[{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28599","id":988064399,"node_id":"MDU6SXNzdWU5ODgwNjQzOTk=","number":28599,"title":"S3 function/engine: How to convert DateTime from ISO 8601 string?","user":{"login":"ecc256","id":15874603,"node_id":"MDQ6VXNlcjE1ODc0NjAz","avatar_url":"https://avatars.githubusercontent.com/u/15874603?v=4","gravatar_id":"","url":"https://api.github.com/users/ecc256","html_url":"https://github.com/ecc256","followers_url":"https://api.github.com/users/ecc256/followers","following_url":"https://api.github.com/users/ecc256/following{/other_user}","gists_url":"https://api.github.com/users/ecc256/gists{/gist_id}","starred_url":"https://api.github.com/users/ecc256/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ecc256/subscriptions","organizations_url":"https://api.github.com/users/ecc256/orgs","repos_url":"https://api.github.com/users/ecc256/repos","events_url":"https://api.github.com/users/ecc256/events{/privacy}","received_events_url":"https://api.github.com/users/ecc256/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-09-03T19:56:34Z","updated_at":"2021-09-04T02:27:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Guys,\r\nI have somewhat similar [issue](https://github.com/ClickHouse/ClickHouse/issues/19351) with S3 import.\r\nI use:\r\n```\r\nINSERT INTO {table}\r\nSELECT *\r\nFROM s3(\r\n  '...',\r\n  '...',\r\n  '...',\r\n  'CSV',\r\n  '{columns}',\r\n  'gzip');\r\n```\r\nCannot use wildcard, have to do one file at a time, b/c if a file row cannot be parsed due to any error, whole file is not imported...\r\n\r\nBoth `parseDateTimeBestEffort/parseDateTime32BestEffort` can parse `ISO 8601` string just fine, but I'm unable to figure out how/where should I add the setting:\r\n> just add `date_time_input_format = 'best_effort'` into the list of settings\r\n\r\nThe string in question is:\r\n```\r\nSELECT parseDateTimeBestEffort('2021-09-03T00:00:00.014+00:00');\r\nSELECT parseDateTime32BestEffort('2021-09-03T00:00:00.014+00:00');\r\n```\r\n\r\nThe columns list is huge, BTW...","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28599/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28580","id":987824154,"node_id":"MDU6SXNzdWU5ODc4MjQxNTQ=","number":28580,"title":"Unexpected Error in WITH statement in combination with UNION ALL","user":{"login":"wol-e","id":52048829,"node_id":"MDQ6VXNlcjUyMDQ4ODI5","avatar_url":"https://avatars.githubusercontent.com/u/52048829?v=4","gravatar_id":"","url":"https://api.github.com/users/wol-e","html_url":"https://github.com/wol-e","followers_url":"https://api.github.com/users/wol-e/followers","following_url":"https://api.github.com/users/wol-e/following{/other_user}","gists_url":"https://api.github.com/users/wol-e/gists{/gist_id}","starred_url":"https://api.github.com/users/wol-e/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wol-e/subscriptions","organizations_url":"https://api.github.com/users/wol-e/orgs","repos_url":"https://api.github.com/users/wol-e/repos","events_url":"https://api.github.com/users/wol-e/events{/privacy}","received_events_url":"https://api.github.com/users/wol-e/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-03T14:25:08Z","updated_at":"2021-11-03T12:39:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi.\r\n\r\nI wanted to raise this as a bug, but I have no means of checking if this is reproducible on the most recent release, so I raise it here:\r\n \r\nMy clickhouse version is: 21.3.5.42\r\n\r\nThe following code creates the error: \"DB::Exception: Table default.cte_numbers doesn't exist\"\r\n\r\n````\r\nSELECT\r\n    *\r\nFROM (\r\n    WITH cte_numbers AS (SELECT 0 AS number)\r\n    SELECT number FROM cte_numbers\r\n    UNION ALL\r\n    SELECT number FROM cte_numbers\r\n)\r\n\r\n-- Error: DB::Exception: Table default.cte_numbers doesn't exist\r\n--\r\n-- Expected result: \r\n--\r\n-- │ number │\r\n-- │ 0 │ \r\n-- │ 0 │ \r\n````\r\n\r\nSome notes: The same query without the `SELECT * ( ... )` or adding another wrapper returns the expected result, see below.\r\n````\r\n-- without the outer SELECT * FROM (...) it works\r\nWITH cte_numbers AS (SELECT 0 AS number)\r\n\r\nSELECT number FROM cte_numbers\r\nUNION ALL\r\nSELECT number FROM cte_numbers\r\n;\r\n\r\n-- Result as expected: \r\n-- │ number │\r\n-- │ 0 │ \r\n-- │ 0 │ \r\n\r\n-- Adding another SELECT * () wrapper for the union it also works (this is my current work-around)\r\nSELECT\r\n    *\r\nFROM (\r\n    WITH cte_numbers AS (SELECT 0 AS number)\r\n    SELECT * FROM (\r\n    SELECT number FROM cte_numbers\r\n    UNION ALL\r\n    SELECT number FROM cte_numbers\r\n    )\r\n);\r\n\r\n-- Result as expected: \r\n-- │ number │\r\n-- │ 0 │ \r\n-- │ 0 │ \r\n````\r\n\r\n\r\n*Stacktrace*:\r\n\r\nServerException: Code: 60.\r\nDB::Exception: Table default.cte_numbers doesn't exist. Stack trace:\r\n\r\n> 0. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) @ 0x87195e1 in /usr/bin/clickhouse\r\n> 1. void std::__1::__optional_storage_base<DB::Exception, false>::__construct<int const&, char const (&) [23], std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(int const&, char const (&) [23], std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) @ 0xeb15151 in /usr/bin/clickhouse\r\n> 2. DB::DatabaseCatalog::getTableImpl(DB::StorageID const&, DB::Context const&, std::__1::optional<DB::Exception>*) const @ 0xeb07263 in /usr/bin/clickhouse\r\n> 3. DB::DatabaseCatalog::getTable(DB::StorageID const&, DB::Context const&) const @ 0xeb0cd6f in /usr/bin/clickhouse\r\n> 4. DB::JoinedTables::getLeftTableStorage() @ 0xefc7f85 in /usr/bin/clickhouse\r\n> 5. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0xec7160f in /usr/bin/clickhouse\r\n> 6. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xec70bed in /usr/bin/clickhouse\r\n> 7. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xef92d35 in /usr/bin/clickhouse\r\n> 8. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xef91630 in /usr/bin/clickhouse\r\n> 9. DB::InterpreterSelectWithUnionQuery::getSampleBlock(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, bool) @ 0xef938f9 in /usr/bin/clickhouse\r\n> 10. ? @ 0xf140ee3 in /usr/bin/clickhouse\r\n> 11. DB::getDatabaseAndTablesWithColumns(std::__1::vector<DB::ASTTableExpression const*, std::__1::allocator<DB::ASTTableExpression const*> > const&, DB::Context const&) @ 0xf141d0b in /usr/bin/clickhouse\r\n> 12. DB::JoinedTables::resolveTables() @ 0xefc81d4 in /usr/bin/clickhouse\r\n> 13. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0xec719da in /usr/bin/clickhouse\r\n> 14. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xec70bed in /usr/bin/clickhouse\r\n> 15. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xef92d35 in /usr/bin/clickhouse\r\n> 16. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xef91630 in /usr/bin/clickhouse\r\n> 17. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::SelectQueryOptions const&) @ 0xec27a10 in /usr/bin/clickhouse\r\n> 18. ? @ 0xf12f449 in /usr/bin/clickhouse\r\n> 19. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xf12e023 in /usr/bin/clickhouse\r\n> 20. DB::TCPHandler::runImpl() @ 0xf8b995d in /usr/bin/clickhouse\r\n> 21. DB::TCPHandler::run() @ 0xf8cbec9 in /usr/bin/clickhouse\r\n> 22. Poco::Net::TCPServerConnection::start() @ 0x11f801af in /usr/bin/clickhouse\r\n> 23. Poco::Net::TCPServerDispatcher::run() @ 0x11f81bc1 in /usr/bin/clickhouse\r\n> 24. Poco::PooledThread::run() @ 0x120b82e9 in /usr/bin/clickhouse\r\n> 25. Poco::ThreadImpl::runnableEntry(void*) @ 0x120b414a in /usr/bin/clickhouse\r\n> 26. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n> 27. __clone @ 0x12171f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n> ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28580/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28578","id":987814949,"node_id":"MDU6SXNzdWU5ODc4MTQ5NDk=","number":28578,"title":"Clickhouse-server cannot start due to json exception","user":{"login":"vc-nhattran","id":88371702,"node_id":"MDQ6VXNlcjg4MzcxNzAy","avatar_url":"https://avatars.githubusercontent.com/u/88371702?v=4","gravatar_id":"","url":"https://api.github.com/users/vc-nhattran","html_url":"https://github.com/vc-nhattran","followers_url":"https://api.github.com/users/vc-nhattran/followers","following_url":"https://api.github.com/users/vc-nhattran/following{/other_user}","gists_url":"https://api.github.com/users/vc-nhattran/gists{/gist_id}","starred_url":"https://api.github.com/users/vc-nhattran/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vc-nhattran/subscriptions","organizations_url":"https://api.github.com/users/vc-nhattran/orgs","repos_url":"https://api.github.com/users/vc-nhattran/repos","events_url":"https://api.github.com/users/vc-nhattran/events{/privacy}","received_events_url":"https://api.github.com/users/vc-nhattran/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1006992822,"node_id":"MDU6TGFiZWwxMDA2OTkyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/operations","name":"operations","color":"7fe8ba","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-09-03T14:15:09Z","updated_at":"2021-10-12T17:51:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I upgrade from 21.3 (LTS) to the latest version\r\n\r\n```\r\n$ sudo apt-get update\r\n$ sudo apt-get install clickhouse-client clickhouse-server\r\n$ sudo service clickhouse-server restart\r\n```\r\n\r\nI got this error:\r\n![Screenshot from 2021-09-03 21-11-06](https://user-images.githubusercontent.com/88371702/132019683-019051f7-19ab-4d5a-ab62-6c13d88d61eb.png)\r\n\r\n--- error log --- \r\n\r\n2021.09.03 14:13:22.880199 [ 24033 ] {} <Error> Application: JSONException: JSON: begin >= end.\r\n2021.09.03 14:14:09.103884 [ 24408 ] {} <Error> Application: Caught exception while loading metadata: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = JSONException: JSON: begin >= end., Stacktrace (when copying this message, always include the lines below):\r\n\r\n0. JSON::checkInit() const @ 0x13afb7ae in /usr/bin/clickhouse\r\n1. DB::FileChecker::load() @ 0x10b418e7 in /usr/bin/clickhouse\r\n2. DB::FileChecker::FileChecker(std::__1::shared_ptr<DB::IDisk>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x10b41393 in /usr/bin/clickhouse\r\n3. DB::StorageLog::StorageLog(std::__1::shared_ptr<DB::IDisk>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long) @ 0x10b35617 in /usr/bin/clickhouse\r\n4. ? @ 0x10b40917 in /usr/bin/clickhouse\r\n5. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, std::__1::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) const @ 0x10960bc1 in /usr/bin/clickhouse\r\n6. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool) @ 0xfeb8e05 in /usr/bin/clickhouse\r\n7. ? @ 0xfeb6f93 in /usr/bin/clickhouse\r\n8. ? @ 0xfeb7f3f in /usr/bin/clickhouse\r\n9. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8fddc58 in /usr/bin/clickhouse\r\n10. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, voidThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x8fdf7ff in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8fdaf3f in /usr/bin/clickhouse\r\n12. ? @ 0x8fde823 in /usr/bin/clickhouse\r\n13. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n14. __clone @ 0x121a3f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 21.8.5.7 (official build))\r\n2021.09.03 14:14:11.633867 [ 24408 ] {} <Error> Application: JSONException: JSON: begin >= end.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28578/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28576","id":987711211,"node_id":"MDU6SXNzdWU5ODc3MTEyMTE=","number":28576,"title":"UUID conversion on insert to MySQL storage engine is broken in some cases","user":{"login":"kolsys","id":6312063,"node_id":"MDQ6VXNlcjYzMTIwNjM=","avatar_url":"https://avatars.githubusercontent.com/u/6312063?v=4","gravatar_id":"","url":"https://api.github.com/users/kolsys","html_url":"https://github.com/kolsys","followers_url":"https://api.github.com/users/kolsys/followers","following_url":"https://api.github.com/users/kolsys/following{/other_user}","gists_url":"https://api.github.com/users/kolsys/gists{/gist_id}","starred_url":"https://api.github.com/users/kolsys/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kolsys/subscriptions","organizations_url":"https://api.github.com/users/kolsys/orgs","repos_url":"https://api.github.com/users/kolsys/repos","events_url":"https://api.github.com/users/kolsys/events{/privacy}","received_events_url":"https://api.github.com/users/kolsys/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-09-03T12:14:15Z","updated_at":"2021-09-23T11:12:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Clickhouse: 21.3.15.4\r\nMySQL: 8.0.17\r\n\r\nIn some cases UUID conversion is incorrect while inserting to the MySQL storage engine and it's not available for selection after insert.\r\n\r\n**How to reproduce**\r\n\r\nCreate MySQL table:\r\n```\r\ncreate database test;\r\ncreate table mysql_uuid (\r\n id binary(16) PRIMARY KEY\r\n) Engine=InnoDB CHARACTER SET latin1;\r\n```\r\n\r\nCreate Clickhouse table:\r\n```\r\nCREATE TABLE mysql_uuid\r\n(\r\n    `id` FixedString(16)\r\n)\r\nENGINE = MySQL('mysql', 'test', 'mysql_uuid', 'root', 'dev', 0)\r\n```\r\n\r\nInsert test data with broken UUID:\r\n```\r\ninsert into mysql_uuid values(UUIDStringToNum('3c6f395f-c759-450c-8f18-0de417be064f'))\r\n```\r\nInsert test data with working UUID:\r\n```\r\ninsert into mysql_uuid values(UUIDStringToNum('625cf527-1484-460b-a27f-b0b5f0537ca4'))\r\n```\r\n\r\nCheck data with broken UUID:\r\n```\r\nSELECT 1\r\nFROM mysql_uuid\r\nWHERE id = UUIDStringToNum('3c6f395f-c759-450c-8f18-0de417be064f')\r\n\r\nQuery id: 5669870d-838f-457e-8d93-4855f5e859a4\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.006 sec. \r\n```\r\n\r\nCheck data with working UUID:\r\n```\r\nSELECT 1\r\nFROM mysql_uuid\r\nWHERE id = UUIDStringToNum('625cf527-1484-460b-a27f-b0b5f0537ca4')\r\n\r\nQuery id: de695d78-91eb-4b0d-b055-7b2fb2b7e113\r\n\r\n┌─1─┐\r\n│ 1 │\r\n└───┘\r\n\r\n1 rows in set. Elapsed: 0.013 sec. \r\n```\r\n\r\n**Expected behavior**\r\n\r\nQuery with broken UUID must return same result:\r\n```\r\nSELECT 1\r\nFROM mysql_uuid\r\nWHERE id = UUIDStringToNum('3c6f395f-c759-450c-8f18-0de417be064f')\r\n\r\nQuery id: de695d78-91eb-4b0d-b055-7b2fb2b7e113\r\n\r\n┌─1─┐\r\n│ 1 │\r\n└───┘\r\n\r\n1 rows in set. Elapsed: 0.013 sec. \r\n```\r\n\r\n\r\n**Error message and/or stacktrace**\r\nAfter insert sample data MySQL contains two rows:\r\n\r\n```\r\nselect BIN_TO_UUID(id) from mysql_uuid\r\n\r\n\r\nBIN_TO_UUID(id)\r\n--\r\n3c6f395f-c759-4566-8f18-0de417be064f\r\n625cf527-1484-460b-a27f-b0b5f0537ca4\r\n\r\n```\r\n\r\nWhere UUID `625cf527-1484-460b-a27f-b0b5f0537ca4` was converted as expected and in UUID `3c6f395f-c759-450c-8f18-0de417be064f` was changed third part of UUID:\r\n\r\n3c6f395f-c759-**450c**-8f18-0de417be064f\r\n3c6f395f-c759-**4566**-8f18-0de417be064f","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":1,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28576/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28575","id":987708195,"node_id":"MDU6SXNzdWU5ODc3MDgxOTU=","number":28575,"title":"`EXPLAIN AST` doesn't respect `FORMAT` in query body","user":{"login":"Nyoroon","id":182203,"node_id":"MDQ6VXNlcjE4MjIwMw==","avatar_url":"https://avatars.githubusercontent.com/u/182203?v=4","gravatar_id":"","url":"https://api.github.com/users/Nyoroon","html_url":"https://github.com/Nyoroon","followers_url":"https://api.github.com/users/Nyoroon/followers","following_url":"https://api.github.com/users/Nyoroon/following{/other_user}","gists_url":"https://api.github.com/users/Nyoroon/gists{/gist_id}","starred_url":"https://api.github.com/users/Nyoroon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Nyoroon/subscriptions","organizations_url":"https://api.github.com/users/Nyoroon/orgs","repos_url":"https://api.github.com/users/Nyoroon/repos","events_url":"https://api.github.com/users/Nyoroon/events{/privacy}","received_events_url":"https://api.github.com/users/Nyoroon/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-03T12:10:50Z","updated_at":"2021-09-03T12:10:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"ClickHouse version: `21.3.15.4`\r\n\r\n`EXPLAIN` works as expected:\r\n```\r\nEXPLAIN\r\nSELECT 1\r\nFORMAT CSV\r\n\r\n\"Expression ((Projection + Before ORDER BY))\"\r\n\"  SettingQuotaAndLimits (Set limits and quota after reading from storage)\"\r\n\"    ReadFromStorage (SystemOne)\"\r\n```\r\n\r\n`EXPLAIN AST` outputs in `Pretty` format:\r\n```\r\nEXPLAIN AST\r\nSELECT 1\r\nFORMAT CSV\r\n\r\n\r\n┌─explain───────────────────────────┐\r\n│ SelectWithUnionQuery (children 2) │\r\n│  ExpressionList (children 1)      │\r\n│   SelectQuery (children 1)        │\r\n│    ExpressionList (children 1)    │\r\n│     Literal UInt64_1              │\r\n│  Identifier CSV                   │\r\n└───────────────────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28575/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28574","id":987699396,"node_id":"MDU6SXNzdWU5ODc2OTkzOTY=","number":28574,"title":"Arrow format(s) - add DateTime64 support","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"},{"id":1481221764,"node_id":"MDU6TGFiZWwxNDgxMjIxNzY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-datetime","name":"comp-datetime","color":"b5bcff","default":false,"description":"date & time & timezone related"}],"state":"open","locked":false,"assignee":{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false},"assignees":[{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-09-03T12:00:18Z","updated_at":"2021-09-09T15:16:14Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Appears there isn't a way to insert nanotime from arrow\r\n\r\nhttps://clickhouse.tech/docs/en/interfaces/formats/#data-format-arrow\r\n \r\nFrom Arrow to CH :\r\n \r\nFR1: timestamp(ns)-> datetime64\r\nFR2: int64 -> datetime64\r\n\r\nhttps://arrow.apache.org/docs/cpp/api/datatype.html\r\n\r\n> enumerator DATE64\r\n> \r\n>> int64_t milliseconds since the UNIX epoch\r\n> \r\n> enumerator TIMESTAMP\r\n> \r\n>> Exact timestamp encoded with int64 since UNIX epoch Default unit millisecond.\r\n> \r\n> enumerator TIME32\r\n> \r\n>> Time as signed 32-bit integer, representing either seconds or milliseconds since midnight.\r\n> \r\n> enumerator TIME64\r\n> \r\n>> Time as signed 64-bit integer, representing either microseconds or nanoseconds since midnight.\r\n> \r\n\r\n\r\nSee https://github.com/ClickHouse/ClickHouse/blob/8c06abee739f4cf30fbdfe1b08ed4af566e46a9f/src/Processors/Formats/Impl/ArrowColumnToCHColumn.cpp#L181\r\n\r\nProbably it should be optionated somehow, or just deducted from the target column type. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28574/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28541","id":987497668,"node_id":"MDU6SXNzdWU5ODc0OTc2Njg=","number":28541,"title":"a problem of cte and row_number()over() ","user":{"login":"l1t1","id":23301704,"node_id":"MDQ6VXNlcjIzMzAxNzA0","avatar_url":"https://avatars.githubusercontent.com/u/23301704?v=4","gravatar_id":"","url":"https://api.github.com/users/l1t1","html_url":"https://github.com/l1t1","followers_url":"https://api.github.com/users/l1t1/followers","following_url":"https://api.github.com/users/l1t1/following{/other_user}","gists_url":"https://api.github.com/users/l1t1/gists{/gist_id}","starred_url":"https://api.github.com/users/l1t1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l1t1/subscriptions","organizations_url":"https://api.github.com/users/l1t1/orgs","repos_url":"https://api.github.com/users/l1t1/repos","events_url":"https://api.github.com/users/l1t1/events{/privacy}","received_events_url":"https://api.github.com/users/l1t1/received_events","type":"User","site_admin":false},"labels":[{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-09-03T07:33:27Z","updated_at":"2021-09-05T15:25:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"a problem of cte and row_number()over() \r\n\r\nwhen one cte table (t in this case) is refered twice in a line,\r\nnot all of its copies are in the same order\r\n\r\n\r\n```sql\r\nwith \r\ns as (select number+1 i from system.numbers limit (6)),\r\np as (select 0 d,0 r\r\nunion all select 1,pi()/3*i from s\r\nunion all select 2,pi()/3*i from s\r\nunion all select sqrt(3),pi()/6+pi()/3*i from s),\r\nt as(select p.*,row_number()over() rn from p)\r\nselect * from t,t t1  where t.rn=t1.rn\r\n```\r\nreturns\r\n```\r\nd\tr\trn\tt1.d\tt1.r\tt1.rn\r\n0\t0\t1\t0\t0\t1\r\n1\t1.0471975511965976\t2\t2\t1.0471975511965976\t2\r\n1\t2.0943951023931953\t3\t2\t2.0943951023931953\t3\r\n1\t3.141592653589793\t4\t2\t3.141592653589793\t4\r\n1\t4.1887902047863905\t5\t2\t4.1887902047863905\t5\r\n1\t5.235987755982988\t6\t2\t5.235987755982988\t6\r\n1\t6.283185307179586\t7\t2\t6.283185307179586\t7\r\n2\t1.0471975511965976\t8\t1.7320508075688772\t1.5707963267948966\t8\r\n2\t2.0943951023931953\t9\t1.7320508075688772\t2.617993877991494\t9\r\n2\t3.141592653589793\t10\t1.7320508075688772\t3.665191429188092\t10\r\n2\t4.1887902047863905\t11\t1.7320508075688772\t4.71238898038469\t11\r\n2\t5.235987755982988\t12\t1.7320508075688772\t5.759586531581287\t12\r\n2\t6.283185307179586\t13\t1.7320508075688772\t6.806784082777885\t13\r\n1.7320508075688772\t1.5707963267948966\t14\t1\t1.0471975511965976\t14\r\n1.7320508075688772\t2.617993877991494\t15\t1\t2.0943951023931953\t15\r\n1.7320508075688772\t3.665191429188092\t16\t1\t3.141592653589793\t16\r\n1.7320508075688772\t4.71238898038469\t17\t1\t4.1887902047863905\t17\r\n1.7320508075688772\t5.759586531581287\t18\t1\t5.235987755982988\t18\r\n1.7320508075688772\t6.806784082777885\t19\t1\t6.283185307179586\t19\r\n```\r\n\t\t","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28541/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28539","id":987467536,"node_id":"MDU6SXNzdWU5ODc0Njc1MzY=","number":28539,"title":"RFC: idempotent insert retries across shards","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-03T07:07:20Z","updated_at":"2021-09-03T07:11:40Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Sometimes the client doesn't know which shard he sends data to, but needs to avoid duplicates when doing retries (so if during the first try he was connected to shard1, did the insert but don't get the acknowledgment, and do the retry - next time loadbalancer can redirect him to another shard).\r\n\r\nThe idea is to make deduplicate state (block checksums) be shared between all the shards (optionally).\r\n\r\nWe can simply add smth like 'deduplicate_state_zookeeper_path'. \r\n\r\nPossible problems - partial failure (like many MV), but it sounds like if all the tables should have the same logic it should still work.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28539/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28527","id":986787497,"node_id":"MDU6SXNzdWU5ODY3ODc0OTc=","number":28527,"title":"RFC: S3 as 'tape device'","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-09-02T14:12:28Z","updated_at":"2021-09-22T18:12:18Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"I want to MOVE OUT old data/partitions, or WHOLE table to s3, preserving normal folder structure.\r\n\r\nLosing the possibility to do any kind of modifications on that data is 100% ok for me. It is 'data at rest. \r\n\r\n#### Usecase 1: backup 'as is'.\r\n\r\nMonthly partitioning. At the end of the month I just do 2 commands:  \r\n\r\n```sql\r\nOPTIMIZE TABLE foo PARTITION ID '202103';  \r\nALTER TABLE foo COPY PARTITION ID '202103' TO S3 's3://aws/bucket/mytable';\r\n```\r\n\r\nLater I can `ATTACH` that 'offline' table as a whole or some offline partitions\r\n\r\n```sql\r\nATTACH OFFLINE TABLE foo_backup FROM S3 's3://aws/bucket/mytable';  \r\n-- now i can do whatever select on my backup, but no inserts, renames / merges etc.  \r\n  \r\nALTER TABLE foo ATTACH PARTITION ID '202103' FROM foo_backup;  \r\n-- recovery backup of some partition\r\n\r\n-- AND / OR   \r\nALTER TABLE foo COPY PARTITION ID '202103' FROM S3 's3://aws/bucket/mytable';\r\n\r\n-- AND / OR  \r\n-- manually copy the data from s3 bucket to my disk, using fuse or aws commands, distribute it as I want between nodes, etc. \r\n```\r\n\r\n#### Usecase 2: 'Very cold' \\(normally detached\\) storage \r\n\r\nAfter 12 months I do   \r\n```sql\r\nALTER TABLE foo COPY PARTITION ID '202003' TO S3 's3://aws/bucket/mytable'  \r\nALTER TABLE foo DROP PARTITION ID '202003' ;\r\n```\r\nIf needed i can attach it back to a side table   \r\n```sql\r\nATTACH OFFLINE  TABLE foo_backup FROM S3 's3://aws/bucket/mytable'  \r\n```\r\nOR attach it temporary to the main table  \r\n```sql\r\nALTER TABLE foo ATTACH OFFLINE PARTITION '202003' FROM S3 's3://aws/bucket/mytable'\r\n```\r\n\r\n#### Usecase 3. Share dataset and other data.\r\n```sql\r\nALTER TABLE nytaxi COPY TO S3 's3://aws/bucket/mytable';  \r\nDROP TABLE nytaxi; \r\n\r\nATTACH OFFLINE TABLE foo_backup FROM S3 's3://aws/bucket/mytable'\r\n```\r\n\r\ndo any kind of selects. inserts/merges etc are not possible. \r\n\r\nCopy it to the local filesystem if needed using fuse / aws-cli.\r\n\r\n#### Usecase 4. Multiple computing nodes doing queries from the same r/o dataset\r\n\r\nsame as  above\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28527/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28520","id":986626572,"node_id":"MDU6SXNzdWU5ODY2MjY1NzI=","number":28520,"title":"a function like translate function of oracle","user":{"login":"l1t1","id":23301704,"node_id":"MDQ6VXNlcjIzMzAxNzA0","avatar_url":"https://avatars.githubusercontent.com/u/23301704?v=4","gravatar_id":"","url":"https://api.github.com/users/l1t1","html_url":"https://github.com/l1t1","followers_url":"https://api.github.com/users/l1t1/followers","following_url":"https://api.github.com/users/l1t1/following{/other_user}","gists_url":"https://api.github.com/users/l1t1/gists{/gist_id}","starred_url":"https://api.github.com/users/l1t1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l1t1/subscriptions","organizations_url":"https://api.github.com/users/l1t1/orgs","repos_url":"https://api.github.com/users/l1t1/repos","events_url":"https://api.github.com/users/l1t1/events{/privacy}","received_events_url":"https://api.github.com/users/l1t1/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-09-02T12:07:09Z","updated_at":"2021-09-02T14:35:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"https://docs.oracle.com/cd/B19306_01/server.102/b14200/functions196.htm\r\n\r\neg.\r\n\r\nSELECT TRANSLATE('SQL*Plus User''s Guide', ' */''', '___') ;\r\nreturns\r\nSQL_Plus_Users_Guide  \r\n\r\nthe characters arguments maybe utf8, so\r\n\r\nTRANSLATE('哈哈(吗)', ' ()', '【】)\r\nreturns\r\n哈哈【吗】\r\n\r\nmy ugly sql:\r\nSELECT arrayStringConcat(\r\narrayMap(x-> case when x in ['(',')'] then ['【','】'][indexOf(['(',')'],x)] else x end, splitByRegexp('','哈哈(吗)')) \r\n)\r\nas res; ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28520/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28517","id":986602314,"node_id":"MDU6SXNzdWU5ODY2MDIzMTQ=","number":28517,"title":"how to rewrite this SQL in clickhouse","user":{"login":"l1t1","id":23301704,"node_id":"MDQ6VXNlcjIzMzAxNzA0","avatar_url":"https://avatars.githubusercontent.com/u/23301704?v=4","gravatar_id":"","url":"https://api.github.com/users/l1t1","html_url":"https://github.com/l1t1","followers_url":"https://api.github.com/users/l1t1/followers","following_url":"https://api.github.com/users/l1t1/following{/other_user}","gists_url":"https://api.github.com/users/l1t1/gists{/gist_id}","starred_url":"https://api.github.com/users/l1t1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l1t1/subscriptions","organizations_url":"https://api.github.com/users/l1t1/orgs","repos_url":"https://api.github.com/users/l1t1/repos","events_url":"https://api.github.com/users/l1t1/events{/privacy}","received_events_url":"https://api.github.com/users/l1t1/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-09-02T11:46:48Z","updated_at":"2021-09-03T03:10:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"```sql\r\nwith \r\ns as (select number+1 n from system.numbers limit (6)),\r\np as (select 0 d,0 r\r\nunion all select 1,pi()/3*i from s\r\nunion all select 2,pi()/3*i from s\r\nunion all select sqrt(3),pi()/6+pi()/3*i from s),\r\nt as(select p.*,row_number()over() rn from p),\r\ne as(\r\nselect row_number()over() n,p1.rn p1,p2.rn p2,(p1.d*p1.d+p2.d*p2.d-2*p1.d*p2.d*cos(p1.r-p2.r))::int d from t p1,t p2 \r\nwhere p1.rn>p2.rn),\r\ntri as(\r\nselect e1.d from e e1,e e2\r\nwhere e1.n>e2.n and e1.d=e2.d and e1.p1=e2.p1 and exists(select 1 from \r\ne e3 where e3.d=e2.d and(e3.p1,e3.p2) in((e1.p2,e2.p2),(e2.p2,e1.p2))))\r\nselect count(*)cnt,d from tri group by rollup(d);\r\n```\r\nit raised\r\n```\r\nCode: 62. DB::Exception: Syntax error: failed at position 494 ('1') (line 13, col 65): 1 from \r\ne e3 where e3.d=e2.d and(e3.p1,e3.p2) in((e1.p2,e2.p2),(e2.p2,e1.p2))))\r\nselect count(*)cnt,d from tri group by rollup(d);\r\n\r\n. Expected one of: DoubleColon, LIKE, GLOBAL NOT IN, AS, DIV, IS, UUID, OR, QuestionMark, BETWEEN, NOT LIKE, MOD, AND, Comma, alias, IN, ILIKE, Dot, NOT ILIKE, NOT, Arrow, token, NOT IN, GLOBAL IN. (SYNTAX_ERROR) (version 21.10.1.7972 (official build))\r\n``` \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28517/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28514","id":986517124,"node_id":"MDU6SXNzdWU5ODY1MTcxMjQ=","number":28514,"title":"Use MeterializeMySQL ","user":{"login":"Clinan","id":23456211,"node_id":"MDQ6VXNlcjIzNDU2MjEx","avatar_url":"https://avatars.githubusercontent.com/u/23456211?v=4","gravatar_id":"","url":"https://api.github.com/users/Clinan","html_url":"https://github.com/Clinan","followers_url":"https://api.github.com/users/Clinan/followers","following_url":"https://api.github.com/users/Clinan/following{/other_user}","gists_url":"https://api.github.com/users/Clinan/gists{/gist_id}","starred_url":"https://api.github.com/users/Clinan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Clinan/subscriptions","organizations_url":"https://api.github.com/users/Clinan/orgs","repos_url":"https://api.github.com/users/Clinan/repos","events_url":"https://api.github.com/users/Clinan/events{/privacy}","received_events_url":"https://api.github.com/users/Clinan/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-09-02T10:29:20Z","updated_at":"2021-12-05T11:19:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"```\r\n\r\n2021.09.02 06:21:35.339155 [ 570432 ] {5333629d-d697-488f-82c6-55cba9d1dad9} <Error> executeQuery: Code: 100, e.displayText() = DB::Exception: Access denied for user root (version 21.8.4.51 (official build)) (from [::1]:45644) (in query: select * from oms.origin_order_promotion order by origin_order_id desc limit 20;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8f9557a in /usr/bin/clickhouse\r\n1. DB::MySQLClient::handshake() @ 0x1005668d in /usr/bin/clickhouse\r\n2. DB::MySQLClient::connect() @ 0x10055af8 in /usr/bin/clickhouse\r\n3. DB::MaterializeMySQLSyncThread::prepareSynchronized(DB::MaterializeMetadata&) @ 0x1002f3bd in /usr/bin/clickhouse\r\n4. DB::MaterializeMySQLSyncThread::synchronization() @ 0x1002dc7c in /usr/bin/clickhouse\r\n5. ? @ 0x10050594 in /usr/bin/clickhouse\r\n6. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8fd61ff in /usr/bin/clickhouse\r\n7. ? @ 0x8fd9ae3 in /usr/bin/clickhouse\r\n8. start_thread @ 0x814a in /usr/lib64/libpthread-2.28.so\r\n9. clone @ 0xfcdc3 in /usr/lib64/libc-2.28.so\r\n\r\n2021.09.02 06:21:35.339325 [ 570432 ] {5333629d-d697-488f-82c6-55cba9d1dad9} <Error> TCPHandler: Code: 100, e.displayText() = DB::Exception: Access denied for user root, Stack trace:\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8f9557a in /usr/bin/clickhouse\r\n1. DB::MySQLClient::handshake() @ 0x1005668d in /usr/bin/clickhouse\r\n2. DB::MySQLClient::connect() @ 0x10055af8 in /usr/bin/clickhouse\r\n3. DB::MaterializeMySQLSyncThread::prepareSynchronized(DB::MaterializeMetadata&) @ 0x1002f3bd in /usr/bin/clickhouse\r\n4. DB::MaterializeMySQLSyncThread::synchronization() @ 0x1002dc7c in /usr/bin/clickhouse\r\n5. ? @ 0x10050594 in /usr/bin/clickhouse\r\n6. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8fd61ff in /usr/bin/clickhouse\r\n7. ? @ 0x8fd9ae3 in /usr/bin/clickhouse\r\n8. start_thread @ 0x814a in /usr/lib64/libpthread-2.28.so\r\n9. clone @ 0xfcdc3 in /usr/lib64/libc-2.28.so\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28514/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28462","id":985358293,"node_id":"MDU6SXNzdWU5ODUzNTgyOTM=","number":28462,"title":"Wrong predicate pushdown causes parsing issue in toDate","user":{"login":"andrea-s","id":9706626,"node_id":"MDQ6VXNlcjk3MDY2MjY=","avatar_url":"https://avatars.githubusercontent.com/u/9706626?v=4","gravatar_id":"","url":"https://api.github.com/users/andrea-s","html_url":"https://github.com/andrea-s","followers_url":"https://api.github.com/users/andrea-s/followers","following_url":"https://api.github.com/users/andrea-s/following{/other_user}","gists_url":"https://api.github.com/users/andrea-s/gists{/gist_id}","starred_url":"https://api.github.com/users/andrea-s/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/andrea-s/subscriptions","organizations_url":"https://api.github.com/users/andrea-s/orgs","repos_url":"https://api.github.com/users/andrea-s/repos","events_url":"https://api.github.com/users/andrea-s/events{/privacy}","received_events_url":"https://api.github.com/users/andrea-s/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-01T16:30:14Z","updated_at":"2021-09-01T17:05:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"`SELECT *\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM\r\n    (\r\n        select '2021-01-01' as some_txt\r\n        UNION ALL\r\n        SELECT 'invalid' AS some_txt\r\n    )\r\n    WHERE some_txt = '2021-01-01'\r\n)\r\nWHERE toDate(some_txt) >= '2000-01-01'`\r\n\r\nReceived exception from server (version **21.8.4**):\r\nCode: 38. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse date: value is too short: Cannot parse Date from String: While processing (toDate('invalid' AS some_txt) >= '2000-01-01') AND (some_txt = '2021-01-01').\r\n\r\nSomehow it looks like the outer predicate gets pushed onto the inner query - so this forces to use -OrZero or -OrNull whenever there's potential to have invalid data.\r\n\r\nSame behavior on **20.8.6** (can't test other versions right now)","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28462/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28448","id":984994134,"node_id":"MDU6SXNzdWU5ODQ5OTQxMzQ=","number":28448,"title":"hdfs engine. how to split hdfs file? and how to set the parallelism in cluster-level?","user":{"login":"gj-zhang","id":21170229,"node_id":"MDQ6VXNlcjIxMTcwMjI5","avatar_url":"https://avatars.githubusercontent.com/u/21170229?v=4","gravatar_id":"","url":"https://api.github.com/users/gj-zhang","html_url":"https://github.com/gj-zhang","followers_url":"https://api.github.com/users/gj-zhang/followers","following_url":"https://api.github.com/users/gj-zhang/following{/other_user}","gists_url":"https://api.github.com/users/gj-zhang/gists{/gist_id}","starred_url":"https://api.github.com/users/gj-zhang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gj-zhang/subscriptions","organizations_url":"https://api.github.com/users/gj-zhang/orgs","repos_url":"https://api.github.com/users/gj-zhang/repos","events_url":"https://api.github.com/users/gj-zhang/events{/privacy}","received_events_url":"https://api.github.com/users/gj-zhang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":2192299304,"node_id":"MDU6TGFiZWwyMTkyMjk5MzA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-hdfs","name":"comp-hdfs","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-09-01T10:43:10Z","updated_at":"2021-09-02T16:05:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"```sql\r\ncreate table test_hdfs_table on cluster datacenter\r\n(\r\n    `product_name`        String,\r\n    `createdate`          DateTime64(3)\r\n)\r\n    engine = HDFS('hdfs://ns/test/test_hdfs_table/*', 'CSV');\r\n\r\ncreate table test_hfds_mergetree_table on cluster datacenter\r\n(\r\n    `product_name`        String,\r\n    `createdate`          DateTime64(3)\r\n)\r\n    ENGINE = ReplicatedMergeTree('/clickhouse/tables/test/test/test_hfds_mergetree_table/{shard}', '{replica}')\r\n        PARTITION BY toYYYYMM(createdate)\r\n        ORDER BY (product_name)\r\n\r\ncreate table test_hfds_distributed_table on cluster datacenter\r\nas test_hfds_mergetree_table\r\nengine = Distributed(xx, xx, xx,xx);\r\n```\r\n\r\nis data synchronized using the following sql?\r\n```\r\ninsert into test_hfds_mergetree_table select * from test_hdfs_table\r\n```\r\nand i have three questions.\r\n1. How does each node know which part of the data it is responsible for synchronizing?\r\n2. Does parallel reading or writing on a document refer to parallelism on a single node? \r\n3. How to achieve cluster-level parallelism?\r\nplease help.\r\n\r\nNow I use Spark to synchronize data from HDFS to ClickHouse. But there are many problems, such as synchronization speed, clickhouse connection, data consistency, etc. So I want to try HDFS engine table, please help to check this problem\r\n\r\nI have one more question need help.\r\nWe have daily synchronization scenarios.  tables on clickHouse and tables pre-aggregated by materialized views need to be emptied or dropped . How can we quickly implement data import in these scenarios? Is there a way to produce ClickHouse files directly?\r\n\r\nI'm going crazy with hive synchronization to clickhouse and asking for help","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28448/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28440","id":984885308,"node_id":"MDU6SXNzdWU5ODQ4ODUzMDg=","number":28440,"title":"Can sendexternaltablesdata be changed to parallel?","user":{"login":"shadowDy","id":18388143,"node_id":"MDQ6VXNlcjE4Mzg4MTQz","avatar_url":"https://avatars.githubusercontent.com/u/18388143?v=4","gravatar_id":"","url":"https://api.github.com/users/shadowDy","html_url":"https://github.com/shadowDy","followers_url":"https://api.github.com/users/shadowDy/followers","following_url":"https://api.github.com/users/shadowDy/following{/other_user}","gists_url":"https://api.github.com/users/shadowDy/gists{/gist_id}","starred_url":"https://api.github.com/users/shadowDy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shadowDy/subscriptions","organizations_url":"https://api.github.com/users/shadowDy/orgs","repos_url":"https://api.github.com/users/shadowDy/repos","events_url":"https://api.github.com/users/shadowDy/events{/privacy}","received_events_url":"https://api.github.com/users/shadowDy/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1546440057,"node_id":"MDU6TGFiZWwxNTQ2NDQwMDU3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/development","name":"development","color":"fc80df","default":false,"description":"Developement process & source code & implementation details"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-09-01T08:52:37Z","updated_at":"2021-09-02T02:08:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"`void MultiplexedConnections::sendExternalTablesData(std::vector<ExternalTablesData> & data)\r\n{\r\n    std::lock_guard lock(cancel_mutex);\r\n\r\n    if (!sent_query)\r\n        throw Exception(\"Cannot send external tables data: query not yet sent.\", ErrorCodes::LOGICAL_ERROR);\r\n\r\n    if (data.size() != active_connection_count)\r\n        throw Exception(\"Mismatch between replicas and data sources\", ErrorCodes::MISMATCH_REPLICAS_DATA_SOURCES);\r\n\r\n    auto it = data.begin();\r\n    for (ReplicaState & state : replica_states)\r\n    {\r\n        Connection * connection = state.connection;\r\n        if (connection != nullptr)\r\n        {\r\n            connection->sendExternalTablesData(*it);\r\n            ++it;\r\n        }\r\n    }\r\n}`\r\n\r\nFrom the log, the data is sent one by one, resulting in a long query time. Can it be sent in parallel, and the parallelism can be configured?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28440/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28424","id":984088308,"node_id":"MDU6SXNzdWU5ODQwODgzMDg=","number":28424,"title":"When to use skipping index vs ORDER BY for a column","user":{"login":"iameugenejo","id":1054824,"node_id":"MDQ6VXNlcjEwNTQ4MjQ=","avatar_url":"https://avatars.githubusercontent.com/u/1054824?v=4","gravatar_id":"","url":"https://api.github.com/users/iameugenejo","html_url":"https://github.com/iameugenejo","followers_url":"https://api.github.com/users/iameugenejo/followers","following_url":"https://api.github.com/users/iameugenejo/following{/other_user}","gists_url":"https://api.github.com/users/iameugenejo/gists{/gist_id}","starred_url":"https://api.github.com/users/iameugenejo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/iameugenejo/subscriptions","organizations_url":"https://api.github.com/users/iameugenejo/orgs","repos_url":"https://api.github.com/users/iameugenejo/repos","events_url":"https://api.github.com/users/iameugenejo/events{/privacy}","received_events_url":"https://api.github.com/users/iameugenejo/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-31T17:04:57Z","updated_at":"2021-08-31T18:54:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"#### A few questions regarding this topic:\r\n\r\n1. What is the characteristics of a column that makes it suitable to be included in `ORDER BY` or as a `Skipping Index`?\r\n\r\n2. I'm under an impression that generally the columns in `ORDER BY `should be from low cardinality to high cardinality, is this correct?\r\n\r\n\r\n#### Follow-up question:\r\n\r\nI have a set of hierarchical columns, let's say `parent_id`, `child_id`, `grandchild_id`, where their cardinalities are like this - `parent_id` is under 100, `child_id` is under 500, `grandchild_id` is under 50,000.\r\n\r\nI could use dictionary for the relationship if the relationship doesn't change at all, however it's rare but does change time to time, so I'm saving these along with the main id, `grandchild_id`\r\n\r\nThe table is partitioned by the `toStartOfDay(timestamp)`, `timestamp` itself is in `5-minute` segments and the total number of rows per daily partition is about `100 million` rows (there are other fields that make the row count higher, but omitted to make the question simple)\r\n\r\nWhat should the ORDER BY look like? \r\n\r\n#### Characteristics of each columns:\r\n- `parent_id`: \r\n  - **CAN** be used in `WHERE` and **NEVER** used by `GROUP BY`\r\n  - cardinality < 100\r\n- `child_id`: \r\n  - **CAN** be used in `WHERE` or `GROUP BY` with `parent_id` in WHERE\r\n  - cardinality < 500\r\n- `grandchild_id`: \r\n  - **CAN** be used in `WHERE` or `GROUP BY` with `parent_id` or `child_id` in `WHERE`\r\n  - cardinality < 50,000\r\n- `timestamp`: \r\n  - **MUST** be used in `WHERE` and **CAN** be used in `GROUP BY`\r\n  - cardinality == 288 within a partition (`5-minute` chunk, partitioned by `toStartOfDay(timestamp)`)\r\n\r\n\r\n1. `parent_id`, `child_id`, `grandchild_id`, `timestamp`?\r\n\r\n2. `timestamp`, `parent_id`, `child_id`, `grandchild_id`?\r\n\r\n3. `grandchild_id`, `timestamp` with skipping index on `parent_id` and `child_id`?\r\n\r\n4.  `timestamp`, `grandchild_id` with skipping index on `parent_id` and `child_id`?\r\n\r\n5. something else?\r\n\r\nI tested 1 ~ 4, and 3 is coming out with the least `marks to read and ranges`, and I'm not sure if it's due to the small size of the sample (1.5 billion rows) or not.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28424/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28405","id":983819773,"node_id":"MDU6SXNzdWU5ODM4MTk3NzM=","number":28405,"title":"How to build ClickHouse with mysql support?","user":{"login":"htochko","id":1263851,"node_id":"MDQ6VXNlcjEyNjM4NTE=","avatar_url":"https://avatars.githubusercontent.com/u/1263851?v=4","gravatar_id":"","url":"https://api.github.com/users/htochko","html_url":"https://github.com/htochko","followers_url":"https://api.github.com/users/htochko/followers","following_url":"https://api.github.com/users/htochko/following{/other_user}","gists_url":"https://api.github.com/users/htochko/gists{/gist_id}","starred_url":"https://api.github.com/users/htochko/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/htochko/subscriptions","organizations_url":"https://api.github.com/users/htochko/orgs","repos_url":"https://api.github.com/users/htochko/repos","events_url":"https://api.github.com/users/htochko/events{/privacy}","received_events_url":"https://api.github.com/users/htochko/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-31T12:44:36Z","updated_at":"2021-09-01T01:35:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I made a build for Apple Silicon  by manual https://clickhouse.tech/docs/en/development/build-osx/\r\nbut now it refuses to read mysql dictionary. \r\n\"Dictionary source of type `mysql` is disabled because ClickHouse was built without mysql support\"\r\nCould you navigate me to any guide on make build with support? \r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28405/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28384","id":983540035,"node_id":"MDU6SXNzdWU5ODM1NDAwMzU=","number":28384,"title":"How do I make  configuration insert_distributed_sync = 1 specify some tables in the cluster","user":{"login":"Virgiljing","id":35559195,"node_id":"MDQ6VXNlcjM1NTU5MTk1","avatar_url":"https://avatars.githubusercontent.com/u/35559195?v=4","gravatar_id":"","url":"https://api.github.com/users/Virgiljing","html_url":"https://github.com/Virgiljing","followers_url":"https://api.github.com/users/Virgiljing/followers","following_url":"https://api.github.com/users/Virgiljing/following{/other_user}","gists_url":"https://api.github.com/users/Virgiljing/gists{/gist_id}","starred_url":"https://api.github.com/users/Virgiljing/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Virgiljing/subscriptions","organizations_url":"https://api.github.com/users/Virgiljing/orgs","repos_url":"https://api.github.com/users/Virgiljing/repos","events_url":"https://api.github.com/users/Virgiljing/events{/privacy}","received_events_url":"https://api.github.com/users/Virgiljing/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-31T07:32:10Z","updated_at":"2021-08-31T23:35:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"There are many tables in the cluster and only one or two tables are configured with insert_distributed_sync = 1,others are configured with insert_distributed_sync = 0","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28384/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28383","id":983530155,"node_id":"MDU6SXNzdWU5ODM1MzAxNTU=","number":28383,"title":"OpenID Connect / OAuth 2.0 support","user":{"login":"GezimSejdiu","id":5259296,"node_id":"MDQ6VXNlcjUyNTkyOTY=","avatar_url":"https://avatars.githubusercontent.com/u/5259296?v=4","gravatar_id":"","url":"https://api.github.com/users/GezimSejdiu","html_url":"https://github.com/GezimSejdiu","followers_url":"https://api.github.com/users/GezimSejdiu/followers","following_url":"https://api.github.com/users/GezimSejdiu/following{/other_user}","gists_url":"https://api.github.com/users/GezimSejdiu/gists{/gist_id}","starred_url":"https://api.github.com/users/GezimSejdiu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GezimSejdiu/subscriptions","organizations_url":"https://api.github.com/users/GezimSejdiu/orgs","repos_url":"https://api.github.com/users/GezimSejdiu/repos","events_url":"https://api.github.com/users/GezimSejdiu/events{/privacy}","received_events_url":"https://api.github.com/users/GezimSejdiu/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-31T07:19:16Z","updated_at":"2021-09-01T07:41:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi team, \r\n\r\nwhile experimenting a bit with the ClickHouse setup (first of all, thanks a lot for such a great tool and contribution) came to the point of integrating SSO and saw that there are no such options e.g. OpenID Connect or SAML. I came to these: https://clickhouse.tech/docs/en/operations/external-authenticators/ which are supported already (mostly for on-prem setup). \r\n\r\nI also couldn't find it on the roadmap, so was thinking if someone did manage to have a workaround on how to integrate SSO via OpenID (e.g. Azure AD), and would be willing to share it. If not the case, I would like to request this feature request: \r\n\r\n> \"ClickHouse should be able to support OpenID so you can seamlessly connect your ClickHouse cluster with Identity Providers like Azure AD, Keycloak, Auth0, or Okta. To set up OpenID support, we just need to point ClickHouse to the metadata endpoint of your provider, and all relevant configuration information is imported automatically.\"\r\n\r\nBest regards, \r\nGezim","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28383/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28381","id":983242788,"node_id":"MDU6SXNzdWU5ODMyNDI3ODg=","number":28381,"title":"date_trunc with 'month' as first parameter returns Date instead of DateTime","user":{"login":"bgrgndzz","id":38699522,"node_id":"MDQ6VXNlcjM4Njk5NTIy","avatar_url":"https://avatars.githubusercontent.com/u/38699522?v=4","gravatar_id":"","url":"https://api.github.com/users/bgrgndzz","html_url":"https://github.com/bgrgndzz","followers_url":"https://api.github.com/users/bgrgndzz/followers","following_url":"https://api.github.com/users/bgrgndzz/following{/other_user}","gists_url":"https://api.github.com/users/bgrgndzz/gists{/gist_id}","starred_url":"https://api.github.com/users/bgrgndzz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bgrgndzz/subscriptions","organizations_url":"https://api.github.com/users/bgrgndzz/orgs","repos_url":"https://api.github.com/users/bgrgndzz/repos","events_url":"https://api.github.com/users/bgrgndzz/events{/privacy}","received_events_url":"https://api.github.com/users/bgrgndzz/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-30T22:02:49Z","updated_at":"2022-01-11T20:24:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Reading the related issue, I understand that ClickHouse's `date_trunc()` was modelled after Postgres's `DATE_TRUNC()`. However, while Postgres returns `DateTime` for every unit, ClickHouse returns `Date` for any unit larger than `day`. The documentation doesn't include this behavior, but says that the only return type is `DateTime`\r\n\r\nI pass the result of `date_trunc()` into `toUnixTimestamp()` so it's essential that the function returns `DateTime` for every unit.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28381/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28375","id":983148085,"node_id":"MDU6SXNzdWU5ODMxNDgwODU=","number":28375,"title":"Memory overcommit for queries.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1532016596,"node_id":"MDU6TGFiZWwxNTMyMDE2NTk2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/memory","name":"memory","color":"e99695","default":false,"description":"When memory usage is higher than expected"}],"state":"open","locked":false,"assignee":{"login":"novikd","id":10158699,"node_id":"MDQ6VXNlcjEwMTU4Njk5","avatar_url":"https://avatars.githubusercontent.com/u/10158699?v=4","gravatar_id":"","url":"https://api.github.com/users/novikd","html_url":"https://github.com/novikd","followers_url":"https://api.github.com/users/novikd/followers","following_url":"https://api.github.com/users/novikd/following{/other_user}","gists_url":"https://api.github.com/users/novikd/gists{/gist_id}","starred_url":"https://api.github.com/users/novikd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/novikd/subscriptions","organizations_url":"https://api.github.com/users/novikd/orgs","repos_url":"https://api.github.com/users/novikd/repos","events_url":"https://api.github.com/users/novikd/events{/privacy}","received_events_url":"https://api.github.com/users/novikd/received_events","type":"User","site_admin":false},"assignees":[{"login":"novikd","id":10158699,"node_id":"MDQ6VXNlcjEwMTU4Njk5","avatar_url":"https://avatars.githubusercontent.com/u/10158699?v=4","gravatar_id":"","url":"https://api.github.com/users/novikd","html_url":"https://github.com/novikd","followers_url":"https://api.github.com/users/novikd/followers","following_url":"https://api.github.com/users/novikd/following{/other_user}","gists_url":"https://api.github.com/users/novikd/gists{/gist_id}","starred_url":"https://api.github.com/users/novikd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/novikd/subscriptions","organizations_url":"https://api.github.com/users/novikd/orgs","repos_url":"https://api.github.com/users/novikd/repos","events_url":"https://api.github.com/users/novikd/events{/privacy}","received_events_url":"https://api.github.com/users/novikd/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-08-30T19:56:05Z","updated_at":"2021-09-19T11:27:58Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nAllow query to use more memory than it's guaranteed if free memory is available while providing guarantees on possible memory usage for other queries.\r\n\r\nE.g. a server has 512 GiB of memory and limiting every query to default 10 GiB just in case is unreasonable.\r\nBut simply raising the max_memory_usage to 500 GiB is also unreasonable.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nAdd setting `max_guaranteed_memory_usage` to accompany with the existing `max_memory_usage`\r\nand `max_guaranteed_memory_usage_for_user` to accompany with the existing `max_memory_usage_for_user`.\r\n\r\n`max_guaranteed_memory_usage` will denote soft memory limit while `max_memory_usage` will remain for hard memory limit.\r\n\r\nIf `max_memory_usage` is reached, exception will be thrown (the same as current behaviour).\r\n\r\nIf `max_memory_usage_for_user` or `max_server_memory_usage` is reached, we will take the ratio of the query memory usage to the `max_guaranteed_memory_usage` or `max_guaranteed_memory_usage_for_user` (overcommit ratio), of our query and all other running queries in a group (of all queries from the same user or all running queries), sort by this value, and if current query is at the top, we immediately throw exception and signal condvar on query finish. Other queries will wait on condvar but no more than specified amount of time, and continue if memory is freed. In case of timeout, an exception is thrown as usual.\r\n\r\nAdd setting `memory_usage_overcommit_max_wait_microseconds` to limit waiting time.\r\n\r\nThe overcommit_ratio should be exposed in system.processes table.\r\n\r\n\r\n**Notes**\r\n\r\nThe idea is to add waiting on condvar inside MemoryTracker.\r\nI'm not 100% sure if it will work ok, especially in dynamics. But at least it looks fairly easy.\r\n\r\nNo deadlocks should be possible as the most hungry query will throw exception, free resources and signal condvar. Then if memory limit is reached again, the next most hungry query will throw exception and so on.\r\n\r\nTo avoid unnecessary waiting we can force the most hungry query (by overcommit ratio) to throw exception slightly earlier than reaching user/total memory limit (e.g. if it is 90% or one GB to reach the limit).\r\n\r\nWaiting inside (almost) arbitrary place in code can prevent proper utilization of CPU in presense of single query execution pipeline. But it should not be a concern as this wait is guaranteed to be short.\r\n\r\nOne of the goals of this task is to enable fallback to external sorting and external aggregation by default. But it is slightly more difficult because it has to be activated before reaching memory limit (the fallback itself requires some extra memory). To solve it, we can provide settings similar to `max_bytes_before_external_group_by` but in form of ratio or delta related to the memory limit. So, it will be activated before memory limit is reached.\r\n\r\n\r\n**Alternative solutions**\r\n\r\nAnother way is to simply specify limits in form of ratio to free memory. Like every query can use 50% of the remaining memory.\r\nThen single query can occupy half of all RAM and concurrent queries will line up in beutiful geometric sequence. But it does not solve the task.\r\n\r\n\r\n**Additional context**\r\n\r\nSee also #8449.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28375/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28362","id":982827587,"node_id":"MDU6SXNzdWU5ODI4Mjc1ODc=","number":28362,"title":"Why not remove one part when load same part from disk?","user":{"login":"ditgittube","id":12425919,"node_id":"MDQ6VXNlcjEyNDI1OTE5","avatar_url":"https://avatars.githubusercontent.com/u/12425919?v=4","gravatar_id":"","url":"https://api.github.com/users/ditgittube","html_url":"https://github.com/ditgittube","followers_url":"https://api.github.com/users/ditgittube/followers","following_url":"https://api.github.com/users/ditgittube/following{/other_user}","gists_url":"https://api.github.com/users/ditgittube/gists{/gist_id}","starred_url":"https://api.github.com/users/ditgittube/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ditgittube/subscriptions","organizations_url":"https://api.github.com/users/ditgittube/orgs","repos_url":"https://api.github.com/users/ditgittube/repos","events_url":"https://api.github.com/users/ditgittube/events{/privacy}","received_events_url":"https://api.github.com/users/ditgittube/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-30T14:02:24Z","updated_at":"2021-08-31T01:24:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"```\r\n2021.08.26 17:41:24.592403 [ 30574 ] {} <Error> Application: Caught exception while loading metadata: Code: 235, e.displayText() = DB::Exception: Part all_4_4_0 already exists: Cannot attach table `default`.`customer` from metadata file /home/omm/clickhouse/data1/clickhouse_path/store/9ee/9ee11f5d-7d68-45c9-8ffd-498630d4ea33/customer.sql from query ATTACH TABLE default.customer UUID '83c8ba1e-d9ba-4c91-b96d-d54fa666a74a' (`C_CUSTKEY` UInt32, `C_NAME` String, `C_ADDRESS` String, `C_CITY` LowCardinality(String), `C_NATION` LowCardinality(String), `C_REGION` LowCardinality(String), `C_PHONE` String, `C_MKTSEGMENT` LowCardinality(String)) ENGINE = ReplicatedMergeTree('/clickhouse/tables/customer/{shard}', '{replica}') ORDER BY C_CUSTKEY SETTINGS index_granularity = 8192: while loading database `default` from path /home/omm/clickhouse/data1/clickhouse_path/metadata/default, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xd7a79b0 in /home/omm/clickhouse/usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x4847f64 in /home/omm/clickhouse/usr/bin/clickhouse\r\n2. DB::MergeTreeData::loadDataParts(bool)::'lambda'()::operator()() const (.cold) @ 0x4312bc3 in /home/omm/clickhouse/usr/bin/clickhouse\r\n3. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x48a8493 in /home/omm/clickhouse/usr/bin/clickhouse\r\n4. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x48a8b7f in /home/omm/clickhouse/usr/bin/clickhouse\r\n5. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x48a7b23 in /home/omm/clickhouse/usr/bin/clickhouse\r\n6. void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x48a73ef in /home/omm/clickhouse/usr/bin/clickhouse\r\n7. start_thread @ 0x7e15 in /usr/lib64/libpthread-2.17.so\r\n8. __clone @ 0x101fed in /usr/lib64/libc-2.17.so\r\n (version 21.3.4.25)\r\n2021.08.26 17:41:24.592502 [ 30574 ] {} <Information> Application: Shutting down storages.\r\n```\r\nWhy not remove one part when load same part from disk?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28362/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28335","id":982642257,"node_id":"MDU6SXNzdWU5ODI2NDIyNTc=","number":28335,"title":"Type mismatch in IN or VALUES section. Expected: Decimal(76, 0). Got: Decimal256","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1745440304,"node_id":"MDU6TGFiZWwxNzQ1NDQwMzA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-decimal","name":"comp-decimal","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-30T10:27:14Z","updated_at":"2021-09-02T02:09:36Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```\r\nclickhouse-client --query='create table decimal_insert_cast_issue ( a Decimal(76, 0)) engine = TinyLog'\r\n\r\nclickhouse-client --query='INSERT INTO decimal_insert_cast_issue VALUES ({param:Nullable(Decimal(41, 0))})' --param_param=1\r\nCode: 53. DB::Exception: Type mismatch in IN or VALUES section. Expected: Decimal(76, 0). Got: Decimal256: While processing _CAST('1', 'Nullable(Decimal(41, 0))'): While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (TYPE_MISMATCH)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28335/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28334","id":982611447,"node_id":"MDU6SXNzdWU5ODI2MTE0NDc=","number":28334,"title":"Better support for row level security filter with Distributed","user":{"login":"KochetovNicolai","id":4092911,"node_id":"MDQ6VXNlcjQwOTI5MTE=","avatar_url":"https://avatars.githubusercontent.com/u/4092911?v=4","gravatar_id":"","url":"https://api.github.com/users/KochetovNicolai","html_url":"https://github.com/KochetovNicolai","followers_url":"https://api.github.com/users/KochetovNicolai/followers","following_url":"https://api.github.com/users/KochetovNicolai/following{/other_user}","gists_url":"https://api.github.com/users/KochetovNicolai/gists{/gist_id}","starred_url":"https://api.github.com/users/KochetovNicolai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KochetovNicolai/subscriptions","organizations_url":"https://api.github.com/users/KochetovNicolai/orgs","repos_url":"https://api.github.com/users/KochetovNicolai/repos","events_url":"https://api.github.com/users/KochetovNicolai/events{/privacy}","received_events_url":"https://api.github.com/users/KochetovNicolai/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false},"assignees":[{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2021-08-30T09:53:51Z","updated_at":"2021-09-01T03:57:38Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Now, it is possible to use row level security filter with Distributed table. In order to apply it, you need to set up filter condition on every remote host (using xml config or RBAC). However, it may be not convenient to support a full set of users on every data node in coherence.\r\n\r\nPossible solution:\r\n* Allow to specify row level filter for distributed table\r\n* Send filtering condition as a part of remote query (e.g. in client info)\r\n\r\nThis require a change in native protocol. If remote host is old, or row level filter cannot be applied, throw an exception.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28334/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28329","id":982519813,"node_id":"MDU6SXNzdWU5ODI1MTk4MTM=","number":28329,"title":"How to watch the progress when I receive data from MySQL using MaterializeMySQL","user":{"login":"Fire-DB","id":89771129,"node_id":"MDQ6VXNlcjg5NzcxMTI5","avatar_url":"https://avatars.githubusercontent.com/u/89771129?v=4","gravatar_id":"","url":"https://api.github.com/users/Fire-DB","html_url":"https://github.com/Fire-DB","followers_url":"https://api.github.com/users/Fire-DB/followers","following_url":"https://api.github.com/users/Fire-DB/following{/other_user}","gists_url":"https://api.github.com/users/Fire-DB/gists{/gist_id}","starred_url":"https://api.github.com/users/Fire-DB/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Fire-DB/subscriptions","organizations_url":"https://api.github.com/users/Fire-DB/orgs","repos_url":"https://api.github.com/users/Fire-DB/repos","events_url":"https://api.github.com/users/Fire-DB/events{/privacy}","received_events_url":"https://api.github.com/users/Fire-DB/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":2121263626,"node_id":"MDU6TGFiZWwyMTIxMjYzNjI2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mysql","name":"comp-mysql","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-30T08:16:14Z","updated_at":"2021-11-30T08:19:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\nClickhouse can synchronize data from MySQL using MaterializeMySQL. But i don't konw how much data have been synchronized currently. So how to make sure that the data in CH is consistent with that in mysql ?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28329/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28327","id":982510197,"node_id":"MDU6SXNzdWU5ODI1MTAxOTc=","number":28327,"title":"The following SQL has no problem in 20.8.7.15-lts, but will fail in 21.3.4.25-lts (DB::Exception: Double-distributed IN/JOIN subqueries is denied (distributed_product_mode = 'deny'))","user":{"login":"zhanghuajieHIT","id":22310751,"node_id":"MDQ6VXNlcjIyMzEwNzUx","avatar_url":"https://avatars.githubusercontent.com/u/22310751?v=4","gravatar_id":"","url":"https://api.github.com/users/zhanghuajieHIT","html_url":"https://github.com/zhanghuajieHIT","followers_url":"https://api.github.com/users/zhanghuajieHIT/followers","following_url":"https://api.github.com/users/zhanghuajieHIT/following{/other_user}","gists_url":"https://api.github.com/users/zhanghuajieHIT/gists{/gist_id}","starred_url":"https://api.github.com/users/zhanghuajieHIT/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhanghuajieHIT/subscriptions","organizations_url":"https://api.github.com/users/zhanghuajieHIT/orgs","repos_url":"https://api.github.com/users/zhanghuajieHIT/repos","events_url":"https://api.github.com/users/zhanghuajieHIT/events{/privacy}","received_events_url":"https://api.github.com/users/zhanghuajieHIT/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":2825253639,"node_id":"MDU6TGFiZWwyODI1MjUzNjM5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.3-affected","name":"v21.3-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-30T08:05:40Z","updated_at":"2021-08-30T14:19:08Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"fail message：\r\nReceived exception from server (version 21.3.4):\r\nCode: 288. DB::Exception: Received from localhost:9001. DB::Exception: Double-distributed IN/JOIN subqueries is denied (distributed_product_mode = 'deny'). You may rewrite query to use local tables in subqueries, or use GLOBAL keyword, or set distributed_product_mode to suitable value.: While processing db_test.agg_table2_a: While processing RIGHT JOIN (WITH (WITH (SELECT max(max_time_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (SELECT max(timestamp) AS max_time FROM db_test.agg_table2) AS max_time SELECT 'a' AS a, max_time AS max_time_local) AS temp USING (a)) AS max_time SELECT max(c) AS maxC FROM db_test.agg_table1) AS maxC, (SELECT max(max_time_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (SELECT max(timestamp) AS max_time FROM db_test.agg_table2) AS max_time SELECT 'a' AS a, max_time AS max_time_local) AS temp USING (a)) AS max_time SELECT 'a' AS a, maxC AS max_c_local) AS temp USING (a): While processing (WITH (SELECT max(max_time_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (SELECT max(timestamp) AS max_time FROM db_test.agg_table2) AS max_time SELECT 'a' AS a, max_time AS max_time_local) AS temp USING (a)) AS max_time SELECT max(max_c_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (WITH (SELECT max(max_time_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (SELECT max(timestamp) AS max_time FROM db_test.agg_table2) AS max_time SELECT 'a' AS a, max_time AS max_time_local) AS temp USING (a)) AS max_time SELECT max(c) AS maxC FROM db_test.agg_table1) AS maxC, (SELECT max(max_time_local) FROM db_test.agg_table2_a RIGHT JOIN (WITH (SELECT max(timestamp) AS max_time FROM db_test.agg_table2) AS max_time SELECT 'a' AS a, max_time AS max_time_local) AS temp USING (a)) AS max_time SELECT 'a' AS a, maxC AS max_c_local) AS temp USING (a)) AS max_level2_p.\r\n\r\nthe tack trace：\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xd7aad40 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x4847384 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n2. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) (.cold) @ 0x4415c6c in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n3. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n4. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n5. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n6. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n7. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n8. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n9. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n10. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n11. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n12. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n13. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n14. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n15. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n16. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n17. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n18. DB::InDepthNodeVisitor<DB::OneTypeMatcher<DB::(anonymous namespace)::NonGlobalTableData, &(DB::NeedChild::all(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::IAST> const&)), std::__1::shared_ptr<DB::IAST> >, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb164183 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n19. DB::InDepthNodeVisitor<DB::(anonymous namespace)::NonGlobalSubqueryMatcher, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb1651aa in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n20. DB::InDepthNodeVisitor<DB::(anonymous namespace)::NonGlobalSubqueryMatcher, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb16503d in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n21. DB::InDepthNodeVisitor<DB::(anonymous namespace)::NonGlobalSubqueryMatcher, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xb16503d in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n22. DB::InJoinSubqueriesPreprocessor::visit(std::__1::shared_ptr<DB::IAST>&) const @ 0xb1663de in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n23. DB::JoinedTables::rewriteDistributedInAndJoins(std::__1::shared_ptr<DB::IAST>&) @ 0xa58342a in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n24. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0xa51946d in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n25. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa51b298 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n26. DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa545fd3 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n27. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xa544848 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n28. DB::ExecuteScalarSubqueriesMatcher::visit(DB::ASTSubquery const&, std::__1::shared_ptr<DB::IAST>&, DB::ExecuteScalarSubqueriesMatcher::Data&) @ 0xb04f721 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n29. DB::ExecuteScalarSubqueriesMatcher::visit(std::__1::shared_ptr<DB::IAST>&, DB::ExecuteScalarSubqueriesMatcher::Data&) @ 0xb052925 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n30. DB::InDepthNodeVisitor<DB::ExecuteScalarSubqueriesMatcher, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xa61cb2b in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n31. DB::InDepthNodeVisitor<DB::ExecuteScalarSubqueriesMatcher, true, std::__1::shared_ptr<DB::IAST> >::visit(std::__1::shared_ptr<DB::IAST>&) @ 0xa61cb63 in /home/code/release_ver/ClickHouse_Kernel/build/programs/clickhouse\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28327/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28318","id":982415574,"node_id":"MDU6SXNzdWU5ODI0MTU1NzQ=","number":28318,"title":"implement string numeric interpolation feature","user":{"login":"mbtolou","id":6413705,"node_id":"MDQ6VXNlcjY0MTM3MDU=","avatar_url":"https://avatars.githubusercontent.com/u/6413705?v=4","gravatar_id":"","url":"https://api.github.com/users/mbtolou","html_url":"https://github.com/mbtolou","followers_url":"https://api.github.com/users/mbtolou/followers","following_url":"https://api.github.com/users/mbtolou/following{/other_user}","gists_url":"https://api.github.com/users/mbtolou/gists{/gist_id}","starred_url":"https://api.github.com/users/mbtolou/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mbtolou/subscriptions","organizations_url":"https://api.github.com/users/mbtolou/orgs","repos_url":"https://api.github.com/users/mbtolou/repos","events_url":"https://api.github.com/users/mbtolou/events{/privacy}","received_events_url":"https://api.github.com/users/mbtolou/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-30T05:51:54Z","updated_at":"2021-08-30T14:17:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"hi.\r\nimplement string numeric interpolation feature.\r\n\r\nhttps://docs.microsoft.com/en-us/dotnet/standard/base-types/standard-numeric-format-strings\r\n\r\nlike this :\r\n\r\n`\r\nselect stringformat('i have {0:C} dollars .',100)\r\n`\r\nresult:\r\ni have 100.0$ dollars .\r\n\r\n`\r\nselect stringformat('->{0:x20}<- .',\"hello\")\r\n`\r\nresult:\r\n->xxxxxxxxxxxxxxxHello<-\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28318/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28317","id":982407500,"node_id":"MDU6SXNzdWU5ODI0MDc1MDA=","number":28317,"title":"minimal response for formatReadableTimeDelta","user":{"login":"mbtolou","id":6413705,"node_id":"MDQ6VXNlcjY0MTM3MDU=","avatar_url":"https://avatars.githubusercontent.com/u/6413705?v=4","gravatar_id":"","url":"https://api.github.com/users/mbtolou","html_url":"https://github.com/mbtolou","followers_url":"https://api.github.com/users/mbtolou/followers","following_url":"https://api.github.com/users/mbtolou/following{/other_user}","gists_url":"https://api.github.com/users/mbtolou/gists{/gist_id}","starred_url":"https://api.github.com/users/mbtolou/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mbtolou/subscriptions","organizations_url":"https://api.github.com/users/mbtolou/orgs","repos_url":"https://api.github.com/users/mbtolou/repos","events_url":"https://api.github.com/users/mbtolou/events{/privacy}","received_events_url":"https://api.github.com/users/mbtolou/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-30T05:33:05Z","updated_at":"2021-08-30T05:33:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"hi .\r\n\r\nplease write minimal response for formatReadableTimeDelta function:\r\n\r\nlike this :\r\n`SELECT formatReadableTimeDelta(-(1 + 60 + 3600 + 86400 + 30.5 * 86400 + 365 * 86400), 'days');`\r\n\r\n-396 days, 13 hours, 1 minute and 1 second\r\n=====>\r\n-396d.13:01:01\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28317/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28311","id":982069839,"node_id":"MDU6SXNzdWU5ODIwNjk4Mzk=","number":28311,"title":"sessionization of events with PROJECTION feature","user":{"login":"asafcombo","id":12124039,"node_id":"MDQ6VXNlcjEyMTI0MDM5","avatar_url":"https://avatars.githubusercontent.com/u/12124039?v=4","gravatar_id":"","url":"https://api.github.com/users/asafcombo","html_url":"https://github.com/asafcombo","followers_url":"https://api.github.com/users/asafcombo/followers","following_url":"https://api.github.com/users/asafcombo/following{/other_user}","gists_url":"https://api.github.com/users/asafcombo/gists{/gist_id}","starred_url":"https://api.github.com/users/asafcombo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/asafcombo/subscriptions","organizations_url":"https://api.github.com/users/asafcombo/orgs","repos_url":"https://api.github.com/users/asafcombo/repos","events_url":"https://api.github.com/users/asafcombo/events{/privacy}","received_events_url":"https://api.github.com/users/asafcombo/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2021-08-29T11:08:33Z","updated_at":"2021-08-29T17:00:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I am testing an interesting usecase if PROJECTION feature. I wanted to hear your thoughts.\r\n\r\nMy end goal is to create a table that gets INSERTS from separate data streams.\r\n\r\nThe streams have different attributes about the session but all streams can be joined with a unique UUID session id.\r\n\r\nThe target table is an AggregateMergeTree that merges the different attributes into a single event by usage of a  \r\n`SimpleAggregateFunction(max, TYPE)`.\r\n\r\nThe table is ordered by several of the attributes of course for better compression and queries times. \r\nThis in turn creates a requirement to pass all the ORDER by fields to all the streams. \r\n\r\nNow, about projections - I ran the below test. Its goal was to see if I can create a table with 1 merge key, and another composite ORDER BY key.\r\n\r\n```SQL\r\nCREATE TABLE `tests.projection_test`\r\n(\r\n    `timestamp`                         DateTime Codec (DoubleDelta, LZ4),\r\n    `ne`     Int32,\r\n\r\n    `request_id`  String,\r\n    `a` SimpleAggregateFunction(max, String) ,\r\n    `b` SimpleAggregateFunction(max, Int32),\r\n\r\n    PROJECTION projection_1 (SELECT * ORDER BY ne)\r\n\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPARTITION BY toDate(timestamp)\r\nORDER BY (request_id)\r\nSETTINGS index_granularity = 8192, storage_policy = 'ebs_hot_and_cold';\r\n\r\n\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 0, '1111-2222', '', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 0, '1111-2222', 'a', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 0, '0000-2222', 'b', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 10, '0000-2222', 'b', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 10, '1111-3333', 'b', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 9, '1111-4444', 'b', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 8, '1111-5555', 'b', 0);\r\nINSERT INTO tests.projection_test VALUES ('2021-08-29', 8, '1111-5555', 'c', 1);\r\n\r\noptimize table tests.projection_test final;\r\nselect * from tests.projection_test;\r\nselect * from tests.projection_test ORDER BY ne;\r\n```\r\n\r\nObservations:\r\n1. It seems like this strategy works.   `select * from tests.projection_test ORDER BY ne`  does returns expected merged results. trace logs shows projection was used.\r\n2.  using ` WHERE timestamp >= `  uses projection by default ( IDK why that is, but in my use case it is indeed beneficial ).\r\n3.  As expected we doubled the table size.\r\n\r\nTo handle the doubling of the table size, my idea was to manipulate the projection. However I did not see any option that fits the need to drop the primary storage ( default table ), and use instead the projection.\r\nWhat I did was deleting all files not necessary.  \r\n\r\nBefore:\r\n\r\n![image](https://user-images.githubusercontent.com/12124039/131248074-cda8c97d-91fc-4e63-a676-e4d4820cc311.png)\r\n\r\n\r\nAfter:\r\n\r\n![image](https://user-images.githubusercontent.com/12124039/131248100-786ec9d5-dce4-41b3-8f6e-2b9616a21afb.png)\r\n\r\n\r\nThis also worked.  \r\n1. `select * from tests.projection_test;`  - FAILS\r\n2.  `select * from tests.projection_test ORDER BY ne;` - WORKS\r\n3. `select * from tests.projection_test WHERE timestamp >= '2021-08-28'`;  -WORKS \r\n\r\nQuestions:\r\n1. general wdyt ?\r\n2. Any specific issues you see ? \r\n\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28311/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28306","id":981997365,"node_id":"MDExOlB1bGxSZXF1ZXN0NzIxODg5MjAw","number":28306,"title":"JOIN key type inference respects low cardinality and nullability","user":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"labels":[{"id":1302792342,"node_id":"MDU6TGFiZWwxMzAyNzkyMzQy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-bugfix","name":"pr-bugfix","color":"ff4080","default":false,"description":"Pull request with bugfix, not backported by default"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-29T05:14:23Z","updated_at":"2021-09-28T11:11:18Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/28306","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28306","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/28306.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/28306.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- Bug Fix (user-visible misbehaviour in official stable or prestable release)\r\n\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nJOIN key type inference respects low cardinality and nullability. Close #27691\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":1,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28306/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28300","id":981843075,"node_id":"MDU6SXNzdWU5ODE4NDMwNzU=","number":28300,"title":"Describe for input columns / parameters.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-28T14:07:46Z","updated_at":"2021-08-31T01:44:16Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"JDBC / ODBC and other client libraries need a way to find out types of columns for input.\r\n\r\n```\r\ncreate table xxx(a Int64, b String) Engine=Memory;\r\n\r\ndesc(insert into xxx)\r\n┌─name─┬─type───────────\r\n│ a    │ Decimal(38, 6)\r\n│ b    │ String\r\n└──────┴────────────────\r\n\r\ndesc(insert into xxx(a))\r\n┌─name─┬─type───────────\r\n│ a    │ Decimal(38, 6)\r\n└──────┴────────────────\r\n```\r\n\r\nIt's impossible to implement insertion in a JDBC compatible way for types like Decimal/Datetime64/FixedString because such types do not exist in JDBC. But a JDBC driver could do it if it was able to discover required types from ClickHouse.\r\n\r\nOther databases implement this in a PREPARE command. This will be our PREPARE.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28300/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28259","id":981192451,"node_id":"MDU6SXNzdWU5ODExOTI0NTE=","number":28259,"title":"bitmapcontains backward compatibility problem","user":{"login":"Nicholaswang","id":3916488,"node_id":"MDQ6VXNlcjM5MTY0ODg=","avatar_url":"https://avatars.githubusercontent.com/u/3916488?v=4","gravatar_id":"","url":"https://api.github.com/users/Nicholaswang","html_url":"https://github.com/Nicholaswang","followers_url":"https://api.github.com/users/Nicholaswang/followers","following_url":"https://api.github.com/users/Nicholaswang/following{/other_user}","gists_url":"https://api.github.com/users/Nicholaswang/gists{/gist_id}","starred_url":"https://api.github.com/users/Nicholaswang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Nicholaswang/subscriptions","organizations_url":"https://api.github.com/users/Nicholaswang/orgs","repos_url":"https://api.github.com/users/Nicholaswang/repos","events_url":"https://api.github.com/users/Nicholaswang/events{/privacy}","received_events_url":"https://api.github.com/users/Nicholaswang/received_events","type":"User","site_admin":false},"labels":[{"id":845271686,"node_id":"MDU6TGFiZWw4NDUyNzE2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/backward%20compatibility","name":"backward compatibility","color":"28af63","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-27T12:17:27Z","updated_at":"2021-08-28T06:18:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"problem: newer version ClickHouse 'bitmapContains' function does not support bitmap built with nullable value.\r\n\r\nSQL: WITH (SELECT groupBitmapState(toUInt32OrNull('342423'))) AS bitmap SELECT bitmapContains(bitmap, toUInt32OrNull('321321'))\r\n\r\nversion: 20.3.10.75 (which works)\r\n![企业微信截图_16300661247298](https://user-images.githubusercontent.com/3916488/131125653-0f45dc3c-6ba5-443a-bcc5-5bcfc6cd9e69.png)\r\n\r\nversion: 20.8.14.4 (which doesn't work)\r\n![企业微信截图_16300661851932](https://user-images.githubusercontent.com/3916488/131125984-0098a580-217e-41fb-b967-e6da03f54635.png)\r\n\r\nbut it works using toUInt32OrZero:\r\n![企业微信截图_16300662279485](https://user-images.githubusercontent.com/3916488/131126003-bffd4d78-a763-4603-88ea-ac435dcdc401.png)\r\n\r\n\r\nI am wondering if this is a backward compatibility issue.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28259/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28223","id":981006872,"node_id":"MDU6SXNzdWU5ODEwMDY4NzI=","number":28223,"title":"receive_timeout/send_timeout param not effective when execute a distributed join query","user":{"login":"oceanliuyang8","id":89071950,"node_id":"MDQ6VXNlcjg5MDcxOTUw","avatar_url":"https://avatars.githubusercontent.com/u/89071950?v=4","gravatar_id":"","url":"https://api.github.com/users/oceanliuyang8","html_url":"https://github.com/oceanliuyang8","followers_url":"https://api.github.com/users/oceanliuyang8/followers","following_url":"https://api.github.com/users/oceanliuyang8/following{/other_user}","gists_url":"https://api.github.com/users/oceanliuyang8/gists{/gist_id}","starred_url":"https://api.github.com/users/oceanliuyang8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/oceanliuyang8/subscriptions","organizations_url":"https://api.github.com/users/oceanliuyang8/orgs","repos_url":"https://api.github.com/users/oceanliuyang8/repos","events_url":"https://api.github.com/users/oceanliuyang8/events{/privacy}","received_events_url":"https://api.github.com/users/oceanliuyang8/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-27T08:29:41Z","updated_at":"2021-08-27T08:58:20Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":" the default receive/send timeout is 300 seconds，but timeout after one sec later，The cluster has 100 nodes and the tableA is about 30 million rows。\r\nhow can i trace this problem，no other useful information in log file（ set send_logs_level='warning'）\r\n\r\n![image](https://user-images.githubusercontent.com/89071950/131096798-7f58df75-977e-4fbb-bb74-d4918aa036d4.png)\r\n\r\n![image](https://user-images.githubusercontent.com/89071950/131101514-b9d9ec56-4309-49b1-9858-c6b79ac818e7.png)\r\n\r\n2021.08.27 **14:46:16.417793** [ 4844 ] {746b2e0c-9638-4f6f-b4e8-2cc3de091f44} <Error> executeQuery: Code: 209, e.displayText() = DB::NetException: **Timeout exceeded while writing to socket** (10.216.60.11:41301) (version 21.6.8.62 (official build)) (from 10.216.60.11:41301) (in query: SELECT ..._local WHERE ...),\r\n2021.08.27 **14:46:15.621569** [ 4844 ] {746b2e0c-9638-4f6f-b4e8-2cc3de091f44} <Debug> **executeQuery**: (from 10.216.60.11:41301, initial_query_id: f48a7e09-ca0c-4a3e-8ef0-b79254c85f8a, using production parser) SELECT ..._local WHERE ...","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28223/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28218","id":980851044,"node_id":"MDU6SXNzdWU5ODA4NTEwNDQ=","number":28218,"title":"support kafka_security_protocol setting in CREATE TABLE DDL","user":{"login":"wangqinghuan","id":22762150,"node_id":"MDQ6VXNlcjIyNzYyMTUw","avatar_url":"https://avatars.githubusercontent.com/u/22762150?v=4","gravatar_id":"","url":"https://api.github.com/users/wangqinghuan","html_url":"https://github.com/wangqinghuan","followers_url":"https://api.github.com/users/wangqinghuan/followers","following_url":"https://api.github.com/users/wangqinghuan/following{/other_user}","gists_url":"https://api.github.com/users/wangqinghuan/gists{/gist_id}","starred_url":"https://api.github.com/users/wangqinghuan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangqinghuan/subscriptions","organizations_url":"https://api.github.com/users/wangqinghuan/orgs","repos_url":"https://api.github.com/users/wangqinghuan/repos","events_url":"https://api.github.com/users/wangqinghuan/events{/privacy}","received_events_url":"https://api.github.com/users/wangqinghuan/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-27T04:08:09Z","updated_at":"2021-08-29T05:07:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"*Use case**\r\n\r\n> we have 2 kafka cluster, one is PLAIN_TEXT and another is SASL_PLAIN_TEXT. Currently, we need to configure topic-level kafka  setting to tell clickhouse kafka_security_protocol used for each topic. Could we move this setting to DDL to simplify configure?\r\ncreate table TEST(\r\n   ...\r\n)\r\n    engine = Kafka \r\nSETTINGS \r\n...\r\nkafka_security_protocol = 'PLAIN_TEXT';\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28218/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28216","id":980818071,"node_id":"MDU6SXNzdWU5ODA4MTgwNzE=","number":28216,"title":"Extend range of Datetime64 and Date32","user":{"login":"liuneng1994","id":16730247,"node_id":"MDQ6VXNlcjE2NzMwMjQ3","avatar_url":"https://avatars.githubusercontent.com/u/16730247?v=4","gravatar_id":"","url":"https://api.github.com/users/liuneng1994","html_url":"https://github.com/liuneng1994","followers_url":"https://api.github.com/users/liuneng1994/followers","following_url":"https://api.github.com/users/liuneng1994/following{/other_user}","gists_url":"https://api.github.com/users/liuneng1994/gists{/gist_id}","starred_url":"https://api.github.com/users/liuneng1994/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liuneng1994/subscriptions","organizations_url":"https://api.github.com/users/liuneng1994/orgs","repos_url":"https://api.github.com/users/liuneng1994/repos","events_url":"https://api.github.com/users/liuneng1994/events{/privacy}","received_events_url":"https://api.github.com/users/liuneng1994/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-08-27T02:52:12Z","updated_at":"2021-11-11T12:41:14Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Not long ago, I submitted a pull request for Date32（https://github.com/ClickHouse/ClickHouse/pull/25774）, in order to solve the problem of too small date range, but for some banking systems, there are still data dates before 1925. I want to extend the existing date range, for example to 1900.\r\n\r\nI want to know if this approach can be accepted by the community.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28216/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28215","id":980814046,"node_id":"MDU6SXNzdWU5ODA4MTQwNDY=","number":28215,"title":"Different predicate push down behaviour with and w/o alias name for table","user":{"login":"zhangjmruc","id":66244986,"node_id":"MDQ6VXNlcjY2MjQ0OTg2","avatar_url":"https://avatars.githubusercontent.com/u/66244986?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjmruc","html_url":"https://github.com/zhangjmruc","followers_url":"https://api.github.com/users/zhangjmruc/followers","following_url":"https://api.github.com/users/zhangjmruc/following{/other_user}","gists_url":"https://api.github.com/users/zhangjmruc/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjmruc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjmruc/subscriptions","organizations_url":"https://api.github.com/users/zhangjmruc/orgs","repos_url":"https://api.github.com/users/zhangjmruc/repos","events_url":"https://api.github.com/users/zhangjmruc/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjmruc/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-27T02:42:11Z","updated_at":"2021-08-27T02:42:11Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"We normally like to add simple aliases for tables in join. But CH has different behavior on predicate push down when with and without alias for a table. Is this expected behavior or not?\r\n\r\n===== details =======\r\nDDL of two tables:\r\ncreate table t(a int, b int, c int) engine=MergeTree order by a;\r\ncreate table t2(a2 int, b2 int, c2 int) engine=MergeTree order by a2;\r\n\r\n-- insert data\r\ninsert into t values(1,1,1),(2,2,2),(3,3,3),(4,4,4);\r\ninsert into t2 values(1,1,1),(2,2,2),(3,3,3),(4,4,4);\r\n\r\nCase 1: two tables use original table name, no alias.\r\n-- \"t.b=5\" is put to prewhere condition, \"t2.b2=4\" is put to prewhere of t2.\r\nexplain syntax select * from t inner join t2 on t.a=t2.a2 where t.b=5 and t2.b2=4;\r\n┌─explain────────────────────┐\r\n│ SELECT                     │\r\n│     a,                     │\r\n│     b,                     │\r\n│     c,                     │\r\n│     a2,                    │\r\n│     b2,                    │\r\n│     c2                     │\r\n│ FROM t                     │\r\n│ ALL INNER JOIN             │\r\n│ (                          │\r\n│     SELECT                 │\r\n│         a2,                │\r\n│         b2,                │\r\n│         c2                 │\r\n│     FROM t2                │\r\n│     **PREWHERE b2 = 4**        │\r\n│ ) AS t2 ON a = a2          │\r\n│ **PREWHERE b = 5**             │\r\n│ WHERE (b = 5) AND (b2 = 4) │\r\n└────────────────────────────┘\r\n\r\nCase 2: one right table t2 has alias name t2_as\r\n-- \"t.b=5\" is put to prewhere condition, BUT \"t2.b2 =4\" not.\r\nexplain syntax select * from t inner join t2 **t2_as** on t.a=t2_as.a2 where t.b=5 and t2_as.b2=4;\r\n\r\n┌─explain──────────────────────────────┐\r\n│ SELECT                               │\r\n│     a,                               │\r\n│     b,                               │\r\n│     c,                               │\r\n│     a2,                              │\r\n│     b2,                              │\r\n│     c2                               │\r\n│ FROM t                               │\r\n│ ALL INNER JOIN t2 AS t2_as ON a = a2 │\r\n│ **PREWHERE b = 5**                       │\r\n│ WHERE (b = 5) AND (b2 = 4)           │\r\n└──────────────────────────────────────┘\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28215/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28196","id":980379821,"node_id":"MDU6SXNzdWU5ODAzNzk4MjE=","number":28196,"title":"clickhouse failing to start after ZK connection drop / heap errors","user":{"login":"kushagra391","id":8533562,"node_id":"MDQ6VXNlcjg1MzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/8533562?v=4","gravatar_id":"","url":"https://api.github.com/users/kushagra391","html_url":"https://github.com/kushagra391","followers_url":"https://api.github.com/users/kushagra391/followers","following_url":"https://api.github.com/users/kushagra391/following{/other_user}","gists_url":"https://api.github.com/users/kushagra391/gists{/gist_id}","starred_url":"https://api.github.com/users/kushagra391/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kushagra391/subscriptions","organizations_url":"https://api.github.com/users/kushagra391/orgs","repos_url":"https://api.github.com/users/kushagra391/repos","events_url":"https://api.github.com/users/kushagra391/events{/privacy}","received_events_url":"https://api.github.com/users/kushagra391/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-26T15:30:37Z","updated_at":"2022-01-24T13:08:51Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n\r\nWe have a 3 * 1 node CH cluster that went into rolling restart after a ZK failure (heap error + connection drop) \r\n\r\n\r\n### Summary\r\n\r\n- issue started with an alert for a long running query\r\n  - upon checking, i saw a `DROP TABLE T SYNC` query being stuck forever (> 12 hours)\r\n    - killing the SQL didn't work \r\n  - tried creating / dropping new MT tables, but these were stuck as well.\r\n- tried deleting the table's node in ZK via `.zCli.sh` + `deleteall`\r\n- issue still persisted, and i tried restarting CH server\r\n\r\n\r\nupon restarting clickhouse pods, ZK shows heap error and bunch of connection errors, while one of CH pods failed to restart\r\n\r\n```shell\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:54,856 [myid:2] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,162 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):Leader@1296] - Have quorum of supporters, sids: [ [1, 2],[1, 2] ]; starting up and setting last processed zxid: 0x1200000000\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,243 [myid:1] - WARN  [NIOWorkerThread-1:ZooKeeperServer@1013] - Connection request from old client /172.18.152.209:36888; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper Heap dump file created [1087344431 bytes in 9.622 secs]\r\nzookeeper-0 kubernetes-zookeeper #\r\nzookeeper-0 kubernetes-zookeeper # java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper # -XX:OnOutOfMemoryError=\"kill -9 %p\"\r\nzookeeper-0 kubernetes-zookeeper #   Executing /bin/sh -c \"kill -9 1\"..\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:19,120 [myid:1] - ERROR [CommitProcWorkThread-1:NIOServerCnxnFactory$1@92] - Thread Thread[CommitProcWorkThread-1,5,main] died\r\nzookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.Arrays.copyOf(Arrays.java:3236)\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,735 [myid:1] - WARN  [NIOWorkerThread-2:ZooKeeperServer@1013] - Connection request from old client /172.18.191.30:55148; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,736 [myid:1] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:42646\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:25,030 [myid:1] - ERROR\r\n```\r\n\r\non CH side, pods went into rolling restart with errors like\r\n\r\n```shell\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.738178 [ 84 ] {} <Error> Application: Caught exception while loading metadata: Code: 999, e.displayText() = Coordination::Exception: All connection tries failed while connecting to ZooKeeper. nodes: 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod  (Connection loss): Cannot attach table ... from metadata file ...sql from query ATTACH TABLE ... while loading database ... from path ..., \r\n\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280211 [ 84 ] {} <Information> Application: shutting down\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280220 [ 84 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280287 [ 85 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.303513 [ 1 ] {} <Information> Application: Child process exited normally with code 70.\r\n```\r\n\r\n### Full Logs\r\n\r\nPasting full logs below\r\n\r\nZK failures after first restart\r\n\r\n<details>\r\n<summary>Show More</summary>\r\n<hr>\r\n\r\n```\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:54,542 [myid:2] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:03:54,846 [myid:3] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:59642\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:54,844 [myid:2] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:54,851 [myid:1] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,162 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):Leader@1296] - Have quorum of supporters, sids: [ [1, 2],[1, 2] ]; starting up and setting last processed zxid: 0x1200000000\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,164 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):CommitProcessor@256] - Configuring CommitProcessor with 1 worker threads.\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,165 [myid:1] - INFO  [QuorumPeer[myid=1](plain=/0.0.0.0:2181)(secure=disabled):ContainerManager@64] - Using checkIntervalMs=60000 maxPerMinute=10000\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:55,171 [myid:2] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:55,171 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0.0.0.0:2181)(secure=disabled):Learner@529] - Learner received UPTODATE message\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:03:55,172 [myid:3] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:55,178 [myid:2] - INFO  [QuorumPeer[myid=2](plain=/0.0.0.0:2181)(secure=disabled):CommitProcessor@256] - Configuring CommitProcessor with 1 worker threads.\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:55,183 [myid:2] - INFO  [SyncThread:2:FileTxnLog@216] - Creating new log file: log.11000000ca\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,243 [myid:1] - WARN  [NIOWorkerThread-1:ZooKeeperServer@1013] - Connection request from old client /172.18.152.209:36888; will be dropped if server is in r-o mode\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:03:55,241 [myid:3] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:55,244 [myid:2] - WARN  [QuorumPeer[myid=2](plain=/0.0.0.0:2181)(secure=disabled):Follower@125] - Got zxid 0x1200000001 expected 0x1\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:03:55,401 [myid:3] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:59644\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:03:55,617 [myid:1] - WARN  [NIOWorkerThread-2:ZooKeeperServer@1013] - Connection request from old client /172.18.191.30:54982; will be dropped if server is in r-o mode\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:03:55,615 [myid:3] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:03:59,587 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:34912\r\nookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper Dumping heap to java_pid1.hprof ...\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:04:09,596 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:34960\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:04:13,664 [myid:2] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:34972\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:04:13,690 [myid:3] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nookeeper-2 kubernetes-zookeeper 2021-08-11 01:04:14,851 [myid:3] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:59690\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:04:15,405 [myid:3] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:59692\r\nzookeeper-0 kubernetes-zookeeper Heap dump file created [1087344431 bytes in 9.622 secs]\r\nzookeeper-0 kubernetes-zookeeper #\r\nzookeeper-0 kubernetes-zookeeper # java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper # -XX:OnOutOfMemoryError=\"kill -9 %p\"\r\nzookeeper-0 kubernetes-zookeeper #   Executing /bin/sh -c \"kill -9 1\"...\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:04:19,587 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:35008\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:19,120 [myid:1] - ERROR [CommitProcWorkThread-1:NIOServerCnxnFactory$1@92] - Thread Thread[CommitProcWorkThread-1,5,main] died\r\nzookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.Arrays.copyOf(Arrays.java:3236)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.DataOutputStream.writeLong(DataOutputStream.java:224)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeLong(BinaryOutputArchive.java:59)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.data.Stat.serialize(Stat.java:131)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.proto.GetDataResponse.serialize(GetDataResponse.java:51)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.ServerCnxn.sendResponse(ServerCnxn.java:79)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:690)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:492)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:949)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:298)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:155)\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nzookeeper-0 kubernetes-zookeeper \tat java.lang.Thread.run(Thread.java:748)\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:20,541 [myid:1] - ERROR [CommitProcWorkThread-2:NIOServerCnxnFactory$1@92] - Thread Thread[CommitProcWorkThread-2,5,main] died\r\nzookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.Arrays.copyOf(Arrays.java:3236)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\r\nzookeeper-0 kubernetes-zookeeper \tat java.io.DataOutputStream.writeLong(DataOutputStream.java:224)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeLong(BinaryOutputArchive.java:59)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.data.Stat.serialize(Stat.java:131)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.proto.GetDataResponse.serialize(GetDataResponse.java:51)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.jute.BinaryOutputArchive.writeRecord(BinaryOutputArchive.java:123)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.ServerCnxn.sendResponse(ServerCnxn.java:79)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:690)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:492)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.quorum.Leader$ToBeAppliedRequestProcessor.processRequest(Leader.java:949)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.quorum.CommitProcessor$CommitWorkRequest.doWork(CommitProcessor.java:298)\r\nzookeeper-0 kubernetes-zookeeper \tat org.apache.zookeeper.server.WorkerService$ScheduledWorkRequest.run(WorkerService.java:155)\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\r\nzookeeper-0 kubernetes-zookeeper \tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\r\nzookeeper-0 kubernetes-zookeeper \tat java.lang.Thread.run(Thread.java:748)\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:21,548 [myid:1] - WARN  [NIOWorkerThread-1:ZooKeeperServer@1013] - Connection request from old client /172.18.191.30:55098; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:23,281 [myid:1] - ERROR [CommitProcWorkThread-3:NIOServerCnxnFactory$1@92] - Thread Thread[CommitProcWorkThread-3,5,main] died\r\nzookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:23,281 [myid:1] - WARN  [NIOWorkerThread-2:ZooKeeperServer@1013] - Connection request from old client /172.18.152.209:36936; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:23,547 [myid:1] - WARN  [NIOWorkerThread-2:ZooKeeperServer@1013] - Connection request from old client /172.18.152.209:36968; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:23,281 [myid:1] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:42622\r\nzookeeper-1 kubernetes-zookeeper 2021-08-11 01:04:23,661 [myid:2] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:35020\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:23,548 [myid:1] - ERROR [CommitProcWorkThread-4:NIOServerCnxnFactory$1@92] - Thread Thread[CommitProcWorkThread-4,5,main] died\r\nzookeeper-0 kubernetes-zookeeper java.lang.OutOfMemoryError: Java heap space\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,466 [myid:1] - INFO  [NIOWorkerThread-2:QuorumZooKeeperServer@157] - Submitting global closeSession request for session 0x10722e27acb0000\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,732 [myid:1] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:42620\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,735 [myid:1] - WARN  [NIOWorkerThread-2:ZooKeeperServer@1013] - Connection request from old client /172.18.191.30:55148; will be dropped if server is in r-o mode\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:24,736 [myid:1] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:42646\r\nzookeeper-2 kubernetes-zookeeper 2021-08-11 01:04:24,845 [myid:3] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:59714\r\nzookeeper-0 kubernetes-zookeeper 2021-08-11 01:04:25,030 [myid:1] - ERROR\r\n```\r\n\r\n<hr>\r\n</details>\r\n\r\n\r\nAfter repeated rolling restarts, I terminated clickhouse cluste, but retained EBS (no data loss)\r\nNow when i bring it up, ZK does not show heap errors, but just leader election / timeout errors\r\n\r\n<details>\r\n<summary>Show More</summary>\r\n<hr>\r\n\r\n```shell\r\nzookeeper-1 kubernetes-zookeeper 2021-08-26 14:58:48,529 [myid:2] - INFO  [WorkerReceiver[myid=2]:FastLeaderElection@679] - Notification: 2 (message format version), 3 (n.leader), 0xa00000144 (n.zxid), 0xb (n.round), LOOKING (n.state), 3 (n.sid), 0xa (n.peerEPoch), LOOKING (my state)0 (n.config version)\r\nzookeeper-2 kubernetes-zookeeper 2021-08-26 14:58:48,526 [myid:3] - INFO  [QuorumPeer[myid=3](plain=/0.0.0.0:2181)(secure=disabled):FastLeaderElection@919] - Notification time out: 60000\r\nzookeeper-1 kubernetes-zookeeper 2021-08-26 14:58:49,595 [myid:2] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:56140\r\nzookeeper-0 kubernetes-zookeeper 2021-08-26 14:58:50,572 [myid:1] - INFO  [NIOWorkerThread-4:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:51318\r\nzookeeper-0 kubernetes-zookeeper 2021-08-26 14:58:51,425 [myid:1] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:51320\r\nzookeeper-1 kubernetes-zookeeper 2021-08-26 14:58:53,656 [myid:2] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:56152\r\nzookeeper-2 kubernetes-zookeeper 2021-08-26 14:58:54,851 [myid:3] - INFO  [NIOWorkerThread-1:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:40616\r\nzookeeper-2 kubernetes-zookeeper 2021-08-26 14:58:55,410 [myid:3] - INFO  [NIOWorkerThread-2:NIOServerCnxn@518] - Processing ruok command from /127.0.0.1:40618\r\nzookeeper-0 kubernetes-zookeeper 2021-08-26 14:58:59,382 [myid:1] - WARN  [NIOWorkerThread-4:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-0 kubernetes-zookeeper 2021-08-26 14:58:59,383 [myid:1] - WARN  [NIOWorkerThread-2:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-2 kubernetes-zookeeper 2021-08-26 14:58:59,384 [myid:3] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\nzookeeper-1 kubernetes-zookeeper 2021-08-26 14:58:59,388 [myid:2] - WARN  [NIOWorkerThread-1:NIOServerCnxn@370] - Exception causing close of session 0x0: ZooKeeperServer not running\r\n```\r\n\r\n<hr>\r\n</details>\r\n\r\nClickhouse pods continue to fail with the same errors\r\n\r\n<details>\r\n<summary>Show More</summary>\r\n<hr>\r\n\r\n```shell\r\n⌁59% [kushagraverma:~] $ stern -n clickhouse chi-rollworks-production-cluster-1-0-0\r\n+ chi-rollworks-production-cluster-1-0-0 › clickhouse-pod\r\n+ chi-rollworks-production-cluster-1-0-0 › clickhouse-log\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Processing configuration file '/etc/clickhouse-server/config.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/conf.d/chop-generated-macros.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/conf.d/chop-generated-zookeeper.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/01-clickhouse-listen.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/02-clickhouse-logger.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/03-clickhouse-querylog.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/04-clickhouse-partlog.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/config.d/chop-generated-settings.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Logging debug to /var/log/clickhouse-server/clickhouse-server.log\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Logging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Logging debug to console\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.023092 [ 1 ] {} <Information> Application: Will watch for the process with pid 84\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.023208 [ 84 ] {} <Information> Application: Forked a child process to watch\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.023770 [ 84 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.069037 [ 84 ] {} <Information> : Starting ClickHouse 21.1.2.15 with revision 54445, build id: E40526A12E9A8F3819A18694F6B798F10C624D5C, PID 84\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.069179 [ 84 ] {} <Information> Application: starting up\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.180634 [ 84 ] {} <Information> Application: Calculated checksum of the binary: F6479E166DD7E8ABFA19525726186F71, integrity check passed.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.180686 [ 84 ] {} <Information> Application: It looks like the process has no CAP_IPC_LOCK capability, binary mlock will be disabled. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_ipc_lock=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181112 [ 84 ] {} <Debug> Application: rlimit on number of file descriptors is 1048576\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181137 [ 84 ] {} <Debug> Application: Initializing DateLUT.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181167 [ 84 ] {} <Debug> Application: Setting up /var/lib/clickhouse/tmp/ to store temporary data in it\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181228 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/_backup_uploads\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181242 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/_clickhouse_exports\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181253 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/data\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181263 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/benchmarks\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181274 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/_s32ch_uploads\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181284 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/backups\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181293 [ 84 ] {} <Debug> Application: Skipped file in temporary path /var/lib/clickhouse/tmp/pi_test\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181524 [ 84 ] {} <Debug> Application: Configuration parameter 'interserver_http_host' doesn't exist or exists and empty. Will use 'chi-rollworks-production-cluster-1-0-0.chi-rollworks-production-cluster-1-0.clickhouse.svc.cluster.local' as replica host.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.181692 [ 84 ] {} <Information> SensitiveDataMaskerConfigRead: 1 query masking rules loaded.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.182459 [ 84 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Processing configuration file '/etc/clickhouse-server/users.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/conf.d/chop-generated-macros.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/conf.d/chop-generated-zookeeper.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/users.d/01-clickhouse-user.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/users.d/02-clickhouse-default-profile.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Merging configuration file '/etc/clickhouse-server/users.d/chop-generated-users.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Saved preprocessed configuration to '/var/lib/clickhouse/preprocessed_configs/users.xml'.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.183602 [ 84 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.184261 [ 84 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.184753 [ 84 ] {} <Debug> Access(user directories): Added users.xml access storage 'users.xml', path: /etc/clickhouse-server/users.xml\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.187891 [ 84 ] {} <Debug> Access(user directories): Added local directory access storage 'local directory', path: /var/lib/clickhouse/access/\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.188011 [ 84 ] {} <Information> Application: Setting max_server_memory_usage was set to 112.03 GiB (124.47 GiB available * 0.90 max_server_memory_usage_to_ram_ratio)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.188321 [ 84 ] {} <Information> Application: Loading metadata from /var/lib/clickhouse/\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.191813 [ 84 ] {} <Information> DatabaseOrdinary (system): Total 11 tables and 0 dictionaries.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193098 [ 92 ] {} <Debug> system.query_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193104 [ 91 ] {} <Debug> system.crash_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193155 [ 97 ] {} <Debug> system.part_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193216 [ 95 ] {} <Debug> system.part_log_0: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193272 [ 90 ] {} <Debug> system.asynchronous_metric_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193288 [ 94 ] {} <Debug> system.asynchronous_metric_log_0: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193866 [ 96 ] {} <Debug> system.metric_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.193954 [ 93 ] {} <Debug> system.metric_log_0: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.194222 [ 95 ] {} <Debug> system.part_log_0: Loaded data parts (0 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.196496 [ 91 ] {} <Debug> system.crash_log: Loaded data parts (1 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.197016 [ 95 ] {} <Debug> system.query_log_0: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.197383 [ 91 ] {} <Debug> system.trace_log: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.202740 [ 95 ] {} <Debug> system.query_log_0: Loaded data parts (0 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.204104 [ 95 ] {} <Debug> system.trace_log_0: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.207263 [ 94 ] {} <Debug> system.asynchronous_metric_log_0: Loaded data parts (12 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.208850 [ 90 ] {} <Debug> system.asynchronous_metric_log: Loaded data parts (17 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.235547 [ 95 ] {} <Debug> system.trace_log_0: Loaded data parts (17 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.238265 [ 91 ] {} <Debug> system.trace_log: Loaded data parts (29 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.243619 [ 93 ] {} <Debug> system.metric_log_0: Loaded data parts (19 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.244307 [ 96 ] {} <Debug> system.metric_log: Loaded data parts (21 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.253192 [ 97 ] {} <Debug> system.part_log: Loaded data parts (63 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258067 [ 92 ] {} <Debug> system.query_log: Loaded data parts (75 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258264 [ 84 ] {} <Information> DatabaseOrdinary (system): Starting up tables.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258380 [ 95 ] {} <Information> BackgroundSchedulePool/BgSchPool: Create BackgroundSchedulePool with 16 threads\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258727 [ 109 ] {} <Debug> system.asynchronous_metric_log (MergerMutator): Selected 4 parts from 202108_238847_238851_1 to 202108_238854_238854_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258771 [ 109 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 276.24 GiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258818 [ 95 ] {} <Debug> system.asynchronous_metric_log (MergerMutator): Merging 4 parts: from 202108_238847_238851_1 to 202108_238854_238854_0 into Compact\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258835 [ 109 ] {} <Debug> system.part_log (MergerMutator): Selected 2 parts from 20210810_258351_260334_1245 to 20210810_260335_260335_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258874 [ 109 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 276.24 GiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258883 [ 95 ] {} <Debug> system.asynchronous_metric_log (MergerMutator): Selected MergeAlgorithm: Horizontal\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258890 [ 130 ] {} <Debug> system.query_log (MergerMutator): Selected 2 parts from 20210810_750679_751469_218 to 20210810_751470_751470_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258928 [ 130 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 276.24 GiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258938 [ 151 ] {} <Debug> system.part_log (MergerMutator): Merging 2 parts: from 20210810_258351_260334_1245 to 20210810_260335_260335_0 into Compact\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258977 [ 132 ] {} <Debug> system.query_log (MergerMutator): Merging 2 parts: from 20210810_750679_751469_218 to 20210810_751470_751470_0 into Wide\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.258989 [ 151 ] {} <Debug> system.part_log (MergerMutator): Selected MergeAlgorithm: Horizontal\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.259057 [ 132 ] {} <Debug> system.query_log (MergerMutator): Selected MergeAlgorithm: Horizontal\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.286039 [ 84 ] {} <Information> DatabaseOrdinary (company_attributes): Total 12 tables and 0 dictionaries.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.286540 [ 95 ] {} <Debug> system.asynchronous_metric_log (MergerMutator): Merge sorted 549 rows, containing 5 columns (5 merged, 0 gathered) in 0.02770479 sec., 19816.067907390745 rows/sec., 984.71 KiB/sec.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.287183 [ 95 ] {} <Debug> MemoryTracker: Peak memory usage: 4.05 MiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.287519 [ 136 ] {} <Debug> company_attributes.first_party_company_attributes_1621751895000_f4571818: Loading data parts\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.323469 [ 151 ] {} <Debug> system.part_log (MergerMutator): Merge sorted 32825 rows, containing 19 columns (19 merged, 0 gathered) in 0.064514894 sec., 508797.2399055635 rows/sec., 148.98 MiB/sec.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.347751 [ 151 ] {} <Debug> MemoryTracker: Peak memory usage: 12.30 MiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.445664 [ 132 ] {} <Debug> system.query_log (MergerMutator): Merge sorted 5858 rows, containing 51 columns (51 merged, 0 gathered) in 0.186681097 sec., 31379.71703691028 rows/sec., 95.33 MiB/sec.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.456983 [ 132 ] {} <Debug> MemoryTracker: Peak memory usage: 154.50 MiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:22.548567 [ 136 ] {} <Debug> company_attributes.first_party_company_attributes_1621751895000_f4571818: Loaded data parts (243 items)\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.738178 [ 84 ] {} <Error> Application: Caught exception while loading metadata: Code: 999, e.displayText() = Coordination::Exception: All connection tries failed while connecting to ZooKeeper. nodes: 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod  (Connection loss): Cannot attach table ... from metadata file ...sql from query ATTACH TABLE ... while loading database ... from path ...\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 0. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error, int) @ 0xfb46cd3 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 1. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error) @ 0xfb46f52 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2. Coordination::ZooKeeper::connect(std::__1::vector<Coordination::ZooKeeper::Node, std::__1::allocator<Coordination::ZooKeeper::Node> > const&, Poco::Timespan) @ 0xfb9d691 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 3. Coordination::ZooKeeper::ZooKeeper(std::__1::vector<Coordination::ZooKeeper::Node, std::__1::allocator<Coordination::ZooKeeper::Node> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Timespan, Poco::Timespan, Poco::Timespan) @ 0xfb9c1b2 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 4. zkutil::ZooKeeper::init(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xfb5dafa in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 5. zkutil::ZooKeeper::ZooKeeper(Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xfb5ebf4 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 6. std::__1::__compressed_pair_elem<zkutil::ZooKeeper, 1, false>::__compressed_pair_elem<Poco::Util::AbstractConfiguration const&, char const (&) [10], 0ul, 1ul>(std::__1::piecewise_construct_t, std::__1::tuple<Poco::Util::AbstractConfiguration const&, char const (&) [10]>, std::__1::__tuple_indices<0ul, 1ul>) @ 0xeb0db5e in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 7. ? @ 0xeaeea93 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 8. DB::StorageReplicatedMergeTree::StorageReplicatedMergeTree(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata const&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::MergeTreeData::MergingParams const&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool, bool) @ 0xf3e45b1 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 9. std::__1::shared_ptr<DB::StorageReplicatedMergeTree> ext::shared_ptr_helper<DB::StorageReplicatedMergeTree>::create<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, bool const&, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, DB::MergeTreeData::MergingParams&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >, bool const&, bool&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, bool const&, DB::StorageID const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageInMemoryMetadata&, DB::Context&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&, DB::MergeTreeData::MergingParams&, std::__1::unique_ptr<DB::MergeTreeSettings, std::__1::default_delete<DB::MergeTreeSettings> >&&, bool const&, bool&) @ 0xf7c35d3 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 10. ? @ 0xf7bc888 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 11. std::__1::__function::__func<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&), std::__1::allocator<std::__1::shared_ptr<DB::IStorage> (*)(DB::StorageFactory::Arguments const&)>, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) @ 0xf55d63d in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 12. DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) const @ 0xf327caa in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 13. DB::createTableFromAST(DB::ASTCreateQuery, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool) @ 0xebdd04f in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 14. ? @ 0xebd284d in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 15. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x86441bd in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 16. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x864671f in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 17. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x86415ed in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 18. ? @ 0x86451a3 in /usr/bin/clickhouse\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 19. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 20. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod  (version 21.1.2.15 (official build))\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.739428 [ 84 ] {} <Information> Application: Shutting down storages.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.739451 [ 84 ] {} <Information> Context: Shutdown disk default\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.760207 [ 143 ] {} <Debug> SystemLog (system.metric_log): Will use existing table system.metric_log for MetricLog\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:29.762661 [ 143 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 276.24 GiB.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.260077 [ 84 ] {} <Debug> system.asynchronous_metric_log: Removing part from filesystem 202108_238847_238851_1\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.260916 [ 84 ] {} <Debug> system.asynchronous_metric_log: Removing part from filesystem 202108_238852_238852_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.261892 [ 84 ] {} <Debug> system.asynchronous_metric_log: Removing part from filesystem 202108_238853_238853_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.262431 [ 84 ] {} <Debug> system.asynchronous_metric_log: Removing part from filesystem 202108_238854_238854_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.262684 [ 84 ] {} <Debug> system.part_log: Removing part from filesystem 20210810_258351_260334_1245\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.264177 [ 84 ] {} <Debug> system.part_log: Removing part from filesystem 20210810_260335_260335_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.264758 [ 84 ] {} <Debug> system.query_log: Removing part from filesystem 20210810_750679_751469_218\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.269151 [ 84 ] {} <Debug> system.query_log: Removing part from filesystem 20210810_751470_751470_0\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.278715 [ 84 ] {} <Debug> Application: Shut down storages.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.278796 [ 84 ] {} <Debug> Application: Destroyed global context.\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280176 [ 84 ] {} <Error> Application: Coordination::Exception: All connection tries failed while connecting to ZooKeeper. nodes: 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod Code: 33, e.displayText() = DB::Exception: Cannot read all data. Bytes read: 0. Bytes expected: 4.: while receiving handshake from ZooKeeper (version 21.1.2.15 (official build)), 10.100.179.231:2181\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod  (Connection loss): Cannot attach table ... from metadata file ...sql from query ATTACH TABLE ... while loading database ... from path ...\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280211 [ 84 ] {} <Information> Application: shutting down\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280220 [ 84 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.280287 [ 85 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\nchi-rollworks-production-cluster-1-0-0 clickhouse-pod 2021.08.11 00:45:30.303513 [ 1 ] {} <Information> Application: Child process exited normally with code 70.\r\n- chi-rollworks-production-cluster-1-0-0 › clickhouse-pod\r\n+ chi-rollworks-production-cluster-1-0-0 › clickhouse-pod\r\n```\r\n\r\n<hr>\r\n</details>\r\n\r\n---\r\n\r\nCluster details\r\n\r\n- ClickHouse version: 21.1.2.15 \r\n- deployed on EKS v.17 using clickhouse-operator v13.5\r\n\r\nZK relevant details\r\n- cluster has been running for > 1 year\r\n- ZK version: 3.5.6, heap size is 1G on t2.medium (4g RAM)\r\n- ZK snapshots are around 900MB per file, total 8.8G \r\n- CH data is 2 TB / node\r\n\r\nCan someone please help in understanding why such situations can occur and any suggestions for avoiding it in future.\r\n\r\nThanks !","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28196/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28192","id":980211841,"node_id":"MDU6SXNzdWU5ODAyMTE4NDE=","number":28192,"title":"DateTime64 (Decimal64) comparsion much slower than UInt64 or DateTime","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-26T12:53:49Z","updated_at":"2021-08-26T12:53:49Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the situation**\r\nEquality, non-equality conditions over DateTime64 column.\r\nNot sure about how decimal values being stored on disk / in memory, but when we do compare column and literal, we can convert literal value to the same precise as decimal column and just compare bit's like in UInt64, so decimal datatype shouldn't have any impact for performance.\r\n\r\n**How to reproduce**\r\nClickhouse version 21.8\r\n\r\n```\r\nSELECT count()\r\nFROM numbers_mt(10000000000)\r\nWHERE materialize(toDateTime64(1000000, 3)) = toDateTime64(1000000, 3)\r\n\r\n┌─────count()─┐\r\n│ 10000000000 │\r\n└─────────────┘\r\n\r\n1 rows in set. Elapsed: 1.679 sec. Processed 10.00 billion rows, 80.00 GB (5.95 billion rows/s., 47.64 GB/s.)\r\n\r\nSELECT count()\r\nFROM numbers_mt(10000000000)\r\nWHERE materialize(toDecimal64(toDateTime64(1000000, 3), 3)) = toDecimal64(toDateTime64(1000000, 3), 3)\r\n\r\nQuery id: 98b50c04-9067-4d6c-9ae2-346f9a64d8d6\r\n\r\n┌─────count()─┐\r\n│ 10000000000 │\r\n└─────────────┘\r\n\r\n1 rows in set. Elapsed: 1.812 sec. Processed 10.00 billion rows, 80.00 GB (5.52 billion rows/s., 44.16 GB/s.)\r\n\r\n\r\nSELECT count()\r\nFROM numbers_mt(10000000000)\r\nWHERE materialize(toUInt64(toDateTime64(1000000, 3))) = toUInt64(toDateTime64(1000000, 3))\r\n\r\n┌─────count()─┐\r\n│ 10000000000 │\r\n└─────────────┘\r\n\r\n1 rows in set. Elapsed: 0.975 sec. Processed 10.00 billion rows, 80.00 GB (10.26 billion rows/s., 82.06 GB/s.)\r\n\r\n\r\nSELECT count()\r\nFROM numbers_mt(10000000000)\r\nWHERE materialize(toDateTime(1000000)) = toDateTime(1000000)\r\n\r\n┌─────count()─┐\r\n│ 10000000000 │\r\n└─────────────┘\r\n\r\n1 rows in set. Elapsed: 0.508 sec. Processed 10.00 billion rows, 80.00 GB (19.70 billion rows/s., 157.57 GB/s.)\r\n```\r\n\r\n**Expected performance**\r\nDateTime64 compare with literal would have at least half of DateTime performance or the same as UInt64.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28192/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28175","id":979993980,"node_id":"MDU6SXNzdWU5Nzk5OTM5ODA=","number":28175,"title":"Timeout exceeded while writing to socket","user":{"login":"shadowDy","id":18388143,"node_id":"MDQ6VXNlcjE4Mzg4MTQz","avatar_url":"https://avatars.githubusercontent.com/u/18388143?v=4","gravatar_id":"","url":"https://api.github.com/users/shadowDy","html_url":"https://github.com/shadowDy","followers_url":"https://api.github.com/users/shadowDy/followers","following_url":"https://api.github.com/users/shadowDy/following{/other_user}","gists_url":"https://api.github.com/users/shadowDy/gists{/gist_id}","starred_url":"https://api.github.com/users/shadowDy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shadowDy/subscriptions","organizations_url":"https://api.github.com/users/shadowDy/orgs","repos_url":"https://api.github.com/users/shadowDy/repos","events_url":"https://api.github.com/users/shadowDy/events{/privacy}","received_events_url":"https://api.github.com/users/shadowDy/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-08-26T08:47:48Z","updated_at":"2021-09-02T02:10:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"For a global join query, an error is reported after the child node query is executed for less than 1s. Timeout exceeded while writing to socket\r\nlog：\r\n\r\n![image](https://user-images.githubusercontent.com/18388143/130931946-0acf57e5-1bcc-4fb2-a4f6-cff889e61f16.png)\r\n\r\nthe system.settings send_timeout is 300s。\r\n\r\nthe driver node throw exception:\r\n`Code: 210. DB::Exception: Received from xxxx. DB::Exception: Received from xxxx. DB::Exception: Connection reset by peer, while reading from socket (xxxx): while receiving packet from xxxx: While executing Remote.`","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28175/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28166","id":979951120,"node_id":"MDU6SXNzdWU5Nzk5NTExMjA=","number":28166,"title":"Logical error: 'From array size and to array size mismatch'","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-26T07:59:59Z","updated_at":"2021-08-26T07:59:59Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/28081/aea22ec6a1b35172cc409f78bb0ca1dd76c86b40/fuzzer_debug/report.html#fail1\r\n\r\n``` sql\r\nSELECT [255, NULL, NULL], NULL, [256, -2147483647], bitmapToArray(bitmapTransform(z, CAST([1025, -9223372036854775808], 'Array(UInt32)'), replace.from)) FROM bitmap_column_expr_test3 ORDER BY z ASC NULLS FIRST, CAST([-9223372036854775807], 'Array(UInt32)') ASC NULLS FIRST, [1, 0] ASC NULLS FIRST, [NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL, NULL] ASC NULLS FIRST, [-9223372036854775807, -2147483649, 65537] ASC\r\n```\r\n\r\n```\r\n2021.08.26 09:56:52.986693 [ 248 ] {6466ac25-89b3-4975-a687-bbde870c1afe} <Fatal> : Logical error: 'From array size and to array size mismatch'.\r\n2021.08.26 09:56:52.986708 [ 267 ] {6466ac25-89b3-4975-a687-bbde870c1afe} <Fatal> : Logical error: 'From array size and to array size mismatch'.\r\n2021.08.26 09:56:52.986817 [ 270 ] {6466ac25-89b3-4975-a687-bbde870c1afe} <Fatal> : Logical error: 'From array size and to array size mismatch'.\r\n2021.08.26 09:56:52.987942 [ 279 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.26 09:56:52.988147 [ 280 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.26 09:56:52.988318 [ 281 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.26 09:56:52.988373 [ 279 ] {} <Fatal> BaseDaemon: (version 21.10.1.7896, build id: 0A49A39B90BAAEABE3FAD22309B71D23F90D6469) (from thread 248) (query_id: 6466ac25-89b3-4975-a687-bbde870c1afe) Received signal Aborted (6)\r\n2021.08.26 09:56:52.988572 [ 280 ] {} <Fatal> BaseDaemon: (version 21.10.1.7896, build id: 0A49A39B90BAAEABE3FAD22309B71D23F90D6469) (from thread 270) (query_id: 6466ac25-89b3-4975-a687-bbde870c1afe) Received signal Aborted (6)\r\n2021.08.26 09:56:52.988626 [ 281 ] {} <Fatal> BaseDaemon: (version 21.10.1.7896, build id: 0A49A39B90BAAEABE3FAD22309B71D23F90D6469) (from thread 267) (query_id: 6466ac25-89b3-4975-a687-bbde870c1afe) Received signal Aborted (6)\r\n2021.08.26 09:56:52.988760 [ 279 ] {} <Fatal> BaseDaemon:\r\n2021.08.26 09:56:52.988867 [ 280 ] {} <Fatal> BaseDaemon:\r\n2021.08.26 09:56:52.988999 [ 281 ] {} <Fatal> BaseDaemon:\r\n2021.08.26 09:56:52.989169 [ 279 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f0960bbf18b 0x7f0960b9e859 0x137b4f66 0x137b5075 0x16783921 0x1678019d 0x16470efe 0x1ddbba8d 0x1ddbc5d4 0x1e6b3862 0x1e6b2cd6 0x201bdf89 0x1bdb6aa5 0x1fe1c548 0x1fe89a1c 0x1fe8997f 0x1fe8991d 0x1fe898dd 0x1fe898b5 0x1fe8987d 0x13803da9 0x13802ed5 0x1fe8826d 0x1fe88c59 0x1fe8ab09 0x1fe8aa5d 0x1fe8aa01 0x1fe8a912 0x1fe8a7ec 0x1fe8a6fd 0x1fe8a6bd 0x1fe8a695 0x1fe8a660 0x13803da9 0x13802ed5 0x13829eae 0x138311c4 0x1383111d 0x13831045 0x13830962 0x7f0960d85609\r\n2021.08.26 09:56:52.989334 [ 280 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f0960bbf18b 0x7f0960b9e859 0x137b4f66 0x137b5075 0x16783921 0x1678019d 0x16470efe 0x1ddbba8d 0x1ddbc5d4 0x1e6b3862 0x1e6b2cd6 0x201bdf89 0x1bdb6aa5 0x1fe1c548 0x1fe89a1c 0x1fe8997f 0x1fe8991d 0x1fe898dd 0x1fe898b5 0x1fe8987d 0x13803da9 0x13802ed5 0x1fe8826d 0x1fe88c59 0x1fe8ab09 0x1fe8aa5d 0x1fe8aa01 0x1fe8a912 0x1fe8a7ec 0x1fe8a6fd 0x1fe8a6bd 0x1fe8a695 0x1fe8a660 0x13803da9 0x13802ed5 0x13829eae 0x138311c4 0x1383111d 0x13831045 0x13830962 0x7f0960d85609\r\n2021.08.26 09:56:52.989655 [ 279 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:52.989586 [ 281 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f0960bbf18b 0x7f0960b9e859 0x137b4f66 0x137b5075 0x16783921 0x1678019d 0x16470efe 0x1ddbba8d 0x1ddbc5d4 0x1e6b3862 0x1e6b2cd6 0x201bdf89 0x1bdb6aa5 0x1fe1c548 0x1fe89a1c 0x1fe8997f 0x1fe8991d 0x1fe898dd 0x1fe898b5 0x1fe8987d 0x13803da9 0x13802ed5 0x1fe8826d 0x1fe88c59 0x1fe8ab09 0x1fe8aa5d 0x1fe8aa01 0x1fe8a912 0x1fe8a7ec 0x1fe8a6fd 0x1fe8a6bd 0x1fe8a695 0x1fe8a660 0x13803da9 0x13802ed5 0x13829eae 0x138311c4 0x1383111d 0x13831045 0x13830962 0x7f0960d85609\r\n2021.08.26 09:56:52.989876 [ 280 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:52.989897 [ 279 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:52.990085 [ 280 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:52.990184 [ 281 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:52.990420 [ 281 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.26 09:56:53.146087 [ 280 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x137b4f66 in /workspace/clickhouse\r\n2021.08.26 09:56:53.146197 [ 281 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x137b4f66 in /workspace/clickhouse\r\n2021.08.26 09:56:53.146442 [ 279 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x137b4f66 in /workspace/clickhouse\r\n2021.08.26 09:56:53.284874 [ 281 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x137b5075 in /workspace/clickhouse\r\n2021.08.26 09:56:53.285466 [ 280 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x137b5075 in /workspace/clickhouse\r\n2021.08.26 09:56:53.286706 [ 279 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x137b5075 in /workspace/clickhouse\r\n2021.08.26 09:56:53.644847 [ 281 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:643: COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionBitmapTransform::executeIntType<unsigned long>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, unsigned long) const @ 0x16783921 in /workspace/clickhouse\r\n2021.08.26 09:56:53.644838 [ 280 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:643: COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionBitmapTransform::executeIntType<unsigned long>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, unsigned long) const @ 0x16783921 in /workspace/clickhouse\r\n2021.08.26 09:56:53.644823 [ 279 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:643: COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::FunctionBitmapTransform::executeIntType<unsigned long>(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, unsigned long) const @ 0x16783921 in /workspace/clickhouse\r\n2021.08.26 09:56:54.002507 [ 281 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:540: DB::FunctionBitmapTransform::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x1678019d in /workspace/clickhouse\r\n2021.08.26 09:56:54.002708 [ 279 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:540: DB::FunctionBitmapTransform::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x1678019d in /workspace/clickhouse\r\n2021.08.26 09:56:54.002905 [ 280 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsBitmap.h:540: DB::FunctionBitmapTransform::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x1678019d in /workspace/clickhouse\r\n2021.08.26 09:56:54.111651 [ 279 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Functions/IFunctionAdaptors.h:21: DB::FunctionToExecutableFunctionAdaptor::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x16470efe in /workspace/clickhouse\r\n2021.08.26 09:56:54.111709 [ 281 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Functions/IFunctionAdaptors.h:21: DB::FunctionToExecutableFunctionAdaptor::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x16470efe in /workspace/clickhouse\r\n2021.08.26 09:56:54.111880 [ 280 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Functions/IFunctionAdaptors.h:21: DB::FunctionToExecutableFunctionAdaptor::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0x16470efe in /workspace/clickhouse\r\n2021.08.26 09:56:54.448546 [ 279 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:217: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbba8d in /workspace/clickhouse\r\n2021.08.26 09:56:54.448546 [ 280 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:217: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbba8d in /workspace/clickhouse\r\n2021.08.26 09:56:54.448578 [ 281 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:217: DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbba8d in /workspace/clickhouse\r\n2021.08.26 09:56:54.775015 [ 279 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:260: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbc5d4 in /workspace/clickhouse\r\n2021.08.26 09:56:54.781788 [ 281 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:260: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbc5d4 in /workspace/clickhouse\r\n2021.08.26 09:56:54.781788 [ 280 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:260: DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0x1ddbc5d4 in /workspace/clickhouse\r\n2021.08.26 09:56:55.360948 [ 279 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:601: DB::executeAction(DB::ExpressionActions::Action const&, DB::(anonymous namespace)::ExecutionContext&, bool) @ 0x1e6b3862 in /workspace/clickhouse\r\n2021.08.26 09:56:55.374624 [ 280 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:601: DB::executeAction(DB::ExpressionActions::Action const&, DB::(anonymous namespace)::ExecutionContext&, bool) @ 0x1e6b3862 in /workspace/clickhouse\r\n2021.08.26 09:56:55.374713 [ 281 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:601: DB::executeAction(DB::ExpressionActions::Action const&, DB::(anonymous namespace)::ExecutionContext&, bool) @ 0x1e6b3862 in /workspace/clickhouse\r\n2021.08.26 09:56:55.856818 [ 279 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:718: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x1e6b2cd6 in /workspace/clickhouse\r\n2021.08.26 09:56:55.880394 [ 280 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:718: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x1e6b2cd6 in /workspace/clickhouse\r\n2021.08.26 09:56:55.880400 [ 281 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionActions.cpp:718: DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x1e6b2cd6 in /workspace/clickhouse\r\n2021.08.26 09:56:56.038517 [ 279 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/ExpressionTransform.cpp:23: DB::ExpressionTransform::transform(DB::Chunk&) @ 0x201bdf89 in /workspace/clickhouse\r\n2021.08.26 09:56:56.061198 [ 280 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/ExpressionTransform.cpp:23: DB::ExpressionTransform::transform(DB::Chunk&) @ 0x201bdf89 in /workspace/clickhouse\r\n2021.08.26 09:56:56.061198 [ 281 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/ExpressionTransform.cpp:23: DB::ExpressionTransform::transform(DB::Chunk&) @ 0x201bdf89 in /workspace/clickhouse\r\n2021.08.26 09:56:56.310953 [ 279 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.h:43: DB::ISimpleTransform::transform(DB::Chunk&, DB::Chunk&) @ 0x1bdb6aa5 in /workspace/clickhouse\r\n2021.08.26 09:56:56.337529 [ 281 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.h:43: DB::ISimpleTransform::transform(DB::Chunk&, DB::Chunk&) @ 0x1bdb6aa5 in /workspace/clickhouse\r\n2021.08.26 09:56:56.337529 [ 280 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.h:43: DB::ISimpleTransform::transform(DB::Chunk&, DB::Chunk&) @ 0x1bdb6aa5 in /workspace/clickhouse\r\n2021.08.26 09:56:56.438739 [ 279 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.cpp:89: DB::ISimpleTransform::work() @ 0x1fe1c548 in /workspace/clickhouse\r\n2021.08.26 09:56:56.468216 [ 281 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.cpp:89: DB::ISimpleTransform::work() @ 0x1fe1c548 in /workspace/clickhouse\r\n2021.08.26 09:56:56.468223 [ 280 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Processors/ISimpleTransform.cpp:89: DB::ISimpleTransform::work() @ 0x1fe1c548 in /workspace/clickhouse\r\n2021.08.26 09:56:56.965685 [ 279 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88: DB::executeJob(DB::IProcessor*) @ 0x1fe89a1c in /workspace/clickhouse\r\n2021.08.26 09:56:57.003380 [ 280 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88: DB::executeJob(DB::IProcessor*) @ 0x1fe89a1c in /workspace/clickhouse\r\n2021.08.26 09:56:57.003385 [ 281 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88: DB::executeJob(DB::IProcessor*) @ 0x1fe89a1c in /workspace/clickhouse\r\n2021.08.26 09:56:57.481180 [ 279 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105: DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const @ 0x1fe8997f in /workspace/clickhouse\r\n2021.08.26 09:56:57.524162 [ 280 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105: DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const @ 0x1fe8997f in /workspace/clickhouse\r\n2021.08.26 09:56:57.524166 [ 281 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105: DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const @ 0x1fe8997f in /workspace/clickhouse\r\n2021.08.26 09:56:57.989552 [ 279 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe8991d in /workspace/clickhouse\r\n2021.08.26 09:56:58.056474 [ 281 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe8991d in /workspace/clickhouse\r\n2021.08.26 09:56:58.056675 [ 280 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe8991d in /workspace/clickhouse\r\n2021.08.26 09:56:58.496875 [ 279 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe898dd in /workspace/clickhouse\r\n2021.08.26 09:56:58.577542 [ 280 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe898dd in /workspace/clickhouse\r\n2021.08.26 09:56:58.577543 [ 281 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1fe898dd in /workspace/clickhouse\r\n2021.08.26 09:56:59.001443 [ 279 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() @ 0x1fe898b5 in /workspace/clickhouse\r\n2021.08.26 09:56:59.104916 [ 281 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() @ 0x1fe898b5 in /workspace/clickhouse\r\n2021.08.26 09:56:59.104914 [ 280 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() @ 0x1fe898b5 in /workspace/clickhouse\r\n2021.08.26 09:56:59.524207 [ 279 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8987d in /workspace/clickhouse\r\n2021.08.26 09:56:59.622920 [ 281 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8987d in /workspace/clickhouse\r\n2021.08.26 09:56:59.622920 [ 280 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8987d in /workspace/clickhouse\r\n2021.08.26 09:56:59.777259 [ 279 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:56:59.878977 [ 281 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:56:59.878983 [ 280 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:57:00.024593 [ 279 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:00.139074 [ 280 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:00.139075 [ 281 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:00.525829 [ 279 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:600: DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1fe8826d in /workspace/clickhouse\r\n2021.08.26 09:57:00.643522 [ 281 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:600: DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1fe8826d in /workspace/clickhouse\r\n2021.08.26 09:57:00.643517 [ 280 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:600: DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1fe8826d in /workspace/clickhouse\r\n2021.08.26 09:57:01.024183 [ 279 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:485: DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) @ 0x1fe88c59 in /workspace/clickhouse\r\n2021.08.26 09:57:01.139325 [ 281 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:485: DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) @ 0x1fe88c59 in /workspace/clickhouse\r\n2021.08.26 09:57:01.139325 [ 280 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:485: DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) @ 0x1fe88c59 in /workspace/clickhouse\r\n2021.08.26 09:57:01.535382 [ 279 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:788: DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const @ 0x1fe8ab09 in /workspace/clickhouse\r\n2021.08.26 09:57:01.658099 [ 281 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:788: DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const @ 0x1fe8ab09 in /workspace/clickhouse\r\n2021.08.26 09:57:01.658343 [ 280 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:788: DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const @ 0x1fe8ab09 in /workspace/clickhouse\r\n2021.08.26 09:57:02.046291 [ 279 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) @ 0x1fe8aa5d in /workspace/clickhouse\r\n2021.08.26 09:57:02.178446 [ 280 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) @ 0x1fe8aa5d in /workspace/clickhouse\r\n2021.08.26 09:57:02.178446 [ 281 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) @ 0x1fe8aa5d in /workspace/clickhouse\r\n2021.08.26 09:57:02.560529 [ 279 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) @ 0x1fe8aa01 in /workspace/clickhouse\r\n2021.08.26 09:57:02.699724 [ 281 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) @ 0x1fe8aa01 in /workspace/clickhouse\r\n2021.08.26 09:57:02.699724 [ 280 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) @ 0x1fe8aa01 in /workspace/clickhouse\r\n2021.08.26 09:57:03.091977 [ 279 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) @ 0x1fe8a912 in /workspace/clickhouse\r\n2021.08.26 09:57:03.220103 [ 281 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) @ 0x1fe8a912 in /workspace/clickhouse\r\n2021.08.26 09:57:03.220273 [ 280 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) @ 0x1fe8a912 in /workspace/clickhouse\r\n2021.08.26 09:57:03.603319 [ 279 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:182: ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() @ 0x1fe8a7ec in /workspace/clickhouse\r\n2021.08.26 09:57:03.737291 [ 280 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:182: ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() @ 0x1fe8a7ec in /workspace/clickhouse\r\n2021.08.26 09:57:03.737590 [ 281 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:182: ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() @ 0x1fe8a7ec in /workspace/clickhouse\r\n2021.08.26 09:57:04.121547 [ 279 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) @ 0x1fe8a6fd in /workspace/clickhouse\r\n2021.08.26 09:57:04.262089 [ 281 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) @ 0x1fe8a6fd in /workspace/clickhouse\r\n2021.08.26 09:57:04.262131 [ 280 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) @ 0x1fe8a6fd in /workspace/clickhouse\r\n2021.08.26 09:57:04.645083 [ 279 ] {} <Fatal> BaseDaemon: 34. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&...) @ 0x1fe8a6bd in /workspace/clickhouse\r\n2021.08.26 09:57:04.781929 [ 280 ] {} <Fatal> BaseDaemon: 34. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&...) @ 0x1fe8a6bd in /workspace/clickhouse\r\n2021.08.26 09:57:04.781929 [ 281 ] {} <Fatal> BaseDaemon: 34. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&...) @ 0x1fe8a6bd in /workspace/clickhouse\r\n2021.08.26 09:57:05.160855 [ 279 ] {} <Fatal> BaseDaemon: 35. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()>::operator()() @ 0x1fe8a695 in /workspace/clickhouse\r\n2021.08.26 09:57:05.306913 [ 280 ] {} <Fatal> BaseDaemon: 35. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()>::operator()() @ 0x1fe8a695 in /workspace/clickhouse\r\n2021.08.26 09:57:05.307583 [ 281 ] {} <Fatal> BaseDaemon: 35. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()>::operator()() @ 0x1fe8a695 in /workspace/clickhouse\r\n2021.08.26 09:57:05.689113 [ 279 ] {} <Fatal> BaseDaemon: 36. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8a660 in /workspace/clickhouse\r\n2021.08.26 09:57:05.831805 [ 281 ] {} <Fatal> BaseDaemon: 36. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8a660 in /workspace/clickhouse\r\n2021.08.26 09:57:05.831943 [ 280 ] {} <Fatal> BaseDaemon: 36. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x1fe8a660 in /workspace/clickhouse\r\n2021.08.26 09:57:05.943817 [ 279 ] {} <Fatal> BaseDaemon: 37. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:57:06.086150 [ 281 ] {} <Fatal> BaseDaemon: 37. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:57:06.089162 [ 280 ] {} <Fatal> BaseDaemon: 37. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x13803da9 in /workspace/clickhouse\r\n2021.08.26 09:57:06.195447 [ 279 ] {} <Fatal> BaseDaemon: 38. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:06.287234 [ 279 ] {} <Fatal> BaseDaemon: 39. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:270: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x13829eae in /workspace/clickhouse\r\n2021.08.26 09:57:06.337509 [ 281 ] {} <Fatal> BaseDaemon: 38. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:06.342437 [ 280 ] {} <Fatal> BaseDaemon: 38. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x13802ed5 in /workspace/clickhouse\r\n2021.08.26 09:57:06.388792 [ 279 ] {} <Fatal> BaseDaemon: 40. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:136: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x138311c4 in /workspace/clickhouse\r\n2021.08.26 09:57:06.430061 [ 281 ] {} <Fatal> BaseDaemon: 39. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:270: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x13829eae in /workspace/clickhouse\r\n2021.08.26 09:57:06.433665 [ 280 ] {} <Fatal> BaseDaemon: 39. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:270: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x13829eae in /workspace/clickhouse\r\n2021.08.26 09:57:06.488267 [ 279 ] {} <Fatal> BaseDaemon: 41. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x1383111d in /workspace/clickhouse\r\n2021.08.26 09:57:06.533064 [ 281 ] {} <Fatal> BaseDaemon: 40. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:136: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x138311c4 in /workspace/clickhouse\r\n2021.08.26 09:57:06.535713 [ 280 ] {} <Fatal> BaseDaemon: 40. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:136: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x138311c4 in /workspace/clickhouse\r\n2021.08.26 09:57:06.587799 [ 279 ] {} <Fatal> BaseDaemon: 42. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) @ 0x13831045 in /workspace/clickhouse\r\n2021.08.26 09:57:06.632323 [ 281 ] {} <Fatal> BaseDaemon: 41. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x1383111d in /workspace/clickhouse\r\n2021.08.26 09:57:06.633179 [ 280 ] {} <Fatal> BaseDaemon: 41. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x1383111d in /workspace/clickhouse\r\n2021.08.26 09:57:06.685802 [ 279 ] {} <Fatal> BaseDaemon: 43. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0x13830962 in /workspace/clickhouse\r\n2021.08.26 09:57:06.686088 [ 279 ] {} <Fatal> BaseDaemon: 44. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.08.26 09:57:06.728942 [ 280 ] {} <Fatal> BaseDaemon: 42. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) @ 0x13831045 in /workspace/clickhouse\r\n2021.08.26 09:57:06.730220 [ 281 ] {} <Fatal> BaseDaemon: 42. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) @ 0x13831045 in /workspace/clickhouse\r\n2021.08.26 09:57:06.824612 [ 280 ] {} <Fatal> BaseDaemon: 43. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0x13830962 in /workspace/clickhouse\r\n2021.08.26 09:57:06.824880 [ 280 ] {} <Fatal> BaseDaemon: 44. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.08.26 09:57:06.826953 [ 281 ] {} <Fatal> BaseDaemon: 43. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0x13830962 in /workspace/clickhouse\r\n2021.08.26 09:57:06.827278 [ 281 ] {} <Fatal> BaseDaemon: 44. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.08.26 09:57:08.024645 [ 279 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: C8BB9316A66DA2966BBB63CDCE713234. There is no information about the reference checksum.\r\n2021.08.26 09:57:08.145092 [ 280 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: C8BB9316A66DA2966BBB63CDCE713234. There is no information about the reference checksum.\r\n2021.08.26 09:57:08.155665 [ 281 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: C8BB9316A66DA2966BBB63CDCE713234. There is no information about the reference checksum.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28166/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28163","id":979844542,"node_id":"MDU6SXNzdWU5Nzk4NDQ1NDI=","number":28163,"title":"Is there any globalLock on jdbcfunciton?","user":{"login":"lichenglin","id":8294900,"node_id":"MDQ6VXNlcjgyOTQ5MDA=","avatar_url":"https://avatars.githubusercontent.com/u/8294900?v=4","gravatar_id":"","url":"https://api.github.com/users/lichenglin","html_url":"https://github.com/lichenglin","followers_url":"https://api.github.com/users/lichenglin/followers","following_url":"https://api.github.com/users/lichenglin/following{/other_user}","gists_url":"https://api.github.com/users/lichenglin/gists{/gist_id}","starred_url":"https://api.github.com/users/lichenglin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lichenglin/subscriptions","organizations_url":"https://api.github.com/users/lichenglin/orgs","repos_url":"https://api.github.com/users/lichenglin/repos","events_url":"https://api.github.com/users/lichenglin/events{/privacy}","received_events_url":"https://api.github.com/users/lichenglin/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-26T05:15:57Z","updated_at":"2021-09-15T03:07:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> Make sure to check documentation https://clickhouse.yandex/docs/en/ first. If the question is concise and probably has a short answer, asking it in Telegram chat https://telegram.me/clickhouse_en is probably the fastest way to find the answer. For more complicated questions, consider asking them on StackOverflow with \"clickhouse\" tag https://stackoverflow.com/questions/tagged/clickhouse \r\n\r\n> If you still prefer GitHub issues, remove all this text and ask your question here.\r\n\r\nI hava exucte a very long sql such as \"insert into table select * from jdbc(.....)\" where jdbc(....) has about 1 blilion rows.\r\nIt takes a very long time to finish the sql.\r\n\r\nBut I found that when the sql excuting , the other sql that use jdbc fuction and jdbcTable will block until the sql above has been done.\r\n\r\nEvery jdbcfunciton must be excueted one by one? sync??\r\n\r\nI have change the default timeout to a very long time. inlude the config in server.json 、vertx.json、datasource.json.\r\n\r\nThe work.num of bridge use the default value\r\n\r\nIf I missed something??\r\n\r\nThank U for your Reply\r\n\r\nthe version of of jdbc-bridge and ck are both latest.\r\n\r\nand the config of brige seems ok.\r\n\r\n![image](https://user-images.githubusercontent.com/8294900/130904917-30c97cb9-f50e-4177-b9da-9f91876a7215.png)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28163/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28161","id":979804971,"node_id":"MDU6SXNzdWU5Nzk4MDQ5NzE=","number":28161,"title":"Why Exception \"DataPartWriterInMemory supports only one write\" exist in MergeTreeDataPartWriterInMemory::write ?","user":{"login":"mo-avatar","id":59680192,"node_id":"MDQ6VXNlcjU5NjgwMTky","avatar_url":"https://avatars.githubusercontent.com/u/59680192?v=4","gravatar_id":"","url":"https://api.github.com/users/mo-avatar","html_url":"https://github.com/mo-avatar","followers_url":"https://api.github.com/users/mo-avatar/followers","following_url":"https://api.github.com/users/mo-avatar/following{/other_user}","gists_url":"https://api.github.com/users/mo-avatar/gists{/gist_id}","starred_url":"https://api.github.com/users/mo-avatar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mo-avatar/subscriptions","organizations_url":"https://api.github.com/users/mo-avatar/orgs","repos_url":"https://api.github.com/users/mo-avatar/repos","events_url":"https://api.github.com/users/mo-avatar/events{/privacy}","received_events_url":"https://api.github.com/users/mo-avatar/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-26T03:53:05Z","updated_at":"2021-08-26T04:38:19Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When I explore the code base of inserting, I find some thing like this:\r\nvoid MergeTreeDataPartWriterInMemory::write(\r\n    const Block & block, const IColumn::Permutation * permutation)\r\n{\r\n    if (part_in_memory->block)\r\n        throw Exception(\"DataPartWriterInMemory supports only one write\", ErrorCodes::LOGICAL_ERROR);\r\n'''\r\n}\r\nAccording to my personal understanding, in IStorage::write method we create diffierent kind of part based on diffierent size, and  the dataset in a part can only be  a subset of dataset in block which is transformed to IStorage::write method。The block in part_in_memory can only be created in  MergeTreeDataPartWriterInMemory::write or MergeTreeDataPartWriterInMemory::finish. And I didn't see any possibility to re-write the block( or I just missed it). \r\n\r\nSo the question is why the exception should exist?\r\n\r\nAnd if the block may be re-write some how, does it mean we can break the rule \"DataPartWriterInMemory supports only one write\",  and find some way to make the MergeTreeDataPartInMemory to hold diffierent blocks of data to reduce the in memory parts num ?  Or the combine use of  SquashingBlockOutputStream in InterpreterInsertQuery and params to control part type is enough.\r\n\r\nThanks!\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28161/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28159","id":979712663,"node_id":"MDU6SXNzdWU5Nzk3MTI2NjM=","number":28159,"title":"I can not connect via postgres interface","user":{"login":"l-we","id":6325345,"node_id":"MDQ6VXNlcjYzMjUzNDU=","avatar_url":"https://avatars.githubusercontent.com/u/6325345?v=4","gravatar_id":"","url":"https://api.github.com/users/l-we","html_url":"https://github.com/l-we","followers_url":"https://api.github.com/users/l-we/followers","following_url":"https://api.github.com/users/l-we/following{/other_user}","gists_url":"https://api.github.com/users/l-we/gists{/gist_id}","starred_url":"https://api.github.com/users/l-we/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l-we/subscriptions","organizations_url":"https://api.github.com/users/l-we/orgs","repos_url":"https://api.github.com/users/l-we/repos","events_url":"https://api.github.com/users/l-we/events{/privacy}","received_events_url":"https://api.github.com/users/l-we/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-26T00:18:19Z","updated_at":"2022-01-07T06:47:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"ClickHouse server version 21.8.4.51 (official build).\r\npsql (PostgreSQL) 13.4 (Debian 13.4-1.pgdg110+1)\r\nEncryption :password_sha256_hex\r\n\r\n```\r\npsql -h 192.168.10.10 -U default -p 9005 \"sslmode=disable\" -W  \r\nERROR:  Authentication method is not supported\r\n```\r\n\r\n```\r\n{} <Error> PostgreSQLHandler: DB::Exception: Authentication type [ERRFMT] is not supported.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28159/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28158","id":979661747,"node_id":"MDU6SXNzdWU5Nzk2NjE3NDc=","number":28158,"title":"suggest add an embedded version on windows","user":{"login":"l1t1","id":23301704,"node_id":"MDQ6VXNlcjIzMzAxNzA0","avatar_url":"https://avatars.githubusercontent.com/u/23301704?v=4","gravatar_id":"","url":"https://api.github.com/users/l1t1","html_url":"https://github.com/l1t1","followers_url":"https://api.github.com/users/l1t1/followers","following_url":"https://api.github.com/users/l1t1/following{/other_user}","gists_url":"https://api.github.com/users/l1t1/gists{/gist_id}","starred_url":"https://api.github.com/users/l1t1/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/l1t1/subscriptions","organizations_url":"https://api.github.com/users/l1t1/orgs","repos_url":"https://api.github.com/users/l1t1/repos","events_url":"https://api.github.com/users/l1t1/events{/privacy}","received_events_url":"https://api.github.com/users/l1t1/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T22:16:06Z","updated_at":"2021-08-25T22:16:06Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"sth like this:\r\nhttps://pypi.org/project/monetdbe/","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28158/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28144","id":979410658,"node_id":"MDU6SXNzdWU5Nzk0MTA2NTg=","number":28144,"title":"[RFC] HTTP Multipart response","user":{"login":"Algunenano","id":664253,"node_id":"MDQ6VXNlcjY2NDI1Mw==","avatar_url":"https://avatars.githubusercontent.com/u/664253?v=4","gravatar_id":"","url":"https://api.github.com/users/Algunenano","html_url":"https://github.com/Algunenano","followers_url":"https://api.github.com/users/Algunenano/followers","following_url":"https://api.github.com/users/Algunenano/following{/other_user}","gists_url":"https://api.github.com/users/Algunenano/gists{/gist_id}","starred_url":"https://api.github.com/users/Algunenano/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Algunenano/subscriptions","organizations_url":"https://api.github.com/users/Algunenano/orgs","repos_url":"https://api.github.com/users/Algunenano/repos","events_url":"https://api.github.com/users/Algunenano/events{/privacy}","received_events_url":"https://api.github.com/users/Algunenano/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-25T16:32:18Z","updated_at":"2021-08-26T09:52:08Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nCurrently there are 2 things I miss from the HTTP Handler that the TCP handler has  (well, there are more but these are the 2 I'm currently looking at):\r\n\r\n* Mixed data and errors. If the server has started returning data to the client and an exception happens, the client will receive the exception message as if it was part of the data.\r\n* You can't get server logs with the response (`send_logs_level` setting). \r\n\r\n**Describe the solution you'd like**\r\n\r\nBy adding another option (`http_multipart_response`), the response from the HTTP Handler would change to `multipart/form-data;` and the body of the HTTP message would contain at least one form with information from the server. Currently I see 3 possible forms but more could be added in the future if necessary:\r\n* data (Content type varies depending on the format): Contains the response from the query. \r\n* logs (`Content-Type: text/plain; charset=utf-8`): Contains the text server logs in utf-8.\r\n* error (`Content-Type: text/plain; charset=utf-8`): Contains the raised exception message.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nOne thing that I considered to get the logs out was to do so via custom headers, but that has multiple issues. First, headers are limited in size, and second, they can contain only ASCII characters.\r\n\r\n**Additional context**\r\n\r\nI have a super early concept working right now, but needs polishing and testing to confirm it's possible to read the output from different sources and that HTTP standards are followed (currently I'm adding data manually to the buffer).\r\n\r\nAn example using curl:\r\n\r\n```\r\n$ curl -vvv 'http://localhost:8123/?wait_end_of_query=1&http_multipart_response=1&send_logs_level=debug' -d \"Select count(distinct repository) from github_events FORMAT CSV SETTINGS send_logs_level='debug'\"\r\n*   Trying 127.0.0.1:8123...\r\n* Connected to localhost (127.0.0.1) port 8123 (#0)\r\n> POST /?wait_end_of_query=1&http_multipart_response=1&send_logs_level=debug HTTP/1.1\r\n> Host: localhost:8123\r\n> User-Agent: curl/7.78.0\r\n> Accept: */*\r\n> Content-Length: 96\r\n> Content-Type: application/x-www-form-urlencoded\r\n> \r\n* Mark bundle as not supporting multiuse\r\n< HTTP/1.1 200 OK\r\n< Date: Wed, 25 Aug 2021 16:09:08 GMT\r\n< Connection: Keep-Alive\r\n< Content-Type: multipart/form-data;boundary=\"boundary\"\r\n< X-ClickHouse-Server-Display-Name: Mordor.localdomain\r\n< Transfer-Encoding: chunked\r\n< X-ClickHouse-Query-Id: 0e4dc496-052e-4da4-a21a-7fd379e7e845\r\n< X-ClickHouse-Format: CSV\r\n< X-ClickHouse-Timezone: Europe/Madrid\r\n< Keep-Alive: timeout=3\r\n< X-ClickHouse-Summary: {\"read_rows\":\"0\",\"read_bytes\":\"0\",\"written_rows\":\"0\",\"written_bytes\":\"0\",\"total_rows_to_read\":\"0\"}\r\n< \r\n\r\n--boundary\r\nContent-Disposition: form-data; name=\"data\"\r\nContent-Type: text/csv; charset=UTF-8; header=absent\r\n\r\n9384072\r\n\r\n--boundary\r\nContent-Disposition: form-data; name=\"logs\"\r\nContent-Type: text/plain; charset=utf-8\r\n\r\n2021-08-25 18:09:08.810174 [ 386786 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> MemoryTracker: Current memory usage (for query): 1.04 GiB.\r\n2021-08-25 18:09:09.065846 [ 386790 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> MemoryTracker: Current memory usage (for query): 2.02 GiB.\r\n2021-08-25 18:09:09.140416 [ 386811 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7221810 to 1 rows (from 232.73 MiB) in 0.748490634 sec. (9648497.485 rows/sec., 310.93 MiB/sec.)\r\n2021-08-25 18:09:09.142395 [ 386792 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7653864 to 1 rows (from 246.61 MiB) in 0.750463037 sec. (10198855.403 rows/sec., 328.60 MiB/sec.)\r\n2021-08-25 18:09:09.143057 [ 386784 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 6759238 to 1 rows (from 217.80 MiB) in 0.751135051 sec. (8998698.691 rows/sec., 289.96 MiB/sec.)\r\n2021-08-25 18:09:09.143110 [ 386871 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7849801 to 1 rows (from 251.10 MiB) in 0.751214761 sec. (10449476.511 rows/sec., 334.26 MiB/sec.)\r\n2021-08-25 18:09:09.143287 [ 386805 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7607380 to 1 rows (from 244.61 MiB) in 0.751364689 sec. (10124750.486 rows/sec., 325.56 MiB/sec.)\r\n2021-08-25 18:09:09.143486 [ 386767 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 6881026 to 1 rows (from 220.85 MiB) in 0.751560948 sec. (9155646.017 rows/sec., 293.85 MiB/sec.)\r\n2021-08-25 18:09:09.143527 [ 386790 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 5940239 to 1 rows (from 191.69 MiB) in 0.751608367 sec. (7903369.974 rows/sec., 255.04 MiB/sec.)\r\n2021-08-25 18:09:09.144010 [ 386786 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7557442 to 1 rows (from 242.86 MiB) in 0.752094773 sec. (10048523.499 rows/sec., 322.92 MiB/sec.)\r\n2021-08-25 18:09:09.144367 [ 386789 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 6088203 to 1 rows (from 196.29 MiB) in 0.75246376 sec. (8091024.875 rows/sec., 260.86 MiB/sec.)\r\n2021-08-25 18:09:09.144367 [ 386837 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 8221964 to 1 rows (from 264.85 MiB) in 0.75246827 sec. (10926658.741 rows/sec., 351.97 MiB/sec.)\r\n2021-08-25 18:09:09.144564 [ 386778 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 6732747 to 1 rows (from 216.82 MiB) in 0.752636488 sec. (8945549.555 rows/sec., 288.08 MiB/sec.)\r\n2021-08-25 18:09:09.145004 [ 386783 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7890110 to 1 rows (from 254.77 MiB) in 0.753085714 sec. (10477041.130 rows/sec., 338.31 MiB/sec.)\r\n2021-08-25 18:09:09.145762 [ 386870 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7509419 to 1 rows (from 240.96 MiB) in 0.753839428 sec. (9961563.061 rows/sec., 319.64 MiB/sec.)\r\n2021-08-25 18:09:09.145980 [ 386874 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 6277932 to 1 rows (from 202.88 MiB) in 0.754080436 sec. (8325281.628 rows/sec., 269.04 MiB/sec.)\r\n2021-08-25 18:09:09.146309 [ 386852 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 5787716 to 1 rows (from 185.55 MiB) in 0.754391723 sec. (7672030.092 rows/sec., 245.96 MiB/sec.)\r\n2021-08-25 18:09:09.146402 [ 386810 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Debug> AggregatingTransform: Aggregated. 7108829 to 1 rows (from 228.21 MiB) in 0.754489592 sec. (9422037.196 rows/sec., 302.47 MiB/sec.)\r\n2021-08-25 18:09:10.100589 [ 386616 ] {0e4dc496-052e-4da4-a21a-7fd379e7e845} <Information> executeQuery: Read 113087720 rows, 3.55 GiB in 1.710302905 sec., 66121457 rows/sec., 2.08 GiB/sec.\r\n\r\n--boundary--\r\n* Connection #0 to host localhost left intact\r\n```\r\n\r\nOr via Python:\r\n\r\n```\r\nPython 3.9.6 (default, Jun 30 2021, 10:22:16) \r\n[GCC 11.1.0] on linux\r\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\r\n>>> import requests\r\n>>> from requests_toolbelt.multipart import decoder\r\n>>> r = requests.post('http://localhost:8123/?wait_end_of_query=1&http_multipart_response=1&send_logs_level=debug', \"Select count(distinct repository) from github_events FORMAT CSV\")\r\n>>> for part in decoder.MultipartDecoder.from_response(r).parts:\r\n...   print(part.headers)\r\n...   print(part.content.decode('UTF8'))\r\n... \r\n{b'Content-Disposition': b'form-data; name=\"data\"', b'Content-Type': b'text/csv; charset=UTF-8; header=absent'}\r\n9384072\r\n\r\n{b'Content-Disposition': b'form-data; name=\"logs\"', b'Content-Type': b'text/plain; charset=utf-8'}\r\n2021-08-25 18:29:04.437578 [ 386784 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> MemoryTracker: Current memory usage (for query): 1.05 GiB.\r\n2021-08-25 18:29:04.681176 [ 386797 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> MemoryTracker: Current memory usage (for query): 2.02 GiB.\r\n2021-08-25 18:29:04.758312 [ 386815 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7686241 to 1 rows (from 248.14 MiB) in 0.743676495 sec. (10335463.137 rows/sec., 333.67 MiB/sec.)\r\n2021-08-25 18:29:04.758360 [ 386836 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7000714 to 1 rows (from 224.42 MiB) in 0.743739494 sec. (9412857.669 rows/sec., 301.75 MiB/sec.)\r\n2021-08-25 18:29:04.758400 [ 386829 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7669423 to 1 rows (from 246.33 MiB) in 0.743746894 sec. (10311872.307 rows/sec., 331.21 MiB/sec.)\r\n2021-08-25 18:29:04.760080 [ 386861 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7581081 to 1 rows (from 243.39 MiB) in 0.74544971 sec. (10169808.772 rows/sec., 326.50 MiB/sec.)\r\n2021-08-25 18:29:04.760145 [ 386780 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7209589 to 1 rows (from 231.88 MiB) in 0.745504029 sec. (9670757.930 rows/sec., 311.04 MiB/sec.)\r\n2021-08-25 18:29:04.760187 [ 386843 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6699081 to 1 rows (from 215.66 MiB) in 0.745550279 sec. (8985418.138 rows/sec., 289.26 MiB/sec.)\r\n2021-08-25 18:29:04.760645 [ 386797 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 5936686 to 1 rows (from 191.51 MiB) in 0.746033435 sec. (7957667.474 rows/sec., 256.70 MiB/sec.)\r\n2021-08-25 18:29:04.760694 [ 386880 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 7672048 to 1 rows (from 246.90 MiB) in 0.746060954 sec. (10283406.415 rows/sec., 330.94 MiB/sec.)\r\n2021-08-25 18:29:04.760775 [ 386763 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 8825101 to 1 rows (from 283.36 MiB) in 0.746141374 sec. (11827652.651 rows/sec., 379.76 MiB/sec.)\r\n2021-08-25 18:29:04.760806 [ 386822 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6258868 to 1 rows (from 203.24 MiB) in 0.746183763 sec. (8387837.300 rows/sec., 272.38 MiB/sec.)\r\n2021-08-25 18:29:04.761250 [ 386784 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6690862 to 1 rows (from 214.69 MiB) in 0.74660657 sec. (8961697.189 rows/sec., 287.55 MiB/sec.)\r\n2021-08-25 18:29:04.761310 [ 386801 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6634668 to 1 rows (from 212.40 MiB) in 0.746693999 sec. (8885390.815 rows/sec., 284.46 MiB/sec.)\r\n2021-08-25 18:29:04.761355 [ 386830 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 8074255 to 1 rows (from 261.55 MiB) in 0.746719329 sec. (10812971.737 rows/sec., 350.27 MiB/sec.)\r\n2021-08-25 18:29:04.761414 [ 386818 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6838226 to 1 rows (from 219.89 MiB) in 0.746786598 sec. (9156867.596 rows/sec., 294.45 MiB/sec.)\r\n2021-08-25 18:29:04.761639 [ 386781 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 6609245 to 1 rows (from 210.81 MiB) in 0.747021646 sec. (8847461.162 rows/sec., 282.21 MiB/sec.)\r\n2021-08-25 18:29:04.763002 [ 386817 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Debug> AggregatingTransform: Aggregated. 5701632 to 1 rows (from 184.39 MiB) in 0.748377335 sec. (7618659.376 rows/sec., 246.39 MiB/sec.)\r\n2021-08-25 18:29:05.715820 [ 386615 ] {d7d91103-f79e-4aa4-ad6d-61da4507b2df} <Information> executeQuery: Read 113087720 rows, 3.55 GiB in 1.702811592 sec., 66412350 rows/sec., 2.09 GiB/sec.\r\n\r\n>>>\r\n```\r\n\r\n\r\nWhat do you think? Is this worth exploring? Are there any other possibilities to return multiple data chunks in HTTP 1 that I'm missing and would be better suited for this?\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28144/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28131","id":979011504,"node_id":"MDU6SXNzdWU5NzkwMTE1MDQ=","number":28131,"title":"Decrypt function has strange NULL-related behaviour ","user":{"login":"risentveber","id":6682918,"node_id":"MDQ6VXNlcjY2ODI5MTg=","avatar_url":"https://avatars.githubusercontent.com/u/6682918?v=4","gravatar_id":"","url":"https://api.github.com/users/risentveber","html_url":"https://github.com/risentveber","followers_url":"https://api.github.com/users/risentveber/followers","following_url":"https://api.github.com/users/risentveber/following{/other_user}","gists_url":"https://api.github.com/users/risentveber/gists{/gist_id}","starred_url":"https://api.github.com/users/risentveber/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/risentveber/subscriptions","organizations_url":"https://api.github.com/users/risentveber/orgs","repos_url":"https://api.github.com/users/risentveber/repos","events_url":"https://api.github.com/users/risentveber/events{/privacy}","received_events_url":"https://api.github.com/users/risentveber/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T11:04:40Z","updated_at":"2021-08-25T11:06:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Description of the unexpected behaviour**\r\n`decrypt` works in strange manner with `NULL` maybe due to query optimisation at parsing phase.\r\n\r\n**How to reproduce**\r\nThis way it works\r\n```\r\ndecrypt(\r\n'aes-256-gcm', base64Decode(null), \r\nunhex('key in hex 64 characters long goes here'), base64Decode(null)\r\n) as attr FROM table_name WHERE attr_iv is NULL AND attr_encrypted is NULL LIMIT 1;\r\n┌─error_details─┐\r\n│ ᴺᵁᴸᴸ          │\r\n└───────────────┘\r\n```\r\nThis way it doesn't\r\n```\r\ndecrypt(\r\n'aes-256-gcm', base64Decode(attr_encrypted),\r\nunhex('key in hex 64 characters long goes here'), base64Decode(attr_iv)\r\n) as attr FROM table_name WHERE attr_iv is NULL AND attr_encrypted is NULL LIMIT 1;\r\n\r\nCode: 36. DB::Exception: Received from ****. DB::Exception: Encrypted data is smaller than the size\r\n of additional data for AEAD mode, cannot decrypt.: while executing 'FUNCTION decrypt('aes-256-gcm' :: 2, \r\nbase64Decode(attr_encrypted) :: 0, unhex('key in hex 64 characters long goes here') :: 3, \r\nbase64Decode(attr_iv) :: 4) -> decrypt('aes-256-gcm', base64Decode(attr_encrypted), \r\nunhex('key in hex 64 characters long goes here'), base64Decode(attr_iv)) Nullable(String) : 1'.\r\n```\r\n* Clickhouse version - **21.3.15**\r\n\r\n**Expected behavior**\r\nBoth cases should return the same result - `NULL`.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28131/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28130","id":979005722,"node_id":"MDExOlB1bGxSZXF1ZXN0NzE5NDg5MTM3","number":28130,"title":"add zorder curve base","user":{"login":"bakey","id":1566848,"node_id":"MDQ6VXNlcjE1NjY4NDg=","avatar_url":"https://avatars.githubusercontent.com/u/1566848?v=4","gravatar_id":"","url":"https://api.github.com/users/bakey","html_url":"https://github.com/bakey","followers_url":"https://api.github.com/users/bakey/followers","following_url":"https://api.github.com/users/bakey/following{/other_user}","gists_url":"https://api.github.com/users/bakey/gists{/gist_id}","starred_url":"https://api.github.com/users/bakey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bakey/subscriptions","organizations_url":"https://api.github.com/users/bakey/orgs","repos_url":"https://api.github.com/users/bakey/repos","events_url":"https://api.github.com/users/bakey/events{/privacy}","received_events_url":"https://api.github.com/users/bakey/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"assignees":[{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":20,"created_at":"2021-08-25T10:59:18Z","updated_at":"2021-12-09T14:59:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/28130","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28130","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/28130.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/28130.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n    Continued work of PR #6286 \r\n    Try to finish work of the zorder curve function  supporting\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28130/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28129","id":978956838,"node_id":"MDU6SXNzdWU5Nzg5NTY4Mzg=","number":28129,"title":"Clickhouse -all the replica turns into read-only mode","user":{"login":"chandrabhushansingh16","id":57935627,"node_id":"MDQ6VXNlcjU3OTM1NjI3","avatar_url":"https://avatars.githubusercontent.com/u/57935627?v=4","gravatar_id":"","url":"https://api.github.com/users/chandrabhushansingh16","html_url":"https://github.com/chandrabhushansingh16","followers_url":"https://api.github.com/users/chandrabhushansingh16/followers","following_url":"https://api.github.com/users/chandrabhushansingh16/following{/other_user}","gists_url":"https://api.github.com/users/chandrabhushansingh16/gists{/gist_id}","starred_url":"https://api.github.com/users/chandrabhushansingh16/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chandrabhushansingh16/subscriptions","organizations_url":"https://api.github.com/users/chandrabhushansingh16/orgs","repos_url":"https://api.github.com/users/chandrabhushansingh16/repos","events_url":"https://api.github.com/users/chandrabhushansingh16/events{/privacy}","received_events_url":"https://api.github.com/users/chandrabhushansingh16/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T10:03:57Z","updated_at":"2021-08-25T10:03:57Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi all\r\nFollowing is my Clickhouse details\r\nclickhouse-server: 20.12.4 revision 54442 \r\nNumber of replicas: 2\r\nNumber of shards : 2\r\n\r\nI am facing a problem in which Clickhouse frequently turns into read-only mode. It happens for both the instances of Clickhouse\r\n\r\nFollowing workaround does not work:\r\n 1. sudo -u clickhouse touch /var/lib/clickhouse/flags/force_restore_data\r\n 2. Restart the server\r\n\r\nsystem.replicas table shows \"is_readonly\" flag true.\r\n\r\nHow I can remove the is_readonly = 1 to 0 , so that insertion in table can work as usual.\r\nOr if there is any way to make the tables writable.\r\n\r\nThanks\r\n\r\ncc @den-crane\r\n\r\n_Originally posted by @chandrabhushansingh16 in https://github.com/ClickHouse/ClickHouse/issues/20014#issuecomment-905360588_","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28129/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28127","id":978868856,"node_id":"MDU6SXNzdWU5Nzg4Njg4NTY=","number":28127,"title":"load data to mergetree table, how  remove duplicate data when fail and retry","user":{"login":"jiangjiguang","id":12368495,"node_id":"MDQ6VXNlcjEyMzY4NDk1","avatar_url":"https://avatars.githubusercontent.com/u/12368495?v=4","gravatar_id":"","url":"https://api.github.com/users/jiangjiguang","html_url":"https://github.com/jiangjiguang","followers_url":"https://api.github.com/users/jiangjiguang/followers","following_url":"https://api.github.com/users/jiangjiguang/following{/other_user}","gists_url":"https://api.github.com/users/jiangjiguang/gists{/gist_id}","starred_url":"https://api.github.com/users/jiangjiguang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jiangjiguang/subscriptions","organizations_url":"https://api.github.com/users/jiangjiguang/orgs","repos_url":"https://api.github.com/users/jiangjiguang/repos","events_url":"https://api.github.com/users/jiangjiguang/events{/privacy}","received_events_url":"https://api.github.com/users/jiangjiguang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T08:31:08Z","updated_at":"2021-08-25T08:31:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Load data from HDFS parquet files to mergetree table, my process is below:\r\n1、CREATE TABLE table_src AS table_standard ENGINE = HDFS('hdfs:xxxxxx.parquet' , Parquet)\r\n2、INSERT INTO table_temp SELECT * FROM table_src\r\n3、ALTER TABLE table_temp MOVE PARTITION 'xxxxxx' TO TABLE table_prd\r\n\r\nbut, it may fail when loading data from a parquet file, now i will add retry.\r\nAs far as i know, the loading data has not transaction mechanism, and part of data will be written to table,  how do I to remove duplicate data ? \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28127/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28126","id":978797187,"node_id":"MDU6SXNzdWU5Nzg3OTcxODc=","number":28126,"title":"Data type problem","user":{"login":"zhanggaohe","id":61952723,"node_id":"MDQ6VXNlcjYxOTUyNzIz","avatar_url":"https://avatars.githubusercontent.com/u/61952723?v=4","gravatar_id":"","url":"https://api.github.com/users/zhanggaohe","html_url":"https://github.com/zhanggaohe","followers_url":"https://api.github.com/users/zhanggaohe/followers","following_url":"https://api.github.com/users/zhanggaohe/following{/other_user}","gists_url":"https://api.github.com/users/zhanggaohe/gists{/gist_id}","starred_url":"https://api.github.com/users/zhanggaohe/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhanggaohe/subscriptions","organizations_url":"https://api.github.com/users/zhanggaohe/orgs","repos_url":"https://api.github.com/users/zhanggaohe/repos","events_url":"https://api.github.com/users/zhanggaohe/events{/privacy}","received_events_url":"https://api.github.com/users/zhanggaohe/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T07:09:42Z","updated_at":"2021-08-25T07:11:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":null,"reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28126/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28121","id":978741119,"node_id":"MDU6SXNzdWU5Nzg3NDExMTk=","number":28121,"title":"Full name column appears in left join of multiple tables","user":{"login":"peterZhiFate","id":74392381,"node_id":"MDQ6VXNlcjc0MzkyMzgx","avatar_url":"https://avatars.githubusercontent.com/u/74392381?v=4","gravatar_id":"","url":"https://api.github.com/users/peterZhiFate","html_url":"https://github.com/peterZhiFate","followers_url":"https://api.github.com/users/peterZhiFate/followers","following_url":"https://api.github.com/users/peterZhiFate/following{/other_user}","gists_url":"https://api.github.com/users/peterZhiFate/gists{/gist_id}","starred_url":"https://api.github.com/users/peterZhiFate/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/peterZhiFate/subscriptions","organizations_url":"https://api.github.com/users/peterZhiFate/orgs","repos_url":"https://api.github.com/users/peterZhiFate/repos","events_url":"https://api.github.com/users/peterZhiFate/events{/privacy}","received_events_url":"https://api.github.com/users/peterZhiFate/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-25T05:43:35Z","updated_at":"2021-08-25T05:43:35Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When I execute the following sql statement, when I left join two tables, it column names are correct, as shown in picture 1.\r\n![picture1](https://user-images.githubusercontent.com/74392381/130733049-68b1555c-e124-4944-a3e2-b4cc0166ae23.png)\r\n\r\nBut when I left join the three tables, I can see that the column names are incorrect. As shown in picture two.\r\n\r\n![picture2](https://user-images.githubusercontent.com/74392381/130733060-10b4bbad-e306-4248-8573-4455e2f3d249.png)\r\n\r\nIs there any solution?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28121/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28108","id":978615443,"node_id":"MDU6SXNzdWU5Nzg2MTU0NDM=","number":28108,"title":"geo centroid function how to use in clickhouse","user":{"login":"linger000","id":31945909,"node_id":"MDQ6VXNlcjMxOTQ1OTA5","avatar_url":"https://avatars.githubusercontent.com/u/31945909?v=4","gravatar_id":"","url":"https://api.github.com/users/linger000","html_url":"https://github.com/linger000","followers_url":"https://api.github.com/users/linger000/followers","following_url":"https://api.github.com/users/linger000/following{/other_user}","gists_url":"https://api.github.com/users/linger000/gists{/gist_id}","starred_url":"https://api.github.com/users/linger000/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/linger000/subscriptions","organizations_url":"https://api.github.com/users/linger000/orgs","repos_url":"https://api.github.com/users/linger000/repos","events_url":"https://api.github.com/users/linger000/events{/privacy}","received_events_url":"https://api.github.com/users/linger000/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false},"assignees":[{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-08-25T01:29:24Z","updated_at":"2021-08-25T22:06:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> Make sure to check documentation https://clickhouse.yandex/docs/en/ first. If the question is concise and probably has a short answer, asking it in Telegram chat https://telegram.me/clickhouse_en is probably the fastest way to find the answer. For more complicated questions, consider asking them on StackOverflow with \"clickhouse\" tag https://stackoverflow.com/questions/tagged/clickhouse \r\n\r\n> If you still prefer GitHub issues, remove all this text and ask your question here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28108/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28107","id":978569184,"node_id":"MDU6SXNzdWU5Nzg1NjkxODQ=","number":28107,"title":"Add query fuzzer on server side.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-24T23:50:14Z","updated_at":"2021-08-24T23:50:14Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nEnable AST based query fuzzing for every kind of tests.\r\n\r\n**Describe the solution you'd like**\r\n\r\nSimilarly to `clickhouse-client`, the server should accumulate the set of AST nodes and use it for query fuzzing.\r\nFor every received query, the server should also execute configured amount of fuzzed queries.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28107/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28087","id":978193966,"node_id":"MDU6SXNzdWU5NzgxOTM5NjY=","number":28087,"title":"Index analysis for monotonic functions not working with toDate(datetime64)","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"},{"id":1481221764,"node_id":"MDU6TGFiZWwxNDgxMjIxNzY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-datetime","name":"comp-datetime","color":"b5bcff","default":false,"description":"date & time & timezone related"}],"state":"open","locked":false,"assignee":{"login":"Enmk","id":1110183,"node_id":"MDQ6VXNlcjExMTAxODM=","avatar_url":"https://avatars.githubusercontent.com/u/1110183?v=4","gravatar_id":"","url":"https://api.github.com/users/Enmk","html_url":"https://github.com/Enmk","followers_url":"https://api.github.com/users/Enmk/followers","following_url":"https://api.github.com/users/Enmk/following{/other_user}","gists_url":"https://api.github.com/users/Enmk/gists{/gist_id}","starred_url":"https://api.github.com/users/Enmk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Enmk/subscriptions","organizations_url":"https://api.github.com/users/Enmk/orgs","repos_url":"https://api.github.com/users/Enmk/repos","events_url":"https://api.github.com/users/Enmk/events{/privacy}","received_events_url":"https://api.github.com/users/Enmk/received_events","type":"User","site_admin":false},"assignees":[{"login":"Enmk","id":1110183,"node_id":"MDQ6VXNlcjExMTAxODM=","avatar_url":"https://avatars.githubusercontent.com/u/1110183?v=4","gravatar_id":"","url":"https://api.github.com/users/Enmk","html_url":"https://github.com/Enmk","followers_url":"https://api.github.com/users/Enmk/followers","following_url":"https://api.github.com/users/Enmk/following{/other_user}","gists_url":"https://api.github.com/users/Enmk/gists{/gist_id}","starred_url":"https://api.github.com/users/Enmk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Enmk/subscriptions","organizations_url":"https://api.github.com/users/Enmk/orgs","repos_url":"https://api.github.com/users/Enmk/repos","events_url":"https://api.github.com/users/Enmk/events{/privacy}","received_events_url":"https://api.github.com/users/Enmk/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-08-24T14:58:50Z","updated_at":"2021-08-25T15:27:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Looks like clickhouse doesn't recognize that toDate of datetime64 column is monotonic function.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes, 21.10 \r\n\r\n**How to reproduce**\r\n\r\n```\r\nCREATE TABLE xxxx (key UInt32, ts DateTime64(3)) ENGINE=MergeTree ORDER BY (toDate(ts));\r\nCREATE TABLE xxxx_2 (key UInt32, ts DateTime64(3)) ENGINE=MergeTree ORDER BY (toDate(ts),ts);\r\nCREATE TABLE xxxx_3 (key UInt32, ts DateTime64(3)) ENGINE=MergeTree ORDER BY (toDate(toDateTime(ts)),ts);\r\nINSERT INTO xxxx SELECT 1, now64() + number/1000 FROM numbers(100000000);\r\nINSERT INTO xxxx_2 SELECT 1, now64() + number/1000 FROM numbers(100000000);\r\nINSERT INTO xxxx_3 SELECT 1, now64() + number/1000 FROM numbers(100000000);\r\n\r\nSELECT count()\r\nFROM xxxx\r\nWHERE (ts >= '2021-08-24 18:10:00') AND (ts < '2021-08-24 18:40:00')\r\n\r\nQuery id: 8bfb020b-a0c0-4d11-8142-599ead96de4a\r\n\r\n┌─count()─┐\r\n│ 1800000 │\r\n└─────────┘\r\n\r\n1 rows in set. Elapsed: 0.021 sec. Processed 22.98 million rows, 183.83 MB (1.07 billion rows/s., 8.60 GB/s.)\r\n\r\nSELECT count()\r\nFROM xxxx_2\r\nWHERE (ts >= '2021-08-24 18:10:00') AND (ts < '2021-08-24 18:40:00')\r\n\r\nQuery id: 070c035a-44d1-41d2-b6f6-2455167c9ce5\r\n\r\n┌─count()─┐\r\n│ 1800000 │\r\n└─────────┘\r\n\r\n1 rows in set. Elapsed: 0.008 sec. Processed 1.82 million rows, 14.55 MB (228.99 million rows/s., 1.83 GB/s.)\r\n \r\n SELECT count()\r\nFROM xxxx_3\r\nWHERE (ts >= '2021-08-24 18:10:00') AND (ts < '2021-08-24 18:40:00')\r\n\r\nQuery id: 5e692987-fe67-4ea6-b0ca-314e28636331\r\n\r\n┌─count()─┐\r\n│ 1800000 │\r\n└─────────┘\r\n\r\n1 rows in set. Elapsed: 0.008 sec. Processed 1.82 million rows, 14.55 MB (239.13 million rows/s., 1.91 GB/s.)\r\n\r\n ```\r\n\r\n**Expected behavior**\r\n\r\nQueries would read the same amount of rows.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n> If applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\n\r\nRelated https://github.com/ClickHouse/ClickHouse/pull/10026\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28087/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28065","id":977809505,"node_id":"MDExOlB1bGxSZXF1ZXN0NzE4NDk5MjQy","number":28065,"title":"ALTER PRIMARY KEY","user":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-24T07:49:24Z","updated_at":"2021-11-17T23:32:44Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/28065","html_url":"https://github.com/ClickHouse/ClickHouse/pull/28065","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/28065.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/28065.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nAllow changing primary key of existing tables.  https://github.com/ClickHouse/ClickHouse/issues/147  https://github.com/ClickHouse/ClickHouse/issues/2210   https://github.com/ClickHouse/ClickHouse/issues/7052\r\n\r\nDetailed description / Documentation draft:\r\n.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065/reactions","total_count":7,"+1":7,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28065/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28062","id":977766899,"node_id":"MDU6SXNzdWU5Nzc3NjY4OTk=","number":28062,"title":"Move DB files to another server","user":{"login":"nikgul1995","id":11719834,"node_id":"MDQ6VXNlcjExNzE5ODM0","avatar_url":"https://avatars.githubusercontent.com/u/11719834?v=4","gravatar_id":"","url":"https://api.github.com/users/nikgul1995","html_url":"https://github.com/nikgul1995","followers_url":"https://api.github.com/users/nikgul1995/followers","following_url":"https://api.github.com/users/nikgul1995/following{/other_user}","gists_url":"https://api.github.com/users/nikgul1995/gists{/gist_id}","starred_url":"https://api.github.com/users/nikgul1995/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikgul1995/subscriptions","organizations_url":"https://api.github.com/users/nikgul1995/orgs","repos_url":"https://api.github.com/users/nikgul1995/repos","events_url":"https://api.github.com/users/nikgul1995/events{/privacy}","received_events_url":"https://api.github.com/users/nikgul1995/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-24T06:51:31Z","updated_at":"2021-08-24T21:35:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello.\r\nI have DB installed on Ubuntu server.\r\nI want to move DB files on another server (file server with samba).\r\nWill it be work:\r\n1. I create a folder on another server\r\n2. I move folder /var/lib/clickhouse from current server to file server\r\n3. On current server I mount file-server`s folder to /var/lib/clickhouse\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28062/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28034","id":977179499,"node_id":"MDU6SXNzdWU5NzcxNzk0OTk=","number":28034,"title":"Inserting to DateTime64 with best_effort and specific values returns an error (edge case)","user":{"login":"gyfis","id":1632140,"node_id":"MDQ6VXNlcjE2MzIxNDA=","avatar_url":"https://avatars.githubusercontent.com/u/1632140?v=4","gravatar_id":"","url":"https://api.github.com/users/gyfis","html_url":"https://github.com/gyfis","followers_url":"https://api.github.com/users/gyfis/followers","following_url":"https://api.github.com/users/gyfis/following{/other_user}","gists_url":"https://api.github.com/users/gyfis/gists{/gist_id}","starred_url":"https://api.github.com/users/gyfis/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gyfis/subscriptions","organizations_url":"https://api.github.com/users/gyfis/orgs","repos_url":"https://api.github.com/users/gyfis/repos","events_url":"https://api.github.com/users/gyfis/events{/privacy}","received_events_url":"https://api.github.com/users/gyfis/received_events","type":"User","site_admin":false},"labels":[{"id":1481221764,"node_id":"MDU6TGFiZWwxNDgxMjIxNzY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-datetime","name":"comp-datetime","color":"b5bcff","default":false,"description":"date & time & timezone related"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"Enmk","id":1110183,"node_id":"MDQ6VXNlcjExMTAxODM=","avatar_url":"https://avatars.githubusercontent.com/u/1110183?v=4","gravatar_id":"","url":"https://api.github.com/users/Enmk","html_url":"https://github.com/Enmk","followers_url":"https://api.github.com/users/Enmk/followers","following_url":"https://api.github.com/users/Enmk/following{/other_user}","gists_url":"https://api.github.com/users/Enmk/gists{/gist_id}","starred_url":"https://api.github.com/users/Enmk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Enmk/subscriptions","organizations_url":"https://api.github.com/users/Enmk/orgs","repos_url":"https://api.github.com/users/Enmk/repos","events_url":"https://api.github.com/users/Enmk/events{/privacy}","received_events_url":"https://api.github.com/users/Enmk/received_events","type":"User","site_admin":false},"assignees":[{"login":"Enmk","id":1110183,"node_id":"MDQ6VXNlcjExMTAxODM=","avatar_url":"https://avatars.githubusercontent.com/u/1110183?v=4","gravatar_id":"","url":"https://api.github.com/users/Enmk","html_url":"https://github.com/Enmk","followers_url":"https://api.github.com/users/Enmk/followers","following_url":"https://api.github.com/users/Enmk/following{/other_user}","gists_url":"https://api.github.com/users/Enmk/gists{/gist_id}","starred_url":"https://api.github.com/users/Enmk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Enmk/subscriptions","organizations_url":"https://api.github.com/users/Enmk/orgs","repos_url":"https://api.github.com/users/Enmk/repos","events_url":"https://api.github.com/users/Enmk/events{/privacy}","received_events_url":"https://api.github.com/users/Enmk/received_events","type":"User","site_admin":false}],"milestone":null,"comments":12,"created_at":"2021-08-23T15:52:47Z","updated_at":"2022-01-27T08:43:10Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"### Describe the bug\r\n\r\nInserting to a DateTime64 column via `INSERT INTO table (...) VALUES (...)` with `date_time_input_format=best_effort` fails when a specific order of the `VALUES` is input. \r\n\r\nThe failing order of VALUES: a string with a millisecond unix timestamp followed by a string with a best-effort-style date tim..\r\n\r\n### Does it reproduce on recent release?\r\n\r\nYes (tested on 21.8.4).\r\n\r\n### How to reproduce\r\n\r\n* Which ClickHouse server version to use: 21.8.4\r\n* Which interface to use, if matters: I don't think it does, but I used clickhouse-client\r\n* Non-default settings, if any: `date_time_input_format=best_effort` input into clickhouse client:\r\n\r\n```\r\ndocker run -it --rm --link some-clickhouse-server:clickhouse-server yandex/clickhouse-client --host clickhouse-server --date_time_input_format=best_effort\r\n```\r\n\r\n* `CREATE TABLE` statements for all tables involved:\r\n\r\n```sql\r\nCREATE TABLE test_table (datetime DateTime64(3, 'UTC') DEFAULT NOW('UTC')) ENGINE MergeTree ORDER BY datetime\r\n```\r\n\r\n### Queries to run that lead to **expected** result\r\n\r\n```sql\r\nINSERT INTO test_table (datetime) VALUES (NULL)\r\nINSERT INTO test_table (datetime) VALUES ('2021-08-01T12:23:34.567Z')\r\nINSERT INTO test_table (datetime) VALUES (1629733517)\r\nINSERT INTO test_table (datetime) VALUES ('1629733517')\r\nINSERT INTO test_table (datetime) VALUES (1629733517000)\r\nINSERT INTO test_table (datetime) VALUES ('1629733517000')\r\nINSERT INTO test_table (datetime) VALUES (NULL), (1629733517), ('2021-08-01T12:23:34.567Z'), ('1629733517'), ('1629733517000')\r\nINSERT INTO test_table (datetime) VALUES ('1629733517'), ('2021-08-01T12:23:34.567Z')\r\nINSERT INTO test_table (datetime) VALUES (1629733517000), ('2021-08-01T12:23:34.567Z')\r\n```\r\n\r\n### Queries to run that lead to **unexpected** result\r\n\r\nWhen you want to insert a **millisecond unix timestamp as string** before a string timestamp, the query fails\r\n\r\n```sql\r\nINSERT INTO test_table (datetime) VALUES ('1629733517000'), ('2021-08-01T12:23:34.567Z')\r\n```\r\n\r\n### Expected behavior\r\n\r\nI expected the rows to insert in any of the previous cases.\r\n\r\n### Error message and/or stacktrace\r\n\r\n> Exception on client:\r\n> Code: 6. DB::Exception: Cannot parse string '2021-08-01T12:23:34.567Z' as DateTime64(3, 'UTC'): syntax error at position 23 (parsed just '2021-08-01T12:23:34.567'): while executing 'FUNCTION CAST(assumeNotNull(_dummy_0) :: 2, 'DateTime64(3, \\'UTC\\')' :: 1) -> CAST(assumeNotNull(_dummy_0), 'DateTime64(3, \\'UTC\\')') DateTime64(3, 'UTC') : 4': data for INSERT was parsed from query\r\n\r\n### Additional info\r\n\r\nThis could be related to some DateTime64 handling changed e.g. here https://github.com/ClickHouse/ClickHouse/pull/21053 though I didn't dig into it more. \r\n\r\nLet me know if I can provide more info! Thanks","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28034/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28019","id":976866055,"node_id":"MDU6SXNzdWU5NzY4NjYwNTU=","number":28019,"title":"Multi-table JOIN  Code: 47, e.displayText() = DB::Exception: Missing columns","user":{"login":"guoqlin","id":49741140,"node_id":"MDQ6VXNlcjQ5NzQxMTQw","avatar_url":"https://avatars.githubusercontent.com/u/49741140?v=4","gravatar_id":"","url":"https://api.github.com/users/guoqlin","html_url":"https://github.com/guoqlin","followers_url":"https://api.github.com/users/guoqlin/followers","following_url":"https://api.github.com/users/guoqlin/following{/other_user}","gists_url":"https://api.github.com/users/guoqlin/gists{/gist_id}","starred_url":"https://api.github.com/users/guoqlin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/guoqlin/subscriptions","organizations_url":"https://api.github.com/users/guoqlin/orgs","repos_url":"https://api.github.com/users/guoqlin/repos","events_url":"https://api.github.com/users/guoqlin/events{/privacy}","received_events_url":"https://api.github.com/users/guoqlin/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-23T10:13:25Z","updated_at":"2021-08-23T10:18:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> Make sure to check documentation https://clickhouse.yandex/docs/en/ first. If the question is concise and probably has a short answer, asking it in Telegram chat https://telegram.me/clickhouse_en is probably the fastest way to find the answer. For more complicated questions, consider asking them on StackOverflow with \"clickhouse\" tag https://stackoverflow.com/questions/tagged/clickhouse \r\n\r\n> If you still prefer GitHub issues, remove all this text and ask your question here.\r\nSELECT\r\n\t`m1nckbdef438o9em38zepg`.`B商品名称` AS `B商品名称`,\r\n\t`m1nckbdef438o9em38zepg`.`同时购买A和B订单数` AS `同时购买A和B订单数2`\r\nFROM\r\n\t`smartbimpp`.`3qcnfud0fy4lbircffaduw` `3qcnfud0fy4lbircffaduw` \r\nRIGHT JOIN `smartbimpp`.`m1nckbdef438o9em38zepg` `m1nckbdef438o9em38zepg` ON\r\n\t`3qcnfud0fy4lbircffaduw`.`A商品名称` = `m1nckbdef438o9em38zepg`.`A商品名称`\r\nLEFT JOIN `smartbimpp`.`avhnzf9rr2vuyesdvryc6w` `avhnzf9rr2vuyesdvryc6w` ON\r\n\t`m1nckbdef438o9em38zepg`.`B商品名称` = `avhnzf9rr2vuyesdvryc6w`.`B商品名称`\r\n\r\n\r\n ClickHouse exception, code: 47, host: 10.10.111.129, port: 8123; Code: 47, e.displayText() = DB::Exception: Missing columns: '同时购买A和B订单数' while processing query: 'SELECT `A商品名称` AS `--3qcnfud0fy4lbircffaduw.A商品名称`, `同时购买A和B订单数`, m1nckbdef438o9em38zepg.`A商品名称` AS `--m1nckbdef438o9em38zepg.A商品名称`, `B商品名称` AS `--m1nckbdef438o9em38zepg.B商品名称` FROM smartbimpp.`3qcnfud0fy4lbircffaduw` AS `3qcnfud0fy4lbircffaduw` ALL RIGHT JOIN \r\n smartbimpp.m1nckbdef438o9em38zepg AS m1nckbdef438o9em38zepg ON `--3qcnfud0fy4lbircffaduw.A商品名称` = `--m1nckbdef438o9em38zepg.A商品名称`', required columns: 'A商品名称' '同时购买A和B订单数' 'm1nckbdef438o9em38zepg.A商品名称' 'B商品名称', maybe you meant:  '['A商品名称']' '['A商品名称']', joined columns: 'm1nckbdef438o9em38zepg.A商品名称' 'B商品名称' 'm1nckbdef438o9em38zepg.同时购买A和B订单数' 'm1nckbdef438o9em38zepg.总订单数' (version 21.2.5.5 (official build))","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28019/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/28013","id":976757394,"node_id":"MDU6SXNzdWU5NzY3NTczOTQ=","number":28013,"title":"Timeout exception will be thrown after a few seconds（5~8S），but the default timeout param is 300 seconds","user":{"login":"oceanliuyang8","id":89071950,"node_id":"MDQ6VXNlcjg5MDcxOTUw","avatar_url":"https://avatars.githubusercontent.com/u/89071950?v=4","gravatar_id":"","url":"https://api.github.com/users/oceanliuyang8","html_url":"https://github.com/oceanliuyang8","followers_url":"https://api.github.com/users/oceanliuyang8/followers","following_url":"https://api.github.com/users/oceanliuyang8/following{/other_user}","gists_url":"https://api.github.com/users/oceanliuyang8/gists{/gist_id}","starred_url":"https://api.github.com/users/oceanliuyang8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/oceanliuyang8/subscriptions","organizations_url":"https://api.github.com/users/oceanliuyang8/orgs","repos_url":"https://api.github.com/users/oceanliuyang8/repos","events_url":"https://api.github.com/users/oceanliuyang8/events{/privacy}","received_events_url":"https://api.github.com/users/oceanliuyang8/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-08-23T08:14:21Z","updated_at":"2021-08-26T02:56:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When execute a distributed join query, **Timeout exception will be thrown after a few seconds when execute sub query（5~8S）**，but the default receive/send timeout is 300 seconds，The cluster has **100 nodes** and the sub-table data is about **30 million rows**。\r\n\r\nBut it will be successfully on the following situations\r\n1、a 10+ nodes cluster \r\n2、add a query conditon on sub-table（about Hundreds of thousands left）\r\n3、left join changed to global join， but will spend hundreds of seconds。\r\n\r\n**Why Timeout exception  after a few seconds when query started？**\r\n\r\n![image](https://user-images.githubusercontent.com/89071950/130410476-8941223e-e8bb-435f-97d7-b269212c3815.png)\r\n\r\n<Error> executeQuery: Code: 209, e.displayText() = DB::NetException: Timeout exceeded while writing to socket (10.216.70.22:46531) (version 21.6.8.62 (official build)) , Stack trace (when copying this message, always include the lines below):\r\n\t\t0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8b7323a in /usr/bin/clickhouse\r\n\t\t1. DB::WriteBufferFromPocoSocket::nextImpl() @ 0xf61934a in /usr/bin/clickhouse\r\n\t\t2. DB::TCPHandler::sendData(DB::Block const&) @ 0x1046f90e in /usr/bin/clickhouse\r\n\t\t3. DB::TCPHandler::processOrdinaryQueryWithProcessors() @ 0x1046d75a in /usr/bin/clickhouse\r\n\t\t4. DB::TCPHandler::runImpl() @ 0x1046695e in /usr/bin/clickhouse\r\n\t\t5. DB::TCPHandler::run() @ 0x104787d9 in /usr/bin/clickhouse","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/28013/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27938","id":976084711,"node_id":"MDU6SXNzdWU5NzYwODQ3MTE=","number":27938,"title":"Why not skip the rename proccess,  continue fetch broken parts from replicas to other local disks when a disk is lost","user":{"login":"ditgittube","id":12425919,"node_id":"MDQ6VXNlcjEyNDI1OTE5","avatar_url":"https://avatars.githubusercontent.com/u/12425919?v=4","gravatar_id":"","url":"https://api.github.com/users/ditgittube","html_url":"https://github.com/ditgittube","followers_url":"https://api.github.com/users/ditgittube/followers","following_url":"https://api.github.com/users/ditgittube/following{/other_user}","gists_url":"https://api.github.com/users/ditgittube/gists{/gist_id}","starred_url":"https://api.github.com/users/ditgittube/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ditgittube/subscriptions","organizations_url":"https://api.github.com/users/ditgittube/orgs","repos_url":"https://api.github.com/users/ditgittube/repos","events_url":"https://api.github.com/users/ditgittube/events{/privacy}","received_events_url":"https://api.github.com/users/ditgittube/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1546440057,"node_id":"MDU6TGFiZWwxNTQ2NDQwMDU3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/development","name":"development","color":"fc80df","default":false,"description":"Developement process & source code & implementation details"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-21T08:43:04Z","updated_at":"2021-08-22T18:26:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"when disk is lost, clickhouse rename part to detached directory, but throw the excecption:\r\n```\r\n2021.08.21 16:13:48.967419 [ 27384 ] {} <Error> default.customer (ReplicatedMergeTreePartCheckThread): void DB::ReplicatedMergeTreePartCheckThread::run(): Code: 107, e.displayText() = DB::Exception: Part directory /home/omm/clickhouse/data2/clickhouse/store/83c/83c8ba1e-d9ba-4c91-b96d-d54fa666a74a/all_1_1_0/ doesn't exist. Most likely it is a logical error., Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xd7a1ef0 in /home/omm/clickhouse/usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x4847654 in /home/omm/clickhouse/usr/bin/clickhouse\r\n2. DB::IMergeTreeDataPart::renameTo(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) const (.cold) @ 0x43027e5 in /home/omm/clickhouse/usr/bin/clickhouse\r\n3. DB::IMergeTreeDataPart::renameToDetached(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0xa98250b in /home/omm/clickhouse/usr/bin/clickhouse\r\n4. DB::MergeTreeData::forgetPartAndMoveToDetached(std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) @ 0xa9c288f in /home/omm/clickhouse/usr/bin/clickhouse\r\n5. DB::ReplicatedMergeTreePartCheckThread::checkPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) (.cold) @ 0x433e635 in /home/omm/clickhouse/usr/bin/clickhouse\r\n6. DB::ReplicatedMergeTreePartCheckThread::run() @ 0xaadd587 in /home/omm/clickhouse/usr/bin/clickhouse\r\n7. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0xae2e56e in /home/omm/clickhouse/usr/bin/clickhouse\r\n8. DB::BackgroundSchedulePool::threadFunction() @ 0xae2ea07 in /home/omm/clickhouse/usr/bin/clickhouse\r\n9. ThreadFromGlobalPool::ThreadFromGlobalPool<DB::BackgroundSchedulePool::BackgroundSchedulePool(unsigned long, unsigned long, char const*)::'lambda'()>(DB::BackgroundSchedulePool::BackgroundSchedulePool(unsigned long, unsigned long, char const*)::'lambda'()&&)::'lambda'()::operator()() @ 0xae2eb46 in /home/omm/clickhouse/usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x48a6f53 in /home/omm/clickhouse/usr/bin/clickhouse\r\n11. void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x48a681f in /home/omm/clickhouse/usr/bin/clickhouse\r\n12. start_thread @ 0x7e15 in /usr/lib64/libpthread-2.17.so\r\n13. __clone @ 0x101fed in /usr/lib64/libc-2.17.so\r\n (version 21.3.4.25)\r\n```\r\nWhy not skip the rename proccess,  continue fetch broken parts from replicas to other local disks?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27938/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27928","id":975558778,"node_id":"MDExOlB1bGxSZXF1ZXN0NzE2NjkyNzE5","number":27928,"title":"Add backward compatibility check in stress test","user":{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false},"labels":[{"id":1309693854,"node_id":"MDU6TGFiZWwxMzA5NjkzODU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-build","name":"pr-build","color":"007700","default":false,"description":"Pull request with build/testing/packaging improvement"},{"id":2250495937,"node_id":"MDU6TGFiZWwyMjUwNDk1OTM3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/force%20tests","name":"force tests","color":"59d16d","default":false,"description":"Force test ignoring fast test output. Also forces full perf test run."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":43,"created_at":"2021-08-20T12:18:50Z","updated_at":"2022-01-27T15:15:04Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/27928","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27928","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/27928.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/27928.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- Build/Testing/Packaging Improvement\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nhttps://github.com/ClickHouse/ClickHouse/issues/25088\r\n\r\n\r\nDetailed description / Documentation draft:\r\n...\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27928/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27902","id":975318314,"node_id":"MDU6SXNzdWU5NzUzMTgzMTQ=","number":27902,"title":"Clickhouse broadcast join table optimize like spark TorrentBroadcast","user":{"login":"lifulong","id":450539,"node_id":"MDQ6VXNlcjQ1MDUzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/450539?v=4","gravatar_id":"","url":"https://api.github.com/users/lifulong","html_url":"https://github.com/lifulong","followers_url":"https://api.github.com/users/lifulong/followers","following_url":"https://api.github.com/users/lifulong/following{/other_user}","gists_url":"https://api.github.com/users/lifulong/gists{/gist_id}","starred_url":"https://api.github.com/users/lifulong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lifulong/subscriptions","organizations_url":"https://api.github.com/users/lifulong/orgs","repos_url":"https://api.github.com/users/lifulong/repos","events_url":"https://api.github.com/users/lifulong/events{/privacy}","received_events_url":"https://api.github.com/users/lifulong/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-20T06:46:42Z","updated_at":"2021-08-23T03:19:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Single shard is bottleneck while broadcast join table to all shard and join table is big, broadcast join table cost too much time**\r\n\r\n**in spark,  torrent broadcast mechanism can used to fix single node bottleneck while broadcast join table or other data to all executors/nodes**\r\n\r\n**refer to https://mallikarjuna_g.gitbooks.io/spark/content/spark-TorrentBroadcast.html**\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27902/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27871","id":974610975,"node_id":"MDU6SXNzdWU5NzQ2MTA5NzU=","number":27871,"title":"Ldap authentication not working and for unknown reason.","user":{"login":"pawellrus","id":20677956,"node_id":"MDQ6VXNlcjIwNjc3OTU2","avatar_url":"https://avatars.githubusercontent.com/u/20677956?v=4","gravatar_id":"","url":"https://api.github.com/users/pawellrus","html_url":"https://github.com/pawellrus","followers_url":"https://api.github.com/users/pawellrus/followers","following_url":"https://api.github.com/users/pawellrus/following{/other_user}","gists_url":"https://api.github.com/users/pawellrus/gists{/gist_id}","starred_url":"https://api.github.com/users/pawellrus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pawellrus/subscriptions","organizations_url":"https://api.github.com/users/pawellrus/orgs","repos_url":"https://api.github.com/users/pawellrus/repos","events_url":"https://api.github.com/users/pawellrus/events{/privacy}","received_events_url":"https://api.github.com/users/pawellrus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":{"login":"traceon","id":10261534,"node_id":"MDQ6VXNlcjEwMjYxNTM0","avatar_url":"https://avatars.githubusercontent.com/u/10261534?v=4","gravatar_id":"","url":"https://api.github.com/users/traceon","html_url":"https://github.com/traceon","followers_url":"https://api.github.com/users/traceon/followers","following_url":"https://api.github.com/users/traceon/following{/other_user}","gists_url":"https://api.github.com/users/traceon/gists{/gist_id}","starred_url":"https://api.github.com/users/traceon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/traceon/subscriptions","organizations_url":"https://api.github.com/users/traceon/orgs","repos_url":"https://api.github.com/users/traceon/repos","events_url":"https://api.github.com/users/traceon/events{/privacy}","received_events_url":"https://api.github.com/users/traceon/received_events","type":"User","site_admin":false},"assignees":[{"login":"traceon","id":10261534,"node_id":"MDQ6VXNlcjEwMjYxNTM0","avatar_url":"https://avatars.githubusercontent.com/u/10261534?v=4","gravatar_id":"","url":"https://api.github.com/users/traceon","html_url":"https://github.com/traceon","followers_url":"https://api.github.com/users/traceon/followers","following_url":"https://api.github.com/users/traceon/following{/other_user}","gists_url":"https://api.github.com/users/traceon/gists{/gist_id}","starred_url":"https://api.github.com/users/traceon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/traceon/subscriptions","organizations_url":"https://api.github.com/users/traceon/orgs","repos_url":"https://api.github.com/users/traceon/repos","events_url":"https://api.github.com/users/traceon/events{/privacy}","received_events_url":"https://api.github.com/users/traceon/received_events","type":"User","site_admin":false}],"milestone":null,"comments":9,"created_at":"2021-08-19T12:28:03Z","updated_at":"2021-08-30T11:15:46Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I am trying to set up Ldap authenticaton on my clickhouse instance.\r\n\r\nconfig.xml\r\n````\r\n    <ldap_servers>\r\n        <someldap>\r\n            <host>ldap.company.ru</host>\r\n            <port>389</port>\r\n            <bind_dn>uid={user_name},ou=**long path with russian characters and spaces**</bind_dn>\r\n            <enable_tls>no</enable_tls>\r\n        </someldap>\r\n    </ldap_servers>\r\n````\r\n\r\nusers.d/user.xml\r\n\r\n````\r\n<yandex>\r\n    <users>\r\n        <user>\r\n            <ldap>\r\n                <server>someldap</server>\r\n            </ldap>\r\n            <profile>default</profile>\r\n            <quota>default</quota>\r\n        </user>\r\n    </users>\r\n</yandex>\r\n````\r\n\r\nWhen I try to authenticate as a user clickhouse throws the error. Trace is below. Unfortunately there is no way to understand what is wrong? May be issue is in russian caharcters and spaces in bind_dn?\r\n\r\n\r\n\r\n````\r\n2021.08.19 15:25:36.448574 [ 8569 ] {} <Error> Access(user directories): user: Authentication failed: Code: 532, e.displayText() = DB::Exception: No such object, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8f9b87a in /usr/bin/clickhouse\r\n1. DB::LDAPClient::diag(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0xfb550cf in /usr/bin/clickhouse\r\n2. DB::LDAPClient::openConnection() @ 0xfb56354 in /usr/bin/clickhouse\r\n3. DB::LDAPSimpleAuthClient::authenticate(std::__1::vector<DB::LDAPClient::RoleSearchParams, std::__1::allocator<DB::LDAPClient::RoleSearchParams> > const*, std::__1::vector<std::__1::set<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::allocator<std::__1::set<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > > >*) @ 0xfb59ea7 in /usr/bin/clickhouse\r\n4. DB::ExternalAuthenticators::checkLDAPCredentials(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::BasicCredentials const&, std::__1::vector<DB::LDAPClient::RoleSearchParams, std::__1::allocator<DB::LDAPClient::RoleSearchParams> > const*, std::__1::vector<std::__1::set<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > >, std::__1::allocator<std::__1::set<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > > >*) const @ 0xfb33d8f in /usr/bin/clickhouse\r\n5. DB::IAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0xfb41bfe in /usr/bin/clickhouse\r\n6. DB::MultipleAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0xfb65858 in /usr/bin/clickhouse\r\n7. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0xfb417fc in /usr/bin/clickhouse\r\n8. DB::Context::setUser(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0xffab939 in /usr/bin/clickhouse\r\n9. DB::TCPHandler::receiveHello() @ 0x10fd82ac in /usr/bin/clickhouse\r\n10. DB::TCPHandler::runImpl() @ 0x10fd16b8 in /usr/bin/clickhouse\r\n11. DB::TCPHandler::run() @ 0x10fe4fd9 in /usr/bin/clickhouse\r\n12. Poco::Net::TCPServerConnection::start() @ 0x13b5bc0f in /usr/bin/clickhouse\r\n13. Poco::Net::TCPServerDispatcher::run() @ 0x13b5d69a in /usr/bin/clickhouse\r\n14. Poco::PooledThread::run() @ 0x13c90519 in /usr/bin/clickhouse\r\n15. Poco::ThreadImpl::runnableEntry(void*) @ 0x13c8c7aa in /usr/bin/clickhouse\r\n16. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n17. clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n (version 21.8.3.44 (official build))\r\n2021.08.19 15:25:36.449193 [ 8569 ] {} <Error> ServerErrorHandler: Code: 516, e.displayText() = DB::Exception: user: Authentication failed: password is incorrect or there is no user with such name, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8f9b87a in /usr/bin/clickhouse\r\n1. DB::IAccessStorage::throwCannotAuthenticate(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xfb41a3f in /usr/bin/clickhouse\r\n2. DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0xfb41882 in /usr/bin/clickhouse\r\n3. DB::Context::setUser(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0xffab939 in /usr/bin/clickhouse\r\n4. DB::TCPHandler::receiveHello() @ 0x10fd82ac in /usr/bin/clickhouse\r\n5. DB::TCPHandler::runImpl() @ 0x10fd16b8 in /usr/bin/clickhouse\r\n6. DB::TCPHandler::run() @ 0x10fe4fd9 in /usr/bin/clickhouse\r\n7. Poco::Net::TCPServerConnection::start() @ 0x13b5bc0f in /usr/bin/clickhouse\r\n8. Poco::Net::TCPServerDispatcher::run() @ 0x13b5d69a in /usr/bin/clickhouse\r\n9. Poco::PooledThread::run() @ 0x13c90519 in /usr/bin/clickhouse\r\n10. Poco::ThreadImpl::runnableEntry(void*) @ 0x13c8c7aa in /usr/bin/clickhouse\r\n11. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n12. clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n (version 21.8.3.44 (official build))\r\n````\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27871/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27870","id":974598396,"node_id":"MDU6SXNzdWU5NzQ1OTgzOTY=","number":27870,"title":"CatBoostModel.cpp calls `dlerror` from libc","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-08-19T12:12:40Z","updated_at":"2021-08-19T12:26:40Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"test_catboost_model_first_evaluate\r\n```\r\n2021.08.19 13:59:13.236203 [ 348 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.19 13:59:13.237234 [ 348 ] {} <Fatal> BaseDaemon: (version 21.9.1.7821 (official build), build id: B93FC7D1AC073387F233DA0D5F4B5D59FC5A5F66) (from thread 194) (query_id: ec2b7d73-664e-46be-8e8a-e48c71f0b91a) Received signal Illegal instruction (4)\r\n2021.08.19 13:59:13.237834 [ 348 ] {} <Fatal> BaseDaemon: Illegal operand.\r\n2021.08.19 13:59:13.238528 [ 348 ] {} <Fatal> BaseDaemon: Stack trace: 0x139acebd 0x1e5d6e4a 0x1e5d17f7 0x1e5ccfa4 0x1e5cc7fa 0x1e5cc74d 0x1e5cc688 0x1e5cc620 0x1e5cc3e0 0x1e5cc064 0x1e5cbe92 0x1e5cbd31 0x1e5cb5dc 0x1e5cb2f7 0x1e5ca59f 0x1e5ca1f6 0x1e590120 0x1e5933d0 0x1e59335c 0x1e5932dc 0x1e59325a 0x1e5931e2 0x1e5a0368 0x1e5a027a 0x1e59cca6 0x1e59ac0b 0x1e598be9 0x1e5c6cff 0x1e5c6ac9 0x1e5c69d2 0x1e593c67 0x1e593bf5 0x1e593f8d 0x1717321d 0x171721b2 0x1631fa33 0x1dc60266 0x1dc5fdb0 0x1dc602c4 0x1e333ca3 0x1e6b61cb\r\n2021.08.19 13:59:13.242518 [ 348 ] {} <Fatal> BaseDaemon: 4. ./obj-x86_64-linux-gnu/../base/harmful/harmful.c:199: dlerror @ 0x139acebd in /usr/bin/clickhouse\r\n2021.08.19 13:59:13.304643 [ 348 ] {} <Fatal> BaseDaemon: 5. ./obj-x86_64-linux-gnu/../src/Common/SharedLibrary.cpp:36: DB::SharedLibrary::getImpl(std::__1::basic_string_view<char, std::__1::char_traits<char> >, bool) @ 0x1e5d6e4a in /usr/bin/clickhouse\r\n2021.08.19 13:59:13.467250 [ 348 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/SharedLibrary.h:24: void* (*DB::SharedLibrary::get<void* (*)()>(std::__1::basic_string_view<char, std::__1::char_traits<char> >))() @ 0x1e5d17f7 in /usr/bin/clickhouse\r\n2021.08.19 13:59:13.627541 [ 348 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Interpreters/CatBoostModel.cpp:456: void DB::(anonymous namespace)::CatBoostLibHolder::load<void* (*)()>(void* (*&)(), std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5ccfa4 in /usr/bin/clickhouse\r\n2021.08.19 13:59:13.785122 [ 348 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Interpreters/CatBoostModel.cpp:464: DB::(anonymous namespace)::CatBoostLibHolder::initAPI() @ 0x1e5cc7fa in /usr/bin/clickhouse\r\n2021.08.19 13:59:13.941766 [ 348 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Interpreters/CatBoostModel.cpp:443: DB::(anonymous namespace)::CatBoostLibHolder::CatBoostLibHolder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0x1e5cc74d in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.097582 [ 348 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886: void std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder>::construct<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(DB::(anonymous namespace)::CatBoostLibHolder*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cc688 in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.254706 [ 348 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:521: void std::__1::allocator_traits<std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder> >::__construct<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(std::__1::integral_constant<bool, true>, std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder>&, DB::(anonymous namespace)::CatBoostLibHolder*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cc620 in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.411101 [ 348 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:482: void std::__1::allocator_traits<std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder> >::construct<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder>&, DB::(anonymous namespace)::CatBoostLibHolder*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cc3e0 in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.568291 [ 348 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2594: std::__1::__shared_ptr_emplace<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder> >::__shared_ptr_emplace<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cc064 in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.735862 [ 348 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3360: std::__1::shared_ptr<DB::(anonymous namespace)::CatBoostLibHolder> std::__1::allocate_shared<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, void>(std::__1::allocator<DB::(anonymous namespace)::CatBoostLibHolder> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cbe92 in /usr/bin/clickhouse\r\n2021.08.19 13:59:14.900351 [ 348 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3369: std::__1::shared_ptr<DB::(anonymous namespace)::CatBoostLibHolder> std::__1::make_shared<DB::(anonymous namespace)::CatBoostLibHolder, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, void>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cbd31 in /usr/bin/clickhouse\r\n2021.08.19 13:59:15.083533 [ 348 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Interpreters/CatBoostModel.cpp:490: DB::(anonymous namespace)::getCatBoostWrapperHolder(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x1e5cb5dc in /usr/bin/clickhouse\r\n2021.08.19 13:59:15.244953 [ 348 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Interpreters/CatBoostModel.cpp:502: DB::CatBoostModel::CatBoostModel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::ExternalLoadableLifetime const&) @ 0x1e5cb2f7 in /usr/bin/clickhouse\r\n2021.08.19 13:59:15.469473 [ 348 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068: std::__1::__unique_if<DB::CatBoostModel>::__unique_single std::__1::make_unique<DB::CatBoostModel, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::ExternalLoadableLifetime&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&, DB::ExternalLoadableLifetime&) @ 0x1e5ca59f in /usr/bin/clickhouse\r\n2021.08.19 13:59:15.691083 [ 348 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalModelsLoader.cpp:30: DB::ExternalModelsLoader::create(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Util::AbstractConfiguration const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x1e5ca1f6 in /usr/bin/clickhouse\r\n2021.08.19 13:59:16.017786 [ 348 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:1498: DB::ExternalLoader::createObject(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) const @ 0x1e590120 in /usr/bin/clickhouse\r\n2021.08.19 13:59:16.418175 [ 348 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:1264: auto DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1::operator()<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) const @ 0x1e5933d0 in /usr/bin/clickhouse\r\n2021.08.19 13:59:16.805223 [ 348 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1&>(fp)(std::__1::forward<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>(fp0), std::__1::forward<DB::ExternalLoader::ObjectConfig const&>(fp0), std::__1::forward<std::__1::shared_ptr<DB::IExternalLoadable const> const&>(fp0))) std::__1::__invoke<DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&>(DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) @ 0x1e59335c in /usr/bin/clickhouse\r\n2021.08.19 13:59:17.162490 [ 348 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:317: std::__1::shared_ptr<DB::IExternalLoadable const> std::__1::__invoke_void_return_wrapper<std::__1::shared_ptr<DB::IExternalLoadable const> >::__call<DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&>(DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) @ 0x1e5932dc in /usr/bin/clickhouse\r\n2021.08.19 13:59:17.524810 [ 348 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1, std::__1::shared_ptr<DB::IExternalLoadable const> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&)>::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) @ 0x1e59325a in /usr/bin/clickhouse\r\n2021.08.19 13:59:17.901028 [ 348 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: std::__1::shared_ptr<DB::IExternalLoadable const> std::__1::__function::__policy_invoker<std::__1::shared_ptr<DB::IExternalLoadable const> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::ExternalLoader::ExternalLoader(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::Logger*)::$_1, std::__1::shared_ptr<DB::IExternalLoadable const> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&)> >(std::__1::__function::__policy_storage const*, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) @ 0x1e5931e2 in /usr/bin/clickhouse\r\n2021.08.19 13:59:18.248391 [ 348 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<std::__1::shared_ptr<DB::IExternalLoadable const> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&)>::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) const @ 0x1e5a0368 in /usr/bin/clickhouse\r\n2021.08.19 13:59:18.585908 [ 348 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<std::__1::shared_ptr<DB::IExternalLoadable const> (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&)>::operator()(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const> const&) const @ 0x1e5a027a in /usr/bin/clickhouse\r\n2021.08.19 13:59:18.924246 [ 348 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:1026: DB::ExternalLoader::LoadingDispatcher::loadSingleObject(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ExternalLoader::ObjectConfig const&, std::__1::shared_ptr<DB::IExternalLoadable const>) @ 0x1e59cca6 in /usr/bin/clickhouse\r\n2021.08.19 13:59:19.254967 [ 348 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:981: DB::ExternalLoader::LoadingDispatcher::doLoading(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, bool, unsigned long, bool, std::__1::shared_ptr<DB::ThreadGroupStatus>) @ 0x1e59ac0b in /usr/bin/clickhouse\r\n2021.08.19 13:59:19.582242 [ 348 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:921: DB::ExternalLoader::LoadingDispatcher::startLoading(DB::ExternalLoader::LoadingDispatcher::Info&, bool, unsigned long) @ 0x1e598be9 in /usr/bin/clickhouse\r\n2021.08.19 13:59:19.975862 [ 348 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:0: DB::ExternalLoader::LoadingDispatcher::loadImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >, bool, std::__1::unique_lock<std::__1::mutex>&)::'lambda'()::operator()() const @ 0x1e5c6cff in /usr/bin/clickhouse\r\n2021.08.19 13:59:20.370266 [ 348 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__mutex_base:405: void std::__1::condition_variable::wait<DB::ExternalLoader::LoadingDispatcher::loadImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >, bool, std::__1::unique_lock<std::__1::mutex>&)::'lambda'()>(std::__1::unique_lock<std::__1::mutex>&, DB::ExternalLoader::LoadingDispatcher::loadImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >, bool, std::__1::unique_lock<std::__1::mutex>&)::'lambda'()) @ 0x1e5c6ac9 in /usr/bin/clickhouse\r\n2021.08.19 13:59:20.758840 [ 348 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:829: DB::ExternalLoader::LoadingDispatcher::loadImpl(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >, bool, std::__1::unique_lock<std::__1::mutex>&) @ 0x1e5c69d2 in /usr/bin/clickhouse\r\n2021.08.19 13:59:21.078837 [ 348 ] {} <Fatal> BaseDaemon: 34. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:595: DB::ExternalLoader::LoadResult DB::ExternalLoader::LoadingDispatcher::tryLoad<DB::ExternalLoader::LoadResult>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >) @ 0x1e593c67 in /usr/bin/clickhouse\r\n2021.08.19 13:59:21.403902 [ 348 ] {} <Fatal> BaseDaemon: 35. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:1354: DB::ExternalLoader::LoadResult DB::ExternalLoader::tryLoad<DB::ExternalLoader::LoadResult, void>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> >) const @ 0x1e593bf5 in /usr/bin/clickhouse\r\n2021.08.19 13:59:21.738466 [ 348 ] {} <Fatal> BaseDaemon: 36. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalLoader.cpp:1366: std::__1::shared_ptr<DB::IExternalLoadable const> DB::ExternalLoader::load<std::__1::shared_ptr<DB::IExternalLoadable const>, void>(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x1e593f8d in /usr/bin/clickhouse\r\n2021.08.19 13:59:22.002970 [ 348 ] {} <Fatal> BaseDaemon: 37. ./obj-x86_64-linux-gnu/../src/Interpreters/ExternalModelsLoader.h:25: DB::ExternalModelsLoader::getModel(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x1717321d in /usr/bin/clickhouse\r\n2021.08.19 13:59:22.256946 [ 348 ] {} <Fatal> BaseDaemon: 38. ./obj-x86_64-linux-gnu/../src/Functions/FunctionsExternalModels.cpp:52: DB::FunctionModelEvaluate::getReturnTypeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&) const @ 0x171721b2 in /usr/bin/clickhouse\r\n2021.08.19 13:59:22.363653 [ 348 ] {} <Fatal> BaseDaemon: 39. ./obj-x86_64-linux-gnu/../src/Functions/IFunctionAdaptors.h:123: DB::FunctionToOverloadResolverAdaptor::getReturnTypeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&) const @ 0x1631fa33 in /usr/bin/clickhouse\r\n2021.08.19 13:59:22.713087 [ 348 ] {} <Fatal> BaseDaemon: 40. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:359: DB::IFunctionOverloadResolver::getReturnTypeWithoutLowCardinality(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&) const @ 0x1dc60266 in /usr/bin/clickhouse\r\n2021.08.19 13:59:23.066185 [ 348 ] {} <Fatal> BaseDaemon: 41. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:314: DB::IFunctionOverloadResolver::getReturnType(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&) const @ 0x1dc5fdb0 in /usr/bin/clickhouse\r\n2021.08.19 13:59:23.419197 [ 348 ] {} <Fatal> BaseDaemon: 42. ./obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:329: DB::IFunctionOverloadResolver::build(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&) const @ 0x1dc602c4 in /usr/bin/clickhouse\r\n2021.08.19 13:59:25.033066 [ 348 ] {} <Fatal> BaseDaemon: 43. ./obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:204: DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node const*, std::__1::allocator<DB::ActionsDAG::Node const*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0x1e333ca3 in /usr/bin/clickhouse\r\n2021.08.19 13:59:25.662213 [ 348 ] {} <Fatal> BaseDaemon: 44. ./obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:570: DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0x1e6b61cb in /usr/bin/clickhouse\r\n2021.08.19 13:59:27.013815 [ 348 ] {} <Fatal> BaseDaemon: Checksum of the binary: 1EFCD06204779A2EAFBED96BAEF63211, integrity check passed.\r\n2021.08.19 13:59:35.964010 [ 190 ] {} <Fatal> Application: Child process was terminated by signal 4.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27870/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27863","id":974576711,"node_id":"MDU6SXNzdWU5NzQ1NzY3MTE=","number":27863,"title":"Can't create VIEW with toInt16('low_cardinality_string') in it.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"KochetovNicolai","id":4092911,"node_id":"MDQ6VXNlcjQwOTI5MTE=","avatar_url":"https://avatars.githubusercontent.com/u/4092911?v=4","gravatar_id":"","url":"https://api.github.com/users/KochetovNicolai","html_url":"https://github.com/KochetovNicolai","followers_url":"https://api.github.com/users/KochetovNicolai/followers","following_url":"https://api.github.com/users/KochetovNicolai/following{/other_user}","gists_url":"https://api.github.com/users/KochetovNicolai/gists{/gist_id}","starred_url":"https://api.github.com/users/KochetovNicolai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KochetovNicolai/subscriptions","organizations_url":"https://api.github.com/users/KochetovNicolai/orgs","repos_url":"https://api.github.com/users/KochetovNicolai/repos","events_url":"https://api.github.com/users/KochetovNicolai/events{/privacy}","received_events_url":"https://api.github.com/users/KochetovNicolai/received_events","type":"User","site_admin":false},"assignees":[{"login":"KochetovNicolai","id":4092911,"node_id":"MDQ6VXNlcjQwOTI5MTE=","avatar_url":"https://avatars.githubusercontent.com/u/4092911?v=4","gravatar_id":"","url":"https://api.github.com/users/KochetovNicolai","html_url":"https://github.com/KochetovNicolai","followers_url":"https://api.github.com/users/KochetovNicolai/followers","following_url":"https://api.github.com/users/KochetovNicolai/following{/other_user}","gists_url":"https://api.github.com/users/KochetovNicolai/gists{/gist_id}","starred_url":"https://api.github.com/users/KochetovNicolai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KochetovNicolai/subscriptions","organizations_url":"https://api.github.com/users/KochetovNicolai/orgs","repos_url":"https://api.github.com/users/KochetovNicolai/repos","events_url":"https://api.github.com/users/KochetovNicolai/events{/privacy}","received_events_url":"https://api.github.com/users/KochetovNicolai/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-08-19T11:45:24Z","updated_at":"2021-12-01T08:03:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\nIf you apply some functions to LowCardinality(String) column in view select statement and in result it ends with numeric datatype, clickhouse complains about having suspicious low cardinality column. \r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes,\r\n\r\nClickhouse version 21.9\r\n\r\n**How to reproduce**\r\n\r\n```\r\nCREATE VIEW test_lc AS\r\nSELECT toInt16(toLowCardinality(materialize('10')))\r\n\r\nReceived exception from server (version 21.9.1):\r\nCode: 455. DB::Exception: Received from localhost:9000. DB::Exception: Creating columns of type LowCardinality(Int16) is prohibited by default due to expected negative impact on performance. It can be enabled with the \"allow_suspicious_low_cardinality_types\" setting.. (SUSPICIOUS_TYPE_FOR_LOW_CARDINALITY)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nIt's possible to create VIEW regardless of how clickhouse behaves with LowCardinality columns.\r\n\r\nRelated to\r\n\r\nSame issue for materialized view:\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/issues/6293\r\nhttps://github.com/ClickHouse/ClickHouse/pull/6428\r\n\r\nMain issue about keeping LC after some functions https://github.com/ClickHouse/ClickHouse/issues/6432","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27863/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27861","id":974564150,"node_id":"MDU6SXNzdWU5NzQ1NjQxNTA=","number":27861,"title":"It's possible to create direct dictionary with LIFETIME, but not use it.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-08-19T11:28:46Z","updated_at":"2022-01-19T16:50:49Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the issue**\r\nYou can create direct dictionary with lifetime trait and clickhouse wouldn't complain, but it would throw exception if you would try to use this dictionary. \r\n\r\n**How to reproduce**\r\nClickhouse version 21.9\r\n\r\n```\r\nCREATE TABLE dict_source (key UInt64, value String) ENGINE=MergeTree ORDER BY key;\r\nINSERT INTO dict_source SELECT number, toString(number) FROM numbers(1000000);\r\nCREATE DICTIONARY dict　(　    `key` UInt64,　    `value` String　)　PRIMARY KEY key　SOURCE(CLICKHOUSE(table 'dict_source'))　LAYOUT(DIRECT()) LIFETIME(300)　;\r\n\r\nSELECT dictGet('dict', 'value', 1::UInt64);\r\n\r\nSELECT dictGet('dict', 'value', CAST('1', 'UInt64'))\r\n\r\nQuery id: d31b3dd7-bd6c-4c7b-b997-f9f7222c3cfa\r\n\r\n\r\n0 rows in set. Elapsed: 0.004 sec.\r\n\r\nReceived exception from server (version 21.9.1):\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: 'lifetime' parameter is redundant for the dictionary' of layout 'direct': While processing dictGet('dict', 'value', CAST('1', 'UInt64')). (BAD_ARGUMENTS)\r\n```\r\n\r\n**Expected behavior**\r\nClickhouse will throw this error on dictionary creation.\r\n\r\n**Error message and/or stacktrace**\r\n```\r\nCode: 36. DB::Exception: Received from localhost:9000. DB::Exception: 'lifetime' parameter is redundant for the dictionary' of layout 'direct': While processing dictGet('dict', 'value', CAST('1', 'UInt64')). (BAD_ARGUMENTS)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27861/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27845","id":974446414,"node_id":"MDU6SXNzdWU5NzQ0NDY0MTQ=","number":27845,"title":"Hints for joins and unions","user":{"login":"GSokol","id":1274970,"node_id":"MDQ6VXNlcjEyNzQ5NzA=","avatar_url":"https://avatars.githubusercontent.com/u/1274970?v=4","gravatar_id":"","url":"https://api.github.com/users/GSokol","html_url":"https://github.com/GSokol","followers_url":"https://api.github.com/users/GSokol/followers","following_url":"https://api.github.com/users/GSokol/following{/other_user}","gists_url":"https://api.github.com/users/GSokol/gists{/gist_id}","starred_url":"https://api.github.com/users/GSokol/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/GSokol/subscriptions","organizations_url":"https://api.github.com/users/GSokol/orgs","repos_url":"https://api.github.com/users/GSokol/repos","events_url":"https://api.github.com/users/GSokol/events{/privacy}","received_events_url":"https://api.github.com/users/GSokol/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2021-08-19T09:11:13Z","updated_at":"2021-08-20T22:56:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nAs far ad Clickouse supports merge joins, I want to utilize it to build DWH with modal layer relying on Anchor modeling or High Normalized Hybrid Modeling. That models require a huge amount of joins. In some cases, it would be joins of rather big datamarts, when merge joins would be preferable, In other cases, the data marts would be relatively small by nature and hash joins would give more performance. So it would be great to have some kind of tool to specify implicitly which kind of join is preferred in the particular case.\r\n\r\n**Describe the solution you'd like**\r\n\r\nGreenplum and Vertical utilize comment-based hints to implicitly suggest the join type. These databases are highly popular in DWH community, so we could inherit this practice.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nMS SQL supports the extended join syntax when you could specify the desired join type with keywords.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27845/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27843","id":974394660,"node_id":"MDU6SXNzdWU5NzQzOTQ2NjA=","number":27843,"title":"Cap timeouts to max_execution_time.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-08-19T08:11:51Z","updated_at":"2021-08-24T18:13:38Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"If `max_execution_time` is lower than some of:\r\n- send_timeout;\r\n- receive_timeout;\r\n- connect_timeout;\r\n- http_send_timeout;\r\n- http_receive_timeout;\r\n- http_connection_timeout;\r\n- lock_acquire_timeout;\r\n- receive_data_timeout_ms;\r\n- hedged_connection_timeout_ms;\r\n- replication_alter_columns_timeout;\r\n- insert_quorum_timeout;\r\n- insert_distributed_timeout;\r\n- distributed_ddl_task_timeout;\r\n- postgresql_connection_pool_wait_timeout;\r\n\r\nThe settings should be lowered to `max_execution_time`.\r\nThe socket options should be modified accordingly.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27843/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27821","id":973737972,"node_id":"MDU6SXNzdWU5NzM3Mzc5NzI=","number":27821,"title":"Wrong language of a section","user":{"login":"mathalex","id":33257111,"node_id":"MDQ6VXNlcjMzMjU3MTEx","avatar_url":"https://avatars.githubusercontent.com/u/33257111?v=4","gravatar_id":"","url":"https://api.github.com/users/mathalex","html_url":"https://github.com/mathalex","followers_url":"https://api.github.com/users/mathalex/followers","following_url":"https://api.github.com/users/mathalex/following{/other_user}","gists_url":"https://api.github.com/users/mathalex/gists{/gist_id}","starred_url":"https://api.github.com/users/mathalex/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mathalex/subscriptions","organizations_url":"https://api.github.com/users/mathalex/orgs","repos_url":"https://api.github.com/users/mathalex/repos","events_url":"https://api.github.com/users/mathalex/events{/privacy}","received_events_url":"https://api.github.com/users/mathalex/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-18T14:36:59Z","updated_at":"2021-08-18T14:36:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"multiSearchAllPositions section in [this file](https://github.com/ClickHouse/ClickHouse/blob/master/docs/ru/sql-reference/functions/string-search-functions.md) is not translated into Russian.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27821/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27813","id":973540709,"node_id":"MDU6SXNzdWU5NzM1NDA3MDk=","number":27813,"title":"SELECT - There is no supertype for types DateTime, String","user":{"login":"AlexMAS","id":816125,"node_id":"MDQ6VXNlcjgxNjEyNQ==","avatar_url":"https://avatars.githubusercontent.com/u/816125?v=4","gravatar_id":"","url":"https://api.github.com/users/AlexMAS","html_url":"https://github.com/AlexMAS","followers_url":"https://api.github.com/users/AlexMAS/followers","following_url":"https://api.github.com/users/AlexMAS/following{/other_user}","gists_url":"https://api.github.com/users/AlexMAS/gists{/gist_id}","starred_url":"https://api.github.com/users/AlexMAS/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/AlexMAS/subscriptions","organizations_url":"https://api.github.com/users/AlexMAS/orgs","repos_url":"https://api.github.com/users/AlexMAS/repos","events_url":"https://api.github.com/users/AlexMAS/events{/privacy}","received_events_url":"https://api.github.com/users/AlexMAS/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-18T10:59:25Z","updated_at":"2021-08-19T08:14:34Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hello!\r\n\r\nI'm faced with an unexpected behaviour which is very similar to #3746.\r\n\r\nIt's easy to reproduce. The next request:\r\n\r\n```sql\r\nSELECT greatest(time, '2021-08-18 10:44:03') AS max_time\r\nFROM\r\n(\r\n    SELECT now() AS time\r\n)\r\n```\r\n\r\nends with the error:\r\n\r\n```\r\nReceived exception from server (version 21.4.6):\r\nCode: 386. DB::Exception: Received from s:9000. DB::Exception: There is no supertype for types DateTime, String because some of them are String/FixedString and some of them are not: While processing greatest(time, '2021-08-18 10:44:03') AS max_time.\r\n```\r\n\r\nPossible workaround is to use `toDateTime()` function to explicit cast the string parameter.\r\n\r\n```sql\r\nSELECT greatest(time, toDateTime('2021-08-18 10:44:03')) AS max_time\r\nFROM\r\n(\r\n    SELECT now() AS time\r\n)\r\n\r\nQuery id: 091311c2-a7c4-4192-a7b6-d9680c9d1f8d\r\n\r\n┌────────────max_time─┐\r\n│ 2021-08-18 10:49:03 │\r\n└─────────────────────┘\r\n\r\n1 rows in set. Elapsed: 0.006 sec.\r\n```\r\n\r\nThe server version is `21.4.6.55`.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27813/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27801","id":973232926,"node_id":"MDU6SXNzdWU5NzMyMzI5MjY=","number":27801,"title":"Add support for non-default ANSI MODE of MySQL sources.","user":{"login":"wangxuanyue","id":22309912,"node_id":"MDQ6VXNlcjIyMzA5OTEy","avatar_url":"https://avatars.githubusercontent.com/u/22309912?v=4","gravatar_id":"","url":"https://api.github.com/users/wangxuanyue","html_url":"https://github.com/wangxuanyue","followers_url":"https://api.github.com/users/wangxuanyue/followers","following_url":"https://api.github.com/users/wangxuanyue/following{/other_user}","gists_url":"https://api.github.com/users/wangxuanyue/gists{/gist_id}","starred_url":"https://api.github.com/users/wangxuanyue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wangxuanyue/subscriptions","organizations_url":"https://api.github.com/users/wangxuanyue/orgs","repos_url":"https://api.github.com/users/wangxuanyue/repos","events_url":"https://api.github.com/users/wangxuanyue/events{/privacy}","received_events_url":"https://api.github.com/users/wangxuanyue/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":2121263626,"node_id":"MDU6TGFiZWwyMTIxMjYzNjI2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mysql","name":"comp-mysql","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2021-08-18T03:14:29Z","updated_at":"2021-08-19T14:13:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"* Which ClickHouse server version to use\r\n21.3.12.2 (official build).\r\n\r\nMySQLDictionarySource: Cannot find table in SHOW TABLE STATUS result.\r\n\r\n```log\r\n2021.08.18 10:47:35.986046 [ 41658 ] {} <Trace> ExternalDictionariesLoader: Start loading object '45e4489a-6353-4e9c-864e-de030e1ed1bb'\r\n2021.08.18 10:47:35.986171 [ 41658 ] {} <Debug> mysqlxx::Pool: Connecting to db@10.2.1.1:3306 as user ruser\r\n2021.08.18 10:47:36.045025 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18677358): sending PING to check if it is alive.\r\n2021.08.18 10:47:36.059678 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18677358): PING ok.\r\n2021.08.18 10:47:36.059697 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18677358): sending PING to check if it is alive.\r\n2021.08.18 10:47:36.074322 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18677358): PING ok.\r\n2021.08.18 10:47:36.074337 [ 41658 ] {} <Trace> MySQLDictionarySource: SHOW TABLE STATUS LIKE 'td\\\\_area'\r\n2021.08.18 10:47:36.074361 [ 41658 ] {} <Trace> mysqlxx::Query: Running MySQL query using connection 18677358\r\n2021.08.18 10:47:36.090609 [ 41658 ] {} <Error> MySQLDictionarySource: Cannot find table in SHOW TABLE STATUS result.\r\n2021.08.18 10:47:36.090630 [ 41658 ] {} <Trace> MySQLDictionarySource: SELECT `area_code`, `area_name` FROM `db`.`td_area`;\r\n2021.08.18 10:47:36.090662 [ 41658 ] {} <Debug> mysqlxx::Pool: Connecting to db@10.2.1.1:3306 as user ruser\r\n2021.08.18 10:47:36.149062 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18725404): sending PING to check if it is alive.\r\n2021.08.18 10:47:36.163600 [ 41658 ] {} <Trace> mysqlxx::Pool: Entry(connection 18725404): PING ok.\r\n2021.08.18 10:47:36.163614 [ 41658 ] {} <Trace> mysqlxx::Query: Running MySQL query using connection 18725404\r\n2021.08.18 10:47:36.181153 [ 41658 ] {} <Trace> ExternalDictionariesLoader: Supposed update time for '45e4489a-6353-4e9c-864e-de030e1ed1bb' is 2021-08-18 10:52:54 (loaded, lifetime [300, 360], no errors)\r\n2021.08.18 10:47:36.181175 [ 41658 ] {} <Trace> ExternalDictionariesLoader: Next update time for '45e4489a-6353-4e9c-864e-de030e1ed1bb' was set to 2021-08-18 10:52:54\r\n```\r\n* run the sql : SHOW TABLE STATUS LIKE 'td\\\\\\\\_area'\r\n![image](https://user-images.githubusercontent.com/22309912/129830796-f9b5eb0f-2383-4172-8173-dcf7a69f8f55.png)\r\n* run sql : SHOW TABLE STATUS LIKE 'td_area'\r\n![image](https://user-images.githubusercontent.com/22309912/129830877-1444e033-4790-4ba3-845a-480ee9bcb207.png)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27801/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27798","id":973036865,"node_id":"MDU6SXNzdWU5NzMwMzY4NjU=","number":27798,"title":"runningAccumulate with window","user":{"login":"sergeevik","id":25515848,"node_id":"MDQ6VXNlcjI1NTE1ODQ4","avatar_url":"https://avatars.githubusercontent.com/u/25515848?v=4","gravatar_id":"","url":"https://api.github.com/users/sergeevik","html_url":"https://github.com/sergeevik","followers_url":"https://api.github.com/users/sergeevik/followers","following_url":"https://api.github.com/users/sergeevik/following{/other_user}","gists_url":"https://api.github.com/users/sergeevik/gists{/gist_id}","starred_url":"https://api.github.com/users/sergeevik/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sergeevik/subscriptions","organizations_url":"https://api.github.com/users/sergeevik/orgs","repos_url":"https://api.github.com/users/sergeevik/repos","events_url":"https://api.github.com/users/sergeevik/events{/privacy}","received_events_url":"https://api.github.com/users/sergeevik/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-17T20:23:29Z","updated_at":"2021-08-18T15:21:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"runningAccumulate accumulate all in group. What if we want to accumulate last n rows, not all.\r\n\r\nFor example we have table from 1 to 10\r\nrunningAccumulate return \r\n\r\n│ 0 │   0 │\r\n│ 1 │   1 │\r\n│ 2 │   3 │\r\n│ 3 │   6 │\r\n│ 4 │  10 │\r\n│ 5 │  15 │\r\n│ 6 │  21 │\r\n│ 7 │  28 │\r\n│ 8 │  36 │\r\n│ 9 │  45 │\r\n\r\nIn my suggestion, call runningAccumulate(column, 2), and get result:\r\n\r\n│ 0 │   0 │\r\n│ 1 │   1 │\r\n│ 2 │   3 │\r\n│ 3 │   5 │ (2+3)\r\n│ 4 │  7 │ (3+4)\r\n│ 5 │  9 │ (5+4)\r\n│ 6 │  11 │ (5+6)\r\n│ 7 │  13 │ (6+7)\r\n│ 8 │  15 │ (7+8)\r\n│ 9 │  17 │ (8+9)\r\n\r\nUse case: for me is DAU/MAU report when i need calculate active users for every day with 30 day before\r\ntoday minus 30 day\r\nyesterday minus 30 day\r\n...\r\n\r\nNow i use materialized view with arrayJoin(30). Get this idea from [medium](https://medium.com/@andre_bodro/how-to-fast-calculating-mau-in-clickhouse-fd793559b229)\r\n\r\nsome my analytics about performance:\r\nuse ArrayJoin (calculate last 60 day): \r\n30 rows in set. Elapsed: 12.780 sec. Processed 8.84 million rows, 459.69 MB (691.90 thousand rows/s., 35.97 MB/s.) \r\n\r\nuse runningAccumulate (accumulate last 60 day):\r\n27 rows in set. Elapsed: 1.835 sec. Processed 8.37 million rows, 434.96 MB (4.56 million rows/s., 236.97 MB/s.) \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27798/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27761","id":972313547,"node_id":"MDU6SXNzdWU5NzIzMTM1NDc=","number":27761,"title":"Allow type casts between aggregate function states with the same representation","user":{"login":"vmihailenco","id":290976,"node_id":"MDQ6VXNlcjI5MDk3Ng==","avatar_url":"https://avatars.githubusercontent.com/u/290976?v=4","gravatar_id":"","url":"https://api.github.com/users/vmihailenco","html_url":"https://github.com/vmihailenco","followers_url":"https://api.github.com/users/vmihailenco/followers","following_url":"https://api.github.com/users/vmihailenco/following{/other_user}","gists_url":"https://api.github.com/users/vmihailenco/gists{/gist_id}","starred_url":"https://api.github.com/users/vmihailenco/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vmihailenco/subscriptions","organizations_url":"https://api.github.com/users/vmihailenco/orgs","repos_url":"https://api.github.com/users/vmihailenco/repos","events_url":"https://api.github.com/users/vmihailenco/events{/privacy}","received_events_url":"https://api.github.com/users/vmihailenco/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-17T05:23:28Z","updated_at":"2022-01-12T06:42:00Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"For example, it is not possible to change column type from `quantilesTDigest` to `quantilesTDigestWeighted`\r\n\r\n```sql\r\nCREATE TABLE test\r\n(\r\n    `foo` AggregateFunction(quantilesTDigest(0.5), Float32)\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple()\r\n\r\nALTER TABLE test\r\n    MODIFY COLUMN `foo` AggregateFunction(quantilesTDigestWeighted(0.5), Float32, UInt32)\r\n\r\nReceived exception from server (version 21.7.7):\r\nCode: 70. DB::Exception: Received from localhost:9000. DB::Exception: Conversion from AggregateFunction(quantilesTDigest(0.5), Float32) to AggregateFunction(quantilesTDigestWeighted(0.5), Float32, UInt32) is not supported: While processing CAST(foo, 'AggregateFunction(quantilesTDigestWeighted(0.5), Float32, UInt32)') AS foo.\r\n```\r\n\r\nAlso it would be handy to support conversions between `groupUniqArray(100)` and `groupUniqArray(1000)`.\r\n\r\nThe suggestion is to maintain the list of such identical types and use the list only for `MODIFY COLUMN` since I guess performance can be an issue.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27761/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27749","id":971844205,"node_id":"MDU6SXNzdWU5NzE4NDQyMDU=","number":27749,"title":"Distributed Mann-Whitney U test","user":{"login":"zlobober","id":1856442,"node_id":"MDQ6VXNlcjE4NTY0NDI=","avatar_url":"https://avatars.githubusercontent.com/u/1856442?v=4","gravatar_id":"","url":"https://api.github.com/users/zlobober","html_url":"https://github.com/zlobober","followers_url":"https://api.github.com/users/zlobober/followers","following_url":"https://api.github.com/users/zlobober/following{/other_user}","gists_url":"https://api.github.com/users/zlobober/gists{/gist_id}","starred_url":"https://api.github.com/users/zlobober/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zlobober/subscriptions","organizations_url":"https://api.github.com/users/zlobober/orgs","repos_url":"https://api.github.com/users/zlobober/repos","events_url":"https://api.github.com/users/zlobober/events{/privacy}","received_events_url":"https://api.github.com/users/zlobober/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false},"assignees":[{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-08-16T15:15:12Z","updated_at":"2021-08-16T15:15:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"This issue follows up https://github.com/ClickHouse/ClickHouse/pull/16883.\r\n\r\n**Use case**\r\n\r\nIn the distributed setting, the current implementation of mannWhitneyUTest materializes all data at the query initiator in order to (virtually) merge them into single sequence and calculate U-statistics, which is clearly inefficient when amount of data is too large for a single host. \r\n\r\n**Describe the solution you'd like**\r\n\r\nIt is possible, though, to apply a somewhat standard trick in order to overcome the need of distributed sorting. Assuming that both populations are sampled from distributions with finite second moments, one may perform a following two-step procedure:\r\n\r\n1. Choose a small number of buckets, let's say B = 100. Distribute each sample from each population equiprobably into one of the B buckets (obtaining 2B buckets in total). Take average value in each bucket, obtaining a population of size B corresponding to each of the initial populations.\r\n2. Run Mann-Whitney U test for the two obtained populations of size B.\r\n\r\nThis produces a quite practical test for the null hypothesis \"means of two distributions are equal to each\" since, informally speaking, averages over buckets are normally distributed with the same means as the original distributions. In practice, this is often enough to make conclusions; in particular, it is equivalent to the original test under the assumption that original distributions had same shape (i.e. coincide under some shift).\r\n\r\nFinally, it is quite easy to make first step of the algorithm efficient in the distributed setting as it allows using a mergeable state of size O(B) (we need to track only total value and count in each bucket).\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNothing comes to my mind as the original U-value calculation procedure forces us to obtain a sorted sequence of samples in-memory. We must roughen initial data in some way. \r\n\r\n**Additional context**\r\n\r\nSimilar approach is used in internal AB-evaluation service in Yandex in order to deal with the large populations.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27749/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27730","id":971544219,"node_id":"MDU6SXNzdWU5NzE1NDQyMTk=","number":27730,"title":"Better tooling around materialized columns","user":{"login":"macobo","id":148820,"node_id":"MDQ6VXNlcjE0ODgyMA==","avatar_url":"https://avatars.githubusercontent.com/u/148820?v=4","gravatar_id":"","url":"https://api.github.com/users/macobo","html_url":"https://github.com/macobo","followers_url":"https://api.github.com/users/macobo/followers","following_url":"https://api.github.com/users/macobo/following{/other_user}","gists_url":"https://api.github.com/users/macobo/gists{/gist_id}","starred_url":"https://api.github.com/users/macobo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/macobo/subscriptions","organizations_url":"https://api.github.com/users/macobo/orgs","repos_url":"https://api.github.com/users/macobo/repos","events_url":"https://api.github.com/users/macobo/events{/privacy}","received_events_url":"https://api.github.com/users/macobo/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-16T09:29:09Z","updated_at":"2021-08-16T09:29:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> (you don't have to strictly follow this form)\r\n\r\n**Use case**\r\n\r\nWe use materialized columns extensively in our analytics app to speed up queries on a lot of free-form json data where the structure is not static.\r\n\r\nManaging these columns is however a pain because:\r\n- Adding them, the data starts trickling in only forwards\r\n- To populate the data for historic data, you need to OPTIMIZE the whole data or partition. https://clickhouse.tech/docs/en/sql-reference/statements/optimize/\r\n\r\n**Describe the solution you'd like**\r\n\r\nExtend `optimize` statement to allow materializing only one column.\r\n\r\n`OPTIMIZE TABLE [db.]name [ON CLUSTER cluster] [COLUMN mat_column] [PARTITION partition | PARTITION ID 'partition_id']`\r\n\r\nEx: `OPTIMIZE TABLE mytable ON CLUSTER cluster COLUMN mycolumn`\r\n\r\n**Describe alternatives you've considered**\r\n\r\nOptimizing our whole dataset every time we materialize a column. This runs into issues due to heavy I/O\r\n\r\n**Additional context**\r\n\r\n\r\n> Add any other context or screenshots about the feature request here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27730/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27728","id":971504073,"node_id":"MDU6SXNzdWU5NzE1MDQwNzM=","number":27728,"title":"DB::NetException: Connection reset by peer, while reading from socket","user":{"login":"shadowDy","id":18388143,"node_id":"MDQ6VXNlcjE4Mzg4MTQz","avatar_url":"https://avatars.githubusercontent.com/u/18388143?v=4","gravatar_id":"","url":"https://api.github.com/users/shadowDy","html_url":"https://github.com/shadowDy","followers_url":"https://api.github.com/users/shadowDy/followers","following_url":"https://api.github.com/users/shadowDy/following{/other_user}","gists_url":"https://api.github.com/users/shadowDy/gists{/gist_id}","starred_url":"https://api.github.com/users/shadowDy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shadowDy/subscriptions","organizations_url":"https://api.github.com/users/shadowDy/orgs","repos_url":"https://api.github.com/users/shadowDy/repos","events_url":"https://api.github.com/users/shadowDy/events{/privacy}","received_events_url":"https://api.github.com/users/shadowDy/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-16T08:41:59Z","updated_at":"2021-08-16T11:33:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":" I execute an SQL and do not try global join, \r\nWhen I execute on a cluster with only 10 + nodes, the data will be returned successfully.\r\n\r\nWhen I execute on a 100 + node cluster,this exception will occur stably. The interval between execution and error reporting is about 5-8s.\r\n![image](https://user-images.githubusercontent.com/18388143/129536132-4ba2486f-b421-43a4-bc29-accc6042b04a.png)\r\n\r\n\r\nWhen I change the join to global join, a 300s timeout will be executed.\r\n\r\nVersion: 21.1.2 revision 54443\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27728/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27684","id":971069091,"node_id":"MDU6SXNzdWU5NzEwNjkwOTE=","number":27684,"title":"Fuzzing of network protocols","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-15T07:15:50Z","updated_at":"2021-08-16T00:18:46Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"When clickhouse-server is built with fuzzing, make it act in the following way:\r\n\r\nServer should startup as usual. When it is ready for connections, the function `LLVMFuzzerTestOneInput` will establish a connection to specified port and send random garbage (the fuzzer's data) to it. It should also create another thread that will receive and ignore all server replies from the socket. After garbage is being sent, connection should be closed.\r\n\r\nA corpus can be generated from `strace -s1000 -x -f -e trace=network clickhouse-client`\r\n\r\nNote: it will do network interaction but it will connect to itself (the same process), so the coverage data will be successfully collected.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27684/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27668","id":970775191,"node_id":"MDU6SXNzdWU5NzA3NzUxOTE=","number":27668,"title":"Types of expressions over SimpleAggregateFunctions are inconsistent.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-08-14T00:15:13Z","updated_at":"2021-08-16T00:04:45Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```sql\r\n-------- SUM OK  (expected) --------------------\r\nSELECT\r\n    CAST(5, 'SimpleAggregateFunction(sum, UInt64)') AS x,\r\n    sum(x) AS y,\r\n    toTypeName(y) AS type_y;  \r\n┌─x─┬─y─┬─type_y─┐\r\n│ 5 │ 5 │ UInt64 │\r\n└───┴───┴────────┘\r\n\r\n\r\n-------- MIN is not OK --------------------\r\nSELECT\r\n    CAST(5, 'SimpleAggregateFunction(min, UInt64)') AS x,\r\n    min(x) AS y,\r\n    toTypeName(y) AS type_y;\r\n┌─x─┬─y─┬─type_y───────────────────────────────┐\r\n│ 5 │ 5 │ SimpleAggregateFunction(min, UInt64) │\r\n└───┴───┴──────────────────────────────────────┘\r\n\r\n\r\n-------- MAX is not OK --------------------\r\nSELECT\r\n    CAST(5, 'SimpleAggregateFunction(max, UInt64)') AS x,\r\n    max(x) AS y,\r\n    toTypeName(y) AS type_y;\r\n┌─x─┬─y─┬─type_y───────────────────────────────┐\r\n│ 5 │ 5 │ SimpleAggregateFunction(max, UInt64) │\r\n└───┴───┴──────────────────────────────────────┘\r\n\r\n-------- ANY is not OK --------------------\r\nSELECT\r\n    CAST(5, 'SimpleAggregateFunction(any, UInt64)') AS x,\r\n    any(x) AS y,\r\n    toTypeName(y) AS type_y;\r\n┌─x─┬─y─┬─type_y───────────────────────────────┐\r\n│ 5 │ 5 │ SimpleAggregateFunction(any, UInt64) │\r\n└───┴───┴──────────────────────────────────────┘\r\n\r\n-------- ANYLAST is not OK --------------------\r\nSELECT\r\n    CAST(5, 'SimpleAggregateFunction(anyLast, UInt64)') AS x,\r\n    anyLast(x) AS y,\r\n    toTypeName(y) AS type_y;\r\n┌─x─┬─y─┬─type_y───────────────────────────────────┐\r\n│ 5 │ 5 │ SimpleAggregateFunction(anyLast, UInt64) │\r\n└───┴───┴──────────────────────────────────────────┘","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27668/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27667","id":970731662,"node_id":"MDU6SXNzdWU5NzA3MzE2NjI=","number":27667,"title":"ClickHouse inserted a partial block of data and it broke deduplication","user":{"login":"SaltTan","id":20357526,"node_id":"MDQ6VXNlcjIwMzU3NTI2","avatar_url":"https://avatars.githubusercontent.com/u/20357526?v=4","gravatar_id":"","url":"https://api.github.com/users/SaltTan","html_url":"https://github.com/SaltTan","followers_url":"https://api.github.com/users/SaltTan/followers","following_url":"https://api.github.com/users/SaltTan/following{/other_user}","gists_url":"https://api.github.com/users/SaltTan/gists{/gist_id}","starred_url":"https://api.github.com/users/SaltTan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SaltTan/subscriptions","organizations_url":"https://api.github.com/users/SaltTan/orgs","repos_url":"https://api.github.com/users/SaltTan/repos","events_url":"https://api.github.com/users/SaltTan/events{/privacy}","received_events_url":"https://api.github.com/users/SaltTan/received_events","type":"User","site_admin":false},"labels":[{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":21,"created_at":"2021-08-13T21:54:50Z","updated_at":"2021-12-16T16:40:29Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I was trying to implement deduplication of blocks, and as a test I killed clickhouse-client while it was inserting data from a TSV file using the `input` function.\r\nClickHouse inserted a partial block of data and it broke deduplication.\r\n```\r\nFirst (failed) attempt\r\n\r\n-- The first block of 103425 rows got inserted\r\n2021.08.13 20:03:19.943839 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_2044325925592136338_224670626381852826', 103425 rows\r\n2021.08.13 20:03:19.977457 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Trace> db.table: Renaming temporary part tmp_insert_20210813_211_211_0 to 20210813_424_424_0.\r\n\r\n-- Client has dropped the connection, CANCEL THE QUERY\r\n2021.08.13 20:03:20.717922 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Information> TCPHandler: Client has dropped the connection, cancel the query.\r\n\r\n-- However another block of 51715 rows got inserted anyway\r\n2021.08.13 20:03:21.265719 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_12200150366349168987_12358441307174369580', 51715 rows\r\n2021.08.13 20:03:21.296928 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Trace> db.table: Renaming temporary part tmp_insert_20210813_212_212_0 to 20210813_425_425_0.\r\n\r\n-- The total number of rows reported by the query is 155140 (two blocks of data)\r\n2021.08.13 20:03:21.510984 [ 30177 ] {33b2b70a-7e48-40e3-b431-d3580c5c6735} <Information> executeQuery: Read 155140 rows, 776.61 MiB in 4.751866542 sec., 32648 rows/sec., 163.43 MiB/sec.\r\n\r\n-- Retry\r\n\r\n-- The block of 103425 rows got deduplicated\r\n2021.08.13 20:45:05.817280 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_2044325925592136338_224670626381852826', 103425 rows\r\n2021.08.13 20:45:05.822455 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Information> db.table (Replicated OutputStream): Block with ID 20210813_2044325925592136338_224670626381852826 already exists locally as part 20210813_424_424_0; ignoring it.\r\n\r\n-- The second block is 103414 rows, not 51715 rows, so it gets inserted successfully\r\n2021.08.13 20:45:09.054894 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_2425217170151591480_6106430219846770241', 103414 rows\r\n2021.08.13 20:45:09.073571 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Trace> db.table: Renaming temporary part tmp_insert_20210813_239_239_0 to 20210813_451_451_0.\r\n\r\n-- The third block\r\n2021.08.13 20:45:13.051942 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_16231016785560754596_9804537966436769992', 103379 rows\r\n2021.08.13 20:45:13.065374 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Trace> db.table: Renaming temporary part tmp_insert_20210813_240_240_0 to 20210813_452_452_0.\r\n\r\n-- The fourth block\r\n2021.08.13 20:45:16.813489 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_14978609919789150052_6694756734307136853', 103604 rows\r\n2021.08.13 20:45:16.822164 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Trace> db.table: Renaming temporary part tmp_insert_20210813_241_241_0 to 20210813_453_453_0.\r\n\r\n-- The fifth block\r\n2021.08.13 20:45:19.335465 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Debug> db.table (Replicated OutputStream): Wrote block with ID '20210813_9047090776261112264_1197278831556285907', 88696 rows\r\n2021.08.13 20:45:19.344424 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Trace> db.table: Renaming temporary part tmp_insert_20210813_242_242_0 to 20210813_454_454_0.\r\n\r\n-- The total number of rows reported by the query is 502518 (five blocks of data)\r\n2021.08.13 20:45:19.613576 [ 21749 ] {115638b0-8ee7-45a8-be6a-594f9a00e417} <Information> executeQuery: Read 502518 rows, 2.45 GiB in 17.397894281 sec., 28883 rows/sec., 144.36 MiB/sec.\r\n\r\n-- The number of rows in the table is 554233\r\nselect count() from db.table;\r\n\r\ncount()\r\n-------\r\n 554233\r\n\r\n554233 = 502518 (the second insert) + 51715 (the partial block from the first insert)\r\n\r\n-- The number of rows in the source file is 502518\r\n$ zcat 20210813200003-1.tsv.gz | wc -l\r\n502518\r\n```\r\n\r\nClickHouse client version 21.8.3.44 (official build).\r\nConnected to ClickHouse server version 21.8.3 revision 54449.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27667/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27653","id":970513780,"node_id":"MDExOlB1bGxSZXF1ZXN0NzEyNDU2MjM4","number":27653,"title":"Implement HasAll specialization for SSE and AVX2","user":{"login":"youennL-cs","id":87176914,"node_id":"MDQ6VXNlcjg3MTc2OTE0","avatar_url":"https://avatars.githubusercontent.com/u/87176914?v=4","gravatar_id":"","url":"https://api.github.com/users/youennL-cs","html_url":"https://github.com/youennL-cs","followers_url":"https://api.github.com/users/youennL-cs/followers","following_url":"https://api.github.com/users/youennL-cs/following{/other_user}","gists_url":"https://api.github.com/users/youennL-cs/gists{/gist_id}","starred_url":"https://api.github.com/users/youennL-cs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/youennL-cs/subscriptions","organizations_url":"https://api.github.com/users/youennL-cs/orgs","repos_url":"https://api.github.com/users/youennL-cs/repos","events_url":"https://api.github.com/users/youennL-cs/events{/privacy}","received_events_url":"https://api.github.com/users/youennL-cs/received_events","type":"User","site_admin":false},"labels":[{"id":1304141686,"node_id":"MDU6TGFiZWwxMzA0MTQxNjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-improvement","name":"pr-improvement","color":"007700","default":false,"description":"Pull request with some product improvements"},{"id":3627025164,"node_id":"LA_kwDOA5dJV87YMAMM","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/can%20be%20tested","name":"can be tested","color":"1d76db","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":19,"created_at":"2021-08-13T15:36:25Z","updated_at":"2022-01-28T19:48:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/27653","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27653","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/27653.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/27653.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\n_Changelog category_:\r\n- Improvement\r\n\r\n\r\n_Changelog entry (a user-readable short description of the changes that goes to CHANGELOG.md)_:\r\nAs talked in [issue 27025](https://github.com/ClickHouse/ClickHouse/issues/27025), there is an improvement of the HasAll function using SIMD instruction (SSE and AVX2).\r\nGtest tests have also been added.\r\n\r\n\r\n_Detailed description / Documentation draft_:\r\nIt still require some discussion, mainly about copy/paste code. Should I continue to copy paste (int and unsigned int are strictly identical but cast is impossible)\r\nSecond thing to discuss is I fail to execute with AVX2. I follow what exist, it compiles and all but doesn't execute on my machine even if I can run AVX2 instructions. I missed something but I don't know what. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27653/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27651","id":970501975,"node_id":"MDU6SXNzdWU5NzA1MDE5NzU=","number":27651,"title":"librdkafka use `system` function from libc ","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-08-13T15:25:47Z","updated_at":"2021-12-21T07:20:58Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Harmful check fails for librdkafka\r\n```\r\n2021.08.13 09:24:10.499196 [ 179 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.08.13 09:24:10.500177 [ 179 ] {} <Fatal> BaseDaemon: (version 21.9.1.7754 (official build), build id: AA047EDD4FDBABD43D2D23351D89438384B3C4CF) (from thread 10) (query_id: 3a077dac-da31-44f3-a3f2-f27b94603d03) Received signal Illegal instruction (4)\r\n2021.08.13 09:24:10.500753 [ 179 ] {} <Fatal> BaseDaemon: Illegal operand.\r\n2021.08.13 09:24:10.501364 [ 179 ] {} <Fatal> BaseDaemon: Stack trace: 0x136179fd 0x23a09a06 0x23a088b6 0x239c90d3 0x238be36c 0x2386db63 0x1eed6cba 0x1eed6c40 0x1eed6a00 0x1eed6684 0x1eed64b2 0x1eec8091 0x1eebc5d5 0x1eebbdc1 0x1df556dd 0x1df527bb 0x1df56bcc 0x1e992ef0 0x1e991085 0x1f54575c 0x1f552aa8 0x23c03cbc 0x23c04544 0x23d5eb23 0x23d5b3dd 0x23d5a168 0x7effb7910609 0x7effb7826293\r\n2021.08.13 09:24:10.505590 [ 179 ] {} <Fatal> BaseDaemon: 4. ./obj-x86_64-linux-gnu/../base/harmful/harmful.c:220: system @ 0x136179fd in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.580346 [ 179 ] {} <Fatal> BaseDaemon: 5. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl_cyrus.c:226: rd_kafka_sasl_cyrus_kinit_refresh @ 0x23a09a06 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.652059 [ 179 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl_cyrus.c:572: rd_kafka_sasl_cyrus_init @ 0x23a088b6 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.723386 [ 179 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka_sasl.c:315: rd_kafka_sasl_init @ 0x239c90d3 in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.780824 [ 179 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/librdkafka/src/rdkafka.c:2280: rd_kafka_new @ 0x238be36c in /usr/bin/clickhouse\r\n2021.08.13 09:24:10.927985 [ 179 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../contrib/cppkafka/src/consumer.cpp:65: cppkafka::Consumer::Consumer(cppkafka::Configuration) @ 0x2386db63 in /usr/bin/clickhouse\r\n2021.08.13 09:24:11.691891 [ 179 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886: void std::__1::allocator<cppkafka::Consumer>::construct<cppkafka::Consumer, cppkafka::Configuration&>(cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6cba in /usr/bin/clickhouse\r\n2021.08.13 09:24:12.452286 [ 179 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:521: void std::__1::allocator_traits<std::__1::allocator<cppkafka::Consumer> >::__construct<cppkafka::Consumer, cppkafka::Configuration&>(std::__1::integral_constant<bool, true>, std::__1::allocator<cppkafka::Consumer>&, cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6c40 in /usr/bin/clickhouse\r\n2021.08.13 09:24:13.211423 [ 179 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:482: void std::__1::allocator_traits<std::__1::allocator<cppkafka::Consumer> >::construct<cppkafka::Consumer, cppkafka::Configuration&>(std::__1::allocator<cppkafka::Consumer>&, cppkafka::Consumer*, cppkafka::Configuration&) @ 0x1eed6a00 in /usr/bin/clickhouse\r\n2021.08.13 09:24:13.973190 [ 179 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2594: std::__1::__shared_ptr_emplace<cppkafka::Consumer, std::__1::allocator<cppkafka::Consumer> >::__shared_ptr_emplace<cppkafka::Configuration&>(std::__1::allocator<cppkafka::Consumer>, cppkafka::Configuration&) @ 0x1eed6684 in /usr/bin/clickhouse\r\n2021.08.13 09:24:14.756760 [ 179 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3360: std::__1::shared_ptr<cppkafka::Consumer> std::__1::allocate_shared<cppkafka::Consumer, std::__1::allocator<cppkafka::Consumer>, cppkafka::Configuration&, void>(std::__1::allocator<cppkafka::Consumer> const&, cppkafka::Configuration&) @ 0x1eed64b2 in /usr/bin/clickhouse\r\n2021.08.13 09:24:15.492782 [ 179 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3369: std::__1::shared_ptr<cppkafka::Consumer> std::__1::make_shared<cppkafka::Consumer, cppkafka::Configuration&, void>(cppkafka::Configuration&) @ 0x1eec8091 in /usr/bin/clickhouse\r\n2021.08.13 09:24:16.197138 [ 179 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Storages/Kafka/StorageKafka.cpp:430: DB::StorageKafka::createReadBuffer(unsigned long) @ 0x1eebc5d5 in /usr/bin/clickhouse\r\n2021.08.13 09:24:16.889068 [ 179 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Storages/Kafka/StorageKafka.cpp:309: DB::StorageKafka::startup() @ 0x1eebbdc1 in /usr/bin/clickhouse\r\n2021.08.13 09:24:17.678482 [ 179 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:1077: DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&) @ 0x1df556dd in /usr/bin/clickhouse\r\n2021.08.13 09:24:18.464234 [ 179 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:937: DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) @ 0x1df527bb in /usr/bin/clickhouse\r\n2021.08.13 09:24:19.255921 [ 179 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:0: DB::InterpreterCreateQuery::execute() @ 0x1df56bcc in /usr/bin/clickhouse\r\n2021.08.13 09:24:19.896872 [ 179 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:565: DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) @ 0x1e992ef0 in /usr/bin/clickhouse\r\n2021.08.13 09:24:20.575817 [ 179 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:926: DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool) @ 0x1e991085 in /usr/bin/clickhouse\r\n2021.08.13 09:24:21.290223 [ 179 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:335: DB::TCPHandler::runImpl() @ 0x1f54575c in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.039072 [ 179 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1670: DB::TCPHandler::run() @ 0x1f552aa8 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.112692 [ 179 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43: Poco::Net::TCPServerConnection::start() @ 0x23c03cbc in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.221010 [ 179 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x23c04544 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.322408 [ 179 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199: Poco::PooledThread::run() @ 0x23d5eb23 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.417239 [ 179 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x23d5b3dd in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.510947 [ 179 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345: Poco::ThreadImpl::runnableEntry(void*) @ 0x23d5a168 in /usr/bin/clickhouse\r\n2021.08.13 09:24:22.511385 [ 179 ] {} <Fatal> BaseDaemon: 30. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.08.13 09:24:22.511691 [ 179 ] {} <Fatal> BaseDaemon: 31. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.08.13 09:24:23.823986 [ 179 ] {} <Fatal> BaseDaemon: Checksum of the binary: 55DDC1145AD182E891203722D68048BB, integrity check passed.\r\n2021.08.13 09:24:32.266474 [ 1 ] {} <Fatal> Application: Child process was terminated by signal 4.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27651/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27611","id":968554129,"node_id":"MDExOlB1bGxSZXF1ZXN0NzEwNjYxNjgw","number":27611,"title":"Aggregator enable compilation with no key","user":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"labels":[{"id":1876999755,"node_id":"MDU6TGFiZWwxODc2OTk5NzU1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-not-for-changelog","name":"pr-not-for-changelog","color":"007700","default":false,"description":"This PR should not be mentioned in the changelog"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-08-12T10:38:44Z","updated_at":"2022-01-24T11:51:30Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/27611","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27611","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/27611.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/27611.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- Not for changelog (changelog entry is not required)","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27611/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27570","id":966831699,"node_id":"MDU6SXNzdWU5NjY4MzE2OTk=","number":27570,"title":"RBAC: new testing version 21.9 is going to change what privileges needed to insert into a tables that is used by one or more Materialized Views. Is it intentional?","user":{"login":"vzakaznikov","id":41681088,"node_id":"MDQ6VXNlcjQxNjgxMDg4","avatar_url":"https://avatars.githubusercontent.com/u/41681088?v=4","gravatar_id":"","url":"https://api.github.com/users/vzakaznikov","html_url":"https://github.com/vzakaznikov","followers_url":"https://api.github.com/users/vzakaznikov/followers","following_url":"https://api.github.com/users/vzakaznikov/following{/other_user}","gists_url":"https://api.github.com/users/vzakaznikov/gists{/gist_id}","starred_url":"https://api.github.com/users/vzakaznikov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vzakaznikov/subscriptions","organizations_url":"https://api.github.com/users/vzakaznikov/orgs","repos_url":"https://api.github.com/users/vzakaznikov/repos","events_url":"https://api.github.com/users/vzakaznikov/events{/privacy}","received_events_url":"https://api.github.com/users/vzakaznikov/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1507867504,"node_id":"MDU6TGFiZWwxNTA3ODY3NTA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-hold","name":"st-hold","color":"e5b890","default":false,"description":"We've paused the work on issue for some reason"},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false},"assignees":[{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2021-08-11T14:30:54Z","updated_at":"2021-10-18T14:03:01Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\n21.9 breaks privileges needed to insert into a tables that used by one or more Materialized Views.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\n21.9 (master)\r\n\r\n**How to reproduce**\r\n\r\nAdd MV over some source table and try to insert into it with the user that only has INSERT privilege for source table and check\r\nthat an ACCESS_DENIED error is returned.\r\n\r\n**Expected behavior**\r\n\r\nPrivileges needed to insert into a source table shall not be affected by the presence or absence of any\r\nnumber of materialized views.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27570/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27559","id":966480514,"node_id":"MDU6SXNzdWU5NjY0ODA1MTQ=","number":27559,"title":"Is there any effective solution for ClickHouse to support GDPR?","user":{"login":"fsgdgsdfgsgd","id":29115270,"node_id":"MDQ6VXNlcjI5MTE1Mjcw","avatar_url":"https://avatars.githubusercontent.com/u/29115270?v=4","gravatar_id":"","url":"https://api.github.com/users/fsgdgsdfgsgd","html_url":"https://github.com/fsgdgsdfgsgd","followers_url":"https://api.github.com/users/fsgdgsdfgsgd/followers","following_url":"https://api.github.com/users/fsgdgsdfgsgd/following{/other_user}","gists_url":"https://api.github.com/users/fsgdgsdfgsgd/gists{/gist_id}","starred_url":"https://api.github.com/users/fsgdgsdfgsgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fsgdgsdfgsgd/subscriptions","organizations_url":"https://api.github.com/users/fsgdgsdfgsgd/orgs","repos_url":"https://api.github.com/users/fsgdgsdfgsgd/repos","events_url":"https://api.github.com/users/fsgdgsdfgsgd/events{/privacy}","received_events_url":"https://api.github.com/users/fsgdgsdfgsgd/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-08-11T11:03:40Z","updated_at":"2021-08-11T14:27:12Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In order to meet GDPR requirements, we want to enable users to choose whether to delete their personal data. We have tried several solutions and I will share them below.\r\n\r\nSolution 1: Use CollapsingMergeTree engine\r\nWe normally use ReplicatedMergeTree as our table engine. When we switched the table engine to CollapsingMergeTree, we noticed that some of our query performance dropped very severely. And because the deletion process of CollapsingMergeTree is asynchronous, users will see inconsistent data.\r\n\r\nSolution 2: Use external dictionary\r\nIn this solution, we store user's personal data in another database. And we use dictionary to query personal data from another database.\r\nThe advantages of this solution is that if we do not need to query the columns related to the user's personal data, it will not affect the query performance. And it's easy to delete data from other databases.\r\n\r\nWe tested two data sources: Redis and ClickHouse. The layout of the dictionary is complex_key_direct.\r\n1) ClickHouse\r\nBecause ClickHouse is not good at deleting data, we use other tools to delete the data and then import it again. Because user's personal data is not large, this method is acceptable.\r\nBut ClickHouse has poor performance as a dictionary source in our case. Queries are very easy to time out.\r\n\r\n2) Redis\r\nThe performance of Redis can meet the requirements, but because the Redis cluster cannot be used as the dictionary source now (maybe because of some bugs, I raised another [issue](https://github.com/ClickHouse/ClickHouse/issues/27123) to discuss it), so this solution has also been abandoned. \r\n\r\nSolution 3: Use ALTER TABLE … DELETE Statement\r\nThis deletion is also asynchronous and we found that when we want to delete a large amount of data, the performance of ClickHouse will drop drastically.\r\n\r\nSolution 4: Use JOIN clause\r\nIn this solution, we store the user's personal data in another table and use JOIN clause to query personal data if needed.\r\nThe performance of this solution is barely possible. We may try some optimizations to see if we can improve performance.\r\n\r\nForgot to mention. We have terabytes of data. And our analysis system is interactive, so the query cannot execute too long.\r\n\r\nDoes anyone have any good suggestions? It would be really helpful.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27559/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27553","id":966099340,"node_id":"MDU6SXNzdWU5NjYwOTkzNDA=","number":27553,"title":"TTL Expired Rows in old partition is not deleted","user":{"login":"roanhe-ts","id":42906151,"node_id":"MDQ6VXNlcjQyOTA2MTUx","avatar_url":"https://avatars.githubusercontent.com/u/42906151?v=4","gravatar_id":"","url":"https://api.github.com/users/roanhe-ts","html_url":"https://github.com/roanhe-ts","followers_url":"https://api.github.com/users/roanhe-ts/followers","following_url":"https://api.github.com/users/roanhe-ts/following{/other_user}","gists_url":"https://api.github.com/users/roanhe-ts/gists{/gist_id}","starred_url":"https://api.github.com/users/roanhe-ts/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/roanhe-ts/subscriptions","organizations_url":"https://api.github.com/users/roanhe-ts/orgs","repos_url":"https://api.github.com/users/roanhe-ts/repos","events_url":"https://api.github.com/users/roanhe-ts/events{/privacy}","received_events_url":"https://api.github.com/users/roanhe-ts/received_events","type":"User","site_admin":false},"labels":[{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2021-08-11T07:09:06Z","updated_at":"2021-10-25T10:05:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> A clear and concise description of what works not as it is supposed to.\r\n\r\nClickhouse version:\r\n```bash\r\nSELECT version()\r\n\r\nQuery id: bf1f155a-953c-496a-a4a5-ed9b99735028\r\n\r\n┌─version()─┐\r\n│ 21.3.3.14 │\r\n└───────────┘\r\n```\r\n\r\nExpired Rows in very old partition is not delete. Today: 2021-08-11\r\n\r\n```bash\r\nSELECT\r\n    table,\r\n    partition,\r\n    name,\r\n    delete_ttl_info_min,\r\n    delete_ttl_info_max\r\nFROM system.parts\r\nWHERE (partition = '20210725') OR (partition = '20210726')\r\n\r\nQuery id: 79fb0beb-1735-410c-8b95-5ce32b46db51\r\n\r\n┌─table─────────────────┬─partition─┬─name─────────────────────────────────────────────────────────┬─delete_ttl_info_min─┬─delete_ttl_info_max─┐\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6184687_6188358_8_6972051   │ 2021-08-10 00:00:00 │ 2021-08-10 01:51:41 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6188359_6191411_5_6972051   │ 2021-08-10 00:02:26 │ 2021-08-10 05:49:54 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6191412_6194180_5_6972051   │ 2021-08-10 00:01:02 │ 2021-08-10 08:58:35 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6194181_6196941_5_6972051   │ 2021-08-10 00:10:52 │ 2021-08-10 10:49:57 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6196942_6199724_5_6972051   │ 2021-08-10 00:06:38 │ 2021-08-10 12:25:40 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6199725_6202296_5_6972051   │ 2021-08-10 00:14:56 │ 2021-08-10 13:44:44 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6202297_6204987_5_6972051   │ 2021-08-10 00:03:44 │ 2021-08-10 15:05:23 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6204988_6214379_6_6972051   │ 2021-08-10 00:00:02 │ 2021-08-10 19:33:15 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6214380_6217360_5_6972051   │ 2021-08-10 00:47:59 │ 2021-08-10 20:22:13 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6217361_6227171_6_6972051   │ 2021-08-10 00:17:31 │ 2021-08-10 23:00:40 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6227172_6229901_5_6972051   │ 2021-08-10 04:26:29 │ 2021-08-10 23:59:59 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6229903_6248927_520_6972051 │ 2021-08-10 05:03:44 │ 2021-08-10 23:59:59 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6248929_6329771_168_6972051 │ 2021-08-10 00:26:29 │ 2021-08-10 23:58:44 │\r\n│ access_log_test_local │ 20210726  │ 4bbbe7974b17cca232d56a19deb2ed25_6333994_6333994_0_6972051   │ 2021-08-10 20:46:31 │ 2021-08-10 20:46:31 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6136323_6139967_9_6972051   │ 2021-08-09 00:00:00 │ 2021-08-09 01:42:37 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6139968_6143051_5_6972051   │ 2021-08-09 00:00:00 │ 2021-08-09 05:22:27 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6143052_6145831_5_6972051   │ 2021-08-09 00:02:48 │ 2021-08-09 08:41:07 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6145833_6149139_5_6972051   │ 2021-08-09 00:00:01 │ 2021-08-09 10:47:12 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6149140_6151806_5_6972051   │ 2021-08-09 00:03:45 │ 2021-08-09 12:12:36 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6151807_6154454_5_6972051   │ 2021-08-09 00:02:03 │ 2021-08-09 13:34:54 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6154455_6157080_5_6972051   │ 2021-08-09 02:23:38 │ 2021-08-09 14:49:13 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6157081_6166931_6_6972051   │ 2021-08-09 00:08:03 │ 2021-08-09 18:44:24 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6166932_6169657_5_6972051   │ 2021-08-09 00:12:13 │ 2021-08-09 19:39:18 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6169658_6179707_6_6972051   │ 2021-08-09 00:49:36 │ 2021-08-09 22:20:10 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6179708_6182596_5_6972051   │ 2021-08-09 00:24:54 │ 2021-08-09 23:12:44 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6184743_6184848_2_6972051   │ 2021-08-09 21:59:10 │ 2021-08-09 23:59:59 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6184850_6187747_77_6972051  │ 2021-08-09 08:01:42 │ 2021-08-09 23:59:59 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6187749_6287772_274_6972051 │ 2021-08-09 01:40:27 │ 2021-08-09 23:59:59 │\r\n│ access_log_test_local │ 20210725  │ f6eecf6d607ddbc8ec23d988c3e9195b_6296709_6296709_0_6972051   │ 2021-08-09 22:49:46 │ 2021-08-09 22:49:46 │\r\n└───────────────────────┴───────────┴──────────────────────────────────────────────────────────────┴─────────────────────┴─────────────────────┘\r\n\r\n\r\nselect count(*) from htest.access_log_test_local where UXTime < toDateTime('2021-07-27 00:00:00', 'Asia/Shanghai')\r\n\r\nSELECT count(*)\r\nFROM htest.access_log_test_local\r\nWHERE UXTime < toDateTime('2021-07-27 00:00:00', 'Asia/Shanghai')\r\n\r\nQuery id: 5bda362d-abab-45bc-9923-78ffcd19feb8\r\n\r\n┌─────count()─┐\r\n│ 13277438516 │\r\n└─────────────┘\r\n\r\n1 rows in set. Elapsed: 1.258 sec. Processed 13.28 billion rows, 53.11 GB (10.56 billion rows/s., 42.23 GB/s.) \r\n```\r\nExpired Rows is delete after I run `optimize table htest.access_log_test_local partition 20210726` manually\r\n\r\n```bash\r\nSELECT count(*)\r\nFROM htest.access_log_test_local\r\nWHERE UXTime < toDateTime('2021-07-27 00:00:00', 'Asia/Shanghai')\r\n\r\nQuery id: cec65576-e3a9-4ed2-866f-273d4d540766\r\n\r\n┌─count()─┐\r\n│       0 │\r\n└─────────┘\r\n\r\n1 rows in set. Elapsed: 0.018 sec. \r\n```\r\n\r\n**Expected behavior**\r\n\r\nExpired Rows is deleted from table.\r\n\r\n**Additional context**\r\n\r\n__Maybe parts in old partition that has not been written for a long time are not selected by merge algorithm?__\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27553/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27509","id":964482448,"node_id":"MDU6SXNzdWU5NjQ0ODI0NDg=","number":27509,"title":"Add build with AVX2 in CI.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":521113272,"node_id":"MDU6TGFiZWw1MjExMTMyNzI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/build","name":"build","color":"800000","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-10T00:07:13Z","updated_at":"2022-01-28T13:36:22Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"It is needed for evalation.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27509/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27502","id":964417245,"node_id":"MDU6SXNzdWU5NjQ0MTcyNDU=","number":27502,"title":"DROP/MODIFY COLUMN for compact part memory usage for tables with thousands of columns.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1532016596,"node_id":"MDU6TGFiZWwxNTMyMDE2NTk2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/memory","name":"memory","color":"e99695","default":false,"description":"When memory usage is higher than expected"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2021-08-09T21:50:22Z","updated_at":"2021-08-16T00:31:08Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"If you have really wide table with many thousands of columns and you want to drop / modify column.\r\nFor wide parts it working fine, but for compact clickhouse would try to allocate a lot of memory and going to be killed via OOM killer.\r\n\r\n\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes, 21.9\r\n\r\n**How to reproduce**\r\n```\r\nclickhouse-client -mn --query=\"SELECT 'CREATE TABLE xxx_really_wide( ' || arrayStringConcat(groupArray('column_'|| toString(number) || ' Nullable(UInt32)'), ',') || ') ENGINE=MergeTree ORDER BY assumeNotNull(column_0)' FROM numbers(6000) FORMAT TSVRaw\" | clickhouse-client -mn\r\n\r\nclickhouse-client -mn --query=\"SELECT arrayStringConcat(replicate('1', range(6000)), ',') FROM numbers(300) FORMAT TSVRaw\" | clickhouse-client -mn --query \"INSERT INTO xxx_really_wide FORMAT CSV \" --send_logs_level='trace'\r\n```\r\n\r\n```\r\nSELECT *\r\nFROM system.merges\r\nFORMAT Vertical\r\n\r\nQuery id: d9751847-a689-49f0-aab3-f868d9c08e34\r\n\r\nRow 1:\r\n──────\r\ndatabase:                    default\r\ntable:                       xxx_really_wide\r\nelapsed:                     84.8952157\r\nprogress:                    0\r\nnum_parts:                   1\r\nsource_part_names:           ['all_1_1_0']\r\nresult_part_name:            all_1_1_0_3\r\nsource_part_paths:           ['/var/lib/clickhouse/data/default/xxx_really_wide/all_1_1_0/']\r\nresult_part_path:            /var/lib/clickhouse/data/default/xxx_really_wide/all_1_1_0_3/\r\npartition_id:                all\r\nis_mutation:                 1\r\ntotal_size_bytes_compressed: 456027\r\ntotal_size_marks:            2\r\nbytes_read_uncompressed:     0\r\nrows_read:                   0\r\nbytes_written_uncompressed:  0\r\nrows_written:                0\r\ncolumns_written:             0\r\nmemory_usage:                16836533272\r\nthread_id:                   16171\r\nmerge_type:\r\nmerge_algorithm:\r\n\r\nRow 2:\r\n──────\r\ndatabase:                    default\r\ntable:                       xxx_really_wide\r\nelapsed:                     84.8947852\r\nprogress:                    0\r\nnum_parts:                   1\r\nsource_part_names:           ['all_2_2_0']\r\nresult_part_name:            all_2_2_0_3\r\nsource_part_paths:           ['/var/lib/clickhouse/data/default/xxx_really_wide/all_2_2_0/']\r\nresult_part_path:            /var/lib/clickhouse/data/default/xxx_really_wide/all_2_2_0_3/\r\npartition_id:                all\r\nis_mutation:                 1\r\ntotal_size_bytes_compressed: 468027\r\ntotal_size_marks:            2\r\nbytes_read_uncompressed:     0\r\nrows_read:                   0\r\nbytes_written_uncompressed:  0\r\nrows_written:                0\r\ncolumns_written:             0\r\nmemory_usage:                16706294896\r\nthread_id:                   16174\r\nmerge_type:\r\nmerge_algorithm:\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27502/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27482","id":964112700,"node_id":"MDU6SXNzdWU5NjQxMTI3MDA=","number":27482,"title":"system.errors add is_retriable column.","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-09T15:25:44Z","updated_at":"2021-08-10T20:39:22Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"When an error is an obvious failure, which is not related to some temporary condition, the next retry doesn't have any sense.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27482/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27450","id":963609074,"node_id":"MDU6SXNzdWU5NjM2MDkwNzQ=","number":27450,"title":"Connection to the remote server failure","user":{"login":"chasingegg","id":18375889,"node_id":"MDQ6VXNlcjE4Mzc1ODg5","avatar_url":"https://avatars.githubusercontent.com/u/18375889?v=4","gravatar_id":"","url":"https://api.github.com/users/chasingegg","html_url":"https://github.com/chasingegg","followers_url":"https://api.github.com/users/chasingegg/followers","following_url":"https://api.github.com/users/chasingegg/following{/other_user}","gists_url":"https://api.github.com/users/chasingegg/gists{/gist_id}","starred_url":"https://api.github.com/users/chasingegg/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/chasingegg/subscriptions","organizations_url":"https://api.github.com/users/chasingegg/orgs","repos_url":"https://api.github.com/users/chasingegg/repos","events_url":"https://api.github.com/users/chasingegg/events{/privacy}","received_events_url":"https://api.github.com/users/chasingegg/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-09T03:23:24Z","updated_at":"2021-08-09T03:23:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\r\n**Describe the unexpected behaviour**\r\nConnect to the ClickHouse through remote server\r\n\r\n**How to reproduce**\r\n* 21.5.1.1 version\r\n* change config.xml by set **<listen_host>::</listen_host>**\r\n* echo 'SELECT 1' | curl 'http://host:8123' -d @-\r\n\r\n**Expected behavior**\r\nSuccessfully connect to the database and return 1 in the console.\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021.08.09 11:21:03.431740 [ 2199543 ] {} <Error> Access(user directories): default: Authentication failed: Code: 195, e.displayText() = DB::Exception: Connections from ::ffff:xx.xx.xx.xx are not allowed, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/exception:133: Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11175f9c in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n1. /home/gaochao.tang/ClickHouse/build/../src/Common/Exception.cpp:57: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8bfb4da in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n2. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:0: DB::IAccessStorage::throwAddressNotAllowed(Poco::Net::IPAddress const&) @ 0xe51c7a8 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n3. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:454: DB::IAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0xe51c6e8 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n4. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:428: DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0xe51c2ac in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n5. /home/gaochao.tang/ClickHouse/build/../src/Access/MultipleAccessStorage.cpp:414: DB::MultipleAccessStorage::loginImpl(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&) const @ 0xe539513 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n6. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:428: DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0xe51c2ac in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n7. /home/gaochao.tang/ClickHouse/build/../src/Interpreters/Context.cpp:0: DB::Context::setUser(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0xe865607 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n8. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/memory:1655: DB::HTTPHandler::authenticateUser(std::__1::shared_ptr<DB::Context>, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0xf3edb14 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n9. /home/gaochao.tang/ClickHouse/build/../src/Server/HTTPHandler.cpp:455: DB::HTTPHandler::processQuery(std::__1::shared_ptr<DB::Context>, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) @ 0xf3ef5bf in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n10. /home/gaochao.tang/ClickHouse/build/../src/Server/HTTPHandler.cpp:911: DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0xf3f5181 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n11. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/include/Poco/AutoPtr.h:215: DB::HTTPServerConnection::run() @ 0xf44616f in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n12. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x110f3117 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n13. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/atomic:1072: Poco::Net::TCPServerDispatcher::run() @ 0x110f3510 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n14. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/src/ThreadPool.cpp:213: Poco::PooledThread::run() @ 0x111d1467 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n15. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::ThreadImpl::runnableEntry(void*) @ 0x111cf036 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n16. start_thread @ 0x74a4 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n17. __clone @ 0xe8d0f in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 21.5.1.1)\r\n2021.08.09 11:21:03.431916 [ 2199543 ] {} <Error> DynamicQueryHandler: Code: 516, e.displayText() = DB::Exception: default: Authentication failed: password is incorrect or there is no user with such name, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/exception:133: Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x11175f9c in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n1. /home/gaochao.tang/ClickHouse/build/../src/Common/Exception.cpp:57: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8bfb4da in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n2. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:0: DB::IAccessStorage::throwCannotAuthenticate(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xe51c4d2 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n3. /home/gaochao.tang/ClickHouse/build/../src/Access/IAccessStorage.cpp:433: DB::IAccessStorage::login(DB::Credentials const&, Poco::Net::IPAddress const&, DB::ExternalAuthenticators const&, bool) const @ 0xe51c321 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n4. /home/gaochao.tang/ClickHouse/build/../src/Interpreters/Context.cpp:0: DB::Context::setUser(DB::Credentials const&, Poco::Net::SocketAddress const&) @ 0xe865607 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n5. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/memory:1655: DB::HTTPHandler::authenticateUser(std::__1::shared_ptr<DB::Context>, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&) @ 0xf3edb14 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n6. /home/gaochao.tang/ClickHouse/build/../src/Server/HTTPHandler.cpp:455: DB::HTTPHandler::processQuery(std::__1::shared_ptr<DB::Context>, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) @ 0xf3ef5bf in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n7. /home/gaochao.tang/ClickHouse/build/../src/Server/HTTPHandler.cpp:911: DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0xf3f5181 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n8. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/include/Poco/AutoPtr.h:215: DB::HTTPServerConnection::run() @ 0xf44616f in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n9. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x110f3117 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n10. /home/gaochao.tang/ClickHouse/build/../contrib/libcxx/include/atomic:1072: Poco::Net::TCPServerDispatcher::run() @ 0x110f3510 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n11. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/src/ThreadPool.cpp:213: Poco::PooledThread::run() @ 0x111d1467 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n12. /home/gaochao.tang/ClickHouse/build/../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::ThreadImpl::runnableEntry(void*) @ 0x111cf036 in /data00/home/gaochao.tang/ClickHouse/build/programs/clickhouse\r\n13. start_thread @ 0x74a4 in /lib/x86_64-linux-gnu/libpthread-2.24.so\r\n14. __clone @ 0xe8d0f in /lib/x86_64-linux-gnu/libc-2.24.so\r\n (version 21.5.1.1)\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27450/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27440","id":963521704,"node_id":"MDU6SXNzdWU5NjM1MjE3MDQ=","number":27440,"title":"Open datasets from Rapid7 internet surveys","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":3360106261,"node_id":"MDU6TGFiZWwzMzYwMTA2MjYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/dataset","name":"dataset","color":"00CCCC","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"assignees":[{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2021-08-08T21:50:36Z","updated_at":"2021-09-16T12:43:47Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://opendata.rapid7.com/","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27440/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27433","id":963499900,"node_id":"MDU6SXNzdWU5NjM0OTk5MDA=","number":27433,"title":"CREATE TABLE w/ Replicated engine should fail when ZK node removal is scheduled","user":{"login":"knoguchi","id":127546,"node_id":"MDQ6VXNlcjEyNzU0Ng==","avatar_url":"https://avatars.githubusercontent.com/u/127546?v=4","gravatar_id":"","url":"https://api.github.com/users/knoguchi","html_url":"https://github.com/knoguchi","followers_url":"https://api.github.com/users/knoguchi/followers","following_url":"https://api.github.com/users/knoguchi/following{/other_user}","gists_url":"https://api.github.com/users/knoguchi/gists{/gist_id}","starred_url":"https://api.github.com/users/knoguchi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/knoguchi/subscriptions","organizations_url":"https://api.github.com/users/knoguchi/orgs","repos_url":"https://api.github.com/users/knoguchi/repos","events_url":"https://api.github.com/users/knoguchi/events{/privacy}","received_events_url":"https://api.github.com/users/knoguchi/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-08T19:31:07Z","updated_at":"2021-08-08T21:59:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the issue**\r\n\r\nWhen you DROP & CREATE **same** replicated tables in short time period, the newly CREATE'd table will be unusable after 8 minutes.\r\n\r\nThe reason is known.  ZK node removal is asynchronous, and the zk node is removed after successful CREATE.  The delay is configurable by `database_atomic_delay_before_drop_table_sec` settings.   See discussion #23035\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n21.3.4.25\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n```\r\nCREATE TABLE default.tbl (\r\n    t DateTime\r\n    name String\r\n) \r\nENGINE=ReplicatedMergeTree('/clickhouse/tables/{layer}-{shard}/default.tbl', '{replica}')\r\nPARTITION BY toYYYYMMDD(t)\r\nORDER BY (t, name)\r\n```\r\n\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\nNo data is necessary.\r\n* Queries to run that lead to unexpected result\r\n```\r\nDROP TABLE tbl ON CLUSTER cluster_name\r\n```\r\nand run the create table again within 8min (default), check error log after 8min.\r\n\r\n**Expected behavior**\r\nIn my opinion CREATE should fail when the same zookeeper node is scheduled for removal.\r\nOr CREATE could cancel zk node removal if the zk node value is exactly same (primary key, order by, etc..)\r\n\r\nAlso it'd be nice to have scheduled removal time in one of the system tables for visibility.\r\n\r\n\r\n**Error message and/or stacktrace**\r\n```\r\n2021.08.08 18:31:43.199248 [ 15947 ] {} <Error> default.tbl (6772ad45-d3c6-4375-b78a-0166d390f323): void DB::StorageReplicatedMergeTree::queueUpdatingTask(): Code: 999, e.displayText() = Coordination::Exception: Can't get data for node /clickhouse/tables/1-005/default.tbl/replicas/1r005/log_pointer: node doesn't exist (No node), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error, int) @ 0xfbfabe3 in /usr/bin/clickhouse\r\n1. Coordination::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Error) @ 0xfbfae62 in /usr/bin/clickhouse\r\n2. zkutil::ZooKeeper::get(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Coordination::Stat*, std::__1::shared_ptr<Poco::Event> const&) @ 0xfc020f2 in /usr/bin/clickhouse\r\n3. DB::ReplicatedMergeTreeQueue::pullLogsToQueue(std::__1::shared_ptr<zkutil::ZooKeeper>, std::__1::function<void (Coordination::WatchResponse const&)>) @ 0xf7771a7 in /usr/bin/clickhouse\r\n4. DB::StorageReplicatedMergeTree::queueUpdatingTask() @ 0xf40c11a in /usr/bin/clickhouse\r\n5. DB::BackgroundSchedulePoolTaskInfo::execute() @ 0xeadfe80 in /usr/bin/clickhouse\r\n6. DB::BackgroundSchedulePool::threadFunction() @ 0xeae1e77 in /usr/bin/clickhouse\r\n7. ? @ 0xeae2c42 in /usr/bin/clickhouse\r\n8. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x865165f in /usr/bin/clickhouse\r\n9. ? @ 0x86550f3 in /usr/bin/clickhouse\r\n10. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n11. clone @ 0x12171f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n (version 21.3.4.25 (official build))\r\n```\r\n\r\n**Additional context**\r\n\r\n`SYNC` modifier for the DROP statement can avoid this problem but that's a side topic.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27433/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27381","id":963189885,"node_id":"MDU6SXNzdWU5NjMxODk4ODU=","number":27381,"title":"Fast remainder support","user":{"login":"sundy-li","id":3325189,"node_id":"MDQ6VXNlcjMzMjUxODk=","avatar_url":"https://avatars.githubusercontent.com/u/3325189?v=4","gravatar_id":"","url":"https://api.github.com/users/sundy-li","html_url":"https://github.com/sundy-li","followers_url":"https://api.github.com/users/sundy-li/followers","following_url":"https://api.github.com/users/sundy-li/following{/other_user}","gists_url":"https://api.github.com/users/sundy-li/gists{/gist_id}","starred_url":"https://api.github.com/users/sundy-li/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/sundy-li/subscriptions","organizations_url":"https://api.github.com/users/sundy-li/orgs","repos_url":"https://api.github.com/users/sundy-li/repos","events_url":"https://api.github.com/users/sundy-li/events{/privacy}","received_events_url":"https://api.github.com/users/sundy-li/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-08-07T09:00:02Z","updated_at":"2021-08-09T02:10:56Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Maybe it can help in `ModuloByConstantImpl`\r\n\r\nRefer to:\r\n\r\nBlog: https://lemire.me/blog/2019/02/08/faster-remainders-when-the-divisor-is-a-constant-beating-compilers-and-libdivide/\r\nPaper: https://arxiv.org/abs/1902.01961\r\nGo: https://github.com/bmkessler/fastdiv\r\nRust: https://docs.rs/strength_reduce/0.2.3/strength_reduce/","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381/reactions","total_count":3,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27381/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27272","id":962529689,"node_id":"MDU6SXNzdWU5NjI1Mjk2ODk=","number":27272,"title":"Memory exceeded with multiple JOINs","user":{"login":"wzisheng","id":87692633,"node_id":"MDQ6VXNlcjg3NjkyNjMz","avatar_url":"https://avatars.githubusercontent.com/u/87692633?v=4","gravatar_id":"","url":"https://api.github.com/users/wzisheng","html_url":"https://github.com/wzisheng","followers_url":"https://api.github.com/users/wzisheng/followers","following_url":"https://api.github.com/users/wzisheng/following{/other_user}","gists_url":"https://api.github.com/users/wzisheng/gists{/gist_id}","starred_url":"https://api.github.com/users/wzisheng/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/wzisheng/subscriptions","organizations_url":"https://api.github.com/users/wzisheng/orgs","repos_url":"https://api.github.com/users/wzisheng/repos","events_url":"https://api.github.com/users/wzisheng/events{/privacy}","received_events_url":"https://api.github.com/users/wzisheng/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-08-06T08:38:58Z","updated_at":"2021-08-06T09:14:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"v21.3.4.25-lts\r\n\r\ni have a sql like this:\r\n`select sum(e.count) from table1 a \r\njoin table2 b on a.id = b.id\r\njoin table3 c on c.id = b.id\r\njoin table4 d on d.id = b.id\r\njoin table5 e on e.id = b.id\r\nwhere a.c1 ... and b.c1 ... and c.c1... or d.c1 ... and e.c1...`\r\n\r\nTotal 5 talbes are less than 10GB, and table1 has 500 columns while the other 4 tables only have 5 columns\r\n\r\nwhen the above sql got `Code: 241. DB::Exception: Memory limit (total) exceeded: would use 56.62 GiB (attempt to allocate chunk of 268435456 bytes), maximum: 56.46 GiB`\r\nlooks like releated to #4904 \r\nwhy would this happen?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27272/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27269","id":962361658,"node_id":"MDU6SXNzdWU5NjIzNjE2NTg=","number":27269,"title":"GLOBAL IN with subqueries very slow","user":{"login":"DeanThompson","id":1628320,"node_id":"MDQ6VXNlcjE2MjgzMjA=","avatar_url":"https://avatars.githubusercontent.com/u/1628320?v=4","gravatar_id":"","url":"https://api.github.com/users/DeanThompson","html_url":"https://github.com/DeanThompson","followers_url":"https://api.github.com/users/DeanThompson/followers","following_url":"https://api.github.com/users/DeanThompson/following{/other_user}","gists_url":"https://api.github.com/users/DeanThompson/gists{/gist_id}","starred_url":"https://api.github.com/users/DeanThompson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DeanThompson/subscriptions","organizations_url":"https://api.github.com/users/DeanThompson/orgs","repos_url":"https://api.github.com/users/DeanThompson/repos","events_url":"https://api.github.com/users/DeanThompson/events{/privacy}","received_events_url":"https://api.github.com/users/DeanThompson/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"},{"id":1507860028,"node_id":"MDU6TGFiZWwxNTA3ODYwMDI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-repro","name":"st-need-repro","color":"e5b890","default":false,"description":"We were not able to reproduce the problem, please help us."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2021-08-06T03:25:11Z","updated_at":"2021-08-26T02:11:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the situation**\r\nExecuting queries against the distributed table, with a GLOBAL IN and subquery, the performance is unexpectedly slow.\r\n\r\n**How to reproduce**\r\n\r\nHaving a distributed table `t_table_all`, with about 1 billion rows in total.\r\n\r\n```\r\nSELECT count(1)\r\nFROM t_table_all\r\nWHERE category_id_3 GLOBAL IN (\r\n  SELECT category_id_3\r\n  FROM t_table_all\r\n  WHERE brand_name = '1234'\r\n    AND ((month >= '2020-01-01') AND (month <= '2021-06-01'))\r\n  GROUP BY category_id_3\r\n)\r\nAND ((month >= '2021-05-01') AND (month <= '2021-06-01'))\r\n```\r\n\r\nThis query takes 20s, but only 2s after replacing the subquery with constant values.\r\n\r\nWe are using 21.6.5 revision 54448, CentOS 7\r\n\r\nFound a post (in Chinese) related to this issue [https://mp.weixin.qq.com/s/Dv7N_td9t5RfhuymkQlPug](https://mp.weixin.qq.com/s/Dv7N_td9t5RfhuymkQlPug). The original author thinks that the subquery produces too many small blocks, he modified the source code of `addExternalStorage` to [use `SquashingTransform` to compact the small blocks](https://github.com/kindred77/ClickHouse/blob/21.3-squashblock-for-globalsubquery/src/Interpreters/GlobalSubqueriesVisitor.h#L154) and get huge performance improvement: ~1 hour to 6 seconds!","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27269/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27233","id":962010870,"node_id":"MDExOlB1bGxSZXF1ZXN0NzA0ODY0MzY2","number":27233,"title":"toDate(Time) with OrNull/OrZero support integer as argument","user":{"login":"zxc111","id":2883637,"node_id":"MDQ6VXNlcjI4ODM2Mzc=","avatar_url":"https://avatars.githubusercontent.com/u/2883637?v=4","gravatar_id":"","url":"https://api.github.com/users/zxc111","html_url":"https://github.com/zxc111","followers_url":"https://api.github.com/users/zxc111/followers","following_url":"https://api.github.com/users/zxc111/following{/other_user}","gists_url":"https://api.github.com/users/zxc111/gists{/gist_id}","starred_url":"https://api.github.com/users/zxc111/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zxc111/subscriptions","organizations_url":"https://api.github.com/users/zxc111/orgs","repos_url":"https://api.github.com/users/zxc111/repos","events_url":"https://api.github.com/users/zxc111/events{/privacy}","received_events_url":"https://api.github.com/users/zxc111/received_events","type":"User","site_admin":false},"labels":[{"id":1304141686,"node_id":"MDU6TGFiZWwxMzA0MTQxNjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-improvement","name":"pr-improvement","color":"007700","default":false,"description":"Pull request with some product improvements"},{"id":3627025164,"node_id":"LA_kwDOA5dJV87YMAMM","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/can%20be%20tested","name":"can be tested","color":"1d76db","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false},"assignees":[{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-08-05T16:47:17Z","updated_at":"2022-01-11T10:18:40Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/27233","html_url":"https://github.com/ClickHouse/ClickHouse/pull/27233","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/27233.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/27233.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- Improvement\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\ntoDate(Time) with OrNull/OrZero support integer as argument\r\n\r\nCloses #9584","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27233/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27232","id":961952697,"node_id":"MDU6SXNzdWU5NjE5NTI2OTc=","number":27232,"title":"How to handle duplicate records when doing data aggregation in Materialized Views","user":{"login":"ohecker","id":8004361,"node_id":"MDQ6VXNlcjgwMDQzNjE=","avatar_url":"https://avatars.githubusercontent.com/u/8004361?v=4","gravatar_id":"","url":"https://api.github.com/users/ohecker","html_url":"https://github.com/ohecker","followers_url":"https://api.github.com/users/ohecker/followers","following_url":"https://api.github.com/users/ohecker/following{/other_user}","gists_url":"https://api.github.com/users/ohecker/gists{/gist_id}","starred_url":"https://api.github.com/users/ohecker/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ohecker/subscriptions","organizations_url":"https://api.github.com/users/ohecker/orgs","repos_url":"https://api.github.com/users/ohecker/repos","events_url":"https://api.github.com/users/ohecker/events{/privacy}","received_events_url":"https://api.github.com/users/ohecker/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-08-05T15:37:19Z","updated_at":"2021-09-18T04:28:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"This question asks for proposed patterns to avoid problems with data aggregation in materialized views when there might be duplicates in the source table.\r\n\r\nThe following setup illustrates the problem (using ClickHouse 21.5):\r\n\r\nWe assume to have a raw data table which contains an entry for every video that a user has watched, including the timestamp.\r\n```\r\nCREATE DATABASE IF NOT EXISTS dedupmv;\r\n  \r\nCREATE TABLE dedupmv.user_videos (\r\n    user_id             UInt64,\r\n    video_id            UInt64,\r\n    viewed_at           DateTime\r\n)\r\nENGINE = ReplacingMergeTree()\r\nORDER BY (user_id, video_id, viewed_at);\r\n```\r\nEnties with same values for all attributes should be regarded as duplicates (maybe arising from some retries in preprocessing stages). Therefore we use ReplacingMergeTree to (eventually) consolidate (remove) them.\r\n\r\nTo provide a fast access to statistical data we define a table with preaggregated data (consumed videos per user and day) and define a MV to update it accordingly:\r\n\r\n```\r\nCREATE TABLE dedupmv.user_videos_daily (\r\n    user_id         UInt64,\r\n    view_date       Date,\r\n    video_count     UInt32\r\n)\r\nENGINE = SummingMergeTree()\r\nORDER BY (user_id, view_date);\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS dedupmv.user_videos_daily_mv\r\nTO dedupmv.user_videos_daily\r\nAS SELECT\r\n    user_id,\r\n    toDate(viewed_at) as view_date,\r\n    count(*) as video_count\r\nFROM dedupmv.user_videos\r\nGROUP BY (user_id, view_date)\r\nORDER BY (user_id, view_date);\r\n```\r\n\r\nWe insert some testdata into the raw data table:\r\n\r\n```\r\nINSERT INTO dedupmv.user_videos FORMAT Values\r\n    (1, 1, '2021-08-04 01:00:00'),\r\n    (1, 2, '2021-08-04 02:00:00'),\r\n    (1, 3, '2021-08-04 03:00:00'),\r\n    (1, 5, '2021-08-05 01:00:00'),\r\n    (2, 1, '2021-08-04 05:00:00');\r\n```\r\n\r\nWe might now query the raw table or the preaggregated data table to get the videos per user and day:\r\n\r\n```\r\nSELECT\r\n    user_id,\r\n    toDate(viewed_at) as view_date,\r\n    count(*) as video_count\r\nFROM dedupmv.user_videos\r\nGROUP BY (user_id, view_date)\r\nORDER BY (user_id, view_date)\r\n```\r\nor\r\n```\r\nSELECT\r\n    user_id,\r\n    view_date,\r\n    sum(video_count) as video_count\r\nFROM dedupmv.user_videos_daily\r\nGROUP BY (user_id, view_date)\r\nORDER BY (user_id, view_date)\r\n```\r\nboth will return the same (expected and desired) result:\r\n\r\nuser_id\t|view_date\t|video_count\r\n----|------|-----\r\n1\t|2021-08-04\t|3\r\n1\t|2021-08-05\t|1\r\n2\t|2021-08-04\t|1\r\n\r\nNow lets assume some duplicates are inserted into the raw data table (resulting from technical or operator error):\r\n```\r\nINSERT INTO dedupmv.user_videos FORMAT Values\r\n    (1, 3, '2021-08-04 03:00:00'),\r\n    (1, 5, '2021-08-05 01:00:00');\r\n```\r\nDepending on whether the ReplacingMergeTree engine of the raw data table has already consolidated the duplicates the above select statement on the raw table _might_ now show a different (unwanted) result which also counts the duplicates:\r\n\r\n|user_id\t|view_date\t|video_count\r\n|----|------|-----\r\n|1\t|2021-08-04\t|4\r\n|1\t|2021-08-05\t|2\r\n|2\t|2021-08-04\t|1\r\n\r\nBut in any case we might force the query to do deduplication by introducing the FINAL keyword:\r\n```\r\nSELECT\r\n    user_id,\r\n    toDate(viewed_at) as view_date,\r\n    count(*) as video_count\r\nFROM dedupmv.user_videos FINAL\r\nGROUP BY user_id, view_date\r\nORDER BY user_id, view_date\r\n```\r\nwhich leads to the desired result with duplicates not counted multiple times. (Alternatively we might force deduplication with \"OPTIMIZE ... FINAL\").\r\n\r\n_So when querying the statistics directly on the raw data table deduplication can be handled._\r\n\r\nBut when querying the number of videos per user and day on the preaggregated table the duplicates are always counted:\r\n```\r\nSELECT\r\n    user_id,\r\n    view_date,\r\n    sum(video_count) as video_count\r\nFROM dedupmv.user_videos_daily\r\nGROUP BY (user_id, view_date)\r\nORDER BY (user_id, view_date)\r\n```\r\nalways returns\r\n\r\n|user_id\t|view_date\t|video_count\r\n|----|------|-----\r\n|1\t|2021-08-04\t|4\r\n|1\t|2021-08-05\t|2\r\n|2\t|2021-08-04\t|1\r\n\r\nThis behavior is known and described and results from the fact that MVs act like insert triggers on the source table and do not have any knowledge on possible merge and deduplipcation actions on the source table.\r\n\r\nSo the question is: **Is there any proposed / best practice solution pattern to handle the duplicate issue when using MVs for data preaggregation as shown in this case?**\r\n\r\n\r\n\r\n \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27232/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27231","id":961916618,"node_id":"MDU6SXNzdWU5NjE5MTY2MTg=","number":27231,"title":"clickhouse-client in batch mode does not exit after network timeout","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-05T14:59:21Z","updated_at":"2021-11-30T06:47:02Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the issue**\r\n```\r\nclickhouse-client -n < 01949_big_regexp_hyperscan.sql \r\nTimeout exceeded while receiving data from server. Waited for 300 seconds, timeout is 300 seconds.\r\n^C\r\n```\r\n\r\n**Expected behavior**\r\nShould display exception and exit.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":2},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27231/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27230","id":961904070,"node_id":"MDU6SXNzdWU5NjE5MDQwNzA=","number":27230,"title":"Maximum parse depth (1000) exceeded on attempt to parse formatted query again","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2532986061,"node_id":"MDU6TGFiZWwyNTMyOTg2MDYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-query-syntax","name":"comp-query-syntax","color":"b5bcff","default":false,"description":"Relates to query parse / aliases resolution etc."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-08-05T14:46:30Z","updated_at":"2021-08-07T05:29:15Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```\r\nCREATE TABLE IF NOT EXISTS parse_depth_error on cluster test_shard_localhost\r\n(\r\n    timestamp DateTime('UTC') NOT NULL,\r\n    col_00 UInt32,\r\n    col_01 UInt32,\r\n    col_02 UInt32,\r\n    col_03 UInt32,\r\n    col_04 UInt32,\r\n    col_05 UInt32,\r\n    col_06 UInt32,\r\n    col_07 UInt32,\r\n    col_08 UInt32,\r\n    col_09 UInt32,\r\n    col_10 UInt32,\r\n    col_11 UInt32,\r\n    col_12 UInt32,\r\n    col_13 UInt32,\r\n    col_14 UInt32,\r\n    col_15 UInt32,\r\n    col_16 UInt32,\r\n    col_17 UInt32,\r\n    col_18 UInt32,\r\n    col_19 UInt32,\r\n    col_20 UInt32,\r\n    col_21 UInt32,\r\n    col_22 UInt32,\r\n    col_23 UInt32,\r\n    col_24 UInt32,\r\n    col_25 UInt32,\r\n    col_26 UInt32,\r\n    col_27 UInt32,\r\n    col_28 UInt32,\r\n    col_29 UInt32,\r\n    col_39 UInt32,\r\n    total UInt32 DEFAULT col_00 + col_01 + col_02 + col_03 + col_04 + col_05 +\r\n                         col_06 + col_07 + col_08 + col_09 + col_10 + col_11 +\r\n                         col_12 + col_13 + col_14 + col_15 + col_16 + col_17 +\r\n                         col_18 + col_19 + col_20 + col_21 + col_22 + col_23 +\r\n                         col_24 + col_25 + col_26 + col_27 + col_28 + col_29 +\r\n                         col_39\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(timestamp)\r\nORDER BY (timestamp);\r\n\r\nReceived exception from server (version 21.9.1):\r\nCode: 306. DB::Exception: Received from localhost:9000. DB::Exception: \r\nThere was an error on [localhost:9000]: Code: 306. DB::Exception: Maximum parse depth (1000) exceeded. \r\nConsider rising max_parser_depth parameter. (TOO_DEEP_RECURSION) (version 21.9.1.7603). (TOO_DEEP_RECURSION)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27230/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27229","id":961887627,"node_id":"MDU6SXNzdWU5NjE4ODc2Mjc=","number":27229,"title":"Compare queries with normalized formatting","user":{"login":"Nyoroon","id":182203,"node_id":"MDQ6VXNlcjE4MjIwMw==","avatar_url":"https://avatars.githubusercontent.com/u/182203?v=4","gravatar_id":"","url":"https://api.github.com/users/Nyoroon","html_url":"https://github.com/Nyoroon","followers_url":"https://api.github.com/users/Nyoroon/followers","following_url":"https://api.github.com/users/Nyoroon/following{/other_user}","gists_url":"https://api.github.com/users/Nyoroon/gists{/gist_id}","starred_url":"https://api.github.com/users/Nyoroon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Nyoroon/subscriptions","organizations_url":"https://api.github.com/users/Nyoroon/orgs","repos_url":"https://api.github.com/users/Nyoroon/repos","events_url":"https://api.github.com/users/Nyoroon/events{/privacy}","received_events_url":"https://api.github.com/users/Nyoroon/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-08-05T14:28:52Z","updated_at":"2021-08-06T05:56:23Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nI want to compare two queries with same semantics, but different format.\r\n\r\n**Describe the solution you'd like**\r\nFunction to normalize query format (like in `SHOW CREATE` statement) or get query hash after re-formatting.\r\nOr function to compare two queries.\r\n\r\n**Describe alternatives you've considered**\r\n`normalizeQuery` and `normalizedQueryHash`, but they don't \"fix\" formatting and can't be used to compare two queries with same semantics, but different formatting:\r\n```\r\nSELECT normalizedQueryHash('select 1') = normalizedQueryHash('SELECT 1')\r\n\r\n┌─equals(normalizedQueryHash('select 1'), normalizedQueryHash('SELECT 1'))─┐\r\n│                                                                        0 │\r\n└──────────────────────────────────────────────────────────────────────────┘\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27229/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/27223","id":961755550,"node_id":"MDU6SXNzdWU5NjE3NTU1NTA=","number":27223,"title":"DB ENGINE MySQL， Create table occurs Exception: MergeTree storages require data path.  v 21.7","user":{"login":"900827zy","id":19402940,"node_id":"MDQ6VXNlcjE5NDAyOTQw","avatar_url":"https://avatars.githubusercontent.com/u/19402940?v=4","gravatar_id":"","url":"https://api.github.com/users/900827zy","html_url":"https://github.com/900827zy","followers_url":"https://api.github.com/users/900827zy/followers","following_url":"https://api.github.com/users/900827zy/following{/other_user}","gists_url":"https://api.github.com/users/900827zy/gists{/gist_id}","starred_url":"https://api.github.com/users/900827zy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/900827zy/subscriptions","organizations_url":"https://api.github.com/users/900827zy/orgs","repos_url":"https://api.github.com/users/900827zy/repos","events_url":"https://api.github.com/users/900827zy/events{/privacy}","received_events_url":"https://api.github.com/users/900827zy/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-08-05T12:05:35Z","updated_at":"2021-09-08T10:39:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"  I am a fish to CK.\r\n  Today I create my clickhouse db like this CREATE DATABASE [IF NOT EXISTS] db_name [ON CLUSTER cluster]\r\nENGINE = MySQL('host:port', ['database' | database], 'user', 'password') ,  \"https://clickhouse.tech/docs/en/engines/database-engines/mysql/\".  The tables  in mysql can be seen in ck.\r\n  Then I create table in mysql, insert data in mysql, and update data in mysql,  these changes shows correct in ck. \r\n  At last, I drop the table \"t_test\" in ck, but it still in mysql. So, I try re-create the table in ck like this \"CREATE TABLE IF NOT EXISTS tb_stat(regionId String, groupId String, in int, out int, statDate DateTime) ENGINE = SummingMergeTree partition by (toYYYYMM(statDate), regionId) ORDER BY (toStartOfHour(statDate), regionId, groupId);\", then I get an Exception \"Received exception from server (version 21.1.2):\r\nCode: 79. DB::Exception: Received from localhost:9000. DB::Exception: MergeTree storages require data path. \"\r\n  Em....., I want to know that, while DB Engine is Mysql, it is not allowed to create table in ck ? Also I want to recover my table.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/27223/timeline","performed_via_github_app":null}]

[{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30679","id":1036042680,"node_id":"I_kwDOA5dJV849wMG4","number":30679,"title":"Error in executing 'group by' with 'prewhere'","user":{"login":"pic-Nick","id":5773437,"node_id":"MDQ6VXNlcjU3NzM0Mzc=","avatar_url":"https://avatars.githubusercontent.com/u/5773437?v=4","gravatar_id":"","url":"https://api.github.com/users/pic-Nick","html_url":"https://github.com/pic-Nick","followers_url":"https://api.github.com/users/pic-Nick/followers","following_url":"https://api.github.com/users/pic-Nick/following{/other_user}","gists_url":"https://api.github.com/users/pic-Nick/gists{/gist_id}","starred_url":"https://api.github.com/users/pic-Nick/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pic-Nick/subscriptions","organizations_url":"https://api.github.com/users/pic-Nick/orgs","repos_url":"https://api.github.com/users/pic-Nick/repos","events_url":"https://api.github.com/users/pic-Nick/events{/privacy}","received_events_url":"https://api.github.com/users/pic-Nick/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-26T09:13:59Z","updated_at":"2021-10-26T09:13:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Steps to reproduce:\r\n```\r\ncreate table table1 (\r\n    col1 Int32,\r\n    col2 Int32\r\n)\r\nENGINE = MergeTree\r\npartition by tuple()\r\norder by col1;\r\n```\r\n\r\n```\r\nwith :id as pid\r\nselect a.col1, sum(a.col2) as summ\r\nfrom table1 a\r\nprewhere (pid is null or a.col2 = pid)\r\ngroup by a.col1;\r\n```\r\nWhen `null` is passed as `:id` parameter, the query above produces error:\r\n```\r\nCode: 215, e.displayText() = DB::Exception: Column `col2` is not under aggregate function and not in GROUP BY: While processing col2 (version 21.8.3.44 (official build))\r\n```\r\nIt doesn't matter, if the table is distributed or not.\r\nIf you replace `prewhere` with `where` or pass non-null value or remove `is null` part of the condition, the query executes without error.\r\n\r\nThis reproducible on 21.8.3.44 and above.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30679/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30659","id":1035418979,"node_id":"I_kwDOA5dJV849tz1j","number":30659,"title":"HTAP support/ Queries with join","user":{"login":"itzikiusa","id":37109688,"node_id":"MDQ6VXNlcjM3MTA5Njg4","avatar_url":"https://avatars.githubusercontent.com/u/37109688?v=4","gravatar_id":"","url":"https://api.github.com/users/itzikiusa","html_url":"https://github.com/itzikiusa","followers_url":"https://api.github.com/users/itzikiusa/followers","following_url":"https://api.github.com/users/itzikiusa/following{/other_user}","gists_url":"https://api.github.com/users/itzikiusa/gists{/gist_id}","starred_url":"https://api.github.com/users/itzikiusa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/itzikiusa/subscriptions","organizations_url":"https://api.github.com/users/itzikiusa/orgs","repos_url":"https://api.github.com/users/itzikiusa/repos","events_url":"https://api.github.com/users/itzikiusa/events{/privacy}","received_events_url":"https://api.github.com/users/itzikiusa/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"gingerwizard","id":12695796,"node_id":"MDQ6VXNlcjEyNjk1Nzk2","avatar_url":"https://avatars.githubusercontent.com/u/12695796?v=4","gravatar_id":"","url":"https://api.github.com/users/gingerwizard","html_url":"https://github.com/gingerwizard","followers_url":"https://api.github.com/users/gingerwizard/followers","following_url":"https://api.github.com/users/gingerwizard/following{/other_user}","gists_url":"https://api.github.com/users/gingerwizard/gists{/gist_id}","starred_url":"https://api.github.com/users/gingerwizard/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gingerwizard/subscriptions","organizations_url":"https://api.github.com/users/gingerwizard/orgs","repos_url":"https://api.github.com/users/gingerwizard/repos","events_url":"https://api.github.com/users/gingerwizard/events{/privacy}","received_events_url":"https://api.github.com/users/gingerwizard/received_events","type":"User","site_admin":false},"assignees":[{"login":"gingerwizard","id":12695796,"node_id":"MDQ6VXNlcjEyNjk1Nzk2","avatar_url":"https://avatars.githubusercontent.com/u/12695796?v=4","gravatar_id":"","url":"https://api.github.com/users/gingerwizard","html_url":"https://github.com/gingerwizard","followers_url":"https://api.github.com/users/gingerwizard/followers","following_url":"https://api.github.com/users/gingerwizard/following{/other_user}","gists_url":"https://api.github.com/users/gingerwizard/gists{/gist_id}","starred_url":"https://api.github.com/users/gingerwizard/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gingerwizard/subscriptions","organizations_url":"https://api.github.com/users/gingerwizard/orgs","repos_url":"https://api.github.com/users/gingerwizard/repos","events_url":"https://api.github.com/users/gingerwizard/events{/privacy}","received_events_url":"https://api.github.com/users/gingerwizard/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-25T18:02:47Z","updated_at":"2022-01-24T15:50:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\nWanted to know if there are any plans of supporting new table engines which will be more suitable for small transactions(even with table limit of 100mill records), which will also support updates and deletes directly?\r\nmaybe even embedding a different database within(like embedded rocksdb)\r\n\r\non that note(or different one)\r\nAny plans on optimizing join queries?\r\nMainly the filters which are being applied, so that less data will be joined\r\n\r\nthanks, Itzik\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30659/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30645","id":1035163928,"node_id":"I_kwDOA5dJV849s1kY","number":30645,"title":"TTL Delete for a table is not executing as expected","user":{"login":"toddyocum","id":17409648,"node_id":"MDQ6VXNlcjE3NDA5NjQ4","avatar_url":"https://avatars.githubusercontent.com/u/17409648?v=4","gravatar_id":"","url":"https://api.github.com/users/toddyocum","html_url":"https://github.com/toddyocum","followers_url":"https://api.github.com/users/toddyocum/followers","following_url":"https://api.github.com/users/toddyocum/following{/other_user}","gists_url":"https://api.github.com/users/toddyocum/gists{/gist_id}","starred_url":"https://api.github.com/users/toddyocum/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/toddyocum/subscriptions","organizations_url":"https://api.github.com/users/toddyocum/orgs","repos_url":"https://api.github.com/users/toddyocum/repos","events_url":"https://api.github.com/users/toddyocum/events{/privacy}","received_events_url":"https://api.github.com/users/toddyocum/received_events","type":"User","site_admin":false},"labels":[{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":7,"created_at":"2021-10-25T13:49:48Z","updated_at":"2022-01-19T16:13:16Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nThe TTL partition delete is not working, eventually causing disk to fill.\r\n\r\n**How to reproduce**\r\n\r\n```\r\nSELECT version()\r\n┌─version()─┐\r\n │ 21.3.4.25         │\r\n└──────┘\r\n```\r\n\r\nHere is one example of a table from one box in the cluster that has fallen behind\r\n\r\n```\r\nCREATE TABLE atr.a_2866_r_5min_staging_local\r\n(\r\n    `timestamp` DateTime('UTC'),\r\n    `direction` LowCardinality(String),\r\n    `IPv6_Internal_Top_Talker` IPv6,\r\n    `Router` Int32,\r\n    `Interface` Int32,\r\n    `IPv6_External_Top_Talker` IPv6,\r\n    `bytes` UInt64,\r\n    `packets` UInt64\r\n)\r\nENGINE = SummingMergeTree((bytes, packets))\r\nPARTITION BY toStartOfFiveMinute(timestamp)\r\nORDER BY (timestamp, direction, IPv6_Internal_Top_Talker, Router, Interface, IPv6_External_Top_Talker)\r\nTTL timestamp + toIntervalDay(2)\r\nSETTINGS index_granularity = 8192, ttl_only_drop_parts = 1, merge_with_ttl_timeout = 3600\r\n\r\nSELECT\r\n    now(),\r\n    database,\r\n    table,\r\n    min(min_time) AS mini_time\r\nFROM system.parts\r\nWHERE active AND (database LIKE '%atr%') AND (table LIKE '%a_2866_r_5min_staging_local%')\r\nGROUP BY\r\n    database,\r\n    table\r\nORDER BY mini_time ASC\r\nFORMAT Vertical\r\n\r\nQuery id: 3157fc5f-487a-4894-9f70-bf3439b10eb7\r\n\r\nRow 1:\r\n──────\r\nnow():     2021-10-25 13:27:56\r\ndatabase:  atr\r\ntable:     a_2866_r_5min_staging_local\r\nmini_time: 2021-08-12 02:45:00\r\n\r\n```\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\nHere is another box in the same cluster that has NOT fallen behind\r\n\r\n```\r\nCREATE TABLE atr.a_2866_r_5min_staging_local\r\n(\r\n    `timestamp` DateTime('UTC'),\r\n    `direction` LowCardinality(String),\r\n    `IPv6_Internal_Top_Talker` IPv6,\r\n    `Router` Int32,\r\n    `Interface` Int32,\r\n    `IPv6_External_Top_Talker` IPv6,\r\n    `bytes` UInt64,\r\n    `packets` UInt64\r\n)\r\nENGINE = SummingMergeTree((bytes, packets))\r\nPARTITION BY toStartOfFiveMinute(timestamp)\r\nORDER BY (timestamp, direction, IPv6_Internal_Top_Talker, Router, Interface, IPv6_External_Top_Talker)\r\nTTL timestamp + toIntervalDay(2)\r\nSETTINGS index_granularity = 8192, ttl_only_drop_parts = 1, merge_with_ttl_timeout = 3600\r\n\r\nSELECT\r\n    now(),\r\n    database,\r\n    table,\r\n    min(min_time) AS mini_time\r\nFROM system.parts\r\nWHERE active AND (database LIKE '%atr%') AND (table LIKE '%a_2866_r_5min_staging_local%')\r\nGROUP BY\r\n    database,\r\n    table\r\nORDER BY mini_time ASC\r\nFORMAT Vertical\r\n\r\nQuery id: f307009d-5fc1-49c6-be27-8aab04f365d2\r\n\r\nRow 1:\r\n──────\r\nnow():     2021-10-25 13:30:17\r\ndatabase:  atr\r\ntable:     a_2866_r_5min_staging_local\r\nmini_time: 2021-10-23 13:35:00\r\n\r\n1 rows in set. Elapsed: 0.011 sec.\r\n```\r\n\r\n**Error message and/or stacktrace**\r\n\r\nI've seen no error messages or stack traces\r\n\r\nI enabled and examined `system.parts_log` and don't see any errors\r\n\r\n**Additional context**\r\n\r\nI have a deployment of 6 node in a cluster, each is ingesting data to their local clickhouse and processing the data using materialized views.\r\n\r\nI had reported my details in comments on various other TTL issues that seemed similar, but decided to finally create a separate issue, in case what I'm seeing is different from the other TTL issues.\r\n\r\nAt various times, a node's TTL logic will get stuck such that no further TTL delete operations occur.\r\n\r\nI can sometimes manually get things back on track by restarting clickhouse or manually executing an \r\n\r\n`ALTER TABLE <table> MATERIALIZE TTL; `\r\n\r\nBut eventually the condition reoccurs to various nodes in the deployment. The pattern does not seem specific to a node.\r\n\r\nI examined` system.merges`, and don't see an excessive amount of merges occurring that would prevent the TTL Delete merges from happening, it is often idle when I query it's count.\r\n\r\nI built from source and added some logging which may or may not be useful.\r\n\r\nThe best theory I've seen is that `merges_with_ttl_counter` is getting incremented but not always decremented\r\n\r\n\r\nHere I tracked the state of `merges_with_ttl_counter` in the` MergeList` class\r\n\r\n```\r\n2021.10.25 12:47:05.707708 [ 45446 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 1\r\n2021.10.25 12:47:05.707951 [ 45445 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 2\r\n2021.10.25 12:47:05.712395 [ 46416 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 1\r\n2021.10.25 12:47:05.713148 [ 45965 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 0\r\n2021.10.25 12:47:05.724472 [ 45458 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 1\r\n2021.10.25 12:47:05.724824 [ 45490 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 2\r\n2021.10.25 12:47:05.806905 [ 46310 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 1\r\n2021.10.25 12:47:05.884811 [ 46406 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 0\r\n2021.10.25 12:47:05.900383 [ 45480 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 1\r\n2021.10.25 12:47:05.910629 [ 45458 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 2\r\n2021.10.25 12:47:05.914648 [ 45542 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 3\r\n2021.10.25 12:47:05.922241 [ 45468 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 4\r\n2021.10.25 12:47:05.922322 [ 45514 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 5\r\n2021.10.25 12:47:05.923386 [ 45490 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 6\r\n2021.10.25 12:47:05.938196 [ 45445 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 7\r\n2021.10.25 12:47:05.938314 [ 45465 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 8\r\n2021.10.25 12:47:05.938325 [ 45548 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 9\r\n2021.10.25 12:47:05.939553 [ 45459 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 10\r\n2021.10.25 12:47:05.939638 [ 45521 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 11\r\n2021.10.25 12:47:05.942382 [ 45534 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 12\r\n2021.10.25 12:47:05.944084 [ 45511 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 13\r\n2021.10.25 12:47:05.947070 [ 45495 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 14\r\n2021.10.25 12:47:05.954170 [ 45480 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 15\r\n2021.10.25 12:47:05.954494 [ 45446 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 16\r\n2021.10.25 12:47:05.955354 [ 45797 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 15\r\n2021.10.25 12:47:05.962591 [ 45836 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 14\r\n2021.10.25 12:47:05.973794 [ 45487 ] {} <Information> TYocumMergeList: TYOCUM bookMergeWithTTL merges_with_ttl_counter 15\r\n2021.10.25 12:47:05.980989 [ 46497 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 14\r\n2021.10.25 12:47:05.985542 [ 46279 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 13\r\n2021.10.25 12:47:05.992818 [ 46504 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 12\r\n2021.10.25 12:47:05.994989 [ 45707 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 11\r\n2021.10.25 12:47:05.997867 [ 45945 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 10\r\n2021.10.25 12:47:05.999221 [ 45877 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 9\r\n2021.10.25 12:47:06.001886 [ 46405 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 8\r\n2021.10.25 12:47:06.002306 [ 45786 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 7\r\n2021.10.25 12:47:06.010537 [ 45928 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 6\r\n2021.10.25 12:47:06.032072 [ 46250 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 5\r\n2021.10.25 12:47:06.040663 [ 46569 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 4\r\n2021.10.25 12:47:06.042887 [ 46387 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 3\r\n2021.10.25 12:47:06.094217 [ 46025 ] {} <Information> TYocumMergeList: TYOCUM onEntryDestroy merges_with_ttl_counter 2\r\n```\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30645/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30643","id":1035032178,"node_id":"I_kwDOA5dJV849sVZy","number":30643,"title":"Evaluate `rpmalloc`","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-10-25T11:43:46Z","updated_at":"2021-12-01T03:57:41Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Create a PR with integrated https://github.com/mjansson/rpmalloc\r\nand wait for the results of performance tests (that will show the difference in performance and memory usage).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30643/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30642","id":1035007412,"node_id":"I_kwDOA5dJV849sPW0","number":30642,"title":"countDistinctIf with Date use much more memory than UInt16","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-25T11:17:00Z","updated_at":"2021-10-25T11:38:04Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the situation**\r\ncountDistinctIf with Date datatype use more memory than UInt16\r\n\r\n**How to reproduce**\r\nClickHouse version 21.11\r\n\r\n```\r\nSELECT countDistinctIf(materialize(toUInt16(today())), 1)\r\nFROM numbers_mt(100000000)\r\nGROUP BY number % 1000000\r\nFORMAT `Null`\r\n\r\nPeak memory usage (for query): 1.51 GiB.\r\n\r\n0 rows in set. Elapsed: 2.465 sec. Processed 100.00 million rows, 800.00 MB (40.57 million rows/s., 324.57 MB/s.)\r\n\r\nSELECT countDistinctIf(materialize(today()), 1)\r\nFROM numbers_mt(100000000)\r\nGROUP BY number % 1000000\r\nFORMAT `Null`\r\n\r\nPeak memory usage (for query): 4.51 GiB.\r\n\r\n0 rows in set. Elapsed: 4.455 sec. Processed 100.00 million rows, 800.00 MB (22.44 million rows/s., 179.56 MB/s.)\r\n```\r\n\r\n\r\n**Expected performance**\r\nSpeed and memory usage should be  the same as for UInt16\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30642/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30634","id":1034744912,"node_id":"I_kwDOA5dJV849rPRQ","number":30634,"title":"INSERT INTO ERRIR  Connection reset by peer,No message received","user":{"login":"yangguoxiang","id":12947667,"node_id":"MDQ6VXNlcjEyOTQ3NjY3","avatar_url":"https://avatars.githubusercontent.com/u/12947667?v=4","gravatar_id":"","url":"https://api.github.com/users/yangguoxiang","html_url":"https://github.com/yangguoxiang","followers_url":"https://api.github.com/users/yangguoxiang/followers","following_url":"https://api.github.com/users/yangguoxiang/following{/other_user}","gists_url":"https://api.github.com/users/yangguoxiang/gists{/gist_id}","starred_url":"https://api.github.com/users/yangguoxiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yangguoxiang/subscriptions","organizations_url":"https://api.github.com/users/yangguoxiang/orgs","repos_url":"https://api.github.com/users/yangguoxiang/repos","events_url":"https://api.github.com/users/yangguoxiang/events{/privacy}","received_events_url":"https://api.github.com/users/yangguoxiang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false},"assignees":[{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-10-25T06:49:54Z","updated_at":"2022-01-24T22:25:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"```\r\nCREATE TABLE IF NOT EXISTS south.alarm_file_info_flink (\r\n\t`vehicle_no` String,\r\n\t`vehicle_color` Int16,\r\n\t`alarm_time` DateTime,\r\n\t`alarm_code` Int32,\r\n\t`alarm_level` Int32,\r\n\t`file_img_url` String,\r\n\t`file_original_url` String,\r\n\t`create_time` DateTime DEFAULT now(),\r\n\t`data_flag` Int8 DEFAULT -1\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/south/alarm_file_info_flink', '{replica}')\r\nPARTITION BY toYYYYMM(alarm_time)\r\nORDER BY (vehicle_no, alarm_code, alarm_time)\r\nSETTINGS index_granularity = 8192;\r\n```\r\n> INSERT INTO ：An error is reported after the operation has lasted a certain amount of time\r\n```\r\n2021.10.25 10:47:09.359958 [ 97917 ] {} <Error> south.alarm_file_info_flink: auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &) const: Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Net::ConnectionResetException::ConnectionResetException(int) @ 0x11d385e9 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n1. Poco::Net::SocketImpl::error(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x11d4d56c in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n2. Poco::Net::SocketImpl::receiveBytes(void*, int, int) @ 0x11d4e92b in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n3. Poco::Net::HTTPSession::receive(char*, int) @ 0x11d24876 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n4. Poco::Net::HTTPHeaderStreamBuf::readFromDevice(char*, long) @ 0x11d1ba3f in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n5. Poco::BasicBufferedStreamBuf<char, std::__1::char_traits<char>, Poco::Net::HTTPBufferAllocator>::underflow() @ 0x11d0f590 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n6. std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow() @ 0x1382324a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n7. std::__1::basic_istream<char, std::__1::char_traits<char> >::get() @ 0x1382576a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n8. Poco::Net::HTTPResponse::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x11d21d51 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n9. Poco::Net::HTTPClientSession::receiveResponse(Poco::Net::HTTPResponse&) @ 0x11d1459a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n10. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::call(Poco::URI, Poco::Net::HTTPResponse&) @ 0xf35d693 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n11. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::ReadWriteBufferFromHTTPBase(std::__1::shared_ptr<DB::UpdatablePooledSession>, Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, Poco::Net::HTTPBasicCredentials const&, unsigned long, std::__1::vector<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >, DB::RemoteHostFilter const&) @ 0xf35a2c7 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n12. DB::PooledReadWriteBufferFromHTTP::PooledReadWriteBufferFromHTTP(Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, DB::ConnectionTimeouts const&, Poco::Net::HTTPBasicCredentials const&, unsigned long, unsigned long, unsigned long) @ 0xf350860 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n13. DB::DataPartsExchange::Fetcher::fetchPart(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, DB::ConnectionTimeouts const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xf34f06a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n14. std::__1::shared_ptr<DB::IMergeTreeDataPart> std::__1::__function::__policy_invoker<std::__1::shared_ptr<DB::IMergeTreeDataPart> ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long, std::__1::shared_ptr<zkutil::ZooKeeper>)::$_21, std::__1::shared_ptr<DB::IMergeTreeDataPart> ()> >(std::__1::__function::__policy_storage const*) @ 0xf223f27 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n15. DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long, std::__1::shared_ptr<zkutil::ZooKeeper>) @ 0xf1c6405 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n16. DB::StorageReplicatedMergeTree::executeFetch(DB::ReplicatedMergeTreeLogEntry&) @ 0xf1c08e9 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n17. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0xf1ab5bf in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n18. bool std::__1::__function::__policy_invoker<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageReplicatedMergeTree::processQueueEntry(std::__1::shared_ptr<DB::ReplicatedMergeTreeQueue::SelectedEntry>)::$_15, bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)> >(std::__1::__function::__policy_storage const*, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&) @ 0xf22161f in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n19. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0xf5593fc in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n20. DB::StorageReplicatedMergeTree::processQueueEntry(std::__1::shared_ptr<DB::ReplicatedMergeTreeQueue::SelectedEntry>) @ 0xf1d862d in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n21. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0xf34a347 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n22. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x853c9c8 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n23. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x853e3df in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n24. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x853a01f in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n25. void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x853d493 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n26. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n27. __clone @ 0xfe8dd in /usr/lib64/libc-2.17.so\r\n (version 21.3.16.5 (official build))\r\n2021.10.25 10:47:09.365197 [ 98046 ] {} <Error> south.alarm_file_info_flink: auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &) const: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = No message received, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Net::HTTPResponse::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x11d225f0 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n1. Poco::Net::HTTPClientSession::receiveResponse(Poco::Net::HTTPResponse&) @ 0x11d1459a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n2. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::call(Poco::URI, Poco::Net::HTTPResponse&) @ 0xf35d693 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n3. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::ReadWriteBufferFromHTTPBase(std::__1::shared_ptr<DB::UpdatablePooledSession>, Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, Poco::Net::HTTPBasicCredentials const&, unsigned long, std::__1::vector<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >, DB::RemoteHostFilter const&) @ 0xf35a2c7 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n4. DB::PooledReadWriteBufferFromHTTP::PooledReadWriteBufferFromHTTP(Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, DB::ConnectionTimeouts const&, Poco::Net::HTTPBasicCredentials const&, unsigned long, unsigned long, unsigned long) @ 0xf350860 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n5. DB::DataPartsExchange::Fetcher::fetchPart(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, DB::ConnectionTimeouts const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0xf34f06a in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n6. std::__1::shared_ptr<DB::IMergeTreeDataPart> std::__1::__function::__policy_invoker<std::__1::shared_ptr<DB::IMergeTreeDataPart> ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long, std::__1::shared_ptr<zkutil::ZooKeeper>)::$_21, std::__1::shared_ptr<DB::IMergeTreeDataPart> ()> >(std::__1::__function::__policy_storage const*) @ 0xf223f27 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n7. DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long, std::__1::shared_ptr<zkutil::ZooKeeper>) @ 0xf1c6405 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n8. DB::StorageReplicatedMergeTree::executeFetch(DB::ReplicatedMergeTreeLogEntry&) @ 0xf1c08e9 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n9. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0xf1ab5bf in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n10. bool std::__1::__function::__policy_invoker<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageReplicatedMergeTree::processQueueEntry(std::__1::shared_ptr<DB::ReplicatedMergeTreeQueue::SelectedEntry>)::$_15, bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)> >(std::__1::__function::__policy_storage const*, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&) @ 0xf22161f in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n11. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0xf5593fc in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n12. DB::StorageReplicatedMergeTree::processQueueEntry(std::__1::shared_ptr<DB::ReplicatedMergeTreeQueue::SelectedEntry>) @ 0xf1d862d in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n13. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::IBackgroundJobExecutor::jobExecutingTask()::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0xf34a347 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n14. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x853c9c8 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n15. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...)::'lambda'()::operator()() @ 0x853e3df in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n16. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x853a01f in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n17. void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) @ 0x853d493 in /usr/lib/debug/.build-id/6c/382f2dc3800e5b9a71b0a288692bf78fd3ef8e.debug\r\n18. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n19. __clone @ 0xfe8dd in /usr/lib64/libc-2.17.so\r\n (version 21.3.16.5 (official build))\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30634/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30632","id":1034670382,"node_id":"I_kwDOA5dJV849q9Eu","number":30632,"title":"COVID-19 open dataset","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":3360106261,"node_id":"MDU6TGFiZWwzMzYwMTA2MjYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/dataset","name":"dataset","color":"00CCCC","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-25T04:39:25Z","updated_at":"2021-10-25T04:39:25Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://github.com/GoogleCloudPlatform/covid-19-open-data\r\nSee also https://github.com/GoogleCloudPlatform/covid-19-open-data/pull/514\r\n\r\nYou can load data into ClickHouse with the following query:\r\n```sql\r\nCREATE TABLE covid ENGINE = MergeTree ORDER BY (location_key, date) \r\n  AS SELECT * FROM url('https://storage.googleapis.com/covid19-open-data/v3/epidemiology.csv', CSVWithNames, \r\n  'date Date, location_key LowCardinality(String), new_confirmed Int32, new_deceased Int32, new_recovered Int32, new_tested Int32, cumulative_confirmed Int32, cumulative_deceased Int32, cumulative_recovered Int32, cumulative_tested Int32')\r\n```\r\n\r\nYou can also process it directly with `clickhouse-local`:\r\n```bash\r\nclickhouse-local --input-format CSVWithNames --structure 'date Date, location_key String, new_confirmed Int32, new_deceased Int32, new_recovered Int32, new_tested Int32, cumulative_confirmed Int32, cumulative_deceased Int32, cumulative_recovered Int32, cumulative_tested Int32' --query \"SELECT * FROM table\" < epidemiology.csv\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30632/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30631","id":1034660709,"node_id":"I_kwDOA5dJV849q6tl","number":30631,"title":"Option `--format` for `clickhouse-local` should set both input and output formats.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-10-25T04:20:41Z","updated_at":"2022-01-19T00:00:33Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"`clickhouse-local` can take three options: `--input-format`, `--output-format` and `--format`.\r\nCurrently the `--format` option only sets the output format.\r\n\r\nLet's make it set default input and output formats that can be overriden by `--input-format` and `--output-format`.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30631/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30629","id":1034634703,"node_id":"I_kwDOA5dJV849q0XP","number":30629,"title":"NYPD Complaint Data","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":3360106261,"node_id":"MDU6TGFiZWwzMzYwMTA2MjYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/dataset","name":"dataset","color":"00CCCC","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-25T03:25:11Z","updated_at":"2021-10-25T03:25:11Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Public dataset related to crime data (complaints), quite small - 7.38 million records (no much crime in NYC).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30629/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30627","id":1034545295,"node_id":"I_kwDOA5dJV849qeiP","number":30627,"title":"Amazon reviews dataset","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":3360106261,"node_id":"MDU6TGFiZWwzMzYwMTA2MjYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/dataset","name":"dataset","color":"00CCCC","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-10-24T23:46:38Z","updated_at":"2021-10-25T01:45:11Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Found here: https://habr.com/en/post/512084/\r\n\r\nDescription: https://s3.amazonaws.com/amazon-reviews-pds/readme.html\r\n\r\nLicense:\r\n\r\n> In addition to the license rights granted under the Conditions of Use, Amazon or its content providers grant you a limited, non-exclusive, non-transferable, non-sublicensable, revocable license to access and use the Reviews Library for purposes of academic research. You may not resell, **republish**, or make any commercial use of the Reviews Library or its contents, including use of the Reviews Library for commercial research, such as research related to a funding or consultancy contract, internship, or other relationship in which the results are provided for a fee or **delivered to a for-profit organization**. You may not (a) link or associate content in the Reviews Library with any personal information (including Amazon customer accounts), or (b) attempt to determine the identity of the author of any content in the Reviews Library. If you violate any of the foregoing conditions, your license to access and use the Reviews Library will automatically terminate without prejudice to any of the other rights or remedies Amazon may have.\r\n\r\nTLDR: I can do research, but we cannot make it accessible through our Playground or publish in our docs.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30627/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30600","id":1034289751,"node_id":"I_kwDOA5dJV849pgJX","number":30600,"title":"Add `url` function to SQL","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-24T00:12:56Z","updated_at":"2021-10-24T00:12:56Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nDownload a bunch or URLs into String.\r\n\r\n**Additional context**\r\n\r\nWe already have table function `url` but sometimes I need to download multiple URLs from a column in table.\r\nThat's why I also need `url` as an ordinary function.\r\n\r\n**Describe the solution you'd like**\r\n\r\nThis task is extraordinarily deep and complex.\r\nAnd we don't really need this feature in ClickHouse.\r\n\r\n- the function should be unavaliable if `readonly = 1`;\r\n- it should be enabled or disabled by a separate setting;\r\n- it should respect `remote_host_filter` to avoid SSRF;\r\n- by default it should not allow redirects (again, SSRF) and max number of redirects should be tuned;\r\n- max_network_bandwidth should be respected, or we can add a separate setting for this specific bandwidth;\r\n- timeouts for DNS resolution, connection and reading data should be tuneable;\r\n- it should support exponential backoff and max number of tries on errors;\r\n- it should support returning empty result or throwing exception in case of error;\r\n- downloading a bunch of URLs (from a block of data) should be performed concurrently and asynchronously, this should include DNS resolution, TCP connection, TLS handshake, reading from socket, performing redirects, etc;\r\n- identical URLs should not be downloaded twice;\r\n- max size of download data;\r\n- tuneable HTTP headers.\r\n\r\nOut of scope:\r\n- HTTP/2 support.\r\n\r\nIf for some reason you want to show your friends how to do fully asynchronous HTTP client in C++, maybe you want to implement this task.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n`executable` dictionary with `cache` layout and `curl` inside a script.\r\nBut it will not respect `remote_host_filter`.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30600/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30588","id":1034206804,"node_id":"PR_kwDOA5dJV84tlJXO","number":30588,"title":"Fix library bridge exception `Cannot read from istream`","user":{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false},"labels":[{"id":1304141686,"node_id":"MDU6TGFiZWwxMzA0MTQxNjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-improvement","name":"pr-improvement","color":"007700","default":false,"description":"Pull request with some product improvements"}],"state":"open","locked":false,"assignee":{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false},"assignees":[{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-23T16:43:05Z","updated_at":"2022-01-09T19:11:38Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30588","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30588","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30588.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30588.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- Improvement\r\n\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nFix library bridge error `Cannot read from istream`.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30588/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30584","id":1034160234,"node_id":"I_kwDOA5dJV849pAhq","number":30584,"title":"Generated preprocessed_configs/config.yaml is actually in XML format","user":{"login":"pmed","id":1122386,"node_id":"MDQ6VXNlcjExMjIzODY=","avatar_url":"https://avatars.githubusercontent.com/u/1122386?v=4","gravatar_id":"","url":"https://api.github.com/users/pmed","html_url":"https://github.com/pmed","followers_url":"https://api.github.com/users/pmed/followers","following_url":"https://api.github.com/users/pmed/following{/other_user}","gists_url":"https://api.github.com/users/pmed/gists{/gist_id}","starred_url":"https://api.github.com/users/pmed/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pmed/subscriptions","organizations_url":"https://api.github.com/users/pmed/orgs","repos_url":"https://api.github.com/users/pmed/repos","events_url":"https://api.github.com/users/pmed/events{/privacy}","received_events_url":"https://api.github.com/users/pmed/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-23T12:50:01Z","updated_at":"2021-10-23T12:53:26Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**How to reproduce**\r\n\r\n0. Create a config.yaml file:\r\n\r\n```yaml\r\nmax_server_memory_usage_to_ram_ratio: 0.5\r\n```\r\n\r\n1. Run ClickHouse with this configuration:\r\n\r\n`clickhouse-local -C config.yaml --verbose`\r\n\r\n>  ClickHouse % out/programs/clickhouse-local -C config.yaml --verbose\r\nProcessing configuration file 'config.yaml'.\r\nSaved preprocessed configuration to './preprocessed_configs/config.yaml'.\r\nInitialized background executor for merges and mutations with num_threads=16, num_tasks=32\r\nInitialized background executor for move operations with num_threads=8, num_tasks=8\r\nInitialized background executor for fetches with num_threads=8, num_tasks=8\r\nInitialized background executor for common operations (e.g. clearing old parts) with num_threads=8, num_tasks=8\r\nstd::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in create_directory: Permission denied [/var/lib/clickhouse]\r\nCannot print extra info for Poco::Exception (version 21.11.1.1)\r\n\r\n2. It generates a `preprocessed_configs/config.yaml` that is actually an XML:\r\n```xml\r\n<!-- This file was generated automatically.\r\n     Do not edit it: it is likely to be discarded and generated again before it's read next time.\r\n     Files used to generate this file:\r\n       config.yaml      -->\r\n<clickhouse><max_server_memory_usage_to_ram_ratio>0.5</max_server_memory_usage_to_ram_ratio></clickhouse>\r\n```\r\n\r\n3. An attempt to start ClickHouse with the generated config leads to the error:\r\n\r\n> Processing configuration file 'preprocessed_configs/config.yaml'.\r\nCode: 585. DB::Exception: Unable to parse YAML configuration file preprocessed_configs/config.yaml, yaml-cpp: error at line 2, column 20: illegal map value. (CANNOT_PARSE_YAML), Stack trace (when copying this message, always include the lines below):\r\n\r\n<Empty trace>\r\n (version 21.11.1.1)\r\n\r\n* Which ClickHouse server version to use: version 21.11.1.1 (recently built from master branch)\r\n\r\n**Expected behavior**\r\n\r\nThe generated config filename should have `.xml` extension, i.e. `preprocessed_configs/config.yaml.xml`, in order the ClickHouse can determinate the configuration format by the file extension:\r\n\r\n> ClickHouse % out/programs/clickhouse-local -C preprocessed_configs/config.yaml.xml   \r\nProcessing configuration file 'preprocessed_configs/config.yaml.xml'.\r\nSaved preprocessed configuration to './preprocessed_configs/config.yaml.xml'.\r\nstd::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in create_directory: Permission denied [/var/lib/clickhouse]\r\nCannot print extra info for Poco::Exception (version 21.11.1.1)` \r\n\r\n**Error message and/or stacktrace**\r\n\r\nFor some reason clickhouse-local doesn't start on macOS Big Sur 11.6 with a custom config due to an exception:\r\n\r\n> std::exception. Code: 1001, type: std::__1::__fs::filesystem::filesystem_error, e.what() = filesystem error: in create_directory: Permission denied [/var/lib/clickhouse]\r\nCannot print extra info for Poco::Exception (version 21.11.1.1)\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30584/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30559","id":1033454242,"node_id":"I_kwDOA5dJV849mUKi","number":30559,"title":"Excessive logging for s3: Failed to find credentials process's profile","user":{"login":"anotherbugmaster","id":3992246,"node_id":"MDQ6VXNlcjM5OTIyNDY=","avatar_url":"https://avatars.githubusercontent.com/u/3992246?v=4","gravatar_id":"","url":"https://api.github.com/users/anotherbugmaster","html_url":"https://github.com/anotherbugmaster","followers_url":"https://api.github.com/users/anotherbugmaster/followers","following_url":"https://api.github.com/users/anotherbugmaster/following{/other_user}","gists_url":"https://api.github.com/users/anotherbugmaster/gists{/gist_id}","starred_url":"https://api.github.com/users/anotherbugmaster/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anotherbugmaster/subscriptions","organizations_url":"https://api.github.com/users/anotherbugmaster/orgs","repos_url":"https://api.github.com/users/anotherbugmaster/repos","events_url":"https://api.github.com/users/anotherbugmaster/events{/privacy}","received_events_url":"https://api.github.com/users/anotherbugmaster/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2020016497,"node_id":"MDU6TGFiZWwyMDIwMDE2NDk3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-s3","name":"comp-s3","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-22T11:00:19Z","updated_at":"2022-01-24T13:50:05Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When selecting from S3 table with `use_environment_credentials=true` CH writes lots of messages like this\r\n\r\n```\r\n<Error> AWSClient: ProcessCredentialsProvider: Failed to find credential process's profile: default\r\n```\r\n\r\nI've dug in the CH and AWS SDK codebase a little, and it seems like AWS Client is supposed to cache credentials obtained using instance metastore in local filesystem and then use them till the expiration date comes, but that doesn't happen for some reason, hence this error.\r\n\r\nI've also tried to save credentials obtained from metastore in `/nonexistent/.aws/` (clickhouse user home directory) manually, and these messages were gone before CH made another request to metastore and made credentials saved in a file obsolete.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes, tested with 21.10.2.15\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\n\t* 21.10.2.15\r\n* Which interface to use, if matters\r\n\t* Doesn't matter\r\n* Non-default settings, if any\r\n\t```\r\n\t<use_environment_credentials>true</use_environment_credentials>\r\n\t```\r\n* `CREATE TABLE` statements for all tables involved\r\n\t```\r\n\t\tCREATE TABLE s3_table (\r\n  \t\t\tjson String\r\n\t\t)\r\n\t\tENGINE = S3(\r\n  \t\t\t'https://s3.amazonaws.com/bucket/prefix/*.gz',\r\n  \t\t\t'JSONAsString'\r\n\t\t);\r\n\t```\r\n* Queries to run that lead to unexpected result\r\n\t```\r\n\tSELECT * FROM s3_table LIMIT 1000000;\r\n\t``` \r\n\r\n**Expected behavior**\r\n\r\nNot to have lots of error messages in logs\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\n<Error> AWSClient: ProcessCredentialsProvider: Failed to find credential process's profile: default\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30559/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30531","id":1032920934,"node_id":"I_kwDOA5dJV849kR9m","number":30531,"title":"Map of different data types","user":{"login":"ilugid","id":59707064,"node_id":"MDQ6VXNlcjU5NzA3MDY0","avatar_url":"https://avatars.githubusercontent.com/u/59707064?v=4","gravatar_id":"","url":"https://api.github.com/users/ilugid","html_url":"https://github.com/ilugid","followers_url":"https://api.github.com/users/ilugid/followers","following_url":"https://api.github.com/users/ilugid/following{/other_user}","gists_url":"https://api.github.com/users/ilugid/gists{/gist_id}","starred_url":"https://api.github.com/users/ilugid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ilugid/subscriptions","organizations_url":"https://api.github.com/users/ilugid/orgs","repos_url":"https://api.github.com/users/ilugid/repos","events_url":"https://api.github.com/users/ilugid/events{/privacy}","received_events_url":"https://api.github.com/users/ilugid/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-21T20:38:19Z","updated_at":"2021-10-21T20:38:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nWe have a table in Hive with a multiple nested data structure. Each of the nested sections has a mix of data types, such as Int, Array and Strings.\r\n\r\nWe are trying to use it as a dictionary in Clickhouse.\r\n\r\nWe are unable to create mappings due to limitation of Map type declaration.\r\n1. It supports only one set of Key, Value declaration.\r\n2. There is no provision to add sub-nests.\r\n\r\ne.g hive schema.\r\n(\r\n`regular_col1` boolean,\r\n`regular_col2` string,\r\n`nested_top1` struct<\r\n    `nested_sub1`: struct<\r\n        `nested_sub1_2-1`: struct<\r\n            `nested_sub1_2_3-1`: array<string>,\r\n            `nested_sub1_2_3-2`: struct<\r\n                `nested_sub1_2_3_2-1`: string,\r\n                `nested_sub1_2_3_2-2`: string\r\n    \t        >,\r\n            `nested_sub1_2_3-3`: struct<\r\n                `nested_sub1_2_3_3-1`: string,\r\n                `nested_sub1_2_3_3-2`: bigint\r\n                >\r\n            >\r\n        >,\r\n    `nested_sub2`: struct<\r\n        `nested_sub2_2-1`: string,\r\n        `nested_sub2_2-2`: string,\r\n        `nested_sub2_2-3`: bigint,\r\n        >\r\n    >\r\n)\r\n\r\n**Describe the solution you'd like**\r\nAbility to map above hive table in something sort of like below\r\n\r\nCREATE TABLE clickhouse_nested_maps\r\n(\r\n`regular_col1` Boolean,\r\n`regular_col2` String,\r\n`nested_top1` Map\r\n        (\r\n            String, Map\r\n            (\r\n                String, Map\r\n                   (\r\n                       String: Array(String),\r\n                       String, Map\r\n                       (\r\n                            String, String #####Single declaration can handle both set of strings.\r\n                       )\r\n                       String: Map\r\n                       (\r\n                           String, String,\r\n                           String, Int64,\r\n                       )\r\n                     )\r\n             )\r\n             String, Map\r\n             (\r\n                 String, String,\r\n                 String, String,\r\n                 String, Int64,\r\n             )\r\n          )\r\n)\r\nENGINE = HDFS('hdfs://hadoop:9000/user/hive/warehouse/nested_hive_table/*', 'Parquet')\r\n\r\n\r\n**Describe alternatives you've considered**\r\nConvert original table to a more flat format.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30531/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30530","id":1032888349,"node_id":"I_kwDOA5dJV849kKAd","number":30530,"title":"Virtual column for row / granula number in part and index scan by it","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-21T19:56:33Z","updated_at":"2021-10-21T19:56:33Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nReverse index via projections\r\n\r\n**Describe the solution you'd like**\r\n\r\n_row_number and _granula_number virtual columns which can be used for efficient access to specific rows in part.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30530/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30528","id":1032850321,"node_id":"I_kwDOA5dJV849kAuR","number":30528,"title":"clickhouse-format can't parse complex queries","user":{"login":"siradjev","id":10959667,"node_id":"MDQ6VXNlcjEwOTU5NjY3","avatar_url":"https://avatars.githubusercontent.com/u/10959667?v=4","gravatar_id":"","url":"https://api.github.com/users/siradjev","html_url":"https://github.com/siradjev","followers_url":"https://api.github.com/users/siradjev/followers","following_url":"https://api.github.com/users/siradjev/following{/other_user}","gists_url":"https://api.github.com/users/siradjev/gists{/gist_id}","starred_url":"https://api.github.com/users/siradjev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/siradjev/subscriptions","organizations_url":"https://api.github.com/users/siradjev/orgs","repos_url":"https://api.github.com/users/siradjev/repos","events_url":"https://api.github.com/users/siradjev/events{/privacy}","received_events_url":"https://api.github.com/users/siradjev/received_events","type":"User","site_admin":false},"labels":[{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-21T19:08:00Z","updated_at":"2022-01-18T22:38:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nclickhouse-format can't parse complex queries. \r\nCode: 306, e.displayText() = DB::Exception: Maximum parse depth (1000) exceeded. Consider rising max_parser_depth parameter., Stack trace (when copying this message, always include the lines below):\r\n\r\n**How to reproduce**\r\nwrite any query that requires adjustment of parser settings:\r\nmax_ast_depth\r\nmax_ast_elements\r\nmax_parser_depth \r\nmax_query_size\r\nmax_subquery_depth\r\n\r\n**Expected behavior**\r\nclickhouse-format shall support parameters of clickhouse-client related to query parsing. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30528/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30527","id":1032844493,"node_id":"I_kwDOA5dJV849j_TN","number":30527,"title":"S3 metadata restore - restoring to before a mutation on one table causes metadata restore of a different table to fail.","user":{"login":"MyroTk","id":44327070,"node_id":"MDQ6VXNlcjQ0MzI3MDcw","avatar_url":"https://avatars.githubusercontent.com/u/44327070?v=4","gravatar_id":"","url":"https://api.github.com/users/MyroTk","html_url":"https://github.com/MyroTk","followers_url":"https://api.github.com/users/MyroTk/followers","following_url":"https://api.github.com/users/MyroTk/following{/other_user}","gists_url":"https://api.github.com/users/MyroTk/gists{/gist_id}","starred_url":"https://api.github.com/users/MyroTk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MyroTk/subscriptions","organizations_url":"https://api.github.com/users/MyroTk/orgs","repos_url":"https://api.github.com/users/MyroTk/repos","events_url":"https://api.github.com/users/MyroTk/events{/privacy}","received_events_url":"https://api.github.com/users/MyroTk/received_events","type":"User","site_admin":false},"labels":[{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-21T19:00:47Z","updated_at":"2021-10-22T19:05:02Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe what's wrong**\r\nUsing s3 metadata to restore a table (table0) to before a mutation causes the next table (table1) restore to fail. If I restore table0 to after the mutation or the mutation doesn't happen, table1 is restored correctly.\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\n21.8.9.13\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\nAll tables are created and attached this way but the names are different.\r\n```\r\n[CREATE | ATTACH] TABLE [table_name] (id UInt64, x UInt64)\r\nENGINE = MergeTree()\r\nPARTITION BY id\r\nORDER BY id\r\nSETTINGS storage_policy='external'\r\n```\r\n* Configs\r\nnode1 - bucket is 'root'\r\n```\r\n <yandex>\r\n   <!--config uid: 87d31bae_3288_11ec_825f_99724a71d554-->\r\n   <storage_configuration>\r\n     <disks>\r\n       <external>\r\n         <type>s3</type>\r\n         <endpoint>http://minio1:9001/root/data/</endpoint>\r\n         <access_key_id>minio</access_key_id>\r\n         <secret_access_key>minio123</secret_access_key>\r\n         <send_metadata>true</send_metadata>\r\n         <list_object_keys_size>1</list_object_keys_size>\r\n       </external>\r\n     </disks>\r\n     <policies>\r\n       <external>\r\n         <volumes>\r\n           <external>\r\n             <disk>external</disk>\r\n           </external>\r\n         </volumes>\r\n       </external>\r\n     </policies>\r\n   </storage_configuration>\r\n </yandex>\r\n```\r\nnode2 - bucket is 'root2'\r\n```\r\n <yandex>\r\n   <!--config uid: 87d31bae_3288_11ec_825f_99724a71d554-->\r\n   <storage_configuration>\r\n     <disks>\r\n       <external>\r\n         <type>s3</type>\r\n         <endpoint>http://minio1:9001/root2/data/</endpoint>\r\n         <access_key_id>minio</access_key_id>\r\n         <secret_access_key>minio123</secret_access_key>\r\n         <send_metadata>true</send_metadata>\r\n         <list_object_keys_size>1</list_object_keys_size>\r\n       </external>\r\n     </disks>\r\n     <policies>\r\n       <external>\r\n         <volumes>\r\n           <external>\r\n             <disk>external</disk>\r\n           </external>\r\n         </volumes>\r\n       </external>\r\n     </policies>\r\n   </storage_configuration>\r\n </yandex>\r\n```\r\n* Queries to run that lead to unexpected result\r\n\r\non node1:\r\nCreate table0\r\n`INSERT INTO table0 VALUES (1,2)`\r\n`ALTER TABLE table0 FREEZE`\r\nGet the revision number from /var/lib/clickhouse/disks/external/shadow/1/revision.txt - revision number before mutation\r\n`ALTER TABLE table0 UPDATE x = 1 WHERE 1`\r\n\r\non node2:\r\nCreate restore file at /var/lib/clickhouse/disks/external/restore:\r\n```\r\nrevision=[revision number]\r\nsource_bucket=root\r\n```\r\n`SYSTEM RESTART DISK external`\r\nAttach table0\r\n`SELECT sum(x) FROM table0`  - should be 2 as it's before the ALTER UPDATE.\r\n\r\non both node1 and node2:\r\n`DROP TABLE table0 SYNC`\r\nremove /var/lib/clickhouse/disks/external/*\r\nremove /var/lib/clickhouse/shadow/*\r\n\r\non node1 for the rest:\r\nCreate table1\r\n`INSERT INTO table1 VALUES (1,2)`\r\n`DETACH TABLE table1`\r\nRemove /var/lib/clickhouse/disks/external/*\r\nCreate empty restore file at /var/lib/clickhouse/disks/external/restore\r\n`SYSTEM RESTART DISK external`\r\n`ATTACH TABLE table1`\r\n`SELECT count(*) FROM table1` - Returns 0 even though it should be 1. If the `ALTER UPDATE` doesn't happen or table0 is restore to after the mutation, this returns 1.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30527/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30510","id":1032681279,"node_id":"I_kwDOA5dJV849jXc_","number":30510,"title":"S3 metadata - send_metadata in configs does not work with aws s3.","user":{"login":"MyroTk","id":44327070,"node_id":"MDQ6VXNlcjQ0MzI3MDcw","avatar_url":"https://avatars.githubusercontent.com/u/44327070?v=4","gravatar_id":"","url":"https://api.github.com/users/MyroTk","html_url":"https://github.com/MyroTk","followers_url":"https://api.github.com/users/MyroTk/followers","following_url":"https://api.github.com/users/MyroTk/following{/other_user}","gists_url":"https://api.github.com/users/MyroTk/gists{/gist_id}","starred_url":"https://api.github.com/users/MyroTk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MyroTk/subscriptions","organizations_url":"https://api.github.com/users/MyroTk/orgs","repos_url":"https://api.github.com/users/MyroTk/repos","events_url":"https://api.github.com/users/MyroTk/events{/privacy}","received_events_url":"https://api.github.com/users/MyroTk/received_events","type":"User","site_admin":false},"labels":[{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-21T16:13:31Z","updated_at":"2021-10-22T19:13:58Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe what's wrong**\r\nAdding an aws s3 disk to the config files with `<send_metadata>true</send_metadata>` and then restarting ClickHouse causes it to not come up.\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\n21.8.9.13\r\n\r\nconfig file:\r\n```\r\n <yandex>\r\n   <!--config uid: 87d31bae_3288_11ec_825f_99724a71d554-->\r\n   <storage_configuration>\r\n     <disks>\r\n       <external>\r\n         <type>s3</type>\r\n         <endpoint>[redacted]</endpoint>\r\n         <access_key_id>[redacted]</access_key_id>\r\n         <secret_access_key>[redacted]</secret_access_key>\r\n         <send_metadata>true</send_metadata>\r\n         <list_object_keys_size>1</list_object_keys_size>\r\n       </external>\r\n     </disks>\r\n     <policies>\r\n       <external>\r\n         <volumes>\r\n           <external>\r\n             <disk>external</disk>\r\n           </external>\r\n         </volumes>\r\n       </external>\r\n     </policies>\r\n   </storage_configuration>\r\n </yandex>\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30510/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30498","id":1032541501,"node_id":"PR_kwDOA5dJV84tf0Cd","number":30498,"title":"Add fault injection in ZooKeeper client","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1876999755,"node_id":"MDU6TGFiZWwxODc2OTk5NzU1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-not-for-changelog","name":"pr-not-for-changelog","color":"007700","default":false,"description":"This PR should not be mentioned in the changelog"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-10-21T14:04:09Z","updated_at":"2022-01-17T15:26:20Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30498","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30498","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30498.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30498.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- Not for changelog (changelog entry is not required)\r\n\r\n\r\nDetailed description / Documentation draft:\r\nRandomly expire session in ZooKeeper client, enable fault injection in Stress tests\r\n\r\n**Do not merge until some green commits appear in master.**","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30498/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30496","id":1032522505,"node_id":"I_kwDOA5dJV849iwsJ","number":30496,"title":"dictGet in the column TTL expression","user":{"login":"ibradwan","id":67283130,"node_id":"MDQ6VXNlcjY3MjgzMTMw","avatar_url":"https://avatars.githubusercontent.com/u/67283130?v=4","gravatar_id":"","url":"https://api.github.com/users/ibradwan","html_url":"https://github.com/ibradwan","followers_url":"https://api.github.com/users/ibradwan/followers","following_url":"https://api.github.com/users/ibradwan/following{/other_user}","gists_url":"https://api.github.com/users/ibradwan/gists{/gist_id}","starred_url":"https://api.github.com/users/ibradwan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ibradwan/subscriptions","organizations_url":"https://api.github.com/users/ibradwan/orgs","repos_url":"https://api.github.com/users/ibradwan/repos","events_url":"https://api.github.com/users/ibradwan/events{/privacy}","received_events_url":"https://api.github.com/users/ibradwan/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-21T13:47:31Z","updated_at":"2021-10-26T19:46:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there,\r\n\r\nI would like to have a conditional column TTL based on a value I retrieve from a dictionary. For example, I need to apply a different TTL for the column `FieldX` based on whether the session is favorited or not.\r\n\r\nThe dictionary:\r\n``` SQL\r\nCREATE DICTIONARY db.temp_dict\r\n(\r\n    Id UInt32,\r\n    IsFavorite UInt8 DEFAULT 0\r\n)\r\nPRIMARY KEY SessionId\r\nSOURCE(MYSQL(PORT 3306 USER 'XXXX' PASSWORD 'YYYY' REPLICA (HOST 'HHHH' PRIORITY 1) DB 'mysql_db' TABLE 'favorites'))\r\nLIFETIME(MIN 300 MAX 360)\r\nLAYOUT(FLAT())\r\n```\r\n\r\nFor the table:\r\n``` SQL\r\nCREATE TABLE IF NOT EXISTS db.temp_table\r\n(\r\n    Timestamp DateTime,\r\n\r\n    ProjectId UInt64,\r\n    UserId UInt32,\r\n    SessionId UInt32,\r\n\r\n    FieldX String TTL if(dictGet('db.temp_dict', 'IsFavorite', toUInt64(SessionId)), Timestamp + toIntervalDay(365), Timestamp + toIntervalDay(14))\r\n)\r\nENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(Timestamp)\r\nORDER BY (ProjectId, Date, intHash32(UserId), SessionId);\r\n```\r\n\r\nWhen I run the create table query, I get this error:\r\n```\r\nCode: 36. DB::Exception: Received from localhost:9440. DB::Exception: TTL expression cannot contain non-deterministic functions, but contains function dictGet.\r\n```\r\n\r\nIs there a way to achieve this?\r\n\r\nI saw this comment (https://github.com/ClickHouse/ClickHouse/issues/29533#issuecomment-930306802), but when trying to do so, I get an error that the `WHERE` is not expected.\r\n\r\nMy version: 21.7.11.3\r\n\r\nThank you.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30496/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30486","id":1032309414,"node_id":"I_kwDOA5dJV849h8qm","number":30486,"title":"Errors related to replication","user":{"login":"matiza5","id":36193329,"node_id":"MDQ6VXNlcjM2MTkzMzI5","avatar_url":"https://avatars.githubusercontent.com/u/36193329?v=4","gravatar_id":"","url":"https://api.github.com/users/matiza5","html_url":"https://github.com/matiza5","followers_url":"https://api.github.com/users/matiza5/followers","following_url":"https://api.github.com/users/matiza5/following{/other_user}","gists_url":"https://api.github.com/users/matiza5/gists{/gist_id}","starred_url":"https://api.github.com/users/matiza5/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/matiza5/subscriptions","organizations_url":"https://api.github.com/users/matiza5/orgs","repos_url":"https://api.github.com/users/matiza5/repos","events_url":"https://api.github.com/users/matiza5/events{/privacy}","received_events_url":"https://api.github.com/users/matiza5/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-21T10:12:44Z","updated_at":"2021-10-28T07:43:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\n\r\nI have some issues with clickhouse and don't have any clue how to deal with it.\r\nI use Clickhouse (21.7.5) on Kubernetes (1.20) with following setup: 4 shards, each contains 2 replicas. I use KafkaEngine on every clickhouse pod, which connects within single consumer group to own kafka's partition. Then materialized view also running on each node moves kafka messages to distributed table.\r\nAfter fresh install everything works fine, but when I did rolling restart of all nodes I started receiving such errors on first replica on every shard:\r\n```\r\n2021.10.21 07:54:09.060338 [ 365 ] {} <Error> apicalls.events_1min: auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &) const: Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Net::ConnectionResetException::ConnectionResetException(int) @ 0x13b64a69 in /usr/bin/clickhouse\r\n1. Poco::Net::SocketImpl::error(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x13b71f9c in /usr/bin/clickhouse\r\n2. Poco::Net::SocketImpl::receiveBytes(void*, int, int) @ 0x13b733eb in /usr/bin/clickhouse\r\n3. Poco::Net::HTTPSession::receive(char*, int) @ 0x13b51b16 in /usr/bin/clickhouse\r\n4. Poco::Net::HTTPHeaderStreamBuf::readFromDevice(char*, long) @ 0x13b48ebf in /usr/bin/clickhouse\r\n5. Poco::BasicBufferedStreamBuf<char, std::__1::char_traits<char>, Poco::Net::HTTPBufferAllocator>::underflow() @ 0x13b3d1f0 in /usr/bin/clickhouse\r\n6. std::__1::basic_streambuf<char, std::__1::char_traits<char> >::uflow() @ 0x158b0f2a in ?\r\n7. std::__1::basic_istream<char, std::__1::char_traits<char> >::get() @ 0x158b36ea in ?\r\n8. Poco::Net::HTTPResponse::read(std::__1::basic_istream<char, std::__1::char_traits<char> >&) @ 0x13b4f4f1 in /usr/bin/clickhouse\r\n9. Poco::Net::HTTPClientSession::receiveResponse(Poco::Net::HTTPResponse&) @ 0x13b41fcd in /usr/bin/clickhouse\r\n10. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::call(Poco::URI, Poco::Net::HTTPResponse&) @ 0x10c549ef in /usr/bin/clickhouse\r\n11. DB::detail::ReadWriteBufferFromHTTPBase<std::__1::shared_ptr<DB::UpdatablePooledSession> >::ReadWriteBufferFromHTTPBase(std::__1::shared_ptr<DB::UpdatablePooledSession>, Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, Poco::Net::HTTPBasicCredentials const&, unsigned long, std::__1::vector<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > >, DB::RemoteHostFilter const&) @ 0x10c51582 in /usr/bin/clickhouse\r\n12. DB::PooledReadWriteBufferFromHTTP::PooledReadWriteBufferFromHTTP(Poco::URI, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void (std::__1::basic_ostream<char, std::__1::char_traits<char> >&)>, DB::ConnectionTimeouts const&, Poco::Net::HTTPBasicCredentials const&, unsigned long, unsigned long, unsigned long) @ 0x10c47f18 in /usr/bin/clickhouse\r\n13. DB::DataPartsExchange::Fetcher::fetchPart(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::Context const>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, DB::ConnectionTimeouts const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Throttler>, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::optional<DB::CurrentlySubmergingEmergingTagger>*, bool, std::__1::shared_ptr<DB::IDisk>) @ 0x10c44d83 in /usr/bin/clickhouse\r\n14. ? @ 0x10af52a5 in /usr/bin/clickhouse\r\n15. DB::StorageReplicatedMergeTree::fetchPart(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, unsigned long, std::__1::shared_ptr<zkutil::ZooKeeper>) @ 0x10a5af94 in /usr/bin/clickhouse\r\n16. DB::StorageReplicatedMergeTree::executeFetch(DB::ReplicatedMergeTreeLogEntry&) @ 0x10a53149 in /usr/bin/clickhouse\r\n17. DB::StorageReplicatedMergeTree::executeLogEntry(DB::ReplicatedMergeTreeLogEntry&) @ 0x10a32954 in /usr/bin/clickhouse\r\n18. ? @ 0x10af241f in /usr/bin/clickhouse\r\n19. DB::ReplicatedMergeTreeQueue::processEntry(std::__1::function<std::__1::shared_ptr<zkutil::ZooKeeper> ()>, std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&, std::__1::function<bool (std::__1::shared_ptr<DB::ReplicatedMergeTreeLogEntry>&)>) @ 0x10e870ac in /usr/bin/clickhouse\r\n20. DB::StorageReplicatedMergeTree::processQueueEntry(std::__1::shared_ptr<DB::ReplicatedMergeTreeQueue::SelectedEntry>) @ 0x10a7d2bd in /usr/bin/clickhouse\r\n21. ? @ 0x10c3ce57 in /usr/bin/clickhouse\r\n22. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x8feda98 in /usr/bin/clickhouse\r\n23. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x8fef63f in /usr/bin/clickhouse\r\n24. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x8fead7f in /usr/bin/clickhouse\r\n25. ? @ 0x8fee663 in /usr/bin/clickhouse\r\n26. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n27. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 21.8.7.22 (official build))\r\n```\r\n\r\nI receive ~10-15 errors each minute. Everything seems to works fine, but I'm not sure about data consitency and I'd be more happy, when those errors were gone. (false-positives while using automatic logs monitoring, alerting etc.)\r\nI started digging and found this (it show different parts over and over, it's not about single part):\r\n\r\n```\r\nSELECT\r\n    table,\r\n    postpone_reason,\r\n    last_exception\r\nFROM system.replication_queue\r\n\r\nQuery id: b5c3140e-d8a3-4cce-aa29-8db431321854\r\n\r\n┌─table───────┬─postpone_reason───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─last_exception──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\r\n│ events_1h   │ Not executing log entry queue-0000214598 for part 20211017_142379_152980_4841 because another log entry for the same part is being processed. This shouldn't happen often.                                │ Code: 234, e.displayText() = DB::Exception: No active replica has part 20211017_142379_152980_4841 or covering part (version 21.8.7.22 (official build)) │\r\n│ events_1h   │ Not executing log entry queue-0000214589 for part 20211017_152979_152979_0 because it is covered by part 20211017_142379_152980_4841 that is currently executing.                                         │ Poco::Exception. Code: 1000, e.code() = 104, e.displayText() = Connection reset by peer (version 21.8.7.22 (official build))                              │\r\n│ events_1h   │ Not executing log entry queue-0000214592 for part 20211017_152975_152980_1 because it is covered by part 20211017_142379_152980_4841 that is currently executing.                                         │\r\n│ events_1h   │ Not executing log entry queue-0000214600 of type MERGE_PARTS for part 20211017_142379_152981_4842 because part 20211017_142379_152980_4841 is not ready yet (log entry for that part is being processed). │\r\n└─────────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\nI don't know what I can do to get rid of this. It seems that is some replication issue/lag/???. Maybe I should adjust some `max_block_size values`? or maybe `kafka_flush_interval_ms`? I went through https://kb.altinity.com/altinity-kb-integrations/altinity-kb-kafka/altinity-kb-kafka-main-parsing-loop/ but I don't think that my settings are somehow wrong.\r\n\r\nThis is my current insert rates:\r\n```\r\nSELECT\r\n    ProfileEvent_InsertedRows,\r\n    event_time\r\nFROM system.metric_log\r\nORDER BY event_time DESC\r\nLIMIT 20\r\n\r\nQuery id: a4be9688-7faa-4bcd-9d03-7894afe75878\r\n\r\n┌─ProfileEvent_InsertedRows─┬──────────event_time─┐\r\n│                     30038 │ 2021-10-21 10:04:56 │\r\n│                     19814 │ 2021-10-21 10:04:55 │\r\n│                         0 │ 2021-10-21 10:04:54 │\r\n│                         0 │ 2021-10-21 10:04:53 │\r\n│                        39 │ 2021-10-21 10:04:52 │\r\n│                         0 │ 2021-10-21 10:04:51 │\r\n│                     98743 │ 2021-10-21 10:04:50 │\r\n│                     98745 │ 2021-10-21 10:04:49 │\r\n│                     42386 │ 2021-10-21 10:04:48 │\r\n│                      6609 │ 2021-10-21 10:04:47 │\r\n│                         0 │ 2021-10-21 10:04:46 │\r\n│                         0 │ 2021-10-21 10:04:45 │\r\n│                        40 │ 2021-10-21 10:04:44 │\r\n│                         0 │ 2021-10-21 10:04:43 │\r\n│                    165783 │ 2021-10-21 10:04:42 │\r\n│                     51969 │ 2021-10-21 10:04:41 │\r\n│                     24687 │ 2021-10-21 10:04:40 │\r\n│                     15382 │ 2021-10-21 10:04:39 │\r\n│                         0 │ 2021-10-21 10:04:38 │\r\n│                        40 │ 2021-10-21 10:04:37 │\r\n└───────────────────────────┴─────────────────────┘\r\n```\r\n\r\nAnd my current tables schema:\r\n```\r\nCREATE DATABASE IF NOT EXISTS apicalls ON CLUSTER metrics;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.kafka_queue ON CLUSTER metrics (\r\n    `message` String\r\n) ENGINE = Kafka SETTINGS kafka_broker_list = 'kafka-kafka-0.kafka-kafka-brokers.kafka.svc.cluster.local:9092,kafka-kafka-1.kafka-kafka-brokers.kafka.svc.cluster.local:9092,kafka-kafka-2.kafka-kafka-brokers.kafka.svc.cluster.local:9092', \r\n                          kafka_topic_list = 'apicall-agg',\r\n                          kafka_group_name = 'k8sgroup-raw',\r\n                          kafka_format = 'JSONAsString',\r\n                          kafka_num_consumers = 1,\r\n                          kafka_max_block_size = 1048576;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `user_id` String,\r\n    `user_ip` IPv4,\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `host` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `execution_time_ms` UInt32\r\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/apicalls.events', '{replica}')\r\nPARTITION BY toStartOfDay(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, user_id, process_name, api_version, transport, method, host)\r\nTTL timestamp + toIntervalDay(2)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_distributed ON CLUSTER metrics AS apicalls.events\r\nENGINE = Distributed(metrics, apicalls, events, cityHash64(application_client_id,license_id,process_name,method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.kafka_consumer_to_events ON CLUSTER metrics\r\nTO apicalls.events_distributed\r\nAS WITH JSONExtractRaw(message, 'Body') AS body SELECT\r\n    toDateTime(JSONExtractUInt(message, 'CreatedAt') /1000) AS timestamp,\r\n    JSONExtractString(body, 'application_client_id') AS application_client_id,\r\n    JSONExtractString(body, 'user_id') AS user_id,\r\n    toIPv4(JSONExtractString(body, 'user_ip')) AS user_ip,\r\n    JSONExtractUInt(body, 'license_id') AS license_id,\r\n    JSONExtractString(body, 'process_name') AS process_name,\r\n    JSONExtractString(body, 'api_version') AS api_version,\r\n    JSONExtractString(body, 'transport') AS transport,\r\n    JSONExtractString(body, 'method') AS method,\r\n    JSONExtractString(body, 'host') AS host,\r\n    multiIf(host LIKE 'bamboo%', 'bamboo', extract(host, '\\\\w+$')) AS environment,\r\n    JSONExtractUInt(body, 'execution_time_ms') AS execution_time_ms\r\nFROM apicalls.kafka_queue;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_raw ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `user_id` String,\r\n    `user_ip` IPv4,\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `host` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `execution_time_ms` UInt32\r\n) ENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/apicalls.events_raw', '{replica}')\r\nPARTITION BY toStartOfDay(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, user_id, process_name, api_version, transport, method, host)\r\nTTL timestamp TO VOLUME 'default', timestamp + toIntervalDay(2) TO VOLUME 's3'\r\nSETTINGS storage_policy='tiered', index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_raw_distributed ON CLUSTER metrics AS apicalls.events_raw\r\nENGINE = Distributed(metrics, apicalls, events_raw, cityHash64(timestamp, process_name, method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.events_raw_mv ON CLUSTER metrics\r\nTO apicalls.events_raw_distributed\r\nAS SELECT\r\n    timestamp,\r\n    application_client_id,\r\n    user_id,\r\n    user_ip,\r\n    license_id,\r\n    process_name,\r\n    api_version,\r\n    transport,\r\n    method,\r\n    host,\r\n    environment,\r\n    execution_time_ms\r\nFROM apicalls.events_distributed;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_1min ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `count` UInt32\r\n) ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/apicalls.events_1min', '{replica}', (count))\r\nPARTITION BY toStartOfDay(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, process_name, api_version, transport, method)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_1min_distributed ON CLUSTER metrics AS apicalls.events_1min\r\nENGINE = Distributed(metrics, apicalls, events_1min, cityHash64(timestamp, process_name, method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.events_1min_mv ON CLUSTER metrics\r\nTO apicalls.events_1min_distributed\r\nAS SELECT\r\n    toStartOfMinute(timestamp) as timestamp,\r\n    application_client_id,\r\n    license_id,\r\n    process_name,\r\n    api_version,\r\n    transport,\r\n    method,\r\n    environment,\r\n    count() as count\r\nFROM apicalls.events_distributed\r\nGROUP BY timestamp, application_client_id, license_id, process_name, api_version, transport, method, environment;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_5min ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `count` UInt32\r\n) ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/apicalls.events_5min', '{replica}', (count))\r\nPARTITION BY toStartOfDay(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, process_name, api_version, transport, method)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_5min_distributed ON CLUSTER metrics AS apicalls.events_5min\r\nENGINE = Distributed(metrics, apicalls, events_5min, cityHash64(timestamp, process_name, method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.events_5min_mv on CLUSTER metrics\r\nTO apicalls.events_5min_distributed\r\nAS SELECT\r\n    toStartOfFiveMinute(timestamp) as timestamp,\r\n    application_client_id,\r\n    license_id,\r\n    process_name,\r\n    api_version,\r\n    transport,\r\n    method,\r\n    environment,\r\n    count() as count\r\nFROM apicalls.events_distributed\r\nGROUP BY timestamp, application_client_id, license_id, process_name, api_version, transport, method, environment;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_15min ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `count` UInt32\r\n) ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/apicalls.events_15min', '{replica}', (count))\r\nPARTITION BY toStartOfDay(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, process_name, api_version, transport, method)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_15min_distributed ON CLUSTER metrics AS apicalls.events_15min\r\nENGINE = Distributed(metrics, apicalls, events_15min, cityHash64(timestamp, process_name, method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.events_15min_mv on CLUSTER metrics\r\nTO apicalls.events_15min_distributed\r\nAS SELECT\r\n    toStartOfFifteenMinutes(timestamp) as timestamp,\r\n    application_client_id,\r\n    license_id,\r\n    process_name,\r\n    api_version,\r\n    transport,\r\n    method,\r\n    environment,\r\n    count() as count\r\nFROM apicalls.events_distributed\r\nGROUP BY timestamp, application_client_id, license_id, process_name, api_version, transport, method, environment;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_1h ON CLUSTER metrics (\r\n    `timestamp` DateTime,\r\n    `application_client_id` LowCardinality(String),\r\n    `license_id` UInt64,\r\n    `process_name` LowCardinality(String),\r\n    `api_version` LowCardinality(String),\r\n    `transport` LowCardinality(String),\r\n    `method` LowCardinality(String),\r\n    `environment` LowCardinality(String),\r\n    `count` UInt32\r\n) ENGINE = ReplicatedSummingMergeTree('/clickhouse/tables/{shard}/apicalls.events_1h', '{replica}', (count))\r\nPARTITION BY toStartOfWeek(timestamp)\r\nORDER BY (timestamp, application_client_id, license_id, process_name, api_version, transport, method)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE IF NOT EXISTS apicalls.events_1h_distributed ON CLUSTER metrics AS apicalls.events_1h\r\nENGINE = Distributed(metrics, apicalls, events_1h, cityHash64(timestamp, process_name, method));\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS apicalls.events_1h_mv ON CLUSTER metrics\r\nTO apicalls.events_1h_distributed\r\nAS SELECT\r\n    toStartOfHour(timestamp) as timestamp,\r\n    application_client_id,\r\n    license_id,\r\n    process_name,\r\n    api_version,\r\n    transport,\r\n    method,\r\n    environment,\r\n    count() as count\r\nFROM apicalls.events_distributed\r\nGROUP BY timestamp, application_client_id, license_id, process_name, api_version, transport, method, environment;\r\n```\r\nAny clues?\r\nThanks","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30486/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30485","id":1032301761,"node_id":"I_kwDOA5dJV849h6zB","number":30485,"title":"with rollup cannot work with toString ","user":{"login":"compasses","id":10161171,"node_id":"MDQ6VXNlcjEwMTYxMTcx","avatar_url":"https://avatars.githubusercontent.com/u/10161171?v=4","gravatar_id":"","url":"https://api.github.com/users/compasses","html_url":"https://github.com/compasses","followers_url":"https://api.github.com/users/compasses/followers","following_url":"https://api.github.com/users/compasses/following{/other_user}","gists_url":"https://api.github.com/users/compasses/gists{/gist_id}","starred_url":"https://api.github.com/users/compasses/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/compasses/subscriptions","organizations_url":"https://api.github.com/users/compasses/orgs","repos_url":"https://api.github.com/users/compasses/repos","events_url":"https://api.github.com/users/compasses/events{/privacy}","received_events_url":"https://api.github.com/users/compasses/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-21T10:05:15Z","updated_at":"2021-10-26T05:33:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nA clear and concise description of what works not as it is supposed to.\r\n\r\nThe rollup function seems cannot work with toString function. As I know, the rollup implementation depend on the order of group key in the sql, and the toString function will be removed during the group by optimization , because toString is an injective function. And then the order of group by keys will be changed which lead to wrong reslut.\r\n\r\nLike the sql:\r\n\r\nselect a,toString(b),c from table group by c,b,a with rollup  \r\n\r\nwill give wrong result,  which different with: \r\n\r\nselect a,b,c from table group by c,b,a with rollup  \r\n\r\nBut they should have same result. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30485/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30480","id":1032112747,"node_id":"I_kwDOA5dJV849hMpr","number":30480,"title":"Whether ClickHouse supports JobSchedule ","user":{"login":"16601746291","id":49640575,"node_id":"MDQ6VXNlcjQ5NjQwNTc1","avatar_url":"https://avatars.githubusercontent.com/u/49640575?v=4","gravatar_id":"","url":"https://api.github.com/users/16601746291","html_url":"https://github.com/16601746291","followers_url":"https://api.github.com/users/16601746291/followers","following_url":"https://api.github.com/users/16601746291/following{/other_user}","gists_url":"https://api.github.com/users/16601746291/gists{/gist_id}","starred_url":"https://api.github.com/users/16601746291/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/16601746291/subscriptions","organizations_url":"https://api.github.com/users/16601746291/orgs","repos_url":"https://api.github.com/users/16601746291/repos","events_url":"https://api.github.com/users/16601746291/events{/privacy}","received_events_url":"https://api.github.com/users/16601746291/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"tom-clickhouse","id":97666923,"node_id":"U_kgDOBdJHaw","avatar_url":"https://avatars.githubusercontent.com/u/97666923?v=4","gravatar_id":"","url":"https://api.github.com/users/tom-clickhouse","html_url":"https://github.com/tom-clickhouse","followers_url":"https://api.github.com/users/tom-clickhouse/followers","following_url":"https://api.github.com/users/tom-clickhouse/following{/other_user}","gists_url":"https://api.github.com/users/tom-clickhouse/gists{/gist_id}","starred_url":"https://api.github.com/users/tom-clickhouse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tom-clickhouse/subscriptions","organizations_url":"https://api.github.com/users/tom-clickhouse/orgs","repos_url":"https://api.github.com/users/tom-clickhouse/repos","events_url":"https://api.github.com/users/tom-clickhouse/events{/privacy}","received_events_url":"https://api.github.com/users/tom-clickhouse/received_events","type":"User","site_admin":false},"assignees":[{"login":"tom-clickhouse","id":97666923,"node_id":"U_kgDOBdJHaw","avatar_url":"https://avatars.githubusercontent.com/u/97666923?v=4","gravatar_id":"","url":"https://api.github.com/users/tom-clickhouse","html_url":"https://github.com/tom-clickhouse","followers_url":"https://api.github.com/users/tom-clickhouse/followers","following_url":"https://api.github.com/users/tom-clickhouse/following{/other_user}","gists_url":"https://api.github.com/users/tom-clickhouse/gists{/gist_id}","starred_url":"https://api.github.com/users/tom-clickhouse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tom-clickhouse/subscriptions","organizations_url":"https://api.github.com/users/tom-clickhouse/orgs","repos_url":"https://api.github.com/users/tom-clickhouse/repos","events_url":"https://api.github.com/users/tom-clickhouse/events{/privacy}","received_events_url":"https://api.github.com/users/tom-clickhouse/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-21T06:33:39Z","updated_at":"2022-01-24T16:44:06Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"in sql server ，we can create job schedule, then job schedule call sql script or procedure，like this(notice  \"...... \"  Meas Some omitted statements)：\r\n\r\nDECLARE @jobid uniqueidentifier, @jobname sysname, @returncode INT\r\nSET @jobname = N'job'\r\nBEGIN\r\n......  \r\n@sql_basic_dim=N'EXECUTE pro_basic_dim_merge' \r\n......  \r\n@command = @sql_basic_dim \r\n......  \r\nEXEC msdb.dbo.sp_add_jobschedule\r\n         @job_id = @jobid,\r\n         @name = 'MAIN_SCHEDULE',\r\n         @freq_type = 4, -- every day\r\n         @freq_interval = 1, --  \r\n         @freq_subday_type = 0x4, -- 0x4 minute，0x8 hour\r\n         @freq_subday_interval = 5, \r\n         @active_start_time = 000500; --  job start time,formate HHMMSS\r\n......  \r\n--  start job\r\nEXEC @returncode = msdb.dbo.sp_start_job @job_id = @jobid\r\nGO\r\n\r\nso, clickhouse have like sql server similar reference  ？ \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30480/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30479","id":1031988019,"node_id":"I_kwDOA5dJV849guMz","number":30479,"title":"how to boost insert select  from  remote table ","user":{"login":"674345386","id":7128380,"node_id":"MDQ6VXNlcjcxMjgzODA=","avatar_url":"https://avatars.githubusercontent.com/u/7128380?v=4","gravatar_id":"","url":"https://api.github.com/users/674345386","html_url":"https://github.com/674345386","followers_url":"https://api.github.com/users/674345386/followers","following_url":"https://api.github.com/users/674345386/following{/other_user}","gists_url":"https://api.github.com/users/674345386/gists{/gist_id}","starred_url":"https://api.github.com/users/674345386/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/674345386/subscriptions","organizations_url":"https://api.github.com/users/674345386/orgs","repos_url":"https://api.github.com/users/674345386/repos","events_url":"https://api.github.com/users/674345386/events{/privacy}","received_events_url":"https://api.github.com/users/674345386/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-21T02:21:25Z","updated_at":"2021-10-21T03:54:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"hi, when i run query :\r\n\r\ninsert into db.tb1_local select * from remote ('11.60.#.#',db.tb1_local,'username','passwd') where partition_col =  '2021-10-10 23:00:00'  SETTINGS max_execution_time=650,max_insert_threads=300,max_threads=300;\r\n\r\nit took almost :  Elapsed: 482.117 sec. Processed 17.26 million rows, 228.62 GB (35.80 thousand rows/s., 474.19 MB/s.) \r\n500sec ,  \r\nit looks like it was a little bit slow than the  common INSERT ,\r\nhow can i boost this  INSERT SELECT FROM REMOTE ? \r\n\r\nthx ~","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30479/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30478","id":1031915203,"node_id":"I_kwDOA5dJV849gcbD","number":30478,"title":"ASOF JOIN LowCardinality and constant, Expected String, got Const(String)","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-20T23:38:50Z","updated_at":"2021-10-21T00:13:31Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```sql\r\ncreate table tz(A LowCardinality(String), t DateTime) Engine = Memory;\r\n\r\n\r\nSELECT *\r\nFROM ( SELECT 'x' A, toDateTime(0) t) AS T\r\nASOF INNER JOIN tz on T.A = tz.A and tz.t <= T.t\r\n\r\nReceived exception from server (version 21.10.2):\r\nCode: 44. DB::Exception: Received from localhost:9000. DB::Exception: Invalid column type for \r\nColumnUnique::insertRangeFrom. Expected String, got Const(String). (ILLEGAL_COLUMN)\r\n\r\n```\r\nno issue in case of `A String`\r\n\r\nWA: `materialize` or `toLowCardinality`\r\n```sql\r\nSELECT *\r\nFROM ( SELECT materialize('x') A, toDateTime(0) t) AS T\r\nASOF INNER JOIN tz on T.A = tz.A and tz.t <= T.t\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30478/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30445","id":1031271207,"node_id":"I_kwDOA5dJV849d_Mn","number":30445,"title":"about MaterializeMySQL  change PARTITION BY toYYYYMM() ","user":{"login":"vegastar002","id":4318676,"node_id":"MDQ6VXNlcjQzMTg2NzY=","avatar_url":"https://avatars.githubusercontent.com/u/4318676?v=4","gravatar_id":"","url":"https://api.github.com/users/vegastar002","html_url":"https://github.com/vegastar002","followers_url":"https://api.github.com/users/vegastar002/followers","following_url":"https://api.github.com/users/vegastar002/following{/other_user}","gists_url":"https://api.github.com/users/vegastar002/gists{/gist_id}","starred_url":"https://api.github.com/users/vegastar002/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vegastar002/subscriptions","organizations_url":"https://api.github.com/users/vegastar002/orgs","repos_url":"https://api.github.com/users/vegastar002/repos","events_url":"https://api.github.com/users/vegastar002/events{/privacy}","received_events_url":"https://api.github.com/users/vegastar002/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-20T11:01:27Z","updated_at":"2021-12-14T19:07:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"clickhouse version : 21.3\r\nmysql version : 5.7\r\n\r\nI have a MaterializeMySQL  table, it's structure:\r\n\r\n\r\nCREATE TABLE ck.t1\r\n(\r\n    `XZQH` String,\r\n    `JJDBH` String,\r\n    `SJDBH` String,\r\n    `BJFSDM` String,\r\n    `JJLXDM` String,\r\n    `CLLXDM` String,\r\n    `JJDWDM` String,\r\n    `JJYGH` String,\r\n    `JJYXM` String,\r\n    `JJTBH` String,\r\n    `JJTIP` String,\r\n    `HRSJ` String,\r\n    `BJSJ` DateTime,\r\n    `HZSJ` String,\r\n    `BJDH` String,\r\n    `SJXZQH` String,\r\n    `BDLAJBZ` String,\r\n    `RKSJ` String,\r\n    `BJZLDM` String,\r\n    `SFCKZDJQ` String,\r\n    `AY` String,\r\n    `_sign` Int8 MATERIALIZED 1,\r\n    `_version` UInt64 MATERIALIZED 1\r\n)\r\nENGINE = ReplacingMergeTree(_version)\r\nORDER BY (XZQH, JJDBH)\r\nSETTINGS index_granularity = 8192\r\n\r\ni know MaterializeMySQL  default partition is  intDiv(id, 4294967) . \r\nbut I frequently  sql  is  like  \" where  datetime1 > xxx and datetime1 < yyy \"  .\r\nso I want to ask how to change to PARTITION BY toYYYYMM()  ?\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30445/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30434","id":1031098872,"node_id":"PR_kwDOA5dJV84tbIAh","number":30434,"title":"RFC: save memory and improve mark cache hit rate by compressing marks in memory","user":{"login":"jorisgio","id":1108145,"node_id":"MDQ6VXNlcjExMDgxNDU=","avatar_url":"https://avatars.githubusercontent.com/u/1108145?v=4","gravatar_id":"","url":"https://api.github.com/users/jorisgio","html_url":"https://github.com/jorisgio","followers_url":"https://api.github.com/users/jorisgio/followers","following_url":"https://api.github.com/users/jorisgio/following{/other_user}","gists_url":"https://api.github.com/users/jorisgio/gists{/gist_id}","starred_url":"https://api.github.com/users/jorisgio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jorisgio/subscriptions","organizations_url":"https://api.github.com/users/jorisgio/orgs","repos_url":"https://api.github.com/users/jorisgio/repos","events_url":"https://api.github.com/users/jorisgio/events{/privacy}","received_events_url":"https://api.github.com/users/jorisgio/received_events","type":"User","site_admin":false},"labels":[],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2021-10-20T08:01:07Z","updated_at":"2022-01-07T11:24:46Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30434","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30434","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30434.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30434.patch","merged_at":null},"body":"When using tables with hundreds of billions of rows and 10s of columns, storing all marks in memory can easily use several hundreds of gigabytes. Although mark cache is officially a cache, running queries with high miss rate has a sizeable performance impact. One solution would be to increase granularity, but this sacrifices performances of small queries. \r\nBut actually, marks data representation is very wasteful. Offset in compressed block is usually 0, so it takes 8 bytes to store zeros, and offset field is monotonic. Data can easily be compressed in memory, which allows to keep all marks in ram when using servers with good ram/disks space ratio.\r\nThis is a rough implementation of such idea, we have been running it on production for more than a year, and it has had only beneficial effects.\r\n\r\nThat being said, the implementation is pretty basic. It split marks into blocks, and compress each blocks with ZSTD, with a cache buffer for one decompressed space. It uses a mutex, which is not optimal, but does not seem to affect perf too much. A proper implemenation would probably use Delta coding codec instead of zstd, and it should probably be an opt-in feature, but submitting this in the current form to know if there is interest in such a feature. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":2,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30434/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30421","id":1030815142,"node_id":"I_kwDOA5dJV849cP2m","number":30421,"title":"date column + WITH FILL timestamp return meaningless dates.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-19T23:08:33Z","updated_at":"2021-10-19T23:08:33Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\n\r\nIf you will try to use date column with WITH FILL based on DateTime values ClickHouse will silently overflow values.\r\nWhich can be unexpected if you use toStartOfInterval functions with MONTH / WEEK / DAY / HOUR precision, because some of them returns Date and some of them return DateTime.\r\n\r\n**How to reproduce**\r\nClickHouse 21.11\r\n\r\n```\r\nSELECT today() AS a\r\nORDER BY a ASC WITH FILL FROM now() - toIntervalMonth(1) TO now() + toIntervalDay(1) STEP 82600\r\n\r\n┌──────────a─┐\r\n│ 2021-10-20 │\r\n└────────────┘\r\n┌──────────a─┐\r\n│ 2105-12-14 │\r\n│ 1973-03-29 │\r\n│ 2019-12-17 │\r\n│ 2066-09-05 │\r\n│ 2113-05-26 │\r\n│ 1980-09-08 │\r\n│ 2027-05-29 │\r\n│ 2074-02-15 │\r\n│ 2120-11-05 │\r\n│ 1988-02-19 │\r\n│ 2034-11-08 │\r\n│ 2081-07-28 │\r\n│ 2128-04-17 │\r\n│ 1995-08-01 │\r\n│ 2042-04-20 │\r\n│ 2089-01-07 │\r\n│ 2135-09-28 │\r\n│ 2003-01-11 │\r\n│ 2049-09-30 │\r\n│ 2096-06-19 │\r\n│ 2143-03-10 │\r\n│ 2010-06-23 │\r\n│ 2057-03-12 │\r\n│ 2103-12-01 │\r\n│ 1971-03-16 │\r\n│ 2017-12-03 │\r\n│ 2064-08-22 │\r\n│ 2111-05-13 │\r\n│ 1978-08-26 │\r\n│ 2025-05-15 │\r\n│ 2072-02-02 │\r\n│ 2118-10-23 │\r\n│ 1986-02-05 │\r\n└────────────┘\r\n\r\n34 rows in set. Elapsed: 0.013 sec.\r\n\r\n\r\nSELECT today() AS a\r\nORDER BY a ASC WITH FILL FROM now() - toIntervalSecond(1) TO now() + toIntervalSecond(1) STEP 1\r\n\r\n┌──────────a─┐\r\n│ 2021-10-20 │\r\n└────────────┘\r\n┌──────────a─┐\r\n│ 2025-07-01 │\r\n│ 2025-07-02 │\r\n└────────────┘\r\n```\r\n\r\n**Expected behavior**\r\nDisallow mixing Date/DateTime datatypes here or correctly convert them.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30421/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30420","id":1030811209,"node_id":"I_kwDOA5dJV849cO5J","number":30420,"title":"toStartOfInterval(now(),INTERVAL 1 WEEK) and toStartOfWeek(now()) use different mode.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-19T23:01:28Z","updated_at":"2021-10-19T23:01:28Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\ntoStartOf functions uses different mode for rounding, which can be confusing. \r\nIt's also not possible to pass round mode to `toStartOfInterval(now(), toIntervalWeek(1))` function.\r\n\r\n**How to reproduce**\r\nClickHouse version 21.11\r\n\r\n```\r\nSELECT\r\n    toStartOfInterval(now(), toIntervalWeek(1)) AS interval,\r\n    toStartOfWeek(now()) AS week,\r\n    date_trunc('week', now()) AS trunc\r\n\r\n┌───interval─┬───────week─┬──────trunc─┐\r\n│ 2021-10-18 │ 2021-10-17 │ 2021-10-18 │\r\n└────────────┴────────────┴────────────┘\r\n```\r\n\r\n**Expected behavior**\r\nFunctions return the same result\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30420/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30415","id":1030771936,"node_id":"I_kwDOA5dJV849cFTg","number":30415,"title":"timeZoneNormalize function","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-19T21:47:54Z","updated_at":"2021-10-20T10:02:20Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nTable has 2 columns:\r\ntimestamp DateTime,\r\ntimezone LowCardinality(String),\r\n\r\nEach timestamp has individual timezone value and we need somehow to convert those timestamps to a single timezone.\r\n\r\n**Describe the solution you'd like**\r\n\r\n```\r\nSELECT timeZoneNormalize(timestamp, timezone); -- to UTC\r\n\r\nSELECT timeZoneNormalize(timestamp, timezone, 'Europe/Moscow'); -- destination timezone.\r\n```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30415/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30414","id":1030713386,"node_id":"I_kwDOA5dJV849b3Aq","number":30414,"title":"EXPLAIN AST json=1 SELECT ...","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-19T20:27:18Z","updated_at":"2021-10-19T20:27:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\noutput EXPLAIN AST in json format (like EXPLAIN PLAN)\r\n\r\n**Describe the solution you'd like**\r\n\r\n```\r\nEXPLAIN AST json = 1\r\nSELECT 1\r\n\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.11.1):\r\nCode: 115. DB::Exception: Received from localhost:9000. DB::Exception: Settings are not supported for EXPLAIN AST query.. (UNKNOWN_SETTING)\r\n\r\n\r\nEXPLAIN json = 1\r\nSELECT 1\r\n\r\n┌─explain────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐\r\n│ [\r\n  {\r\n    \"Plan\": {\r\n      \"Node Type\": \"Expression\",\r\n      \"Description\": \"(Projection + Before ORDER BY)\",\r\n      \"Plans\": [\r\n        {\r\n          \"Node Type\": \"SettingQuotaAndLimits\",\r\n          \"Description\": \"Set limits and quota after reading from storage\",\r\n          \"Plans\": [\r\n            {\r\n              \"Node Type\": \"ReadFromStorage\",\r\n              \"Description\": \"SystemOne\"\r\n            }\r\n          ]\r\n        }\r\n      ]\r\n    }\r\n  }\r\n] │\r\n└────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘\r\n\r\n\r\nEXPLAIN AST\r\nSELECT 1\r\n\r\n┌─explain───────────────────────────┐\r\n│ SelectWithUnionQuery (children 1) │\r\n│  ExpressionList (children 1)      │\r\n│   SelectQuery (children 1)        │\r\n│    ExpressionList (children 1)    │\r\n│     Literal UInt64_1              │\r\n└───────────────────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30414/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30411","id":1030683747,"node_id":"I_kwDOA5dJV849bvxj","number":30411,"title":"SELECT modifiers RENAME modifier","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-19T19:49:08Z","updated_at":"2021-10-20T02:30:30Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nMake column names after huge amount of APPLY more predictable.\r\n\r\n**Describe the solution you'd like**\r\n\r\n```\r\nWITH groupArray(time) AS time_arr\r\nSELECT * EXCEPT time APPLY groupArray APPLY x -> arrayReverseSort((y, z) -> z, x, time_arr) APPLY arrayDifference APPLY arraySum \r\nFROM\r\n(\r\n    SELECT\r\n        number AS time,\r\n        number % 10 AS value,\r\n        rand() % 10 AS value_2\r\n    FROM numbers(100)\r\n)\r\n\r\n┌─arraySum(arrayDifference(arrayReverseSort(lambda(tuple(y, z), z), groupArray(value), time_arr)))─┬─arraySum(arrayDifference(arrayReverseSort(lambda(tuple(y, z), z), groupArray(value_2), time_arr)))─┐\r\n│                                                                                               -9 │                                                                                                 -3 │\r\n└──────────────────────────────────────────────────────────────────────────────────────────────────┴────────────────────────────────────────────────────────────────────────────────────────────────────┘\r\n\r\n\r\nWITH groupArray(time) AS time_arr\r\nSELECT * EXCEPT time APPLY groupArray APPLY x -> arrayReverseSort((y, z) -> z, x, time_arr) APPLY arrayDifference APPLY arraySum RENAME (col -> col || '_diff_sum')\r\nFROM\r\n(\r\n    SELECT\r\n        number AS time,\r\n        number % 10 AS value,\r\n        rand() % 10 AS value_2\r\n    FROM numbers(100)\r\n)\r\n\r\n┌─value_diff_sum─┬─value_2_diff_sum─┐\r\n│             -9 │               -3 │\r\n└────────────────┴──────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30411/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30410","id":1030680515,"node_id":"I_kwDOA5dJV849bu_D","number":30410,"title":"SELECT modifiers single APPLY keyword for multiple functions","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-19T19:44:30Z","updated_at":"2021-10-20T09:02:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nMake SQL clearer and shorter. \r\n\r\n**Describe the solution you'd like**\r\n\r\n```\r\nSELECT * APPLY abs -> sum  FROM numbers(10)\r\nSELECT * APPLY(abs,sum)  FROM numbers(10)\r\n```\r\n\r\nWhatever syntax you like :)  \r\n\r\n**Additional context**\r\n\r\ntimescale has similar feature\r\nhttps://blog.timescale.com/blog/function-pipelines-building-functional-programming-into-postgresql-using-custom-operators/\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30410/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30408","id":1030650648,"node_id":"I_kwDOA5dJV849bnsY","number":30408,"title":"JSON_MODIFY like in MS SQL server","user":{"login":"morozovsk","id":1822063,"node_id":"MDQ6VXNlcjE4MjIwNjM=","avatar_url":"https://avatars.githubusercontent.com/u/1822063?v=4","gravatar_id":"","url":"https://api.github.com/users/morozovsk","html_url":"https://github.com/morozovsk","followers_url":"https://api.github.com/users/morozovsk/followers","following_url":"https://api.github.com/users/morozovsk/following{/other_user}","gists_url":"https://api.github.com/users/morozovsk/gists{/gist_id}","starred_url":"https://api.github.com/users/morozovsk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/morozovsk/subscriptions","organizations_url":"https://api.github.com/users/morozovsk/orgs","repos_url":"https://api.github.com/users/morozovsk/repos","events_url":"https://api.github.com/users/morozovsk/events{/privacy}","received_events_url":"https://api.github.com/users/morozovsk/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-19T19:05:39Z","updated_at":"2021-10-26T15:54:46Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\nI have string {\"a\":1,\"b\":1} and I need to to update a=2\r\n\r\n**Describe the solution you'd like**\r\n\r\nJSON_MODIFY ('{\"a\":1}', '$.a', 2) = '{\"a\":2}' // update a to 2\r\nJSON_MODIFY ('{\"b\":1}', '$.a', 2) = {\"a\":2,\"b\":1} // add a=2\r\nJSON_MODIFY ('{\"a\":1,\"b\":1}', '$.a', null) = '{\"b\":1}' // delete a\r\n\r\n**Describe alternatives you've considered**\r\n\r\nif(position(str, '\"a\":'), replaceRegexpOne(str, '(\"a\":[0-9]+[,}])', '\"a\":2'), replaceOne('{\"a\":1,\"b\":1}', '}', ',\"a\":2}')\r\nIt is very hard to use and very easy to make mistake. Also I'cant  increment this field.\r\n\r\n**Additional context**\r\n\r\nhttps://docs.microsoft.com/en-us/sql/t-sql/functions/json-modify-transact-sql?view=sql-server-ver15\r\nhttps://www.sqlshack.com/modifying-json-data-using-json_modify-in-sql-server/\r\nhttps://database.guide/json_modify-examples-in-sql-server-t-sql/\r\nhttps://sqlskull.com/2020/10/29/sql-json_modify-function/","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30408/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30407","id":1030649885,"node_id":"I_kwDOA5dJV849bngd","number":30407,"title":"Time Travel (like in other databases)","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-19T19:04:42Z","updated_at":"2021-10-19T19:15:23Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\n```\r\nSELECT * FROM my_table AT(TIMESTAMP => 'Mon, 01 May 2015 16:20:00 -0700'::timestamp_tz);\r\nSELECT * FROM my_table AT(OFFSET => -60*5);\r\n```\r\n\r\nGive ability to run queries against state of table at specific moment of time in past.\r\nWith ability to revert back to this table state. (for example revert change of column type, drop column statement or DELETE WHERE)\r\nSo it has limited usage for append only tables, but with support for light-weight deletes (or updates), people will modify data more and ability to look in the past will became more useful.\r\n\r\nIt's also possible to offload outdated parts / columns to cold and cheap storage like s3. (PITR for backup?)\r\n\r\n**Describe alternatives you've considered**\r\n\r\nFREEZE TABLE and (or) BACKUP TABLE.\r\n\r\n**Additional context**\r\n\r\nSnowflake: https://docs.snowflake.com/en/user-guide/data-time-travel.html\r\n\r\nOracle: https://docs.oracle.com/cd/E11882_01/appdev.112/e41502/adfns_flashback.htm#ADFNS1008\r\n\r\nCockroachDB: https://www.cockroachlabs.com/blog/time-travel-queries-select-witty_subtitle-the_future/\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30407/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30399","id":1030540307,"node_id":"I_kwDOA5dJV849bMwT","number":30399,"title":"Flaky test: test_cluster_copier","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false},"assignees":[{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2021-10-19T16:56:09Z","updated_at":"2021-12-19T12:06:00Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/d004fd96edbc3eb63561a12b846972a3b4961372/integration_tests_(asan).html\r\n```\r\n____________________________ test_copy_simple[True] ____________________________\r\n[gw0] linux -- Python 3.8.10 /usr/bin/python3\r\n\r\nstarted_cluster = <helpers.cluster.ClickHouseCluster object at 0x7f9093fcb460>\r\nuse_sample_offset = True\r\n\r\n    @pytest.mark.parametrize(('use_sample_offset'), [False, True])\r\n    def test_copy_simple(started_cluster, use_sample_offset):\r\n        if use_sample_offset:\r\n>           execute_task(started_cluster, Task1(started_cluster), ['--experimental-use-sample-offset', '1'])\r\n\r\ntest_cluster_copier/test.py:387: \r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\ntest_cluster_copier/test.py:377: in execute_task\r\n    task.check()\r\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \r\n\r\nself = <test_cluster_copier.test.Task1 object at 0x7f9093dd2f40>\r\n\r\n    def check(self):\r\n        assert self.cluster.instances['s0_0_0'].query(\"SELECT count() FROM hits_all\").strip() == \"1002\"\r\n>       assert self.cluster.instances['s1_0_0'].query(\"SELECT count() FROM hits_all\").strip() == \"1002\"\r\nE       AssertionError: assert '927' == '1002'\r\nE         - 1002\r\nE         + 927\r\n\r\ntest_cluster_copier/test.py:109: AssertionError\r\n```\r\n\r\n[Query CI DB](https://play-ci.clickhouse.com/play?user=play#c2VsZWN0IGNoZWNrX25hbWUsIHB1bGxfcmVxdWVzdF9udW1iZXIsIGNoZWNrX3N0YXJ0X3RpbWUsIHRlc3Rfc3RhdHVzLCByZXBvcnRfdXJsLCBjb21taXRfc2hhLCB0ZXN0X25hbWUKZnJvbSBgZ2gtZGF0YWAuY2hlY2tzIAp3aGVyZSAKdGVzdF9zdGF0dXMgbm90IGluIFsnT0snLCAnU1VDQ0VTUycsICdTS0lQUEVEJywgJ1hGQUlMJywgJ1hFUlJPUiddIGFuZCAKLS0gY2hlY2tfbmFtZSBsaWtlICdJbnRlZ3JhdGlvbiUnIGFuZApwdWxsX3JlcXVlc3RfbnVtYmVyID0gMCBhbmQKdGVzdF9uYW1lIExJS0UgJyV0ZXN0X2NsdXN0ZXJfY29waWVyJScgYW5kCmNoZWNrX3N0YXJ0X3RpbWUgPj0gbm93KCkgLSBpbnRlcnZhbCAzMCBkYXkgCm9yZGVyIGJ5IGNoZWNrX3N0YXJ0X3RpbWUgZGVzYyAKbGltaXQgMTAKZm9ybWF0IE1hcmtkb3duCg==\r\n)\r\n| check_name | pull_request_number | check_start_time | test_status | report_url | commit_sha | test_name |\r\n|:-|-:|-:|:-|:-|:-|:-|\r\n| Integration tests (asan) | 0 | 2021-10-18 22:01:32 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/d004fd96edbc3eb63561a12b846972a3b4961372/integration_tests_(asan).html | d004fd96edbc3eb63561a12b846972a3b4961372 | test_cluster_copier/test.py::test_copy_simple[True] |\r\n| Integration tests (release) | 0 | 2021-10-18 20:54:39 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/75c16b8f8924f3e397c5b2e8a4c8da8ddfd7c79e/integration_tests_(release).html | 75c16b8f8924f3e397c5b2e8a4c8da8ddfd7c79e | test_cluster_copier/test.py::test_copy_with_recovering[True] |\r\n| Integration tests (asan) | 0 | 2021-10-18 13:36:04 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/158b4c26b72c5494a831403e8f0392e30fbab386/integration_tests_(asan).html | 158b4c26b72c5494a831403e8f0392e30fbab386 | test_cluster_copier/test.py::test_copy_with_recovering_after_move_faults[False] |\r\n| Integration tests (release) | 0 | 2021-10-18 08:37:02 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/158b4c26b72c5494a831403e8f0392e30fbab386/integration_tests_(release).html | 158b4c26b72c5494a831403e8f0392e30fbab386 | test_cluster_copier/test.py::test_copy_month_to_week_partition |\r\n| Integration tests (release) | 0 | 2021-10-18 08:37:02 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/158b4c26b72c5494a831403e8f0392e30fbab386/integration_tests_(release).html | 158b4c26b72c5494a831403e8f0392e30fbab386 | test_cluster_copier/test.py::test_copy_with_recovering[False] |\r\n| Integration tests (asan) | 0 | 2021-10-18 01:08:29 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/f032fb71f0903e181e92479803a88fb482adbec1/integration_tests_(asan).html | f032fb71f0903e181e92479803a88fb482adbec1 | test_cluster_copier/test.py::test_copy_month_to_week_partition_with_recovering |\r\n| Integration tests (release) | 0 | 2021-10-17 19:02:52 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/f4bfed9d3a7d87e596c2a264617f890a034bfbb0/integration_tests_(release).html | f4bfed9d3a7d87e596c2a264617f890a034bfbb0 | test_cluster_copier/test.py::test_copy_with_recovering[True] |\r\n| Integration tests (asan) | 0 | 2021-10-17 11:13:42 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/e1969e26933e81fbd288bc411a893ea667e5b071/integration_tests_(asan).html | e1969e26933e81fbd288bc411a893ea667e5b071 | test_cluster_copier/test.py::test_copy_month_to_week_partition_with_recovering_after_move_faults |\r\n| Integration tests (asan) | 0 | 2021-10-17 04:49:08 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/0285a15aeca720c4ecccb16755b9d0bd81de0d45/integration_tests_(asan).html | 0285a15aeca720c4ecccb16755b9d0bd81de0d45 | test_cluster_copier/test.py::test_copy_month_to_week_partition_with_recovering_after_move_faults |\r\n| Integration tests (asan) | 0 | 2021-10-17 00:00:14 | FLAKY | https://clickhouse-test-reports.s3.yandex.net/0/0dd8c70d28ad44d0db386dd225732a5d201cfee5/integration_tests_(asan).html | 0dd8c70d28ad44d0db386dd225732a5d201cfee5 | test_cluster_copier/test.py::test_copy_with_recovering[False] |\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30399/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30388","id":1030526806,"node_id":"I_kwDOA5dJV849bJdW","number":30388,"title":"ThreadSanitizer: data race DB::ContextAccess::~ContextAccess()","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false},"assignees":[{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-19T16:40:23Z","updated_at":"2021-10-19T16:40:40Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/49685f0b9a0c8d5cec489574afce428a0ad873d6/stress_test_(thread).html\r\n```\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=499)\r\n  Read of size 8 at 0x7b4c029015c8 by thread T563 (mutexes: write M424036675123337672):\r\n    #0 std::__1::shared_ptr<DB::EnabledSettings const>::~shared_ptr() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3211:9 (clickhouse+0x150184a3)\r\n    #1 DB::ContextAccess::~ContextAccess() obj-x86_64-linux-gnu/../src/Access/ContextAccess.h:64:7 (clickhouse+0x150184a3)\r\n    #2 std::__1::default_delete<DB::ContextAccess>::operator()(DB::ContextAccess*) const obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1397:5 (clickhouse+0x150183ab)\r\n    #3 std::__1::__shared_ptr_pointer<DB::ContextAccess*, std::__1::shared_ptr<DB::ContextAccess>::__shared_ptr_default_delete<DB::ContextAccess, DB::ContextAccess>, std::__1::allocator<DB::ContextAccess> >::__on_zero_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2565:5 (clickhouse+0x150183ab)\r\n    #4 std::__1::__shared_count::__release_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2475:9 (clickhouse+0x15686ac6)\r\n    #5 std::__1::__shared_weak_count::__release_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2517:27 (clickhouse+0x15686ac6)\r\n    #6 std::__1::shared_ptr<DB::ContextAccess const>::~shared_ptr() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3212:19 (clickhouse+0x15686ac6)\r\n    #7 DB::Context::~Context() obj-x86_64-linux-gnu/../src/Interpreters/Context.cpp:455:19 (clickhouse+0x15686ac6)\r\n    #8 std::__1::default_delete<DB::Context>::operator()(DB::Context*) const obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1397:5 (clickhouse+0x156b9d6b)\r\n    #9 std::__1::__shared_ptr_pointer<DB::Context*, std::__1::shared_ptr<DB::Context>::__shared_ptr_default_delete<DB::Context, DB::Context>, std::__1::allocator<DB::Context> >::__on_zero_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2565:5 (clickhouse+0x156b9d6b)\r\n    #10 std::__1::__shared_count::__release_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2475:9 (clickhouse+0x16c2128b)\r\n    #11 std::__1::__shared_weak_count::__release_shared() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2517:27 (clickhouse+0x16c2128b)\r\n    #12 std::__1::shared_ptr<DB::Context>::~shared_ptr() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3212:19 (clickhouse+0x16c2128b)\r\n    #13 DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:809:1 (clickhouse+0x16c2128b)\r\n    #14 DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:929:9 (clickhouse+0x16c25098)\r\n    #15 DB::HTTPServerConnection::run() obj-x86_64-linux-gnu/../src/Server/HTTP/HTTPServerConnection.cpp:58:34 (clickhouse+0x16c93671)\r\n    #16 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a09d222)\r\n    #17 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a09da32)\r\n    #18 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a21f6b5)\r\n    #19 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #20 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\n  Previous write of size 8 at 0x7b4c029015c8 by thread T1915 (mutexes: write M331713088955880912):\r\nThreadSanitizer: growing clock allocator: 8338 out of 4194304*1024\r\n    #0 std::__1::enable_if<(is_move_constructible<std::__1::__shared_weak_count*>::value) && (is_move_assignable<std::__1::__shared_weak_count*>::value), void>::type std::__1::swap<std::__1::__shared_weak_count*>(std::__1::__shared_weak_count*&, std::__1::__shared_weak_count*&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3935:9 (clickhouse+0x15052f5f)\r\n    #1 std::__1::shared_ptr<DB::EnabledSettings const>::swap(std::__1::shared_ptr<DB::EnabledSettings const>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3300:5 (clickhouse+0x15052f5f)\r\n    #2 std::__1::shared_ptr<DB::EnabledSettings const>::operator=(std::__1::shared_ptr<DB::EnabledSettings const>&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3243:34 (clickhouse+0x15052f5f)\r\n    #3 DB::ContextAccess::setUser(std::__1::shared_ptr<DB::User const> const&) const obj-x86_64-linux-gnu/../src/Access/ContextAccess.cpp:177:26 (clickhouse+0x15052f5f)\r\n    #4 DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0::operator()(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) const obj-x86_64-linux-gnu/../src/Access/ContextAccess.cpp:156:9 (clickhouse+0x15058b77)\r\n    #5 decltype(std::__1::forward<DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0&>(fp)(std::__1::forward<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&>(fp0), std::__1::forward<std::__1::shared_ptr<DB::IAccessEntity const> const&>(fp0))) std::__1::__invoke<DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0&, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&>(DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0&, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x15058b77)\r\n    #6 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0&, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&>(DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0&, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x15058b77)\r\nThreadSanitizer: growing clock allocator: 8339 out of 4194304*1024\r\n    #7 std::__1::__function::__default_alloc_func<DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0, void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>::operator()(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x15058b77)\r\n    #8 void std::__1::__function::__policy_invoker<void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::ContextAccess::ContextAccess(DB::AccessControlManager const&, DB::ContextAccessParams const&)::$_0, void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)> >(std::__1::__function::__policy_storage const*, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x15058b77)\r\n    #9 std::__1::__function::__policy_func<void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>::operator()(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x150d9c79)\r\n    #10 std::__1::function<void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>::operator()(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x150d9c79)\r\n    #11 DB::IAccessStorage::notify(std::__1::vector<std::__1::tuple<std::__1::function<void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::shared_ptr<DB::IAccessEntity const> >, std::__1::allocator<std::__1::tuple<std::__1::function<void (StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::shared_ptr<DB::IAccessEntity const> const&)>, StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::shared_ptr<DB::IAccessEntity const> > > > const&) obj-x86_64-linux-gnu/../src/Access/IAccessStorage.cpp:440:9 (clickhouse+0x150d9c79)\r\n    #12 DB::DiskAccessStorage::removeImpl(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&)::$_4::operator()() const obj-x86_64-linux-gnu/../src/Access/DiskAccessStorage.cpp:532:5 (clickhouse+0x150acb7a)\r\n    #13 basic_scope_guard<DB::DiskAccessStorage::removeImpl(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&)::$_4>::invoke() obj-x86_64-linux-gnu/../base/base/../base/scope_guard.h:94:9 (clickhouse+0x150acb7a)\r\n    #14 basic_scope_guard<DB::DiskAccessStorage::removeImpl(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&)::$_4>::~basic_scope_guard() obj-x86_64-linux-gnu/../base/base/../base/scope_guard.h:44:28 (clickhouse+0x150acb7a)\r\n    #15 DB::DiskAccessStorage::removeImpl(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) obj-x86_64-linux-gnu/../src/Access/DiskAccessStorage.cpp:536:1 (clickhouse+0x150acb7a)\r\n    #16 DB::IAccessStorage::remove(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) obj-x86_64-linux-gnu/../src/Access/IAccessStorage.cpp:318:5 (clickhouse+0x150d75a9)\r\n    #17 DB::MultipleAccessStorage::removeImpl(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&) obj-x86_64-linux-gnu/../src/Access/MultipleAccessStorage.cpp:233:21 (clickhouse+0x15102529)\r\n    #18 DB::IAccessStorage::tryRemove(std::__1::vector<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::allocator<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> > > const&)::$_12::operator()() const obj-x86_64-linux-gnu/../src/Access/IAccessStorage.cpp:352:27 (clickhouse+0x150d85a8)\r\n    #19 bool DB::(anonymous namespace)::tryCall<DB::IAccessStorage::tryRemove(std::__1::vector<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::allocator<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> > > const&)::$_12>(DB::IAccessStorage::tryRemove(std::__1::vector<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::allocator<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> > > const&)::$_12 const&) obj-x86_64-linux-gnu/../src/Access/IAccessStorage.cpp:51:13 (clickhouse+0x150d85a8)\r\nThreadSanitizer: growing clock allocator: 8340 out of 4194304*1024\r\n    #20 DB::IAccessStorage::tryRemove(std::__1::vector<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::allocator<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> > > const&) obj-x86_64-linux-gnu/../src/Access/IAccessStorage.cpp:353:13 (clickhouse+0x150d85a8)\r\n    #21 DB::InterpreterDropAccessEntityQuery::execute()::$_0::operator()(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) const obj-x86_64-linux-gnu/../src/Interpreters/InterpreterDropAccessEntityQuery.cpp:39:28 (clickhouse+0x15be2133)\r\n    #22 DB::InterpreterDropAccessEntityQuery::execute() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterDropAccessEntityQuery.cpp:47:9 (clickhouse+0x15be1d03)\r\n    #23 DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:632:32 (clickhouse+0x161b110f)\r\n    #24 DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:947:30 (clickhouse+0x161aeee1)\r\n    #25 DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:316:24 (clickhouse+0x16c73d38)\r\n    #26 DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1837:9 (clickhouse+0x16c87fc7)\r\n    #27 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a09d222)\r\n    #28 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a09da32)\r\n    #29 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a21f6b5)\r\n    #30 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #31 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\n  Location is heap block of size 440 at 0x7b4c02901440 allocated by thread T1714:\r\n    #0 operator new(unsigned long) <null> (clickhouse+0x9b361a8)\r\n    #1 DB::AccessControlManager::ContextAccessCache::getContextAccess(DB::ContextAccessParams const&) obj-x86_64-linux-gnu/../src/Access/AccessControlManager.cpp:74:51 (clickhouse+0x150136a7)\r\n    #2 DB::AccessControlManager::getContextAccess(DB::ContextAccessParams const&) const obj-x86_64-linux-gnu/../src/Access/AccessControlManager.cpp:469:34 (clickhouse+0x1501235d)\r\n    #3 DB::AccessControlManager::getContextAccess(StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> const&, std::__1::vector<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag>, std::__1::allocator<StrongTypedef<wide::integer<128ul, unsigned int>, DB::UUIDTag> > > const&, bool, DB::Settings const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::ClientInfo const&) const obj-x86_64-linux-gnu/../src/Access/AccessControlManager.cpp:463:12 (clickhouse+0x1501235d)\r\n    #4 DB::Context::calculateAccessRights() obj-x86_64-linux-gnu/../src/Interpreters/Context.cpp:760:44 (clickhouse+0x1568b333)\r\n    #5 DB::Context::setCurrentDatabase(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../src/Interpreters/Context.cpp:1191:5 (clickhouse+0x1568a8ad)\r\nThreadSanitizer: growing clock allocator: 8341 out of 4194304*1024\r\n    #6 DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:719:18 (clickhouse+0x16c20642)\r\n    #7 DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:929:9 (clickhouse+0x16c25098)\r\n    #8 DB::HTTPServerConnection::run() obj-x86_64-linux-gnu/../src/Server/HTTP/HTTPServerConnection.cpp:58:34 (clickhouse+0x16c93671)\r\n    #9 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a09d222)\r\n    #10 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a09da32)\r\n    #11 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a21f6b5)\r\n    #12 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #13 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\n  Mutex M424036675123337672 is already destroyed.\r\n\r\n  Mutex M331713088955880912 is already destroyed.\r\n\r\n  Thread T563 'HTTPHandler' (tid=21426, running) created by thread T249 at:\r\n    #0 pthread_create <null> (clickhouse+0x9aa00ad)\r\n    #1 Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6 (clickhouse+0x1a21b9f8)\r\n    #2 Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2 (clickhouse+0x1a21d1cc)\r\nThreadSanitizer: growing clock allocator: 8342 out of 4194304*1024\r\n    #3 Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10 (clickhouse+0x1a2214a6)\r\n    #4 Poco::ThreadPool::getThread() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:461:14 (clickhouse+0x1a2214a6)\r\n    #5 Poco::ThreadPool::startWithPriority(Poco::Thread::Priority, Poco::Runnable&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:365:2 (clickhouse+0x1a221887)\r\n    #6 Poco::Net::TCPServerDispatcher::enqueue(Poco::Net::StreamSocket const&) obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:152:17 (clickhouse+0x1a09df0e)\r\n    #7 Poco::Net::TCPServer::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServer.cpp:148:21 (clickhouse+0x1a09caa7)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\n  Thread T1915 'TCPHandler' (tid=31106, running) created by thread T249 at:\r\n    #0 pthread_create <null> (clickhouse+0x9aa00ad)\r\n    #1 Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6 (clickhouse+0x1a21b9f8)\r\n    #2 Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2 (clickhouse+0x1a21d1cc)\r\n    #3 Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10 (clickhouse+0x1a2214a6)\r\n    #4 Poco::ThreadPool::getThread() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:461:14 (clickhouse+0x1a2214a6)\r\n    #5 Poco::ThreadPool::startWithPriority(Poco::Thread::Priority, Poco::Runnable&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:365:2 (clickhouse+0x1a221887)\r\n    #6 Poco::Net::TCPServerDispatcher::enqueue(Poco::Net::StreamSocket const&) obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:152:17 (clickhouse+0x1a09df0e)\r\n    #7 Poco::Net::TCPServer::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServer.cpp:148:21 (clickhouse+0x1a09caa7)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\n  Thread T1714 'TCPHandler' (tid=24896, running) created by thread T249 at:\r\n    #0 pthread_create <null> (clickhouse+0x9aa00ad)\r\n    #1 Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6 (clickhouse+0x1a21b9f8)\r\n    #2 Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2 (clickhouse+0x1a21d1cc)\r\n    #3 Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10 (clickhouse+0x1a2214a6)\r\n    #4 Poco::ThreadPool::getThread() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:461:14 (clickhouse+0x1a2214a6)\r\n    #5 Poco::ThreadPool::startWithPriority(Poco::Thread::Priority, Poco::Runnable&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:365:2 (clickhouse+0x1a221887)\r\n    #6 Poco::Net::TCPServerDispatcher::enqueue(Poco::Net::StreamSocket const&) obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:152:17 (clickhouse+0x1a09df0e)\r\n    #7 Poco::Net::TCPServer::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServer.cpp:148:21 (clickhouse+0x1a09caa7)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a21d88f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a21bf67)\r\n\r\nSUMMARY: ThreadSanitizer: data race obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:3211:9 in std::__1::shared_ptr<DB::EnabledSettings const>::~shared_ptr()\r\nThreadSanitizer: growing clock allocator: 8343 out of 4194304*1024\r\n==================\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30388/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30387","id":1030486494,"node_id":"I_kwDOA5dJV849a_ne","number":30387,"title":"Speeding up Final by 500% by splitting query into UNION ALL of non overlapping PK ranges","user":{"login":"jorisgio","id":1108145,"node_id":"MDQ6VXNlcjExMDgxNDU=","avatar_url":"https://avatars.githubusercontent.com/u/1108145?v=4","gravatar_id":"","url":"https://api.github.com/users/jorisgio","html_url":"https://github.com/jorisgio","followers_url":"https://api.github.com/users/jorisgio/followers","following_url":"https://api.github.com/users/jorisgio/following{/other_user}","gists_url":"https://api.github.com/users/jorisgio/gists{/gist_id}","starred_url":"https://api.github.com/users/jorisgio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jorisgio/subscriptions","organizations_url":"https://api.github.com/users/jorisgio/orgs","repos_url":"https://api.github.com/users/jorisgio/repos","events_url":"https://api.github.com/users/jorisgio/events{/privacy}","received_events_url":"https://api.github.com/users/jorisgio/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2021-10-19T15:59:07Z","updated_at":"2022-01-10T01:09:59Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Hi, i am looking about optimizing merging of live updated data a recurrent question i know, but it seems a lot slower than it can be. Let's take a table like\r\n```sql\r\nCREATE TABLE test (\r\n    part_key UInt8,\r\n    key1 String,\r\n    key2 String,\r\n    event_time DateTime,\r\n    hash_of_key1_key2 UInt64,\r\n    dimension_1 String,\r\n    metrics_1 UInt64\r\n    )\r\n    PARTITION BY part_key\r\n    ORDER BY (key1, key2)\r\n    Engine = ReplacingMergeTree(event_time)\r\n ```\r\n    \r\nwhere primary key contains fairly large string.\r\nData set is partitioned by key space to reduce merges pressure in relatively large partitions of ~3B rows. Partitioning by date does not really make sense in the access pattern. We store hash_of_key to use the method below, and for various memory optimisations in join (with bloom index), it is simply a hash of key1, key2\r\n    \r\n  At lookup time, if we want to get only the latest rows, there are a lot of solutions : \r\n  - the fastest one is : \r\n  \r\n   ```sql\r\n   SELECT any(key1), any(key2), argMax(dimension_1, event_time), argMax(metrics_1, event_time) FROM test GROUP BY hash_of_key1_key2\r\n   ```\r\n   In an example data set scanning ~50M rows/20G it is a baseline of 1s. But obviously, it consumes a lot of memory, and does not scale much more, because finishing aggregation is very cpu intensive. \r\n   \r\n   - the \"classic\" method is to use FINAL. It is not clear in my head how FINAL actually works, i don't understand wether it performs a full mergeSort like optimize_read_in_order, or if it just accumulates blocks with overlapping data from sorted input stream. The problem is, final is awfully slow.\r\n   In the same data set as above, with max_final_threads=16, max_threads=32, 12 parts to read, it runs in 20s instead of 1s. The advantage being that it runs in almost constant memory\r\n   \r\n   - the \"optimize_aggregation_in_order=1\" way, grouping primary key look roughly similar to FINAL in term of performance. With the difference that it seems to generate a lot more ranges, and split parts ranges into fair sized ranges. I'm not sure what the difference with final here is, but in anycase it is not faster.\r\n   \r\n   - another idea is to avoid merging alltogether, using a set, since it is ReplacingMergeTree\r\n ```sql\r\n SELECT key1, key2, dimension_1, metrics_1\r\n FROM test\r\n WHERE (hash_of_key1_key2, event_time) IN (SELECT hash_of_key1_key2, max(event_time) FROM test GROUP BY hash_of_key1_key2)\r\n ```\r\n This is already faster than FINAL and GROUP BY optimize_aggregation_in_order, and uses less memory than GROUP BY hash_of_key1_key2 but there is a high cost into building temporary set, which dominates the time.\r\n \r\n Now about the actual question. The really fast and low memory way to do this is to split primary key space into fair ranges. In my case, i'm using 500k rows blocks. Then i generate a very big UNION ALL query : \r\n \r\n```sql\r\nSELECT * FROM test FINAL \r\n WHERE \r\n   key1 > 'start_key1' OR (key1 = 'start_key1 AND key2 >= 'start_key2')\r\n   AND key1 < 'start_key1 OR (key1 = 'start_key1' AND key2 < 'start_key2')\r\n UNION ALL SELECT * FROM test FINAL \r\n ``` \r\n Note it must use boolean condition to implement lexicographic order, else index code is unable to use tuple to optimize range.\r\n If I generate such a query for the same 50M rows data set, the query runs in 3s and ~7G ram which is bigger than FINAL ram but much lower than GROUP by. Care should be taken to set max_final_threads=1 to prevent over use of ram. Due to index granularity, it adds a bit of over-read, almost 20%, but for a 6 times speedup, i'm willing to buy it.\r\n \r\n This is more than *6 times* faster than using FINAL directly, and it cannot really be solved by using partitions, because reducing partitions to 500k/1m rows make ingestion speed very hard, and Replication stuggles to keep up. \r\n But this is really complex to set up in practices for several reasons : \r\n 1. It is hard to compute fair ranges live, since there is no way to access the primary key index in memory directly in sql, which would allow to do that. SAMPLE key requires adding hash into the PK, and is not so fast for live fetch either. So the solution is to have a table storing ranges rebuilt periodically, very adhoc and inconvenient\r\n 1. It makes using distributed table impossible, so client has to fetch ranges, and generate A HUUUUGE SELECT query with UNION ALL for each shard, using 'remote' function. And i didn't find a way to do it live besides generating union all programatically, which adds a round trip to the server. \r\n \r\n My quesion becomes, is there a way to make CH behaves similarly to this *natively* or at least solutions to simplify generating concatenation queries ?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30387/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30383","id":1030417538,"node_id":"I_kwDOA5dJV849auyC","number":30383,"title":"A bug inserting a value","user":{"login":"Podoproas","id":19357845,"node_id":"MDQ6VXNlcjE5MzU3ODQ1","avatar_url":"https://avatars.githubusercontent.com/u/19357845?v=4","gravatar_id":"","url":"https://api.github.com/users/Podoproas","html_url":"https://github.com/Podoproas","followers_url":"https://api.github.com/users/Podoproas/followers","following_url":"https://api.github.com/users/Podoproas/following{/other_user}","gists_url":"https://api.github.com/users/Podoproas/gists{/gist_id}","starred_url":"https://api.github.com/users/Podoproas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Podoproas/subscriptions","organizations_url":"https://api.github.com/users/Podoproas/orgs","repos_url":"https://api.github.com/users/Podoproas/repos","events_url":"https://api.github.com/users/Podoproas/events{/privacy}","received_events_url":"https://api.github.com/users/Podoproas/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-19T14:58:52Z","updated_at":"2021-11-03T04:03:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When I'm trying to insert some values - there is an error.\r\n\r\nOn Managed Service for ClickHouse on Yandex Cloud I have a DB called \"testing_new_structure\"\r\n`CREATE DATABASE testing_new_structure;`\r\n\r\nI've made a table using this request:\r\n`CREATE TABLE IF NOT EXISTS testing_new_structure.devices (timestamp datetime(),  name String,  device_id UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(timestamp) ORDER BY (timestamp);`\r\n\r\nI'm trying to execute:\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaaaa', 4);`\r\nand getting such a response:\r\n`message: Code: 62, e.displayText() = DB::Exception: Cannot parse expression of type String here: 4) (version 21.8.8.29 (official build))`\r\n\r\nI've figured out that the error is caused because of `'aaaaa'`\r\nIf I change it on any other string that contains _more_ or _less_ then 5 characters (error caused only with ASCII characters) it will succeed.\r\n\r\nIf we try to procede:\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12','aaaaa', 4);`\r\n(without space character before string starts) we will SUCCEED!\r\n\r\nSO:\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'a', 4);` - success\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aa', 4);` - success\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaa', 4);` - success\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaaa', 4);` - success\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaaaa', 4);` - error\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12','aaaaa', 4);` - success\r\n`INSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaaaaa', 4);` -  success\r\n\r\nI do not consider that table is not supposed to store values at exactly 5 ASCII characters. Something wrong with parsing the query.\r\n\r\n**Does it reproduce on recent release?**\r\nI tried to reproduce it on localhost with the latest release:\r\n```\r\nDROP DATABASE testing_new_structure\r\n\r\nQuery id: 1211e237-086a-4df2-a92f-5f897afc97ba\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.005 sec.\r\n\r\nCREATE DATABASE testing_new_structure\r\n\r\nCREATE DATABASE testing_new_structure\r\n\r\nQuery id: 4b60bfee-3765-45a3-8ea1-c4851e75b3b8\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.013 sec.\r\n\r\nCREATE TABLE IF NOT EXISTS testing_new_structure.devices (timestamp datetime(),  name String,  device_id UInt8) ENGINE = MergeTree PARTITION BY toYYYYMM(timestamp) ORDER BY (timestamp);\r\n\r\nCREATE TABLE IF NOT EXISTS testing_new_structure.devices\r\n(\r\n    `timestamp` datetime,\r\n    `name` String,\r\n    `device_id` UInt8\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMM(timestamp)\r\nORDER BY timestamp\r\n\r\nQuery id: 09ce7365-50d5-47b1-85a5-6265b722b4e8\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.016 sec.\r\n\r\nINSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'aaaaa', 4)\r\n\r\nINSERT INTO testing_new_structure.devices VALUES\r\n\r\nQuery id: 0cf2e2f8-aa39-4b4c-a8f5-6b991019786b\r\n\r\nException on client:\r\nCode: 62. DB::Exception: Cannot parse expression of type String here: 4): While executing ValuesBlockInputFormat: data for INSERT was parsed from query. (SYNTAX_ERROR), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x944bdda in /usr/bin/clickhouse\r\n1. DB::ValuesBlockInputFormat::parseExpression(DB::IColumn&, unsigned long) @ 0x11b0f3ea in /usr/bin/clickhouse\r\n2. DB::ValuesBlockInputFormat::readRow(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, unsigned long) @ 0x11b0c53b in /usr/bin/clickhouse\r\n3. DB::ValuesBlockInputFormat::generate() @ 0x11b0b87c in /usr/bin/clickhouse\r\n4. DB::ISource::tryGenerate() @ 0x119a2d55 in /usr/bin/clickhouse\r\n5. DB::ISource::work() @ 0x119a2a5a in /usr/bin/clickhouse\r\n6. ? @ 0x119dd93d in /usr/bin/clickhouse\r\n7. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x119da111 in /usr/bin/clickhouse\r\n8. DB::PipelineExecutor::executeImpl(unsigned long) @ 0x119d816f in /usr/bin/clickhouse\r\n9. DB::PipelineExecutor::execute(unsigned long) @ 0x119d7f39 in /usr/bin/clickhouse\r\n10. ? @ 0x119e4ebf in /usr/bin/clickhouse\r\n11. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x948ce7f in /usr/bin/clickhouse\r\n12. ? @ 0x9490763 in /usr/bin/clickhouse\r\n13. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n14. /build/glibc-OTsEL5/glibc-2.27/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:97: clone @ 0x12188f in /usr/lib/debug/lib/x86_64-linux-gnu/libc-2.27.so\r\n\r\n\r\nConnecting to localhost:9000 as user default.\r\nConnected to ClickHouse server version 21.10.2 revision 54449.\r\n\r\nINSERT INTO testing_new_structure.devices VALUES ('2021-10-12', 'a', 4)\r\n\r\nINSERT INTO testing_new_structure.devices VALUES\r\n\r\nQuery id: 83ae6c0f-3144-4bd1-84ae-ac8ae1e8fb25\r\n\r\nOk.\r\n\r\n1 rows in set. Elapsed: 0.016 sec.\r\n```\r\nIt reproduced.\r\n\r\n**Expected behavior**\r\n\r\n> Inserted row\r\n\r\n**Error message and/or stacktrace**\r\n\r\n`message: Code: 62, e.displayText() = DB::Exception: Cannot parse expression of type String here: 4) (version 21.8.8.29 (official build))`\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30383/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30373","id":1030369944,"node_id":"I_kwDOA5dJV849ajKY","number":30373,"title":"Low performance with mapContains against nested Map column","user":{"login":"pdutta777","id":7411992,"node_id":"MDQ6VXNlcjc0MTE5OTI=","avatar_url":"https://avatars.githubusercontent.com/u/7411992?v=4","gravatar_id":"","url":"https://api.github.com/users/pdutta777","html_url":"https://github.com/pdutta777","followers_url":"https://api.github.com/users/pdutta777/followers","following_url":"https://api.github.com/users/pdutta777/following{/other_user}","gists_url":"https://api.github.com/users/pdutta777/gists{/gist_id}","starred_url":"https://api.github.com/users/pdutta777/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/pdutta777/subscriptions","organizations_url":"https://api.github.com/users/pdutta777/orgs","repos_url":"https://api.github.com/users/pdutta777/repos","events_url":"https://api.github.com/users/pdutta777/events{/privacy}","received_events_url":"https://api.github.com/users/pdutta777/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-10-19T14:18:26Z","updated_at":"2021-11-09T21:54:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the situation**\r\nI have the following structure:\r\n```\r\ncol1 Map(LowCardinality(String),\r\n    Map(LowCardinality(String),\r\n        Tuple(tuple1 Nullable(UInt8),\r\n              tuple2 Nullable(Float64),\r\n              tuple3 Nullable(String)\r\n        )\r\n    )\r\n)\r\n```\r\nand the table has about 85 million rows. If I run a query to count number of items for a \"first\" level map, like `SELECT COUNT(*) FROM profile WHERE mapContains(mapcol, 'somekey')=1 SETTINGS optimize_functions_to_subcolumns=1;` the performance is okay (40 seconds, 2.02 million rows/s).\r\n\r\nBut when I try to select from the map vale (which is a key to the inner map): `SELECT COUNT(*) FROM profile WHERE mapContains(mapcol['somekey'], 'nextkey')=1 SETTINGS optimize_functions_to_subcolumns=1;`, the performance really drops (933 seconds, 87K rows/sec).\r\n\r\nIt does seem that the entire column is being read instead of the second key. Or is there some other optimization or way to efficiently select the map keys?\r\n\r\n\r\n**Expected performance**\r\nI would expect some optimization in mapContains to not read what seems like the entire column values, when only the keys needs to be read\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30373/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30371","id":1030339622,"node_id":"I_kwDOA5dJV849abwm","number":30371,"title":"Is there any best practices for PostgreSQL to Clickhouse replication","user":{"login":"kartaris","id":10552394,"node_id":"MDQ6VXNlcjEwNTUyMzk0","avatar_url":"https://avatars.githubusercontent.com/u/10552394?v=4","gravatar_id":"","url":"https://api.github.com/users/kartaris","html_url":"https://github.com/kartaris","followers_url":"https://api.github.com/users/kartaris/followers","following_url":"https://api.github.com/users/kartaris/following{/other_user}","gists_url":"https://api.github.com/users/kartaris/gists{/gist_id}","starred_url":"https://api.github.com/users/kartaris/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kartaris/subscriptions","organizations_url":"https://api.github.com/users/kartaris/orgs","repos_url":"https://api.github.com/users/kartaris/repos","events_url":"https://api.github.com/users/kartaris/events{/privacy}","received_events_url":"https://api.github.com/users/kartaris/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false},"assignees":[{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2021-10-19T13:53:23Z","updated_at":"2021-10-20T18:32:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi there! Is there any best practices for PostgreSQL to Clickhouse replication using MaterializedPostgreSQL engine(except setting  max_replication_slots and creating  replica index)?\r\nMaybe there are some suggestions on changing database schema in Postgres to make Clickhouse queries more efficient?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30371/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30357","id":1030080445,"node_id":"I_kwDOA5dJV849Zce9","number":30357,"title":"Converting an `Array(String)` into an `Array(LowCardinality(String))` with `arrayMap()` fails","user":{"login":"davidmanzanares","id":1692175,"node_id":"MDQ6VXNlcjE2OTIxNzU=","avatar_url":"https://avatars.githubusercontent.com/u/1692175?v=4","gravatar_id":"","url":"https://api.github.com/users/davidmanzanares","html_url":"https://github.com/davidmanzanares","followers_url":"https://api.github.com/users/davidmanzanares/followers","following_url":"https://api.github.com/users/davidmanzanares/following{/other_user}","gists_url":"https://api.github.com/users/davidmanzanares/gists{/gist_id}","starred_url":"https://api.github.com/users/davidmanzanares/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidmanzanares/subscriptions","organizations_url":"https://api.github.com/users/davidmanzanares/orgs","repos_url":"https://api.github.com/users/davidmanzanares/repos","events_url":"https://api.github.com/users/davidmanzanares/events{/privacy}","received_events_url":"https://api.github.com/users/davidmanzanares/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-10-19T09:35:28Z","updated_at":"2022-01-25T14:48:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Converting an `Array(String)` into an `Array(LowCardinality(String))` with `arrayMap()` and `toLowCardinality()` fails.\r\n\r\n```\r\nclickhouse local -q \"SELECT arrayMap(y -> toLowCardinality(y), ['hi'])\"\r\nCode: 44. DB::Exception: Invalid column type for ColumnUnique::insertRangeFrom. Expected String, got ColumnLowCardinality. (ILLEGAL_COLUMN) (version 21.9.5.15 (official build))\r\n```\r\n\r\nIt fails with a similar error on all tested versions: 21.9.5.15, 21.3.17.2 and 20.7.2.30.\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30357/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30354","id":1029945841,"node_id":"I_kwDOA5dJV849Y7nx","number":30354,"title":"MaterializeMySQL engine doesn't do garbage collection for deleted row","user":{"login":"liangli1213","id":69909759,"node_id":"MDQ6VXNlcjY5OTA5NzU5","avatar_url":"https://avatars.githubusercontent.com/u/69909759?v=4","gravatar_id":"","url":"https://api.github.com/users/liangli1213","html_url":"https://github.com/liangli1213","followers_url":"https://api.github.com/users/liangli1213/followers","following_url":"https://api.github.com/users/liangli1213/following{/other_user}","gists_url":"https://api.github.com/users/liangli1213/gists{/gist_id}","starred_url":"https://api.github.com/users/liangli1213/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/liangli1213/subscriptions","organizations_url":"https://api.github.com/users/liangli1213/orgs","repos_url":"https://api.github.com/users/liangli1213/repos","events_url":"https://api.github.com/users/liangli1213/events{/privacy}","received_events_url":"https://api.github.com/users/liangli1213/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""},{"id":2121263626,"node_id":"MDU6TGFiZWwyMTIxMjYzNjI2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mysql","name":"comp-mysql","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-19T07:17:08Z","updated_at":"2021-10-20T02:49:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe what's wrong**\r\n\r\nFor all ClickHouse version which has supported MaterializeMySQL, in source table (MySQL), do a delete operation, the operation can be synchronized to destination table (ClickHouse), but only mark _sign column to -1, the row will not be deleted or be garbage collected, it will be there forever.\r\n\r\nSo is this a bug? @tavplubix ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30354/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30341","id":1029318249,"node_id":"I_kwDOA5dJV849WiZp","number":30341,"title":"Big Decimal can overflow without exception","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-18T16:07:13Z","updated_at":"2021-10-18T16:07:13Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe what's wrong**\r\n\r\nIt's possible to get meaningless result after big decimal division.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. ClickHouse 21.11, 21.8\r\n\r\n**How to reproduce**\r\n\r\nClickHouse 21.11\r\n\r\n```\r\nSELECT toDecimal128(1, 20) / toDecimal128(1, 20)\r\n\r\nQuery id: 080e9ddd-9060-4095-8752-3b24dca42bb3\r\n\r\n┌─divide(toDecimal128(1, 20), toDecimal128(1, 20))─┐\r\n│                           0.01318113592927845595 │\r\n└──────────────────────────────────────────────────┘\r\n\r\n- P from [ 19 : 38 ] - for Decimal128(S)\r\n\r\nSELECT\r\n    toDecimal128(1, 20) / toDecimal128(1, 20) AS a,\r\n    toDecimal128(1, 19) / toDecimal128(1, 19) AS b\r\n\r\n┌──────────────────────a─┬─b─┐\r\n│ 0.01318113592927845595 │ 1 │\r\n└────────────────────────┴───┘\r\n\r\nSELECT\r\n    [1, 1] AS vs,\r\n    toDecimal128(vs[2], 12) / toDecimal128(vs[1], 12) AS frac_dec128_12,\r\n    toDecimal128(vs[2], 24) / toDecimal128(vs[1], 24) AS frac_dec128_24a,\r\n    toFloat64(vs[2]) / toFloat64(vs[1]) AS frac_float64\r\n\r\nQuery id: 6e0fc55d-14a7-4cab-9d03-ccee9200c6b1\r\n\r\n┌─vs────┬─frac_dec128_12─┬────────────frac_dec128_24a─┬─frac_float64─┐\r\n│ [1,1] │              1 │ 0.000000000018960114910927 │            1 │\r\n\r\n\r\nSELECT toDecimal64(2, 12) / toDecimal64(2, 12)\r\n\r\nQuery id: 5e5681dd-9984-43c9-a317-e9e232cd12ed\r\n\r\n\r\n0 rows in set. Elapsed: 0.003 sec.\r\n\r\nReceived exception from server (version 21.11.1):\r\nCode: 407. DB::Exception: Received from localhost:9000. DB::Exception: Decimal math overflow: While processing toDecimal64(2, 12) / toDecimal64(2, 12). (DECIMAL_OVERFLOW)\r\n```\r\n\r\n\r\n**Expected behavior**\r\n\r\nException most likely.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30341/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30340","id":1029256942,"node_id":"I_kwDOA5dJV849WTbu","number":30340,"title":"SELECT EXP(1.0); returns incorrect result","user":{"login":"InquisitorGray","id":83011939,"node_id":"MDQ6VXNlcjgzMDExOTM5","avatar_url":"https://avatars.githubusercontent.com/u/83011939?v=4","gravatar_id":"","url":"https://api.github.com/users/InquisitorGray","html_url":"https://github.com/InquisitorGray","followers_url":"https://api.github.com/users/InquisitorGray/followers","following_url":"https://api.github.com/users/InquisitorGray/following{/other_user}","gists_url":"https://api.github.com/users/InquisitorGray/gists{/gist_id}","starred_url":"https://api.github.com/users/InquisitorGray/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/InquisitorGray/subscriptions","organizations_url":"https://api.github.com/users/InquisitorGray/orgs","repos_url":"https://api.github.com/users/InquisitorGray/repos","events_url":"https://api.github.com/users/InquisitorGray/events{/privacy}","received_events_url":"https://api.github.com/users/InquisitorGray/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-18T15:13:12Z","updated_at":"2021-10-18T18:22:51Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe what's wrong**\r\n\r\nSELECT EXP(1.0); returns 2.718281828460626, which is not the Euler's number 2.7182818284590452\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nThe bug reproduces on the last release available at https://play.clickhouse.com/\r\n\r\n**How to reproduce**\r\n\r\nRun a query `SELECT EXP(1.0);`\r\n\r\n**Probable cause**\r\nThe iterative algorithm doesn't converge to enough precision while reaching the limit of iterations.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30340/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30330","id":1029072055,"node_id":"I_kwDOA5dJV849VmS3","number":30330,"title":"recovery-after-complete-data-loss in case of Atomic ","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-18T12:50:20Z","updated_at":"2021-10-18T12:50:20Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"https://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#recovery-after-complete-data-loss\r\n\r\nIt's unclear how to copy metadata in case of Atomic database.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30330/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30325","id":1028975034,"node_id":"PR_kwDOA5dJV84tUgRh","number":30325,"title":"Zookeeper load balancing settings.","user":{"login":"nicelulu","id":18606973,"node_id":"MDQ6VXNlcjE4NjA2OTcz","avatar_url":"https://avatars.githubusercontent.com/u/18606973?v=4","gravatar_id":"","url":"https://api.github.com/users/nicelulu","html_url":"https://github.com/nicelulu","followers_url":"https://api.github.com/users/nicelulu/followers","following_url":"https://api.github.com/users/nicelulu/following{/other_user}","gists_url":"https://api.github.com/users/nicelulu/gists{/gist_id}","starred_url":"https://api.github.com/users/nicelulu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nicelulu/subscriptions","organizations_url":"https://api.github.com/users/nicelulu/orgs","repos_url":"https://api.github.com/users/nicelulu/repos","events_url":"https://api.github.com/users/nicelulu/events{/privacy}","received_events_url":"https://api.github.com/users/nicelulu/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"assignees":[{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-10-18T11:12:37Z","updated_at":"2021-12-09T14:59:33Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30325","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30325","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30325.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30325.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- New Feature\r\n\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nThis is close https://github.com/ClickHouse/ClickHouse/issues/29617.\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30325/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30323","id":1028904443,"node_id":"I_kwDOA5dJV849U9X7","number":30323,"title":"It's not possible to define CTE before INSERT in INSERT SELECT query.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-18T10:03:46Z","updated_at":"2021-10-19T13:11:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the issue**\r\nIt's not possible to define CTE before INSERT in INSERT SELECT statement.\r\n\r\n**How to reproduce**\r\nClickHouse 21.11\r\n\r\n```\r\nWITH x AS (SELECT * FROM numbers(1000)) INSERT INTO test_tbl_mem SELECT * FROM x;\r\n\r\nSyntax error: failed at position 41 ('INSERT'):\r\n\r\nWITH x AS (SELECT * FROM numbers(1000)) INSERT INTO test_tbl_mem SELECT * FROM x;\r\n\r\nExpected one of: SELECT, Comma, token\r\n\r\nINSERT INTO test_tbl_mem WITH x AS\r\n    (\r\n        SELECT *\r\n        FROM numbers(1000)\r\n    )\r\nSELECT *\r\nFROM x\r\n\r\nQuery id: eacdd594-49d3-4800-9fd1-7ac0f9593ff4\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.003 sec.\r\n```\r\n\r\n**Expected behavior**\r\nFirst query should work.\r\n\r\n**Additional context**\r\nPostgreSQL docs about CTE + INSERT SELECT\r\n\r\nhttps://www.postgresql.org/docs/14/queries-with.html#QUERIES-WITH-MODIFYING\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30323/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30321","id":1028825930,"node_id":"I_kwDOA5dJV849UqNK","number":30321,"title":"Logical error: Cannot drain connections: cancel first","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false},"assignees":[{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2021-10-18T08:51:13Z","updated_at":"2021-10-20T13:48:09Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/8defd0ab892d615679a8043ff6ba3484d3289274/stress_test_(debug).html#fail1\r\n\r\n```\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:39.930234 [ 1860 ] {} <Fatal> : Logical error: 'Cannot drain connections: cancel first.'.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:39.996168 [ 13718 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.023179 [ 13718 ] {} <Fatal> BaseDaemon: (version 21.11.1.8464 (official build), build id: 2FA8FFD6EF578549D52F6A6A5ED522D89360FAB0) (from thread 1860) (no query) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.043884 [ 13718 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.047617 [ 13718 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fa4d5bc418b 0x7fa4d5ba3859 0x14ee88b8 0x14ee89c2 0x2290f97d 0x20e7b7a7 0x20e7cfa1 0x20e7cf3d 0x20e7cefd 0x20e7ced5 0x20e7cea0 0x14f35526 0x14f345b5 0x14f6296f 0x14f6e404 0x14f6e3bd 0x14f6e361 0x14f6e272 0x14f6e156 0x14f6e01d 0x14f6dfdd 0x14f6dfb5 0x14f6df80 0x14f35526 0x14f345b5 0x14f605cf 0x14f67784 0x14f676fd 0x14f67625 0x14f66f62 0x7fa4d5da6609 0x7fa4d5ca0293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.068758 [ 13718 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.071705 [ 13718 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:40.807126 [ 13718 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x14ee88b8 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.076396 [ 1813 ] {} <Fatal> : Logical error: 'Cannot drain connections: cancel first.'.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.122924 [ 13718 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x14ee89c2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.141765 [ 13724 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.149908 [ 13724 ] {} <Fatal> BaseDaemon: (version 21.11.1.8464 (official build), build id: 2FA8FFD6EF578549D52F6A6A5ED522D89360FAB0) (from thread 1813) (no query) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.152227 [ 13724 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.160366 [ 13724 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fa4d5bc418b 0x7fa4d5ba3859 0x14ee88b8 0x14ee89c2 0x2290f97d 0x20e7b7a7 0x20e7cfa1 0x20e7cf3d 0x20e7cefd 0x20e7ced5 0x20e7cea0 0x14f35526 0x14f345b5 0x14f6296f 0x14f6e404 0x14f6e3bd 0x14f6e361 0x14f6e272 0x14f6e156 0x14f6e01d 0x14f6dfdd 0x14f6dfb5 0x14f6df80 0x14f35526 0x14f345b5 0x14f605cf 0x14f67784 0x14f676fd 0x14f67625 0x14f66f62 0x7fa4d5da6609 0x7fa4d5ca0293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.165567 [ 13724 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.168656 [ 13724 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.758953 [ 13718 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Client/HedgedConnections.cpp:252: DB::HedgedConnections::drain() @ 0x2290f97d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:41.964249 [ 13718 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/QueryPipeline/ConnectionCollector.cpp:79: DB::ConnectionCollector::drainConnections(DB::IConnections&) @ 0x20e7b7a7 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:42.203518 [ 13718 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/QueryPipeline/ConnectionCollector.cpp:50: DB::AsyncDrainTask::operator()() const @ 0x20e7cfa1 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:42.539910 [ 13718 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::AsyncDrainTask&>(fp)()) std::__1::__invoke<DB::AsyncDrainTask&>(DB::AsyncDrainTask&) @ 0x20e7cf3d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.114195 [ 13718 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::AsyncDrainTask&>(DB::AsyncDrainTask&) @ 0x20e7cefd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.358354 [ 7760 ] {} <Fatal> : Logical error: 'Cannot drain connections: cancel first.'.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.359151 [ 13718 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::AsyncDrainTask, void ()>::operator()() @ 0x20e7ced5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.483656 [ 13727 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.486617 [ 13727 ] {} <Fatal> BaseDaemon: (version 21.11.1.8464 (official build), build id: 2FA8FFD6EF578549D52F6A6A5ED522D89360FAB0) (from thread 7760) (no query) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.487499 [ 13727 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.488128 [ 13727 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fa4d5bc418b 0x7fa4d5ba3859 0x14ee88b8 0x14ee89c2 0x2290f97d 0x20e7b7a7 0x20e7cfa1 0x20e7cf3d 0x20e7cefd 0x20e7ced5 0x20e7cea0 0x14f35526 0x14f345b5 0x14f6296f 0x14f6e404 0x14f6e3bd 0x14f6e361 0x14f6e272 0x14f6e156 0x14f6e01d 0x14f6dfdd 0x14f6dfb5 0x14f6df80 0x14f35526 0x14f345b5 0x14f605cf 0x14f67784 0x14f676fd 0x14f67625 0x14f66f62 0x7fa4d5da6609 0x7fa4d5ca0293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.493470 [ 13727 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.527746 [ 13727 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.563889 [ 13718 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::AsyncDrainTask, void ()> >(std::__1::__function::__policy_storage const*) @ 0x20e7cea0 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.731925 [ 13718 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x14f35526 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.884707 [ 13727 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x14ee88b8 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:43.890943 [ 13718 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x14f345b5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.144151 [ 13718 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274: ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x14f6296f in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.187666 [ 13727 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x14ee89c2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.487406 [ 13718 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139: bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x14f6e404 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.811087 [ 13727 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Client/HedgedConnections.cpp:252: DB::HedgedConnections::drain() @ 0x2290f97d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.892358 [ 13718 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<bool>(fp)(std::__1::forward<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&>(fp0)...)) std::__1::__invoke_constexpr<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&...) @ 0x14f6e3bd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:44.984679 [ 13727 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/QueryPipeline/ConnectionCollector.cpp:79: DB::ConnectionCollector::drainConnections(DB::IConnections&) @ 0x20e7b7a7 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:45.192036 [ 13727 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/QueryPipeline/ConnectionCollector.cpp:50: DB::AsyncDrainTask::operator()() const @ 0x20e7cfa1 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:45.211328 [ 13718 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&, std::__1::tuple<>&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&, std::__1::__tuple_indices<std::__1::tuple<>&...>) @ 0x14f6e361 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:45.368450 [ 13727 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::AsyncDrainTask&>(fp)()) std::__1::__invoke<DB::AsyncDrainTask&>(DB::AsyncDrainTask&) @ 0x20e7cf3d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:45.574997 [ 13727 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::AsyncDrainTask&>(DB::AsyncDrainTask&) @ 0x20e7cefd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:45.829349 [ 13727 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::AsyncDrainTask, void ()>::operator()() @ 0x20e7ced5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:46.113224 [ 13727 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::AsyncDrainTask, void ()> >(std::__1::__function::__policy_storage const*) @ 0x20e7cea0 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:46.317710 [ 13727 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x14f35526 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:46.580658 [ 13727 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x14f345b5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:46.756982 [ 13727 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274: ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x14f6296f in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:47.071194 [ 13727 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139: bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x14f6e404 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:47.284074 [ 13727 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<bool>(fp)(std::__1::forward<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&>(fp0)...)) std::__1::__invoke_constexpr<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&...) @ 0x14f6e3bd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:47.578461 [ 13727 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&, std::__1::tuple<>&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&, std::__1::__tuple_indices<std::__1::tuple<>&...>) @ 0x14f6e361 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:48.064245 [ 13727 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&, std::__1::tuple<>&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&) @ 0x14f6e272 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:48.341839 [ 13727 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188: ThreadFromGlobalPool::ThreadFromGlobalPool<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x14f6e156 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:48.498997 [ 13727 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<bool>(fp)(std::__1::forward<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()&>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x14f6e01d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:48.790341 [ 13727 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()&>(bool&&...) @ 0x14f6dfdd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:49.039480 [ 13727 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'(), void ()>::operator()() @ 0x14f6dfb5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:49.309383 [ 13727 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(bool&&, bool ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<bool>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x14f6df80 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:49.406313 [ 13727 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x14f35526 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:49.528767 [ 13727 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x14f345b5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:49.801752 [ 13727 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x14f605cf in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.010363 [ 13727 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x14f67784 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.232661 [ 13727 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x14f676fd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.540415 [ 13727 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) @ 0x14f67625 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.727194 [ 13727 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0x14f66f62 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.744928 [ 13727 ] {} <Fatal> BaseDaemon: 34. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:50.746382 [ 13727 ] {} <Fatal> BaseDaemon: 35. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:09:53.080650 [ 13727 ] {} <Fatal> BaseDaemon: Checksum of the binary: 936A3449BE3B5B63B9CB586FFCDF27FB, integrity check passed.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.17 23:10:02.314526 [ 491 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```\r\n\r\ncc: @Avogar ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30321/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30317","id":1028546093,"node_id":"I_kwDOA5dJV849Tl4t","number":30317,"title":"Unable to connect to HDFS: InvalidParameter: RpcAuth: Unknown auth mechanism type: OAUTHBEARER","user":{"login":"jiangdaokun","id":55648713,"node_id":"MDQ6VXNlcjU1NjQ4NzEz","avatar_url":"https://avatars.githubusercontent.com/u/55648713?v=4","gravatar_id":"","url":"https://api.github.com/users/jiangdaokun","html_url":"https://github.com/jiangdaokun","followers_url":"https://api.github.com/users/jiangdaokun/followers","following_url":"https://api.github.com/users/jiangdaokun/following{/other_user}","gists_url":"https://api.github.com/users/jiangdaokun/gists{/gist_id}","starred_url":"https://api.github.com/users/jiangdaokun/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jiangdaokun/subscriptions","organizations_url":"https://api.github.com/users/jiangdaokun/orgs","repos_url":"https://api.github.com/users/jiangdaokun/repos","events_url":"https://api.github.com/users/jiangdaokun/events{/privacy}","received_events_url":"https://api.github.com/users/jiangdaokun/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-18T02:14:55Z","updated_at":"2021-10-18T08:03:47Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n**Describe the unexpected behaviour**\r\nI want to use HDFS engine on Clickhouse, But an error occurred when I query:\r\nUnable to connect to HDFS: InvalidParameter: RpcAuth: Unknown auth mechanism type: OAUTHBEARER\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n21.8.3.44\r\n\r\n* Non-default settings, if any\r\n <!-- Global configuration options for HDFS engine type -->\r\n    <hdfs>\r\n        <hadoop_kerberos_keytab>/etc/clickhouse-server/clickhouse.keytab</hadoop_kerberos_keytab>\r\n        <hadoop_kerberos_principal>clickhouse@TDHTEST</hadoop_kerberos_principal>\r\n        <hadoop_security_authentication>kerberos</hadoop_security_authentication>\r\n        <libhdfs3_conf>/etc/clickhouse-server/hdfs-site.xml</libhdfs3_conf>\r\n    </hdfs>\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\nCREATE TABLE da.base_agent_info_oper_hdfs\r\n(\r\n    `agent_no` String,\r\n    `agent_name` String,\r\n    `agent_type` String,\r\n    `phoneno` String,\r\n    `hdatasign` String,\r\n    `hloadtime` String,\r\n    `hbatchdate` String,\r\n    `hdatasrc` String\r\n)\r\nENGINE = HDFS('hdfs://172.30.83.203:8020/inceptor1/user/hive/warehouse/da.db/hbxyoper/base_agent_info_oper/*', 'ORC')\r\n--hive\r\nCREATE  TABLE `da.base_agent_info_oper`(\r\n  `agent_no` string DEFAULT NULL, \r\n  `agent_name` string DEFAULT NULL, \r\n  `agent_type` string DEFAULT NULL, \r\n  `phoneno` string DEFAULT NULL, \r\n  `hdatasign` string DEFAULT NULL, \r\n  `hloadtime` string DEFAULT NULL, \r\n  `hbatchdate` string DEFAULT NULL, \r\n  `hdatasrc` string DEFAULT NULL\r\n)\r\nCOMMENT '产品信息维表'\r\nCLUSTERED BY ( \r\n  `agent_no`) \r\nINTO 1 BUCKETS\r\nROW FORMAT DELIMITED FIELDS TERMINATED BY '\\001' STORED AS ORC ;\r\n\r\n* Queries to run that lead to unexpected result\r\nselect * from da.base_agent_info_oper_hdfs ;\r\n\r\n**Error message and/or stacktrace**\r\n2021.10.18 09:36:14.875756 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Debug> executeQuery: (from 172.0.0.1:64688) select * from da.base_agent_info_oper_hdfs FORMAT TabSeparatedWithNamesAndTypes \r\n2021.10.18 09:36:14.875992 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Trace> ContextAccess (default): Access granted: SELECT(agent_no, agent_name, agent_type, phoneno, hdatasign, hloadtime, hbatchdate, hdatasrc) ON da.base_agent_info_oper_hdfs\r\n2021.10.18 09:36:14.876062 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Debug> HDFSClient: running kinit: kinit -R -t \"/etc/clickhouse-server/clickhouse.keytab\" -k clickhouse@TDHTEST|| kinit -t \"/etc/clickhouse-server/clickhouse.keytab\" -k clickhouse@TDHTEST\r\n2021.10.18 09:36:14.876074 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Trace> ShellCommand: Will start shell command '/bin/sh' with arguments 'sh', '-c', 'kinit -R -t \"/etc/clickhouse-server/clickhouse.keytab\" -k clickhouse@TDHTEST|| kinit -t \"/etc/clickhouse-server/clickhouse.keytab\" -k clickhouse@TDHTEST'\r\n2021.10.18 09:36:14.876215 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Trace> ShellCommand: Started shell command '/bin/sh' with pid 8045\r\n2021.10.18 09:36:14.876228 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Trace> ShellCommand: Will wait for shell command pid 8045\r\n2021.10.18 09:36:14.882050 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Trace> ShellCommand: Wait for shell command pid 8045 completed with status 0\r\n2021.10.18 09:36:14.887460 [ 47896 ] {b4814402-ff95-4257-b314-2def8f615c20} <Error> executeQuery: Code: 210, e.displayText() = DB::Exception: Unable to connect to HDFS: InvalidParameter: RpcAuth: Unknown auth mechanism type: OAUTHBEARER (version 21.8.3.44 (official build)) (from 172.30.113.194:64688) (in query: select * from da.base_agent_info_oper_hdfs FORMAT TabSeparatedWithNamesAndTypes ), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8f9b87a in /usr/bin/clickhouse\r\n1. DB::createHDFSFS(hdfsBuilder*) @ 0xfa6c230 in /usr/bin/clickhouse\r\n2. DB::StorageHDFS::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0xfa61524 in /usr/bin/clickhouse\r\n3. DB::IStorage::read(DB::QueryPlan&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x10907137 in /usr/bin/clickhouse\r\n4. DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&) @ 0x103b2cfc in /usr/bin/clickhouse\r\n5. DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>) @ 0x103aab9c in /usr/bin/clickhouse\r\n6. DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x103a9d80 in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x1057bdde in /usr/bin/clickhouse\r\n8. DB::InterpreterSelectWithUnionQuery::execute() @ 0x1057ceb1 in /usr/bin/clickhouse\r\n9. ? @ 0x1073d724 in /usr/bin/clickhouse\r\n10. DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::__1::shared_ptr<DB::Context>, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>, std::__1::optional<DB::FormatSettings> const&) @ 0x107411fd in /usr/bin/clickhouse\r\n11. DB::HTTPHandler::processQuery(std::__1::shared_ptr<DB::Context>, DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) @ 0x10f68d2d in /usr/bin/clickhouse\r\n12. DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) @ 0x10f6c489 in /usr/bin/clickhouse\r\n13. DB::HTTPServerConnection::run() @ 0x10fee050 in /usr/bin/clickhouse\r\n14. Poco::Net::TCPServerConnection::start() @ 0x13b5bc0f in /usr/bin/clickhouse\r\n15. Poco::Net::TCPServerDispatcher::run() @ 0x13b5d69a in /usr/bin/clickhouse\r\n16. Poco::PooledThread::run() @ 0x13c90519 in /usr/bin/clickhouse\r\n17. Poco::ThreadImpl::runnableEntry(void*) @ 0x13c8c7aa in /usr/bin/clickhouse\r\n18. start_thread @ 0x7dd5 in /usr/lib64/libpthread-2.17.so\r\n19. clone @ 0xfeb3d in /usr/lib64/libc-2.17.so\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30317/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30316","id":1028523381,"node_id":"I_kwDOA5dJV849TgV1","number":30316,"title":"Aggregation in external memory is paradoxically fast.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-18T01:43:51Z","updated_at":"2021-12-23T08:59:43Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the situation**\r\n```\r\nSET max_memory_usage = '200G'\r\n```\r\n\r\n```\r\nSELECT rand() AS k, count() AS c FROM numbers_mt(2000000000) GROUP BY k ORDER BY c DESC LIMIT 10\r\n```\r\n66 sec. with 40 threads.\r\n\r\n```\r\nSET max_bytes_before_external_group_by = '10G'\r\nSELECT rand() AS k, count() AS c FROM numbers_mt(2000000000) GROUP BY k ORDER BY c DESC LIMIT 10\r\n```\r\n\r\n64 sec. with 40 threads.\r\n\r\nNote: in fact the data is not written to disk but resides in page cache.\r\nBut it still requires serialization and deserialization, that must be slower.\r\n\r\nThen there is a clue that normal aggregation in memory can be improved.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316/reactions","total_count":1,"+1":0,"-1":0,"laugh":1,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30316/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30300","id":1028202655,"node_id":"I_kwDOA5dJV849SSCf","number":30300,"title":"`NativeIndexed` format: like `Native` but with embedded index.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-17T03:13:11Z","updated_at":"2022-01-28T13:32:25Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nAllow skipping of columns and parallel reading of Native-like format.\r\nIt is already available by index in separate file, that is used in `StripeLog` tables (and it's working perfectly),\r\nbut we want similar feature in single format.\r\n\r\n**Describe the solution you'd like**\r\n\r\nImplement `WriteBuffer` adaptor that will, on `nextImpl` create another buffer in memory instead of writing and reusing the existing buffer. It will provide a method to do explicit write of all of the accumulated buffers.\r\n\r\nAnother words, it will accumulate data in memory until the user will call some method like `finishWriteBlock` explicitly.\r\n\r\nIt can be named `AccumulatingWriteBuffer`. The purpose is to allow overwrite some parts of the previous data before it will be written to the underlying media.\r\n\r\nThen we will do the following for every block in native format:\r\n- skip some bytes for index (num_columns * sizeof(UInt64));\r\n- write columns as usual;\r\n- fill the index according on the actual number of bytes written for every column;\r\n- call `finishWriteBlock`.\r\n\r\n**Caveats**\r\n\r\nConsult with @CurtizJ about proper writing of subcolumns.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n1. Generate 128bit random number and use it as a synchronization marker. It will allow finiding chunk boundaries easily without the need of buffering on write. It will allow parallel parsing, but not parallel reading or skipping of columns.\r\n\r\n2. Parquet format should work decently, but it is more weird (metadata at end of file).\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30300/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30299","id":1028189417,"node_id":"I_kwDOA5dJV849SOzp","number":30299,"title":"Allow to specify single disk instead of a storage_policy for MergeTree tables.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-10-17T01:35:11Z","updated_at":"2022-01-05T20:32:58Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Add `disk` setting to `MergeTreeSettings`. It can refer to disk by name in `storage_configuration` in config.\r\nThe setting will have the same effect as if a storage policy with single disk will be used.\r\n\r\nThe settings `disk` and `storage_policy` cannot be used together.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30299/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30274","id":1027949492,"node_id":"I_kwDOA5dJV849RUO0","number":30274,"title":"SELECT INTO OUTFILE: support writing multiple files.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-16T04:32:49Z","updated_at":"2022-01-28T13:41:35Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\n```\r\nWITH number % 10 AS part SELECT number FROM numbers(1000) INTO OUTFILE 'file_{part}.txt'\r\n```\r\n\r\n**Additional context**\r\n\r\nHold, because it's difficult to analyze that the `part` identifier is used.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30274/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30240","id":1027561680,"node_id":"I_kwDOA5dJV849P1jQ","number":30240,"title":"Materialized view deduplication using a checksum of the main insert.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1350221495,"node_id":"MDU6TGFiZWwxMzUwMjIxNDk1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-matview","name":"comp-matview","color":"b5bcff","default":false,"description":"Materialized views"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-15T15:34:39Z","updated_at":"2021-10-16T00:26:04Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Imagine an internal_insert (under the hood, code level, not SQL level) that has an attribute insert_id (and it's optional, if it's missing then the checksum of a block will be used and insert_id will be populated with that checksum).\r\nThis attribute can be exposed to the SQL level using settings (for example `insert into .... settings insert_id=42`), but it's not mandatory for this FR.\r\n\r\nThe idea is that Materialized Views when they get data from the main internal_insert they also get this inherited insert_id, and they use this insert_id as deduplication checksum and store it in ZK and don't calculate a checksum of the block themselves. \r\nThey also propagate this inherited insert_id to their heirs (cascaded MV).\r\n\r\nThe only main insert (SQL) calculates insert_id as a checksum of the inserted block if it's missing. All nested inserts have insert_id filled always.\r\n\r\nIn this case you can repeat inserts and Materialized views will be consistent because they don't calculate a checksum by themselve.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240/reactions","total_count":2,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":2},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30240/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30216","id":1027145440,"node_id":"I_kwDOA5dJV849OP7g","number":30216,"title":"Batch insert data, got more data","user":{"login":"ShubaoYang","id":41741864,"node_id":"MDQ6VXNlcjQxNzQxODY0","avatar_url":"https://avatars.githubusercontent.com/u/41741864?v=4","gravatar_id":"","url":"https://api.github.com/users/ShubaoYang","html_url":"https://github.com/ShubaoYang","followers_url":"https://api.github.com/users/ShubaoYang/followers","following_url":"https://api.github.com/users/ShubaoYang/following{/other_user}","gists_url":"https://api.github.com/users/ShubaoYang/gists{/gist_id}","starred_url":"https://api.github.com/users/ShubaoYang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ShubaoYang/subscriptions","organizations_url":"https://api.github.com/users/ShubaoYang/orgs","repos_url":"https://api.github.com/users/ShubaoYang/repos","events_url":"https://api.github.com/users/ShubaoYang/events{/privacy}","received_events_url":"https://api.github.com/users/ShubaoYang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-10-15T07:08:09Z","updated_at":"2021-10-26T19:00:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have three million rows of data that I want to insert into the ClickHouse database. I inserted 10,000 rows at a time and ended up with 10 million rows in ClickHouse. After a while, it was restored to 3 million rows. What happened?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30216/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30214","id":1027084136,"node_id":"I_kwDOA5dJV849OA9o","number":30214,"title":"count is different on the PostgreSQL engine","user":{"login":"jiangdaokun","id":55648713,"node_id":"MDQ6VXNlcjU1NjQ4NzEz","avatar_url":"https://avatars.githubusercontent.com/u/55648713?v=4","gravatar_id":"","url":"https://api.github.com/users/jiangdaokun","html_url":"https://github.com/jiangdaokun","followers_url":"https://api.github.com/users/jiangdaokun/followers","following_url":"https://api.github.com/users/jiangdaokun/following{/other_user}","gists_url":"https://api.github.com/users/jiangdaokun/gists{/gist_id}","starred_url":"https://api.github.com/users/jiangdaokun/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jiangdaokun/subscriptions","organizations_url":"https://api.github.com/users/jiangdaokun/orgs","repos_url":"https://api.github.com/users/jiangdaokun/repos","events_url":"https://api.github.com/users/jiangdaokun/events{/privacy}","received_events_url":"https://api.github.com/users/jiangdaokun/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-15T05:02:24Z","updated_at":"2021-10-18T01:52:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\nWhen I use CK's  PostgreSQL engine to integrate greenplum； When I counted the data on both sides, I found that the two sides were different；\r\n\r\nCREATE TABLE da.even_trade_conf_gp (\r\n\tparty_hcode Nullable(String), -- 当事人全局编码\r\n\tcust_name Nullable(String), -- 客户名称\r\n\tidenti_no Nullable(String), -- 证件号码\r\n\tidenti_type Nullable(String), -- 证件类型\r\n\tcust_type Nullable(String), -- 客户类型\r\n\tta_type Nullable(String), -- TA类型\r\n\tta_conf_no Nullable(String), -- 确认流水号\r\n\tconf_date Nullable(String) , -- 确认日期\r\n\tapply_date Nullable(String), -- 申请日期\r\n\tbusi_type Nullable(String), -- 业务类型\r\n\tex_trade_type Nullable(String), -- 外部业务类型\r\n\tta_trade_type Nullable(String), -- 基金交易业务类型\r\n\tta_acct Nullable(String), -- 基金账户\r\n\ttrade_acct Nullable(String), -- 交易账户\r\n\tagentno Nullable(String), -- 销售商编号\r\n\tnetno Nullable(String), -- 网点编号\r\n\toper_netno Nullable(String), -- 操作网点编号\r\n\tfund_hcode Nullable(String), -- 产品全局编码\r\n\tfund_code Nullable(String), -- 产品代码\r\n\tshare_type Nullable(String), -- 份额类型\r\n\tconf_amt Nullable(Decimal128(2)) , -- 确认金额\r\n\tconf_share Nullable(Decimal128(2)) , -- 确认份额\r\n\tapply_amt Nullable(Decimal128(2)) , -- 申请金额\r\n\tapply_share Nullable(Decimal128(2)) , -- 申请份额\r\n\tttl_fee Nullable(Decimal128(2)) , -- 总费用\r\n\tcomm_fee Nullable(Decimal128(2)) , -- 手续费\r\n\ttransfee Nullable(Decimal128(2)) , -- 过户费\r\n\tconsign_fee Nullable(Decimal128(2)) , -- 代销费\r\n\tregister_fee Nullable(Decimal128(2)) , -- 未知\r\n\tfund_asset_fee Nullable(Decimal128(2)) , -- 归基金资产费\r\n\tfinalsettle_comss Nullable(Decimal128(2)) , -- 后收费用\r\n\tback_chrg_agencyno Nullable(Decimal128(2)) , -- 后收费归销售机构\r\n\tback_chrg_assets Nullable(Decimal128(2)) , -- 后收费归基金资产\r\n\toth_fee1 Nullable(Decimal128(2)) , -- 其他费用1\r\n\tint_amt Nullable(Decimal128(2)) , -- 利息\r\n\tint_amt_tax Nullable(Decimal128(2)) , -- 利息税\r\n\tstamptax Nullable(Decimal128(2)) , -- 印花税\r\n\tretrun_perm Nullable(Decimal128(2)) , -- 返还保费\r\n\tint_amt_share Nullable(Decimal128(2)) , -- 认购期利息转份额\r\n\tperfpay Nullable(Decimal128(2)) , -- 业绩报酬\r\n\tbreach_amt Nullable(Decimal128(2)) , -- 违约金\r\n\tcash_unpincome Nullable(Decimal128(2)) , -- 兑现的未付收益\r\n\tcash_share Nullable(Decimal128(2)) , -- 兑付份额\r\n\tconf_unpincome Nullable(Decimal128(2)) , -- 未付收益\r\n\tdeal_price Nullable(Decimal64(8)) , -- 成交价\r\n\tapplyno Nullable(String), -- 申请单号\r\n\tori_applyno Nullable(String), -- 原申请单号\r\n\tta_sponsor_sign Nullable(String), -- TA发起标志\r\n\tbonus_type Nullable(String), -- 分红方式\r\n\tcurr_hcode Nullable(String), -- 币种\r\n\tlarge_redem_sign Nullable(String), -- 巨额赎回处理标志\r\n\texchange_flag Nullable(String), -- 交易所标志\r\n\ttrade_handlestatus_code Nullable(String), -- 交易处理状态代码\r\n\tfail_reason_code Nullable(String), -- 失败原因代码\r\n\topp_fund_code Nullable(String), -- 对方产品代码\r\n\topp_share_type Nullable(String), -- 对方份额类型\r\n\topp_ta_acct Nullable(String), -- 对方基金账户\r\n\topp_agentno Nullable(String), -- 对方销售商编号\r\n\topp_netno Nullable(String), -- 对方网点编号\r\n\tasset_type Nullable(String), -- 资金方式\r\n\tbroker_id Nullable(String), -- 经纪人编号\r\n\tagent_chnl_code Nullable(String), -- 销售渠道编号\r\n\ttrade_mode Nullable(String), -- 交易方式\r\n\tmemo Nullable(String), -- 备注\r\n\tip Nullable(String), -- 未知\r\n\tmac Nullable(String), -- 未知\r\n\timei Nullable(String), -- 国际移动设备识别码\r\n\tuuid Nullable(String), -- 通用唯一识别码\r\n\tagio Nullable(Decimal128(10)) , -- 折扣\r\n\trequest_time Nullable(String), -- 申请时间\r\n\tbusiness_finish_flag Nullable(String), -- 未知\r\n\terror_detail Nullable(String), -- 出错详细信息\r\n\tacct_name Nullable(String), -- 投资人收款银行账户户名\r\n\tacct_no Nullable(String), -- 投资人收款银行账户户名\r\n\tclearing_agency Nullable(String), -- 投资人收款银行账户开户行\r\n\thdatasign Nullable(String), -- 数据标识\r\n\thloadtime Nullable(String), -- 数据时间\r\n\thbatchdate Nullable(String), -- 批次日期\r\n\thdatasrc Nullable(String), -- 数据源\r\n\tpar_month Nullable(String) -- 未知\r\n)\r\nENGINE = PostgreSQL('172.0.0.1:5432', 'gpadmin', 'even_trade_conf', 'hbxyoper', 'Hbxyoper@123','dw');\r\n\r\n-- greenplum\r\nselect count(1) from dw.even_trade_conf where conf_date='20210506'; \r\nresult: 8,260,023\r\nselect count(1) from dw.even_trade_conf ;\r\nresult: 2,095,769,958\r\n\r\n-- ck\r\nselect count(1)  from da.even_trade_conf_gp where conf_date='20210506';\r\nresult: 0\r\nselect count(1)  from da.even_trade_conf_gp ;\r\nresult: 2,788,075,838\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30214/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30184","id":1026598567,"node_id":"I_kwDOA5dJV849MKan","number":30184,"title":"Slow JSONExtract with LowCardinality(String) tuples","user":{"login":"davidmanzanares","id":1692175,"node_id":"MDQ6VXNlcjE2OTIxNzU=","avatar_url":"https://avatars.githubusercontent.com/u/1692175?v=4","gravatar_id":"","url":"https://api.github.com/users/davidmanzanares","html_url":"https://github.com/davidmanzanares","followers_url":"https://api.github.com/users/davidmanzanares/followers","following_url":"https://api.github.com/users/davidmanzanares/following{/other_user}","gists_url":"https://api.github.com/users/davidmanzanares/gists{/gist_id}","starred_url":"https://api.github.com/users/davidmanzanares/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidmanzanares/subscriptions","organizations_url":"https://api.github.com/users/davidmanzanares/orgs","repos_url":"https://api.github.com/users/davidmanzanares/repos","events_url":"https://api.github.com/users/davidmanzanares/events{/privacy}","received_events_url":"https://api.github.com/users/davidmanzanares/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-14T16:29:53Z","updated_at":"2021-10-14T16:29:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n`JSONExtract` of Tuples with `LowCardinality(String)` is significantly slower than with regular strings: ~2.5 slower. \r\n\r\nTested on CH 21.8.8.29\r\n\r\n## Unexpectedly slow example\r\n\r\n**60MB/s**\r\n\r\n```\r\ncat test.ndjson | clickhouse local -q \"\r\n    \r\nSELECT JSONExtract(event, 'Tuple(\r\na LowCardinality(String),\r\nb LowCardinality(String),\r\nc LowCardinality(String),\r\nd LowCardinality(String)\r\n\r\n)') AS json FROM table\" --input-format LineAsString --output-format Native --structure   \"\r\n    event String\" --verbose --logger.level trace >> /dev/null\r\n```\r\n\r\n## Expectedly fast example\r\n\r\n**150 MB/s**\r\n```\r\ntime cat test.ndjson | clickhouse local -q \"\r\n    \r\nSELECT JSONExtract(event, 'Tuple(\r\na String,\r\nb String,\r\nc String,\r\nd String\r\n\r\n)') AS json FROM table\" --input-format LineAsString --output-format Native --structure   \"\r\n    event String\" --verbose --logger.level trace >> /dev/null\r\n```\r\n\r\n## Workaround\r\n\r\n**140 MB/s**\r\n```\r\ncat test.ndjson | clickhouse local -q \"\r\n\r\nSELECT\r\n    toLowCardinality(json.1) AS a,\r\n    toLowCardinality(json.2) AS b,\r\n    toLowCardinality(json.3) AS c,\r\n    toLowCardinality(json.4) AS d\r\n\r\nFROM (SELECT JSONExtract(event, 'Tuple(\r\na String,\r\nb String,\r\nc String,\r\nd String\r\n\r\n)') AS json FROM table)\" --input-format LineAsString --output-format Native --structure   \"\r\n    event String\" --verbose --logger.level trace >> /dev/null\r\n\r\n\r\n```\r\ncc: @Algunenano \r\n\r\n[test.ndjson.zip](https://github.com/ClickHouse/ClickHouse/files/7347611/test.ndjson.zip)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30184/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30156","id":1026065469,"node_id":"I_kwDOA5dJV849KIQ9","number":30156,"title":"Use MaterializedMySQL to report an error:Unhandled data type:245.","user":{"login":"SoHuDrgon","id":11847516,"node_id":"MDQ6VXNlcjExODQ3NTE2","avatar_url":"https://avatars.githubusercontent.com/u/11847516?v=4","gravatar_id":"","url":"https://api.github.com/users/SoHuDrgon","html_url":"https://github.com/SoHuDrgon","followers_url":"https://api.github.com/users/SoHuDrgon/followers","following_url":"https://api.github.com/users/SoHuDrgon/following{/other_user}","gists_url":"https://api.github.com/users/SoHuDrgon/gists{/gist_id}","starred_url":"https://api.github.com/users/SoHuDrgon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SoHuDrgon/subscriptions","organizations_url":"https://api.github.com/users/SoHuDrgon/orgs","repos_url":"https://api.github.com/users/SoHuDrgon/repos","events_url":"https://api.github.com/users/SoHuDrgon/events{/privacy}","received_events_url":"https://api.github.com/users/SoHuDrgon/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-10-14T07:37:22Z","updated_at":"2021-11-08T08:33:50Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"MySQL version:8.0.25-15\r\nClickhouse  version:21.9.4.35\r\n\r\n1.create db on mysql\r\n```\r\nCREATE DATABASE db;\r\nCREATE TABLE db.test (a INT PRIMARY KEY, b INT);\r\nINSERT INTO db.test VALUES (1, 11), (2, 22);\r\n```\r\n\r\n2.use MaterializedMySQL on clickhouse:\r\n```\r\nSET allow_experimental_database_materialize_mysql = 1\r\nCREATE DATABASE db ENGINE = MaterializeMySQL('192.168.11.188:6446', 'db', 'ch_repl', 'xxxxxxxx');\r\n```\r\n\r\n3.the log report\r\n```\r\n2021.10.14 15:27:11.100018 [ 4549 ] {} <Debug> executeQuery: (internal) /*Materialize MySQL step 1: execute MySQL DDL for dump data*/ EXTERNAL DDL FROM MySQL(db, db) CREATE TABLE `test` ( `a` int NOT NULL, `b` int DEFAULT NULL, PRIMARY KEY (`a`) ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_0900_ai_ci\r\n2021.10.14 15:27:11.101071 [ 4549 ] {} <Debug> executeQuery: (internal) /* Rewritten MySQL DDL Query */ CREATE TABLE db.test (`a` Int32, `b` Nullable(Int32), `_sign` Int8() MATERIALIZED 1, `_version` UInt64() MATERIALIZED 1, INDEX _version _version TYPE minmax GRANULARITY 1) ENGINE = ReplacingMergeTree(_version) PARTITION BY intDiv(a, 4294967) ORDER BY tuple(a)\r\n2021.10.14 15:27:11.103660 [ 4549 ] {} <Debug> db.test (0c3a485f-ec57-4be3-8c3a-485fec570be3): Loading data parts\r\n2021.10.14 15:27:11.103794 [ 4549 ] {} <Debug> db.test (0c3a485f-ec57-4be3-8c3a-485fec570be3): There are no data parts\r\n2021.10.14 15:27:11.105240 [ 4549 ] {} <Debug> executeQuery: (internal) /*Materialize MySQL step 1: execute dump data*/ INSERT INTO test(a, b) VALUES\r\n2021.10.14 15:27:11.106001 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): sending PING to check if it is alive.\r\n2021.10.14 15:27:11.106670 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): PING ok.\r\n2021.10.14 15:27:11.106746 [ 4549 ] {} <Trace> mysqlxx::Query: Running MySQL query using connection 408710\r\n2021.10.14 15:27:11.108030 [ 4549 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:11.109625 [ 4549 ] {} <Trace> db.test (0c3a485f-ec57-4be3-8c3a-485fec570be3): Renaming temporary part tmp_insert_0_1_1_0 to 0_1_1_0.\r\n2021.10.14 15:27:11.109827 [ 4549 ] {} <Information> MaterializedMySQLSyncThread(db): Materialize MySQL step 1: dump test, 0.00 rows, 0.00 B in 0.00221912 sec., 0.00 rows/sec., 0.00 B/sec.\r\n2021.10.14 15:27:11.109978 [ 4549 ] {} <Information> MaterializedMySQLSyncThread: MySQL dump database position: \r\n \r\n=== Binlog Position ===\r\nBinlog: binlog.000009\r\nPosition: 940291857\r\nGTIDSets: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaa1:1-1945617\r\n\r\n2021.10.14 15:27:11.110018 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): sending PING to check if it is alive.\r\n2021.10.14 15:27:11.110623 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): PING ok.\r\n2021.10.14 15:27:11.110684 [ 4549 ] {} <Trace> mysqlxx::Query: Running MySQL query using connection 408710\r\n2021.10.14 15:27:12.117170 [ 4549 ] {} <Trace> mysqlxx::Pool: (db@192.168.11.188:6446 as user ch_repl): Iterating through existing MySQL connections\r\n2021.10.14 15:27:12.117295 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): sending PING to check if it is alive.\r\n2021.10.14 15:27:12.117971 [ 4549 ] {} <Trace> mysqlxx::Pool: Entry(connection 408710): PING ok.\r\n2021.10.14 15:27:12.118024 [ 4549 ] {} <Trace> mysqlxx::Query: Running MySQL query using connection 408710\r\n2021.10.14 15:27:12.122181 [ 4549 ] {} <Debug> MaterializedMySQLSyncThread: Skip MySQL event: \r\n \r\n=== FormatDescriptionEvent ===\r\nTimestamp: 1634093562\r\nEvent Type: FormatDescriptionEvent\r\nServer ID: 160\r\nEvent Size: 121\r\nLog Pos: 125\r\nFlags: 0\r\nBinlog Version: 4\r\nServer Version: 8.0.25-15\r\nCreate Timestamp: 0\r\nEvent Header Len: 19\r\n\r\n2021.10.14 15:27:12.122241 [ 4549 ] {} <Debug> MaterializedMySQLSyncThread: Skip MySQL event: \r\n \r\n=== PreviousGTIDsEvent ===\r\nTimestamp: 1634093562\r\nEvent Type: PreviousGTIDsEvent\r\nServer ID: 160\r\nEvent Size: 71\r\nLog Pos: 196\r\nFlags: 128\r\n[DryRun Event]\r\n\r\n2021.10.14 15:27:13.091049 [ 3837 ] {} <Trace> SystemLog (system.part_log): Flushing system log, 1 entries to flush up to offset 4\r\n2021.10.14 15:27:13.092853 [ 3837 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:13.094882 [ 3837 ] {} <Trace> system.part_log (ca22883f-c3af-4039-8a22-883fc3af7039): Renaming temporary part tmp_insert_202110_4_4_0 to 202110_4_4_0.\r\n2021.10.14 15:27:13.095196 [ 3837 ] {} <Trace> SystemLog (system.part_log): Flushed system log up to offset 4\r\n2021.10.14 15:27:13.314128 [ 3841 ] {} <Trace> SystemLog (system.query_thread_log): Flushing system log, 3 entries to flush up to offset 187\r\n2021.10.14 15:27:13.317276 [ 3841 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:13.319580 [ 3841 ] {} <Trace> system.query_thread_log (fffc1a48-79d3-4ae1-bffc-1a4879d3fae1): Renaming temporary part tmp_insert_202110_54_54_0 to 202110_54_54_0.\r\n2021.10.14 15:27:13.319986 [ 3841 ] {} <Trace> SystemLog (system.query_thread_log): Flushed system log up to offset 187\r\n2021.10.14 15:27:13.365578 [ 3840 ] {} <Trace> SystemLog (system.query_log): Flushing system log, 3 entries to flush up to offset 126\r\n2021.10.14 15:27:13.370626 [ 3840 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:13.373520 [ 3840 ] {} <Trace> system.query_log (35246dfc-a11d-4afa-b524-6dfca11d8afa): Renaming temporary part tmp_insert_202110_54_54_0 to 202110_54_54_0.\r\n2021.10.14 15:27:13.373704 [ 3840 ] {} <Trace> SystemLog (system.query_log): Flushed system log up to offset 126\r\n2021.10.14 15:27:14.047504 [ 3845 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushing system log, 4949 entries to flush up to offset 60043183\r\n2021.10.14 15:27:14.050555 [ 3845 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:14.053429 [ 3845 ] {} <Trace> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff): Renaming temporary part tmp_insert_202110_12122_12122_0 to 202110_12122_12122_0.\r\n2021.10.14 15:27:14.053693 [ 3845 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushed system log up to offset 60043183\r\n2021.10.14 15:27:14.053734 [ 3912 ] {} <Debug> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff) (MergerMutator): Selected 6 parts from 202110_12068_12117_9 to 202110_12122_12122_0\r\n2021.10.14 15:27:14.053858 [ 3912 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:14.054034 [ 4545 ] {} <Debug> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff) (MergerMutator): Merging 6 parts: from 202110_12068_12117_9 to 202110_12122_12122_0 into Compact\r\n2021.10.14 15:27:14.054208 [ 4545 ] {} <Debug> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff) (MergerMutator): Selected MergeAlgorithm: Horizontal\r\n2021.10.14 15:27:14.054348 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 26 marks from part 202110_12068_12117_9, total 248159 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.054844 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_12118_12118_0, total 4949 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.055176 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_12119_12119_0, total 4949 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.055471 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_12120_12120_0, total 4949 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.055748 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_12121_12121_0, total 4949 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.056000 [ 4545 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_12122_12122_0, total 4949 rows starting from the beginning of the part\r\n2021.10.14 15:27:14.082745 [ 4545 ] {} <Debug> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff) (MergerMutator): Merge sorted 272904 rows, containing 5 columns (5 merged, 0 gathered) in 0.028754264 sec., 9490905.418410292 rows/sec., 237.19 MiB/sec.\r\n2021.10.14 15:27:14.083791 [ 4545 ] {} <Trace> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff): Renaming temporary part tmp_merge_202110_12068_12122_10 to 202110_12068_12122_10.\r\n2021.10.14 15:27:14.083874 [ 4545 ] {} <Trace> system.asynchronous_metric_log (3aa32c4e-74c6-4cff-baa3-2c4e74c60cff) (MergerMutator): Merged 6 parts: from 202110_12068_12117_9 to 202110_12122_12122_0\r\n2021.10.14 15:27:14.083906 [ 4545 ] {} <Debug> MemoryTracker: Peak memory usage: 4.00 MiB.\r\n2021.10.14 15:27:15.000179 [ 3999 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 1.08 GiB, peak 3.02 GiB, will set to 1.08 GiB (RSS), difference: 2.62 MiB\r\n2021.10.14 15:27:16.629615 [ 3844 ] {} <Trace> SystemLog (system.metric_log): Flushing system log, 7 entries to flush up to offset 84928\r\n2021.10.14 15:27:16.642493 [ 3844 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 416.52 GiB.\r\n2021.10.14 15:27:16.645541 [ 3844 ] {} <Trace> system.metric_log (6dcec7cd-7455-491a-adce-c7cd7455791a): Renaming temporary part tmp_insert_202110_11299_11299_0 to 202110_11299_11299_0.\r\n2021.10.14 15:27:16.646275 [ 3844 ] {} <Trace> SystemLog (system.metric_log): Flushed system log up to offset 84928\r\n2021.10.14 15:27:20.750448 [ 4549 ] {} <Debug> MaterializedMySQLSyncThread: Skip MySQL event: \r\n \r\n=== GTIDEvent ===\r\nTimestamp: 1634196433\r\nEvent Type: GTIDEvent\r\nServer ID: 160\r\nEvent Size: 86\r\nLog Pos: 940291943\r\nFlags: 0\r\nGTID Next: aaaaaaaa-aaaa-aaaa-aaaa-aaaaaaaaaaa1:1945618\r\n\r\n2021.10.14 15:27:20.750537 [ 4549 ] {} <Debug> MaterializedMySQLSyncThread: Skip MySQL event: \r\n \r\n=== QueryEvent ===\r\nTimestamp: 1634196433\r\nEvent Type: QueryEvent\r\nServer ID: 160\r\nEvent Size: 88\r\nLog Pos: 940292031\r\nFlags: 8\r\n[DryRun Event]\r\n\r\n2021.10.14 15:27:20.750586 [ 4549 ] {} <Debug> MaterializedMySQLSyncThread: Skip MySQL event: \r\n \r\n=== RowsQueryEvent ===\r\nTimestamp: 1634196433\r\nEvent Type: RowsQueryEvent\r\nServer ID: 160\r\nEvent Size: 117\r\nLog Pos: 940292148\r\nFlags: 128\r\n[DryRun Event]\r\n\r\n2021.10.14 15:27:20.751064 [ 4549 ] {} <Error> MaterializedMySQLSyncThread: Code: 1002. DB::Exception: ParseMetaData: Unhandled data type:245. (UNKNOWN_EXCEPTION), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x936a17a in /usr/bin/clickhouse\r\n1. DB::MySQLReplication::TableMapEvent::parseMeta(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) @ 0x107c79cb in /usr/bin/clickhouse\r\n2. DB::MySQLReplication::TableMapEvent::parseImpl(DB::ReadBuffer&) @ 0x107c7323 in /usr/bin/clickhouse\r\n3. DB::MySQLReplication::MySQLFlavor::readPayloadImpl(DB::ReadBuffer&) @ 0x107cefc5 in /usr/bin/clickhouse\r\n4. DB::MySQLProtocol::IMySQLReadPacket::readPayload(DB::ReadBuffer&, unsigned char&) @ 0x107b3547 in /usr/bin/clickhouse\r\n5. DB::MySQLProtocol::PacketEndpoint::tryReceivePacket(DB::MySQLProtocol::IMySQLReadPacket&, unsigned long) @ 0x107b404e in /usr/bin/clickhouse\r\n6. DB::MaterializedMySQLSyncThread::synchronization() @ 0x1077fd3d in /usr/bin/clickhouse\r\n7. ? @ 0x107a14f2 in /usr/bin/clickhouse\r\n8. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x93ab19f in /usr/bin/clickhouse\r\n9. ? @ 0x93aea83 in /usr/bin/clickhouse\r\n10. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n11. clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n (version 21.9.4.35 (official build))\r\n```\r\n\r\n4.query the clickhouse\r\n```\r\nselect * from test;\r\n\r\nSELECT *\r\nFROM test\r\n\r\nQuery id: 75b3587d-f7a6-4be7-8cd4-cd0c74ae3033\r\n\r\n\r\n0 rows in set. Elapsed: 0.002 sec. \r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 1002. DB::Exception: Received from localhost:9000. DB::Exception: ParseMetaData: Unhandled data type:245. (UNKNOWN_EXCEPTION)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30156/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30146","id":1025541094,"node_id":"I_kwDOA5dJV849IIPm","number":30146,"title":"Join on an aliased column fails with allow_experimental_projection_optimization","user":{"login":"SaltTan","id":20357526,"node_id":"MDQ6VXNlcjIwMzU3NTI2","avatar_url":"https://avatars.githubusercontent.com/u/20357526?v=4","gravatar_id":"","url":"https://api.github.com/users/SaltTan","html_url":"https://github.com/SaltTan","followers_url":"https://api.github.com/users/SaltTan/followers","following_url":"https://api.github.com/users/SaltTan/following{/other_user}","gists_url":"https://api.github.com/users/SaltTan/gists{/gist_id}","starred_url":"https://api.github.com/users/SaltTan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SaltTan/subscriptions","organizations_url":"https://api.github.com/users/SaltTan/orgs","repos_url":"https://api.github.com/users/SaltTan/repos","events_url":"https://api.github.com/users/SaltTan/events{/privacy}","received_events_url":"https://api.github.com/users/SaltTan/received_events","type":"User","site_admin":false},"labels":[{"id":2673090580,"node_id":"MDU6TGFiZWwyNjczMDkwNTgw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug%20experimental","name":"bug experimental","color":"E99695","default":false,"description":"Bug in the feature that should not be used in production"},{"id":3045757785,"node_id":"MDU6TGFiZWwzMDQ1NzU3Nzg1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-projections","name":"comp-projections","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"assignees":[{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false}],"milestone":null,"comments":5,"created_at":"2021-10-13T17:58:22Z","updated_at":"2022-01-09T20:01:07Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Join on an aliased column fails with `allow_experimental_projection_optimization=1` even though there are no projections on a table.\r\n\r\nThe test case is a self join on different columns that are aliased to perform a 'using' join:\r\n```sql\r\ncreate table mt (id1 Int8, id2 Int8) Engine=MergeTree order by tuple();\r\n\r\n-- Both columns are aliased\r\n-- allow_experimental_projection_optimization is disabled\r\nSELECT id1 AS alias1\r\nFROM mt\r\nALL INNER JOIN\r\n(\r\n    SELECT id2 AS alias1\r\n    FROM mt\r\n) AS t USING (alias1)\r\nSETTINGS allow_experimental_projection_optimization = 0\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\n-- allow_experimental_projection_optimization is enabled\r\nSELECT id1 AS alias1\r\nFROM mt\r\nALL INNER JOIN\r\n(\r\n    SELECT id2 AS alias1\r\n    FROM mt\r\n) AS t USING (alias1)\r\nSETTINGS allow_experimental_projection_optimization = 1\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column id1 in block. There are only columns: id2. (NOT_FOUND_COLUMN_IN_BLOCK)\r\n\r\n-- The column in the subquery is aliased\r\nSELECT id1\r\nFROM mt\r\nALL INNER JOIN\r\n(\r\n    SELECT id2 AS id1\r\n    FROM mt\r\n) AS t USING (id1)\r\nSETTINGS allow_experimental_projection_optimization = 1\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column t.id1 in block. There are only columns: . (NOT_FOUND_COLUMN_IN_BLOCK)\r\n\r\n-- The column in the left table is aliased\r\nSELECT id2 AS id1\r\nFROM mt\r\nALL INNER JOIN\r\n(\r\n    SELECT id1\r\n    FROM mt\r\n) AS t USING (id1)\r\nSETTINGS allow_experimental_projection_optimization = 1\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 10. DB::Exception: Received from localhost:9000. DB::Exception: Not found column id2 in block. There are only columns: id1. (NOT_FOUND_COLUMN_IN_BLOCK)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30146/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30144","id":1025485561,"node_id":"PR_kwDOA5dJV84tKA3s","number":30144,"title":"test to check what happen with tests if allow_experimental_projection_optimization is enabled","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":1876999755,"node_id":"MDU6TGFiZWwxODc2OTk5NzU1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-not-for-changelog","name":"pr-not-for-changelog","color":"007700","default":false,"description":"This PR should not be mentioned in the changelog"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-10-13T16:56:51Z","updated_at":"2021-10-17T10:37:42Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30144","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30144","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30144.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30144.patch","merged_at":null},"body":"Do not merge.\r\n\r\nwe observe that allow_experimental_projection_optimization impacts tables/queries without projections.\r\n\r\nChangelog category (leave one):\r\n- Not for changelog (changelog entry is not required)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30144/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30124","id":1025292835,"node_id":"I_kwDOA5dJV849HLoj","number":30124,"title":"Reading parts in reverse order is not parallelised resulting in very bad performance","user":{"login":"nvartolomei","id":543193,"node_id":"MDQ6VXNlcjU0MzE5Mw==","avatar_url":"https://avatars.githubusercontent.com/u/543193?v=4","gravatar_id":"","url":"https://api.github.com/users/nvartolomei","html_url":"https://github.com/nvartolomei","followers_url":"https://api.github.com/users/nvartolomei/followers","following_url":"https://api.github.com/users/nvartolomei/following{/other_user}","gists_url":"https://api.github.com/users/nvartolomei/gists{/gist_id}","starred_url":"https://api.github.com/users/nvartolomei/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nvartolomei/subscriptions","organizations_url":"https://api.github.com/users/nvartolomei/orgs","repos_url":"https://api.github.com/users/nvartolomei/repos","events_url":"https://api.github.com/users/nvartolomei/events{/privacy}","received_events_url":"https://api.github.com/users/nvartolomei/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-10-13T14:02:38Z","updated_at":"2021-11-01T15:43:41Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Is this a bug or there are difficulties in making reading in reverse order parallel similar to read in order? With high number of parts this sometimes leads to disastrous performance.\r\n\r\n```\r\nef164b805f78 :) select version()\r\n\r\n21.9.4.35\r\n\r\nef164b805f78 :) system stop merges;\r\nef164b805f78 :) create table test engine=MergeTree() order by number as system.numbers\r\nef164b805f78 :) insert into test select * from numbers(1000)\r\nef164b805f78 :) insert into test select * from numbers(1000)\r\nef164b805f78 :) insert into test select * from numbers(1000)\r\nef164b805f78 :) explain pipeline select number from test order by number limit 10\r\n\r\n┌─explain──────────────────────────────┐\r\n│ (Expression)                         │\r\n│ ExpressionTransform                  │\r\n│   (Limit)                            │\r\n│   Limit                              │\r\n│     (FinishSorting)                  │\r\n│     MergingSortedTransform 3 → 1     │\r\n│       (Expression)                   │\r\n│       ExpressionTransform × 3        │\r\n│         (SettingQuotaAndLimits)      │\r\n│           (ReadFromMergeTree)        │\r\n│           MergeTreeInOrder × 3 0 → 1 │\r\n└──────────────────────────────────────┘\r\n\r\nef164b805f78 :) explain pipeline select number from test order by number desc limit 10\r\n\r\n┌─explain────────────────────────────────────┐\r\n│ (Expression)                               │\r\n│ ExpressionTransform                        │\r\n│   (Limit)                                  │\r\n│   Limit                                    │\r\n│     (FinishSorting)                        │\r\n│     MergingSortedTransform 3 → 1           │\r\n│       (Expression)                         │\r\n│       ExpressionTransform × 3              │\r\n│         (SettingQuotaAndLimits)            │\r\n│           (ReadFromMergeTree)              │\r\n│           ReverseTransform                 │\r\n│             MergeTreeReverse 0 → 1         │\r\n│               ReverseTransform             │\r\n│                 MergeTreeReverse 0 → 1     │\r\n│                   ReverseTransform         │\r\n│                     MergeTreeReverse 0 → 1 │\r\n└────────────────────────────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30124/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30113","id":1025237272,"node_id":"I_kwDOA5dJV849G-EY","number":30113,"title":"result of scalar CTE from subquery can't be used in IN clause","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-13T13:14:41Z","updated_at":"2021-10-14T13:24:01Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nIt's not possible to use IN + array from scalar CTE with subquery in it.\r\n\r\n**How to reproduce**\r\nClickHouse version 21.10\r\n\r\n```\r\nWITH (\r\n        SELECT groupArray(number)\r\n        FROM numbers(10)\r\n    ) AS ids\r\nSELECT *\r\nFROM numbers(100)\r\nWHERE number IN (ids)\r\n\r\n\r\n0 rows in set. Elapsed: 0.004 sec.\r\n\r\nReceived exception from server (version 21.10.1):\r\nCode: 53. DB::Exception: Received from localhost:9000. DB::Exception: CAST AS Array can only be performed between same-dimensional Array or String types: while executing 'FUNCTION in(number : 0, ids :: 1) -> in(number, ids) UInt8 : 2'. (TYPE_MISMATCH)\r\n\r\nWITH [0, 1, 2, 3, 4, 5, 6, 7, 8, 9] AS ids\r\nSELECT *\r\nFROM numbers(100)\r\nWHERE number IN (ids)\r\n\r\n┌─number─┐\r\n│      0 │\r\n│      1 │\r\n│      2 │\r\n│      3 │\r\n│      4 │\r\n│      5 │\r\n│      6 │\r\n│      7 │\r\n│      8 │\r\n│      9 │\r\n└────────┘\r\n\r\nWITH (\r\n        SELECT groupArray(number)\r\n        FROM numbers(10)\r\n    ) AS ids\r\nSELECT *\r\nFROM numbers(100)\r\nWHERE [number] IN (ids)\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.009 sec.\r\n\r\nWITH (\r\n        SELECT groupArray(toString(number))\r\n        FROM numbers(10)\r\n    ) AS ids\r\nSELECT *\r\nFROM numbers(100)\r\nWHERE toString(number) IN (ids)\r\n\r\n\r\n0 rows in set. Elapsed: 0.006 sec.\r\n\r\nReceived exception from server (version 21.10.1):\r\nCode: 130. DB::Exception: Received from localhost:9000. DB::Exception: Array does not start with '[' character: while executing 'FUNCTION in(toString(number) :: 2, ids :: 1) -> in(toString(number), ids) UInt8 : 3'. (CANNOT_READ_ARRAY_FROM_TEXT)\r\n```\r\n\r\nClickHouse version 21.8\r\n\r\n```\r\nCREATE TABLE test_tbl_a\r\n(\r\n    `id` UInt64\r\n)\r\nENGINE = MergeTree\r\nORDER BY id\r\n\r\nINSERT INTO test_tbl_a SELECT *\r\nFROM numbers(1000)\r\n\r\n\r\nWITH (\r\n        SELECT groupArrayArray([toUInt64(1)])\r\n    ) AS ids\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT id\r\n    FROM test_tbl_a\r\n    WHERE id IN (ids)\r\n    UNION ALL\r\n    SELECT id AS external_idLast\r\n    FROM test_tbl_a\r\n    WHERE id IN (ids)\r\n)\r\n\r\nQuery id: 9705a753-55a8-4276-8c40-e4bd34d4338f\r\n\r\n\r\n0 rows in set. Elapsed: 0.003 sec.\r\n\r\nReceived exception from server (version 21.8.8):\r\nCode: 179. DB::Exception: Received from localhost:9000. DB::Exception: Different expressions with the same alias ids:\r\n(WITH __getScalar('3590115515134869913_2786030434091612130') AS ids SELECT groupArrayArray([toUInt64(1)])) AS ids\r\nand\r\n__getScalar('3590115515134869913_2786030434091612130') AS ids\r\n: While processing (WITH __getScalar('3590115515134869913_2786030434091612130') AS ids SELECT groupArrayArray([toUInt64(1)])) AS ids.\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30113/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30112","id":1025213745,"node_id":"I_kwDOA5dJV849G4Ux","number":30112,"title":"The 'GenerateRandom' is not implemented for type LowCardinality(String)","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-13T12:53:57Z","updated_at":"2021-10-13T12:53:57Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"```\r\navtokmakov-dev.sas.yp-c.yandex.net :) insert into checks_sample select * from generateRandom('`pull_request_number` UInt32, `commit_sha` LowCardinality(String), `check_name` LowCardinality(String), `check_status` LowCardinality(String), `check_duration_ms` UInt64, `check_start_time` DateTime, `test_name` LowCardinality(String), `test_status` LowCardinality(String), `test_duration_ms` UInt64, `report_url` String, `pull_request_url` String, `commit_url` String, `task_url` String, `base_ref` String, `base_repo` String, `head_ref` String, `head_repo` String') limit 1000000\r\n\r\nINSERT INTO checks_sample SELECT *\r\nFROM generateRandom('`pull_request_number` UInt32, `commit_sha` LowCardinality(String), `check_name` LowCardinality(String), `check_status` LowCardinality(String), `check_duration_ms` UInt64, `check_start_time` DateTime, `test_name` LowCardinality(String), `test_status` LowCardinality(String), `test_duration_ms` UInt64, `report_url` String, `pull_request_url` String, `commit_url` String, `task_url` String, `base_ref` String, `base_repo` String, `head_ref` String, `head_repo` String')\r\nLIMIT 1000000\r\n\r\nQuery id: 11e8f2c6-aa92-4a3f-a436-3b1fa738193a\r\n\r\n\r\n0 rows in set. Elapsed: 2.039 sec. \r\n\r\nReceived exception from server (version 21.11.1):\r\nCode: 48. DB::Exception: Received from localhost:9000. DB::Exception: The 'GenerateRandom' is not implemented for type LowCardinality(String): While executing GenerateRandom. (NOT_IMPLEMENTED)\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30112/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30091","id":1025076894,"node_id":"I_kwDOA5dJV849GW6e","number":30091,"title":"Mongodb table engine is not working","user":{"login":"lala7573","id":4996496,"node_id":"MDQ6VXNlcjQ5OTY0OTY=","avatar_url":"https://avatars.githubusercontent.com/u/4996496?v=4","gravatar_id":"","url":"https://api.github.com/users/lala7573","html_url":"https://github.com/lala7573","followers_url":"https://api.github.com/users/lala7573/followers","following_url":"https://api.github.com/users/lala7573/following{/other_user}","gists_url":"https://api.github.com/users/lala7573/gists{/gist_id}","starred_url":"https://api.github.com/users/lala7573/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lala7573/subscriptions","organizations_url":"https://api.github.com/users/lala7573/orgs","repos_url":"https://api.github.com/users/lala7573/repos","events_url":"https://api.github.com/users/lala7573/events{/privacy}","received_events_url":"https://api.github.com/users/lala7573/received_events","type":"User","site_admin":false},"labels":[{"id":1365689344,"node_id":"MDU6TGFiZWwxMzY1Njg5MzQ0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-foreign-db","name":"comp-foreign-db","color":"b5bcff","default":false,"description":"Integrations with other databases"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-13T10:34:22Z","updated_at":"2021-12-07T23:16:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nI tried both mongodb dictionary and mongodb table engine. \r\nAnd the dictionary source works fine but table engine doesn't work.\r\n\r\n**How to reproduce**\r\n```\r\n# this works well\r\nCREATE DICTIONARY ts_geo_dict (\r\n　    name String,\r\n       key Int64\r\n) PRIMARY KEY naverId\r\nSOURCE(MONGODB(\r\n　    host 'ip'\r\n　    port 30011\r\n　    user 'user'\r\n　    password 'password'\r\n　    db 'db'\r\n　    collection 'collection'　\r\n))　LAYOUT(DIRECT());\r\n\r\n# this doesn't work\r\nCREATE TABLE ts_geo_table (\r\n　    name String,\r\n       key Int64\r\n) ENGINE = MongoDB('ip:30011', 'db', 'collection', 'user', 'password');\r\n```\r\n\r\n**Expected behavior**\r\nboth should works well.\r\n\r\n**Error message and/or stacktrace**\r\nIf applicable, add screenshots to help explain your problem.\r\n```\r\n Poco::Exception. Code: 1000, e.code() = 0, Timeout: connect timed out: ip:30011 (version 21.9.4.35 (official build)) (from local:59692) (in query: select naverId from ts_geo;), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Net::SocketImpl::connect(Poco::Net::SocketAddress const&, Poco::Timespan const&) @ 0x14370e5b in /usr/bin/clickhouse\r\n1. DB::StorageMongoDBSocketFactory::createSocket(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, Poco::Timespan, bool) @ 0x1132099d in /usr/bin/clickhouse\r\n2. Poco::MongoDB::Connection::connect(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::MongoDB::Connection::SocketFactory&) @ 0x142e0551 in /usr/bin/clickhouse\r\n3. Poco::MongoDB::Connection::Connection(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, Poco::MongoDB::Connection::SocketFactory&) @ 0x142dfd45 in /usr/bin/clickhouse\r\n4. DB::StorageMongoDB::connectIfNotConnected() @ 0x1131f3e8 in /usr/bin/clickhouse\r\n5. DB::StorageMongoDB::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x1131f705 in /usr/bin/clickhouse\r\n6. DB::IStorage::read(DB::QueryPlan&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x110bb6ae in /usr/bin/clickhouse\r\n7. DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&) @ 0x10b24e46 in /usr/bin/clickhouse\r\n8. DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>) @ 0x10b1ca9c in /usr/bin/clickhouse\r\n9. DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x10b1bc80 in /usr/bin/clickhouse\r\n10. DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x10cf113e in /usr/bin/clickhouse\r\n11. DB::InterpreterSelectWithUnionQuery::execute() @ 0x10cf2191 in /usr/bin/clickhouse\r\n12. ? @ 0x10ee5c04 in /usr/bin/clickhouse\r\n13. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool) @ 0x10ee3ca3 in /usr/bin/clickhouse\r\n14. DB::TCPHandler::runImpl() @ 0x117a5c30 in /usr/bin/clickhouse\r\n15. DB::TCPHandler::run() @ 0x117b8519 in /usr/bin/clickhouse\r\n16. Poco::Net::TCPServerConnection::start() @ 0x1437788f in /usr/bin/clickhouse\r\n17. Poco::Net::TCPServerDispatcher::run() @ 0x1437931a in /usr/bin/clickhouse\r\n18. Poco::PooledThread::run() @ 0x144ac199 in /usr/bin/clickhouse\r\n19. Poco::ThreadImpl::runnableEntry(void*) @ 0x144a842a in /usr/bin/clickhouse\r\n20. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n21. __clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 1000. DB::Exception: Received from localhost:9000. DB::Exception: Timeout: connect timed out: {ip}:30011. (POCO_EXCEPTION)\r\n```\r\n\r\n**Additional context**\r\nI checked tcp dump of both sql, and it seems both works differently. When connecting to mongodb using table engine, clickhouse sends F instantly.\r\n```\r\n# mongodb table engine\r\n\r\n19:25:13.478360 IP localhost.54580 > {ip}.30011: Flags [S], seq 2737913030, win 42340, options [mss 1460,sackOK,TS val 2852010648 ecr 0,nop,wscale 7], length 0\r\n19:25:13.478483 IP localhost.54580 > {ip}.30011: Flags [.], ack 2847463878, win 331, options [nop,nop,TS val 2852010648 ecr 2228342867], length 0\r\n19:25:13.478513 IP localhost.54580 > {ip}.30011: Flags [F.], seq 0, ack 1, win 331, options [nop,nop,TS val 2852010648 ecr 2228342867], length 0\r\n19:25:13.479011 IP localhost.54580 > {ip}.30011: Flags [.], ack 2, win 331, options [nop,nop,TS val 2852010648 ecr 2228342867], length 0\r\n\r\n# mongodb dictionary \r\n\r\n19:30:06.818616 IP localhost.41948 > {ip}.30011: Flags [S], seq 2449614556, win 42340, options [mss 1460,sackOK,TS val 2852303981 ecr 0,nop,wscale 7], length 0\r\n19:30:06.818761 IP localhost.41948 > {ip}.30011: Flags [.], ack 3110347481, win 331, options [nop,nop,TS val 2852303982 ecr 2228636207], length 0\r\n19:30:06.818827 IP localhost.41948 > {ip}.30011: Flags [P.], seq 0:175, ack 1, win 331, options [nop,nop,TS val 2852303982 ecr 2228636207], length 175\r\n19:30:06.819691 IP localhost.41948 > {ip}.30011: Flags [.], ack 196, win 331, options [nop,nop,TS val 2852303982 ecr 2228636208], length 0\r\n19:30:06.837662 IP localhost.41948 > {ip}.30011: Flags [P.], seq 175:382, ack 196, win 331, options [nop,nop,TS val 2852304000 ecr 2228636208], length 207\r\n19:30:06.838030 IP localhost.41948 > {ip}.30011: Flags [P.], seq 382:485, ack 320, win 331, options [nop,nop,TS val 2852304001 ecr 2228636226], length 103\r\n19:30:06.839136 IP localhost.41948 > {ip}.30011: Flags [P.], seq 485:652, ack 414, win 331, options [nop,nop,TS val 2852304002 ecr 2228636226], length 167\r\n19:30:06.840038 IP localhost.41948 > {ip}.30011: Flags [.], ack 14894, win 261, options [nop,nop,TS val 2852304003 ecr 2228636228], length 0\r\n...\r\n...\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30091/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30089","id":1025058754,"node_id":"I_kwDOA5dJV849GSfC","number":30089,"title":"Expression not supported for dictionaries with file source.","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-13T10:16:36Z","updated_at":"2021-10-13T16:57:26Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nUsing JSON file with complex structure as source in dictionary.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt's possible to use ClickHouse functions in expression for dictionaries with file source.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nUse intermediate view as source with parsing JSON values in it.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30089/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30088","id":1025034346,"node_id":"I_kwDOA5dJV849GMhq","number":30088,"title":"Write multiple rows in one kafka message in avro format","user":{"login":"ilejn","id":360758,"node_id":"MDQ6VXNlcjM2MDc1OA==","avatar_url":"https://avatars.githubusercontent.com/u/360758?v=4","gravatar_id":"","url":"https://api.github.com/users/ilejn","html_url":"https://github.com/ilejn","followers_url":"https://api.github.com/users/ilejn/followers","following_url":"https://api.github.com/users/ilejn/following{/other_user}","gists_url":"https://api.github.com/users/ilejn/gists{/gist_id}","starred_url":"https://api.github.com/users/ilejn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ilejn/subscriptions","organizations_url":"https://api.github.com/users/ilejn/orgs","repos_url":"https://api.github.com/users/ilejn/repos","events_url":"https://api.github.com/users/ilejn/events{/privacy}","received_events_url":"https://api.github.com/users/ilejn/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-13T09:53:43Z","updated_at":"2021-10-13T09:54:58Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\n\r\nCurrently writing to kafka in avro format does not work at all. \r\n\r\n**How to reproduce**\r\n\r\n```\r\ninsert` into test_kafka_avro select number*10 as key, number*100 as value from numbers(5)\r\n```\r\n\r\nagainst\r\n```\r\nCREATE TABLE default.test_kafka_avro\r\n(\r\n    `key` UInt64,\r\n    `value` UInt64\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = 'localhost:9092', kafka_topic_list = 'insert1', kafka_group_name = 'insert1', kafka_format = 'Avro', kafka_row_delimiter = '\\n'\r\n\r\n```\r\nproduces five kafka messages. \r\nThe first one consists of avro header (and may be the first row, not sure) padded by zero bytes up to 1024 bytes, the next four messages are empty.\r\n\r\nPlayed with 21.11.1.1\r\n\r\n**Expected behavior**\r\n\r\nWe need the ability to write several rows per a kafka message in one avro file (one avro file consisted of several rows in a kafka message).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30088/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30087","id":1025020324,"node_id":"I_kwDOA5dJV849GJGk","number":30087,"title":"Add storage policy for existing table","user":{"login":"Jake-00","id":60967034,"node_id":"MDQ6VXNlcjYwOTY3MDM0","avatar_url":"https://avatars.githubusercontent.com/u/60967034?v=4","gravatar_id":"","url":"https://api.github.com/users/Jake-00","html_url":"https://github.com/Jake-00","followers_url":"https://api.github.com/users/Jake-00/followers","following_url":"https://api.github.com/users/Jake-00/following{/other_user}","gists_url":"https://api.github.com/users/Jake-00/gists{/gist_id}","starred_url":"https://api.github.com/users/Jake-00/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Jake-00/subscriptions","organizations_url":"https://api.github.com/users/Jake-00/orgs","repos_url":"https://api.github.com/users/Jake-00/repos","events_url":"https://api.github.com/users/Jake-00/events{/privacy}","received_events_url":"https://api.github.com/users/Jake-00/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-13T09:39:23Z","updated_at":"2021-10-13T14:41:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have created some tables without setting storage policy, after a while, I have been adding new storage policy in config.xml, and wanna add storage policy for existing table, when I use command `alter table db.tableName1 modify setting storage_policy='hdd_in_order'; `  , there is an error below \r\nException: New storage policy \\`default\\` shall contain volumes of old one.\r\nI am confused that I use 'hdd_in_order' in spite of 'default', hope some help !!!\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30087/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30085","id":1024957219,"node_id":"I_kwDOA5dJV849F5sj","number":30085,"title":"Regarding the functional suggestions of MaterializedMySQL","user":{"login":"SoHuDrgon","id":11847516,"node_id":"MDQ6VXNlcjExODQ3NTE2","avatar_url":"https://avatars.githubusercontent.com/u/11847516?v=4","gravatar_id":"","url":"https://api.github.com/users/SoHuDrgon","html_url":"https://github.com/SoHuDrgon","followers_url":"https://api.github.com/users/SoHuDrgon/followers","following_url":"https://api.github.com/users/SoHuDrgon/following{/other_user}","gists_url":"https://api.github.com/users/SoHuDrgon/gists{/gist_id}","starred_url":"https://api.github.com/users/SoHuDrgon/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SoHuDrgon/subscriptions","organizations_url":"https://api.github.com/users/SoHuDrgon/orgs","repos_url":"https://api.github.com/users/SoHuDrgon/repos","events_url":"https://api.github.com/users/SoHuDrgon/events{/privacy}","received_events_url":"https://api.github.com/users/SoHuDrgon/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false},"assignees":[{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-13T08:37:40Z","updated_at":"2022-01-24T16:44:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Due to the accumulation of historical data, some historical data may be deleted at a certain time, but you do not want the clickhouse data to be synchronized to this Delete DML statement.\r\n\r\nCan you ignore a certain type of DML statement when synchronizing MySQL data (such as the Delete statement type ignores the execution).\r\n\r\nThanks!","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30085/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30055","id":1023910747,"node_id":"I_kwDOA5dJV849B6Nb","number":30055,"title":"GRANT DROP VIEW","user":{"login":"Glomywow","id":655091,"node_id":"MDQ6VXNlcjY1NTA5MQ==","avatar_url":"https://avatars.githubusercontent.com/u/655091?v=4","gravatar_id":"","url":"https://api.github.com/users/Glomywow","html_url":"https://github.com/Glomywow","followers_url":"https://api.github.com/users/Glomywow/followers","following_url":"https://api.github.com/users/Glomywow/following{/other_user}","gists_url":"https://api.github.com/users/Glomywow/gists{/gist_id}","starred_url":"https://api.github.com/users/Glomywow/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Glomywow/subscriptions","organizations_url":"https://api.github.com/users/Glomywow/orgs","repos_url":"https://api.github.com/users/Glomywow/repos","events_url":"https://api.github.com/users/Glomywow/events{/privacy}","received_events_url":"https://api.github.com/users/Glomywow/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"genzgd","id":625882,"node_id":"MDQ6VXNlcjYyNTg4Mg==","avatar_url":"https://avatars.githubusercontent.com/u/625882?v=4","gravatar_id":"","url":"https://api.github.com/users/genzgd","html_url":"https://github.com/genzgd","followers_url":"https://api.github.com/users/genzgd/followers","following_url":"https://api.github.com/users/genzgd/following{/other_user}","gists_url":"https://api.github.com/users/genzgd/gists{/gist_id}","starred_url":"https://api.github.com/users/genzgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/genzgd/subscriptions","organizations_url":"https://api.github.com/users/genzgd/orgs","repos_url":"https://api.github.com/users/genzgd/repos","events_url":"https://api.github.com/users/genzgd/events{/privacy}","received_events_url":"https://api.github.com/users/genzgd/received_events","type":"User","site_admin":false},"assignees":[{"login":"genzgd","id":625882,"node_id":"MDQ6VXNlcjYyNTg4Mg==","avatar_url":"https://avatars.githubusercontent.com/u/625882?v=4","gravatar_id":"","url":"https://api.github.com/users/genzgd","html_url":"https://github.com/genzgd","followers_url":"https://api.github.com/users/genzgd/followers","following_url":"https://api.github.com/users/genzgd/following{/other_user}","gists_url":"https://api.github.com/users/genzgd/gists{/gist_id}","starred_url":"https://api.github.com/users/genzgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/genzgd/subscriptions","organizations_url":"https://api.github.com/users/genzgd/orgs","repos_url":"https://api.github.com/users/genzgd/repos","events_url":"https://api.github.com/users/genzgd/events{/privacy}","received_events_url":"https://api.github.com/users/genzgd/received_events","type":"User","site_admin":false},{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-12T14:44:53Z","updated_at":"2022-01-24T22:19:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello! \r\nDo GRANT VIEW permissions work correctly?\r\nwhen a user tries to delete a VIEW with the following rights\r\nGRANT  SELECT, INSERT, CREATE TABLE, CREATE VIEW, DROP VIEW ON dictionaries.test to fasolwbanke_test;\r\nHe gets an error:\r\nDB::Exception: fasolwbanke_test: Not enough privileges. To execute this query it's necessary to have grant DROP TABLE, DROP VIEW ON dictionaries.test. (ACCESS_DENIED)\r\nIs it possible to delete a VIEW without granting rights to DROP TABLE? And is it planned to separate access rights by VIEW and TABLE","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30055/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30045","id":1023810588,"node_id":"I_kwDOA5dJV849Bhwc","number":30045,"title":"Datarace in Arrow","user":{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false},"assignees":[{"login":"Avogar","id":48961922,"node_id":"MDQ6VXNlcjQ4OTYxOTIy","avatar_url":"https://avatars.githubusercontent.com/u/48961922?v=4","gravatar_id":"","url":"https://api.github.com/users/Avogar","html_url":"https://github.com/Avogar","followers_url":"https://api.github.com/users/Avogar/followers","following_url":"https://api.github.com/users/Avogar/following{/other_user}","gists_url":"https://api.github.com/users/Avogar/gists{/gist_id}","starred_url":"https://api.github.com/users/Avogar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Avogar/subscriptions","organizations_url":"https://api.github.com/users/Avogar/orgs","repos_url":"https://api.github.com/users/Avogar/repos","events_url":"https://api.github.com/users/Avogar/events{/privacy}","received_events_url":"https://api.github.com/users/Avogar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-12T13:14:24Z","updated_at":"2021-10-12T13:16:22Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/29934/5bf64c62c271cef95f5cfda1796eef82311c6bdd/stress_test_(thread).html#fail1\r\n\r\n```\r\n==495==Launching Symbolizer process: /usr/bin/llvm-symbolizer-13 --inlines --default-arch=x86_64 \r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=495)\r\n  Read of size 4 at 0x7b0c0000ce64 by thread T809:\r\n    #0 arrow::internal::ThreadPool::ProtectAgainstFork() <null> (clickhouse+0x1b285650)\r\n    #1 arrow::internal::ThreadPool::SpawnReal(arrow::internal::TaskHints, arrow::internal::FnOnce<void ()>, arrow::StopToken, arrow::internal::FnOnce<void (arrow::Status const&)>&&) <null> (clickhouse+0x1b2861ca)\r\n    #2 arrow::io::RandomAccessFile::ReadAsync(arrow::io::IOContext const&, long, long) <null> (clickhouse+0x1b1bb7bc)\r\n    #3 arrow::io::RandomAccessFile::ReadAsync(long, long) <null> (clickhouse+0x1b1bbc0c)\r\n    #4 arrow::ipc::RecordBatchFileReaderImpl::ReadFooterAsync(arrow::internal::Executor*) <null> (clickhouse+0x1b15103f)\r\n    #5 arrow::ipc::RecordBatchFileReaderImpl::ReadFooter() <null> (clickhouse+0x1b150cc9)\r\n    #6 arrow::ipc::RecordBatchFileReaderImpl::Open(arrow::io::RandomAccessFile*, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b140910)\r\n    #7 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b130b3e)\r\n    #8 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b13097a)\r\n    #9 DB::ArrowBlockInputFormat::prepareReader() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:97:35 (clickhouse+0x16c690bb)\r\n    #10 DB::ArrowBlockInputFormat::generate() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:47:13 (clickhouse+0x16c6899e)\r\n    #11 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16bfabf4)\r\n    #12 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16bfa6b2)\r\n    #13 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20 (clickhouse+0x16c4b1a1)\r\n    #14 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13 (clickhouse+0x16c4b1a1)\r\n    #15 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c4b1a1)\r\n    #16 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c4b1a1)\r\n    #17 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c4b1a1)\r\n    #18 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c4b1a1)\r\n    #19 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16c49109)\r\n    #20 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16c49109)\r\n    #21 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17 (clickhouse+0x16c49109)\r\n    #22 DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:440:5 (clickhouse+0x16c47908)\r\n    #23 DB::PullingPipelineExecutor::pull(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingPipelineExecutor.cpp:47:20 (clickhouse+0x16c5938a)\r\n    #24 DB::StorageFileSource::generate() obj-x86_64-linux-gnu/../src/Storages/StorageFile.cpp:413:25 (clickhouse+0x164437e0)\r\n    #25 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16bfabf4)\r\n    #26 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16bfa6b2)\r\n    #27 DB::SourceWithProgress::work() obj-x86_64-linux-gnu/../src/Processors/Sources/SourceWithProgress.cpp:65:30 (clickhouse+0x16ea7109)\r\n    #28 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20 (clickhouse+0x16c4b1a1)\r\n    #29 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13 (clickhouse+0x16c4b1a1)\r\n    #30 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c4b1a1)\r\n    #31 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c4b1a1)\r\n    #32 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c4b1a1)\r\n    #33 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c4b1a1)\r\n    #34 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16c49109)\r\n    #35 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16c49109)\r\n    #36 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17 (clickhouse+0x16c49109)\r\n    #37 DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487:5 (clickhouse+0x16c470c3)\r\n    #38 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:826:9 (clickhouse+0x16c470c3)\r\n    #39 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:407:9 (clickhouse+0x16c46936)\r\n    #40 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16c57bd9)\r\n    #41 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16c57bd9)\r\n    #42 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16c57bd9)\r\n    #43 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16c57bd9)\r\n    #44 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16c57bd9)\r\n    #45 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16c57bd9)\r\n    #46 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c57bd9)\r\n    #47 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c57bd9)\r\n    #48 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c57bd9)\r\n    #49 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c57bd9)\r\n    #50 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9b6baed)\r\n    #51 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9b6baed)\r\n    #52 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9b6baed)\r\n    #53 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9b6f2b0)\r\n    #54 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9b6f2b0)\r\n    #55 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9b6f2b0)\r\n    #56 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9b6f2b0)\r\n\r\n  Previous write of size 4 at 0x7b0c0000ce64 by thread T652:\r\n    #0 arrow::internal::ThreadPool::ProtectAgainstFork() <null> (clickhouse+0x1b28576d)\r\n    #1 arrow::internal::ThreadPool::SpawnReal(arrow::internal::TaskHints, arrow::internal::FnOnce<void ()>, arrow::StopToken, arrow::internal::FnOnce<void (arrow::Status const&)>&&) <null> (clickhouse+0x1b2861ca)\r\n    #2 arrow::io::RandomAccessFile::ReadAsync(arrow::io::IOContext const&, long, long) <null> (clickhouse+0x1b1bb7bc)\r\n    #3 arrow::io::RandomAccessFile::ReadAsync(long, long) <null> (clickhouse+0x1b1bbc0c)\r\n    #4 arrow::ipc::RecordBatchFileReaderImpl::ReadFooterAsync(arrow::internal::Executor*) <null> (clickhouse+0x1b15103f)\r\n    #5 arrow::ipc::RecordBatchFileReaderImpl::ReadFooter() <null> (clickhouse+0x1b150cc9)\r\n    #6 arrow::ipc::RecordBatchFileReaderImpl::Open(arrow::io::RandomAccessFile*, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b140910)\r\n    #7 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b130b3e)\r\n    #8 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b13097a)\r\n    #9 DB::ArrowBlockInputFormat::prepareReader() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:97:35 (clickhouse+0x16c690bb)\r\n    #10 DB::ArrowBlockInputFormat::generate() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:47:13 (clickhouse+0x16c6899e)\r\n    #11 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16bfabf4)\r\n    #12 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16bfa6b2)\r\n    #13 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20 (clickhouse+0x16c4b1a1)\r\n    #14 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13 (clickhouse+0x16c4b1a1)\r\n    #15 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c4b1a1)\r\n    #16 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c4b1a1)\r\n    #17 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c4b1a1)\r\n    #18 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c4b1a1)\r\n    #19 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16c49109)\r\n    #20 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16c49109)\r\n    #21 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17 (clickhouse+0x16c49109)\r\n    #22 DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:440:5 (clickhouse+0x16c47908)\r\n    #23 DB::PullingPipelineExecutor::pull(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingPipelineExecutor.cpp:47:20 (clickhouse+0x16c5938a)\r\n    #24 DB::StorageFileSource::generate() obj-x86_64-linux-gnu/../src/Storages/StorageFile.cpp:413:25 (clickhouse+0x164437e0)\r\n    #25 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16bfabf4)\r\n    #26 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16bfa6b2)\r\n    #27 DB::SourceWithProgress::work() obj-x86_64-linux-gnu/../src/Processors/Sources/SourceWithProgress.cpp:65:30 (clickhouse+0x16ea7109)\r\n    #28 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20 (clickhouse+0x16c4b1a1)\r\n    #29 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13 (clickhouse+0x16c4b1a1)\r\n    #30 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c4b1a1)\r\n    #31 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c4b1a1)\r\n    #32 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c4b1a1)\r\n    #33 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c4b1a1)\r\n    #34 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16c49109)\r\n    #35 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16c49109)\r\n    #36 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17 (clickhouse+0x16c49109)\r\n    #37 DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487:5 (clickhouse+0x16c470c3)\r\n    #38 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:826:9 (clickhouse+0x16c470c3)\r\n    #39 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:407:9 (clickhouse+0x16c46936)\r\n    #40 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16c57bd9)\r\n    #41 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16c57bd9)\r\n    #42 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16c57bd9)\r\n    #43 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16c57bd9)\r\n    #44 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16c57bd9)\r\n    #45 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16c57bd9)\r\n    #46 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c57bd9)\r\n    #47 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c57bd9)\r\n    #48 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c57bd9)\r\n    #49 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c57bd9)\r\n    #50 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9b6baed)\r\n    #51 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9b6baed)\r\n    #52 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9b6baed)\r\n    #53 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9b6f2b0)\r\n    #54 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9b6f2b0)\r\n    #55 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9b6f2b0)\r\n    #56 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9b6f2b0)\r\n\r\n  Location is heap block of size 40 at 0x7b0c0000ce40 allocated by main thread:\r\n    #0 operator new(unsigned long) <null> (clickhouse+0x9af7728)\r\n    #1 arrow::internal::ThreadPool::Make(int) <null> (clickhouse+0x1b286588)\r\n    #2 arrow::internal::ThreadPool::MakeEternal(int) <null> (clickhouse+0x1b286807)\r\n    #3 arrow::io::internal::MakeIOThreadPool() interfaces.cc (clickhouse+0x1b1bc521)\r\n    #4 arrow::io::IOContext::IOContext(arrow::MemoryPool*, arrow::StopToken) <null> (clickhouse+0x1b1b9aa9)\r\n    #5 _GLOBAL__sub_I_interfaces.cc interfaces.cc (clickhouse+0x1b1c2034)\r\n    #6 __libc_csu_init <null> (clickhouse+0x1cc6104c)\r\n\r\n  Thread T809 'QueryPipelineEx' (tid=6147, running) created by thread T270 at:\r\n    #0 pthread_create <null> (clickhouse+0x9a6162d)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x9b6ed26)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'(), void>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x9b6ed26)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9b6a154)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:171:5 (clickhouse+0x9b6aa87)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:169:38 (clickhouse+0x16c4b5d0)\r\n    #6 void std::__1::allocator<ThreadFromGlobalPool>::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886:28 (clickhouse+0x16c4b5d0)\r\n    #7 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::__construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::integral_constant<bool, true>, std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:519:21 (clickhouse+0x16c4b5d0)\r\n    #8 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:481:14 (clickhouse+0x16c4b5d0)\r\n    #9 void std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::__construct_one_at_end<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:926:5 (clickhouse+0x16c46c0d)\r\n    #10 ThreadFromGlobalPool& std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::emplace_back<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1684:9 (clickhouse+0x16c46c0d)\r\n    #11 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:774:21 (clickhouse+0x16c46c0d)\r\n    #12 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:407:9 (clickhouse+0x16c46936)\r\n    #13 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16c57bd9)\r\n    #14 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16c57bd9)\r\n    #15 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16c57bd9)\r\n    #16 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16c57bd9)\r\n    #17 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16c57bd9)\r\n    #18 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16c57bd9)\r\n    #19 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16c57bd9)\r\n    #20 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16c57bd9)\r\n    #21 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16c57bd9)\r\n    #22 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16c57bd9)\r\n    #23 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9b6baed)\r\n    #24 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9b6baed)\r\n    #25 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9b6baed)\r\n    #26 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9b6f2b0)\r\n    #27 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9b6f2b0)\r\n    #28 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9b6f2b0)\r\n    #29 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9b6f2b0)\r\n\r\n  Thread T652 'QueryPipelineEx' (tid=3969, running) created by thread T459 at:\r\n    #0 pthread_create <null> (clickhouse+0x9a6162d)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x9b6ed26)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'(), void>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x9b6ed26)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9b6a154)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:171:5 (clickhouse+0x9b6aa87)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::BackgroundSchedulePool::BackgroundSchedulePool(unsigned long, unsigned long, char const*)::$_1>(DB::BackgroundSchedulePool::BackgroundSchedulePool(unsigned long, unsigned long, char const*)::$_1&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:169:38 (clickhouse+0x1560d6cb)\r\n    #6 DB::BackgroundSchedulePool::BackgroundSchedulePool(unsigned long, unsigned long, char const*) obj-x86_64-linux-gnu/../src/Core/BackgroundSchedulePool.cpp:161:18 (clickhouse+0x1560d6cb)\r\n    #7 void std::__1::__optional_storage_base<DB::BackgroundSchedulePool, false>::__construct<DB::SettingFieldNumber<unsigned long> const&, unsigned long const&, char const (&) [13]>(DB::SettingFieldNumber<unsigned long> const&, unsigned long const&, char const (&) [13]) obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:324:55 (clickhouse+0x155c4848)\r\n    #8 DB::BackgroundSchedulePool& std::__1::optional<DB::BackgroundSchedulePool>::emplace<DB::SettingFieldNumber<unsigned long> const&, unsigned long const&, char const (&) [13], void>(DB::SettingFieldNumber<unsigned long> const&, unsigned long const&, char const (&) [13]) obj-x86_64-linux-gnu/../contrib/libcxx/include/optional:830:15 (clickhouse+0x155c4848)\r\n    #9 DB::Context::getBufferFlushSchedulePool() const obj-x86_64-linux-gnu/../src/Interpreters/Context.cpp:1618:44 (clickhouse+0x155c4848)\r\n    #10 DB::StorageBuffer::StorageBuffer(DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context const>, unsigned long, DB::StorageBuffer::Thresholds const&, DB::StorageBuffer::Thresholds const&, DB::StorageBuffer::Thresholds const&, DB::StorageID const&, bool) obj-x86_64-linux-gnu/../src/Storages/StorageBuffer.cpp:126:29 (clickhouse+0x1666222c)\r\n    #11 std::__1::shared_ptr<DB::StorageBuffer> shared_ptr_helper<DB::StorageBuffer>::create<DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, unsigned long&, DB::StorageBuffer::Thresholds&, DB::StorageBuffer::Thresholds&, DB::StorageBuffer::Thresholds&, DB::StorageID&, bool>(DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>&&, unsigned long&, DB::StorageBuffer::Thresholds&, DB::StorageBuffer::Thresholds&, DB::StorageBuffer::Thresholds&, DB::StorageID&, bool&&) obj-x86_64-linux-gnu/../base/base/../base/shared_ptr_helper.h:17:39 (clickhouse+0x1666d4a2)\r\n    #12 DB::registerStorageBuffer(DB::StorageFactory&)::$_5::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../src/Storages/StorageBuffer.cpp:1146:16 (clickhouse+0x1666d4a2)\r\n    #13 decltype(std::__1::forward<DB::registerStorageBuffer(DB::StorageFactory&)::$_5&>(fp)(std::__1::forward<DB::StorageFactory::Arguments const&>(fp0))) std::__1::__invoke<DB::registerStorageBuffer(DB::StorageFactory&)::$_5&, DB::StorageFactory::Arguments const&>(DB::registerStorageBuffer(DB::StorageFactory&)::$_5&, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x1666d4a2)\r\n    #14 std::__1::shared_ptr<DB::IStorage> std::__1::__invoke_void_return_wrapper<std::__1::shared_ptr<DB::IStorage> >::__call<DB::registerStorageBuffer(DB::StorageFactory&)::$_5&, DB::StorageFactory::Arguments const&>(DB::registerStorageBuffer(DB::StorageFactory&)::$_5&, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:317:16 (clickhouse+0x1666d4a2)\r\n    #15 std::__1::__function::__default_alloc_func<DB::registerStorageBuffer(DB::StorageFactory&)::$_5, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x1666d4a2)\r\n    #16 std::__1::shared_ptr<DB::IStorage> std::__1::__function::__policy_invoker<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::registerStorageBuffer(DB::StorageFactory&)::$_5, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)> >(std::__1::__function::__policy_storage const*, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x1666d4a2)\r\n    #17 std::__1::__function::__policy_func<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16436255)\r\n    #18 std::__1::function<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16436255)\r\n    #19 DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, std::__1::shared_ptr<DB::Context>, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) const obj-x86_64-linux-gnu/../src/Storages/StorageFactory.cpp:215:16 (clickhouse+0x16436255)\r\n    #20 DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:1058:42 (clickhouse+0x1574a804)\r\n    #21 DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:952:20 (clickhouse+0x15746436)\r\n    #22 DB::InterpreterCreateQuery::execute() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:1302:16 (clickhouse+0x1574cb06)\r\n    #23 DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:635:32 (clickhouse+0x160e0a2f)\r\n    #24 DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:950:30 (clickhouse+0x160de801)\r\n    #25 DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:315:24 (clickhouse+0x16ba37df)\r\n    #26 DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1849:9 (clickhouse+0x16bb7d87)\r\n    #27 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x19fe7d42)\r\n    #28 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x19fe8552)\r\n    #29 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a16a275)\r\n    #30 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a16844f)\r\n    #31 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a166b27)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/bin/clickhouse+0x1b285650) in arrow::internal::ThreadPool::ProtectAgainstFork()\r\n==================\r\n```\r\n\r\nNoticed the unpleasant symbol in stacktrace `arrow::internal::ThreadPool`. External thread pools are a bit forbidden in our codebase.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30045/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30027","id":1023557277,"node_id":"I_kwDOA5dJV849Aj6d","number":30027,"title":"Got BAD_SIZE_OF_FILE_IN_DATA_PART Exception, why?","user":{"login":"BhavyaRajSharma","id":40774184,"node_id":"MDQ6VXNlcjQwNzc0MTg0","avatar_url":"https://avatars.githubusercontent.com/u/40774184?v=4","gravatar_id":"","url":"https://api.github.com/users/BhavyaRajSharma","html_url":"https://github.com/BhavyaRajSharma","followers_url":"https://api.github.com/users/BhavyaRajSharma/followers","following_url":"https://api.github.com/users/BhavyaRajSharma/following{/other_user}","gists_url":"https://api.github.com/users/BhavyaRajSharma/gists{/gist_id}","starred_url":"https://api.github.com/users/BhavyaRajSharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BhavyaRajSharma/subscriptions","organizations_url":"https://api.github.com/users/BhavyaRajSharma/orgs","repos_url":"https://api.github.com/users/BhavyaRajSharma/repos","events_url":"https://api.github.com/users/BhavyaRajSharma/events{/privacy}","received_events_url":"https://api.github.com/users/BhavyaRajSharma/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-10-12T09:03:42Z","updated_at":"2021-10-21T12:46:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We have a cluster of one shard in which we have two replicas. we noticed that one clickhouse replica is down and below are the exceptions in the clickhouse logs\r\n\r\n> 2021.10.03 22:45:00.547892 [ 18631 ] {} <Error> auto DB::MergeTreeData::loadDataParts(bool)::(anonymous class)::operator()() const: Code: 228. DB::Exception: /Disk/clickhouse/store/84a/84abff20-5247-4de9-84ab-ff2052474de9/20211001_8896_8896_0/ulihash.bin has unexpected size: 4194304 instead of 6622143. (BAD_SIZE_OF_FILE_IN_DATA_PART), Stack trace (when copying this message, always include the lines below):\r\n> \r\n> 0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9366e7a in /usr/bin/clickhouse\r\n> 1. DB::MergeTreeDataPartChecksum::checkSize(std::__1::shared_ptr<DB::IDisk> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x11506f0a in /usr/bin/clickhouse\r\n> 2. DB::MergeTreeDataPartChecksums::checkSizes(std::__1::shared_ptr<DB::IDisk> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x115075fd in /usr/bin/clickhouse\r\n> 3. DB::IMergeTreeDataPart::checkConsistencyBase() const @ 0x1142aeff in /usr/bin/clickhouse\r\n> 4. DB::MergeTreeDataPartWide::checkConsistency(bool) const @ 0x11518c3b in /usr/bin/clickhouse\r\n> 5. DB::IMergeTreeDataPart::loadColumnsChecksumsIndexes(bool, bool) @ 0x11419ccc in /usr/bin/clickhouse\r\n> 6. ? @ 0x114b827a in /usr/bin/clickhouse\r\n> 7. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x93aabb8 in /usr/bin/clickhouse\r\n> 8. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x93ac75f in /usr/bin/clickhouse\r\n> 9. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x93a7e9f in /usr/bin/clickhouse\r\n> 10. ? @ 0x93ab783 in /usr/bin/clickhouse\r\n> 11. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n> 12. clone @ 0xfe8dd in /usr/lib64/libc-2.17.so\r\n>  (version 21.9.2.17 (official build))\r\n> 2021.10.03 22:45:00.547893 [ 18606 ] {} <Error> auto DB::MergeTreeData::loadDataParts(bool)::(anonymous class)::operator()() const: Code: 228. DB::Exception: /Disk/clickhouse/store/84a/84abff20-5247-4de9-84ab-ff2052474de9/20211001_8904_8904_0/ulihash.bin has unexpected size: 4194304 instead of 6621815. (BAD_SIZE_OF_FILE_IN_DATA_PART), Stack trace (when copying this message, always include the lines below):\r\n> \r\n> 0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9366e7a in /usr/bin/clickhouse\r\n> 1. DB::MergeTreeDataPartChecksum::checkSize(std::__1::shared_ptr<DB::IDisk> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x11506f0a in /usr/bin/clickhouse\r\n> 2. DB::MergeTreeDataPartChecksums::checkSizes(std::__1::shared_ptr<DB::IDisk> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) const @ 0x115075fd in /usr/bin/clickhouse\r\n> 3. DB::IMergeTreeDataPart::checkConsistencyBase() const @ 0x1142aeff in /usr/bin/clickhouse\r\n> 4. DB::MergeTreeDataPartWide::checkConsistency(bool) const @ 0x11518c3b in /usr/bin/clickhouse\r\n> 5. DB::IMergeTreeDataPart::loadColumnsChecksumsIndexes(bool, bool) @ 0x11419ccc in /usr/bin/clickhouse\r\n> 6. ? @ 0x114b827a in /usr/bin/clickhouse\r\n> 7. ThreadPoolImpl<ThreadFromGlobalPool>::worker(std::__1::__list_iterator<ThreadFromGlobalPool, void*>) @ 0x93aabb8 in /usr/bin/clickhouse\r\n> 8. ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...)::'lambda'()::operator()() @ 0x93ac75f in /usr/bin/clickhouse\r\n> 9. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x93a7e9f in /usr/bin/clickhouse\r\n> 10. ? @ 0x93ab783 in /usr/bin/clickhouse\r\n> 11. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n> 12. clone @ 0xfe8dd in /usr/lib64/libc-2.17.so\r\n>  (version 21.9.2.17 (official build))\r\n> \r\n\r\nAfter that we also restarted clickhouse once to see if it resolves the error at the start time  and now we are getting below exception while executing start command and checking for status \r\n\r\n> ● clickhouse-server.service - ClickHouse Server (analytic DBMS for big data)\r\n>    Loaded: loaded (/etc/systemd/system/clickhouse-server.service; enabled; vendor preset: disabled)\r\n>    Active: activating (auto-restart) (Result: exit-code) since Mon 2021-10-04 04:29:25 PDT; 6s ago\r\n>   Process: 8177 ExecStart=/usr/bin/clickhouse-server --config=/etc/clickhouse-server/config.xml --pid-file=/run/clickhouse-server/clickhouse-server.pid (code=exited, status=70)\r\n>  Main PID: 8177 (code=exited, status=70)\r\n> \r\n> Oct 04 04:29:25 DEV-BDSP-Manager-01 systemd[1]: Unit clickhouse-server.service entered failed state.\r\n> Oct 04 04:29:25 DEV-BDSP-Manager-01 systemd[1]: clickhouse-server.service failed.\r\n\r\nBelow is the cluster config we are using \r\n```\r\n<shard>\r\n                        <internal_replication>true</internal_replication>\r\n                        <replica>\r\n                                <default_database>dbuli</default_database>\r\n                                <host>10.49.3.101</host>\r\n                                <port>9100</port>\r\n                        </replica>\r\n                         <replica>\r\n                                <default_database>dbuli</default_database>\r\n                                <host>10.49.3.102</host>\r\n                                <port>9100</port>\r\n                        </replica>\r\n                </shard>\r\n\r\n```\r\nusers.xml \r\n\r\n```\r\n    <profiles>\r\n        <!-- Default settings. -->\r\n        <default>\r\n<load_balancing>random</load_balancing>\r\n<distributed_product_mode>local</distributed_product_mode>\r\n<join_algorithm>prefer_partial_merge</join_algorithm>\r\n<max_memory_usage>30000000000</max_memory_usage>\r\n<max_partitions_per_insert_block>0</max_partitions_per_insert_block>\r\n<max_query_size>1073741824</max_query_size>\r\n<max_ast_elements>999999999</max_ast_elements>\r\n<max_expanded_ast_elements>999999999</max_expanded_ast_elements>\r\n<max_bytes_before_external_sort>21474836480</max_bytes_before_external_sort>\r\n<max_bytes_before_external_group_by>21474836480</max_bytes_before_external_group_by>\r\n<max_final_threads>6</max_final_threads>\r\n<max_threads>6</max_threads>\r\n<insert_quorum>2</insert_quorum>\r\n<optimize_read_in_order>1</optimize_read_in_order>\r\n<do_not_merge_across_partitions_select_final>1</do_not_merge_across_partitions_select_final>\r\n<enable_optimize_predicate_expression_to_final_subquery>1</enable_optimize_predicate_expression_to_final_subquery>\r\n<min_bytes_to_use_direct_io>1</min_bytes_to_use_direct_io>\r\n        </default>\r\n\r\n        <!-- Profile that allows only read queries. -->\r\n        <readonly>\r\n            <readonly>1</readonly>\r\n        </readonly>\r\n    </profiles>\r\n\r\n```\r\n\r\n\r\nCLickhouse version\r\n\r\n> ClickHouse client version 21.9.2.17 (official build).\r\n> Connecting to 10.49.3.102:9100 as user default.\r\n> Connected to ClickHouse server version 21.9.2 revision 54449.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30027/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30002","id":1022932764,"node_id":"I_kwDOA5dJV848-Lcc","number":30002,"title":"Datetime64 INSERT from FORMAT Parquet seems to truncate milliseconds","user":{"login":"schalekamp","id":1189754,"node_id":"MDQ6VXNlcjExODk3NTQ=","avatar_url":"https://avatars.githubusercontent.com/u/1189754?v=4","gravatar_id":"","url":"https://api.github.com/users/schalekamp","html_url":"https://github.com/schalekamp","followers_url":"https://api.github.com/users/schalekamp/followers","following_url":"https://api.github.com/users/schalekamp/following{/other_user}","gists_url":"https://api.github.com/users/schalekamp/gists{/gist_id}","starred_url":"https://api.github.com/users/schalekamp/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/schalekamp/subscriptions","organizations_url":"https://api.github.com/users/schalekamp/orgs","repos_url":"https://api.github.com/users/schalekamp/repos","events_url":"https://api.github.com/users/schalekamp/events{/privacy}","received_events_url":"https://api.github.com/users/schalekamp/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-11T17:01:15Z","updated_at":"2021-10-11T18:26:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In my table schema I have the following datetime field:\r\n`    ts_utc             DateTime64(3, 'UTC')`\r\n\r\nIn a pandas dataframe I have the datetime field as : \r\n`df = df.astype({'ts_utc': 'datetime64[ns]'})`\r\n\r\nSaving the dataframe as parquet:\r\n`df.to_parquet(\"out.paquet\")`\r\n\r\nImporting it into Clickhouse\r\n`    clickhouse-client -h <host> --port 9001 --query=\"INSERT INTO mytable FORMAT Parquet\" <out.parquet`\r\n\r\nNow when we query the table the field ts_utc only shows 1 second precision, the milliseconds are .000\r\nWhen we load the parquet file into a pandas Dataframe then the milliseconds are still there.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30002/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29996","id":1022558370,"node_id":"I_kwDOA5dJV8488wCi","number":29996,"title":"Восстановление данных","user":{"login":"root1336","id":71281018,"node_id":"MDQ6VXNlcjcxMjgxMDE4","avatar_url":"https://avatars.githubusercontent.com/u/71281018?v=4","gravatar_id":"","url":"https://api.github.com/users/root1336","html_url":"https://github.com/root1336","followers_url":"https://api.github.com/users/root1336/followers","following_url":"https://api.github.com/users/root1336/following{/other_user}","gists_url":"https://api.github.com/users/root1336/gists{/gist_id}","starred_url":"https://api.github.com/users/root1336/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/root1336/subscriptions","organizations_url":"https://api.github.com/users/root1336/orgs","repos_url":"https://api.github.com/users/root1336/repos","events_url":"https://api.github.com/users/root1336/events{/privacy}","received_events_url":"https://api.github.com/users/root1336/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":14,"created_at":"2021-10-11T10:58:00Z","updated_at":"2021-10-14T01:19:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Good afternoon, colleagues. Please help, the question is as follows.\r\n\r\nThere are two folders data and metadata, they are taken from the old clickhouse, which was used two years ago and now we need to restore these databases. I copied the data and metadata folders to /var/lib/clickhouse/, it didn't help.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29996/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29994","id":1022540513,"node_id":"I_kwDOA5dJV8488rrh","number":29994,"title":"The Buffer engine violates time thresholds in a strange way","user":{"login":"loyd","id":952180,"node_id":"MDQ6VXNlcjk1MjE4MA==","avatar_url":"https://avatars.githubusercontent.com/u/952180?v=4","gravatar_id":"","url":"https://api.github.com/users/loyd","html_url":"https://github.com/loyd","followers_url":"https://api.github.com/users/loyd/followers","following_url":"https://api.github.com/users/loyd/following{/other_user}","gists_url":"https://api.github.com/users/loyd/gists{/gist_id}","starred_url":"https://api.github.com/users/loyd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/loyd/subscriptions","organizations_url":"https://api.github.com/users/loyd/orgs","repos_url":"https://api.github.com/users/loyd/repos","events_url":"https://api.github.com/users/loyd/events{/privacy}","received_events_url":"https://api.github.com/users/loyd/received_events","type":"User","site_admin":false},"labels":[{"id":1507860028,"node_id":"MDU6TGFiZWwxNTA3ODYwMDI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-repro","name":"st-need-repro","color":"e5b890","default":false,"description":"We were not able to reproduce the problem, please help us."},{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-10-11T10:36:55Z","updated_at":"2021-12-07T09:26:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We are inserting into the `Buffer` table that should flush rows not later than after 15s according to the `max_time` setting:\r\n```sql\r\nCREATE TABLE indicator_log_buffer (\r\n    `name` String,\r\n    `time` DateTime64(9),\r\n    `trace_id` UInt64,\r\n    `value` Float64,\r\n    `publisher` LowCardinality(String)\r\n)\r\nENGINE = Buffer('default', 'indicator_log', 16, 5, 15, 10000, 1000000, 10000000, 100000000)\r\n```\r\n\r\nThe schema of `indicator_log`:\r\n```sql\r\nCREATE TABLE indicator_log (\r\n    `name` String,\r\n    `platform_time` DateTime64(9),\r\n    `trace_id` UInt64,\r\n    `value` Float64,\r\n    `publisher` LowCardinality(String),\r\n    INDEX trace_id_index trace_id TYPE minmax GRANULARITY 3\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY toYYYYMM(platform_time)\r\nORDER BY name, platform_time\r\n```\r\n\r\nHowever, for some unknown reasons, the flush time is constantly violated.\r\n\r\nThe following query shows the difference between the buffer table and the underlying one: \r\n```sql\r\nWITH\r\n    'xxxxx' AS indicator,\r\n    now64(9) - INTERVAL 90 MINUTE AS t0,\r\n    now64(9) - INTERVAL 15 SECOND AS t1\r\nSELECT ts, buf.min, raw.min\r\nFROM (\r\n    SELECT toStartOfInterval(platform_time, INTERVAL 10 SECOND) ts, min(value) min\r\n      FROM indicator_log_buffer\r\n     WHERE name = indicator\r\n       AND platform_time BETWEEN t0 AND t1\r\n     GROUP BY ts\r\n) buf\r\nFULL JOIN (\r\n    SELECT toStartOfInterval(platform_time, INTERVAL 10 SECOND) ts, min(value) min\r\n      FROM indicator_log\r\n     WHERE name = indicator\r\n       AND platform_time BETWEEN t0 AND t1\r\n     GROUP BY ts\r\n) raw\r\nUSING ts\r\nWHERE buf.min != raw.min\r\nORDER BY ts;\r\n\r\n```\r\n\r\nFor instance, I run this at `2021-10-11 10:26:30` and see the next results:\r\n```\r\n┌──────────────────ts─┬─────min─┬─raw.min─┐\r\n│ 2021-10-11 10:16:10 │ 3590.97 │ 3591.02 │\r\n│ 2021-10-11 10:16:20 │  3590.1 │ 3591.07 │\r\n│ 2021-10-11 10:22:30 │    3582 │ 3582.32 │\r\n│ 2021-10-11 10:22:40 │ 3582.07 │       0 │\r\n│ 2021-10-11 10:22:50 │ 3581.74 │       0 │\r\n│ 2021-10-11 10:23:00 │ 3583.21 │       0 │\r\n│ 2021-10-11 10:24:10 │ 3579.71 │       0 │\r\n│ 2021-10-11 10:24:30 │ 3581.44 │ 3581.59 │\r\n│ 2021-10-11 10:24:50 │ 3579.97 │ 3579.99 │\r\n│ 2021-10-11 10:25:00 │ 3579.32 │       0 │\r\n│ 2021-10-11 10:25:10 │ 3579.44 │       0 │\r\n│ 2021-10-11 10:25:20 │ 3580.12 │ 3580.21 │\r\n│ 2021-10-11 10:25:40 │ 3581.07 │       0 │\r\n│ 2021-10-11 10:25:50 │ 3581.26 │       0 │\r\n│ 2021-10-11 10:26:00 │ 3581.17 │       0 │\r\n│ 2021-10-11 10:26:10 │ 3580.53 │       0 │\r\n└─────────────────────┴─────────┴─────────┘\r\n```\r\n\r\nIt seems that some shards are flushed, but some of them are not.\r\n\r\nCH version: 21.6.6 revision 54448\r\n\r\n**Note that using only 1 internal buffer (instead of 16) solves the problem.**\r\n\r\nMaybe related: https://github.com/ClickHouse/ClickHouse/issues/25585\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29994/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29978","id":1022333191,"node_id":"I_kwDOA5dJV84875EH","number":29978,"title":"Consider GROUP_CONCAT / STRING_AGG function ","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1546506700,"node_id":"MDU6TGFiZWwxNTQ2NTA2NzAw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-functions","name":"comp-functions","color":"b5bcff","default":false,"description":"Relates to some SQL function"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-11T06:58:43Z","updated_at":"2021-10-11T08:02:42Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"It's about the same as \r\n\r\n```\r\narrayStringConcat( groupArray(column), ',')\r\n```\r\n\r\nPros: less typing, easier migration from other dbs. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29978/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29968","id":1022117279,"node_id":"I_kwDOA5dJV8487EWf","number":29968,"title":"ability to modify MergeTree settings for MatViews.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1350221495,"node_id":"MDU6TGFiZWwxMzUwMjIxNDk1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-matview","name":"comp-matview","color":"b5bcff","default":false,"description":"Materialized views"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-10T23:11:26Z","updated_at":"2021-10-10T23:11:53Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Now it requires to apply settings to inner table\r\n\r\n```sql\r\nALTER TABLE \".inner.reporting_device_raw_data\" MODIFY TTL ts + INTERVAL 12 MONTH;\r\n```\r\n\r\nAnd it's not easy in case of Atomic database\r\n\r\n```sql\r\nselect uuid from system.tables where name = 'reporting_device_raw_data';\r\nALTER TABLE \".inner_id.{uuid from prev. select}\" MODIFY TTL ts + INTERVAL 12 MONTH;\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29968/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29955","id":1022024890,"node_id":"I_kwDOA5dJV8486ty6","number":29955,"title":"Data Still Exists When Delete mutation is done","user":{"login":"variantf","id":2265407,"node_id":"MDQ6VXNlcjIyNjU0MDc=","avatar_url":"https://avatars.githubusercontent.com/u/2265407?v=4","gravatar_id":"","url":"https://api.github.com/users/variantf","html_url":"https://github.com/variantf","followers_url":"https://api.github.com/users/variantf/followers","following_url":"https://api.github.com/users/variantf/following{/other_user}","gists_url":"https://api.github.com/users/variantf/gists{/gist_id}","starred_url":"https://api.github.com/users/variantf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/variantf/subscriptions","organizations_url":"https://api.github.com/users/variantf/orgs","repos_url":"https://api.github.com/users/variantf/repos","events_url":"https://api.github.com/users/variantf/events{/privacy}","received_events_url":"https://api.github.com/users/variantf/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-10T16:10:33Z","updated_at":"2021-10-10T17:05:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nI have 6 clickhouse node to form a cluster. and there's a table which has 3 shard, and each shard is reside on 2 nodes. A ReplicatedMergeTree is built on each shard(with 2 nodes)\r\n\r\nthe table is very large, which consists nearly 100 billion rows and 10 columns. The mutations is need to delete about 20 billion rows.\r\n\r\nI run the mutations like: 'Alter table stock.order_book_local on cluster prod delete where `TradingDay` in ('a', 'b', 'c' ....)'\r\n\r\nAfter some days run, I found that on the 2 node of one shard, the system.mutations table show that the mutation is done with `is_done` = 1 and `parts_to_do` = 0. However, I can still query the count(*) where {mutations condition} is not zero.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n* 21.8.4 revision 54449\r\n* Non-default settings, if any\r\n* `CREATE TABLE` statements for all tables involved\r\nCREATE TABLE stock.order_book_local\r\n(\r\n    `TradingDay` Date,\r\n    `Channel` Int32,\r\n    `ID` Int64,\r\n    `ExchTimeOffsetUs` Int64,\r\n    `Symbol` LowCardinality(String),\r\n    `Volume` UInt64,\r\n    `Price` Float64,\r\n    `OrderKind` LowCardinality(String),\r\n    `FunctionCode` LowCardinality(String),\r\n    `TradeFlag` LowCardinality(String),\r\n    `BidOrderID` Int64,\r\n    `AskOrderID` Int64,\r\n    `Type` LowCardinality(String),\r\n    `StreamID` LowCardinality(String)\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/shard_{shard}/order_book_local', 'replica_{replica}')\r\nORDER BY (TradingDay, Symbol, intHash32(ID))\r\nSAMPLE BY intHash32(ID)\r\nSETTINGS index_granularity = 8192\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29955/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29950","id":1021905795,"node_id":"I_kwDOA5dJV8486QuD","number":29950,"title":"Support SELECT from JoinEngine with complex key","user":{"login":"Data-bytes305","id":83893570,"node_id":"MDQ6VXNlcjgzODkzNTcw","avatar_url":"https://avatars.githubusercontent.com/u/83893570?v=4","gravatar_id":"","url":"https://api.github.com/users/Data-bytes305","html_url":"https://github.com/Data-bytes305","followers_url":"https://api.github.com/users/Data-bytes305/followers","following_url":"https://api.github.com/users/Data-bytes305/following{/other_user}","gists_url":"https://api.github.com/users/Data-bytes305/gists{/gist_id}","starred_url":"https://api.github.com/users/Data-bytes305/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Data-bytes305/subscriptions","organizations_url":"https://api.github.com/users/Data-bytes305/orgs","repos_url":"https://api.github.com/users/Data-bytes305/repos","events_url":"https://api.github.com/users/Data-bytes305/events{/privacy}","received_events_url":"https://api.github.com/users/Data-bytes305/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-10-10T07:30:09Z","updated_at":"2021-10-22T08:14:25Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"version: 21.10.1.7939\r\n\r\nddl: \r\n\r\n```sql\r\ncreate table default.test_join_engine(buid UInt64, goodsid String, goodsname String ) ENGINE = Join(ANY, LEFT, buid, goodsid);\r\n```\r\n\r\n```sql\r\ninsert into default.test_join_engine(buid, goodsid, goodsname)  values(1, '32', '测试');\r\n```\r\n\r\nQuery\r\n\r\n```sql\r\nselect * from default.test_join_engine;\r\n```\r\nleads to\r\n\r\n```\r\nCode: 121. DB::Exception: Received from 168.30.16.99:9000. DB::Exception: Unsupported JOIN keys in StorageJoin. Type: 11: While executing Join. (UNSUPPORTED_JOIN_KEYS)\r\n```\r\n\r\nbut following is succeed\r\n\r\n```sql\r\nselect joinGet('default.test_join_engine', 'goodsname', toUInt64(1), '32');  \r\n```\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29950/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29922","id":1021724362,"node_id":"I_kwDOA5dJV8485kbK","number":29922,"title":"API for query rewrite (RFC)","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-09T14:37:11Z","updated_at":"2021-10-09T14:38:48Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nSome user interfaces may need to modify query without the risk of implementing their own ad-hoc parsers and corrupting the structure of SQL query.\r\n\r\nExamples:\r\n- query formatting;\r\n- adding and modifying LIMIT clause;\r\n- adding a condition to WHERE to filter the resultset;\r\n- changing ORDER BY elements.\r\n\r\n**Describe the solution you'd like**\r\n\r\nSeveral variants to discuss:\r\n\r\n1. ClickHouse HTTP API can have dedicated handler for this, the settings are passed by URL parameters.\r\n2. Integrate it to SQL dialect similar to EXPLAIN query or even integrate it as an options to the EXPLAIN query.\r\n\r\nVariant 1 is more simple and separated from SQL. For example, imagine what happens when we want to do `EXPLAIN ... EXPLAIN ... FORMAT ... FORMAT ...` query.\r\n\r\nVariant 2 is more versatile and can be used from all available ClickHouse interfaces.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nWe can cover some use cases by out of band modifications of the query via settings.\r\nWe already have #16176. We can also add `additional_filter` setting for adding extra conditions to PREWHERE. But it has many tricky implementation details.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29922/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29915","id":1021625606,"node_id":"I_kwDOA5dJV8485MUG","number":29915,"title":"clickhouse.com is blocked by Adblockers","user":{"login":"anergictcell","id":875703,"node_id":"MDQ6VXNlcjg3NTcwMw==","avatar_url":"https://avatars.githubusercontent.com/u/875703?v=4","gravatar_id":"","url":"https://api.github.com/users/anergictcell","html_url":"https://github.com/anergictcell","followers_url":"https://api.github.com/users/anergictcell/followers","following_url":"https://api.github.com/users/anergictcell/following{/other_user}","gists_url":"https://api.github.com/users/anergictcell/gists{/gist_id}","starred_url":"https://api.github.com/users/anergictcell/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anergictcell/subscriptions","organizations_url":"https://api.github.com/users/anergictcell/orgs","repos_url":"https://api.github.com/users/anergictcell/repos","events_url":"https://api.github.com/users/anergictcell/events{/privacy}","received_events_url":"https://api.github.com/users/anergictcell/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-09T06:57:33Z","updated_at":"2021-10-09T06:57:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the issue**\r\nThe clickhouse.com URL is blocked via the `gravity` list for DNS-based adblocking. This makes it impossible to visit the `clickhouse.com` domain when one is using e.g. PiHole for ad blockling\r\n\r\n**Additional context**\r\nThe URL was added to the blacklist in 2014, so this is most likely an artifact that only needs cleaning up. I already opened an issue with the adblocker list here: https://github.com/StevenBlack/hosts/issues/1781 but wanted to make you aware of the issue nevertheless.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29915/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29895","id":1021082750,"node_id":"I_kwDOA5dJV8483Hx-","number":29895,"title":"Setting max_result_bytes works incorrectly","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2021-10-08T13:11:46Z","updated_at":"2021-12-20T10:14:26Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe what's wrong**\r\n```\r\nrc1a-ity5agjmuhyu6nu9.mdb.yandexcloud.net :) select version()\r\n\r\nSELECT version()\r\n\r\nQuery id: 8ae63ad5-0d2a-4845-9e1b-1414a19322fb\r\n\r\n┌─version()─┐\r\n│ 21.7.11.3 │\r\n└───────────┘\r\n\r\nrc1a-ity5agjmuhyu6nu9.mdb.yandexcloud.net :) with timeout as (select commit_sha from checks where check_name='Functional stateless tests (memory)' and check_start_time between '2021-07-07' and '2021-10-07'  group by commit_sha having count() < 10) select count() from checks where commit_sha in timeout and check_name='Functional stateless tests (memory)'\r\n\r\nWITH timeout AS\r\n    (\r\n        SELECT commit_sha\r\n        FROM checks\r\n        WHERE (check_name = 'Functional stateless tests (memory)') AND ((check_start_time >= '2021-07-07') AND (check_start_time <= '2021-10-07'))\r\n        GROUP BY commit_sha\r\n        HAVING count() < 10\r\n    )\r\nSELECT count()\r\nFROM checks\r\nWHERE (commit_sha IN (timeout)) AND (check_name = 'Functional stateless tests (memory)')\r\n\r\n┌─count()─┐\r\n│     209 │            <-- There are 209 rows\r\n└─────────┘\r\n\r\nrc1a-ity5agjmuhyu6nu9.mdb.yandexcloud.net :) with timeout as (select commit_sha from checks where check_name='Functional stateless tests (memory)' and check_start_time between '2021-07-07' and '2021-10-07'  group by commit_sha having count() < 10) select test_name from checks where commit_sha in timeout and check_name='Functional stateless tests (memory)'\r\n\r\nWITH timeout AS\r\n    (\r\n        SELECT commit_sha\r\n        FROM checks\r\n        WHERE (check_name = 'Functional stateless tests (memory)') AND ((check_start_time >= '2021-07-07') AND (check_start_time <= '2021-10-07'))\r\n        GROUP BY commit_sha\r\n        HAVING count() < 10\r\n    )\r\nSELECT test_name     -- the only difference, filtering conditions are the same\r\nFROM checks\r\nWHERE (commit_sha IN (timeout)) AND (check_name = 'Functional stateless tests (memory)')\r\n...\r\n<data>\r\n...\r\n49 rows in set. Elapsed: 0.426 sec. Processed 1.83 million rows, 29.65 MB (4.29 million rows/s., 69.63 MB/s.)        <-- it returns only 49 rows (number can be different on different runs)\r\n\r\nrc1a-ity5agjmuhyu6nu9.mdb.yandexcloud.net :) with timeout as (select commit_sha from checks where check_name='Functional stateless tests (memory)' and check_start_time between '2021-07-07' and '2021-10-07'  group by commit_sha having count() < 10) select 1 from checks where commit_sha in timeout and check_name='Functional stateless tests (memory)'\r\n\r\nWITH timeout AS\r\n    (\r\n        SELECT commit_sha\r\n        FROM checks\r\n        WHERE (check_name = 'Functional stateless tests (memory)') AND ((check_start_time >= '2021-07-07') AND (check_start_time <= '2021-10-07'))\r\n        GROUP BY commit_sha\r\n        HAVING count() < 10\r\n    )\r\nSELECT 1      -- the only differens\r\nFROM checks\r\nWHERE (commit_sha IN (timeout)) AND (check_name = 'Functional stateless tests (memory)')\r\n...\r\n<data>\r\n...\r\n\r\n209 rows in set. Elapsed: 0.398 sec. Processed 5.64 million rows, 8.97 MB (14.17 million rows/s., 22.54 MB/s.)        <-- 209 rows as expected\r\n\r\nrc1a-ity5agjmuhyu6nu9.mdb.yandexcloud.net :) with timeout as (select commit_sha from checks where check_name='Functional stateless tests (memory)' and check_start_time between '2021-07-07' and '2021-10-07'  group by commit_sha having count() < 10) select test_name from checks where commit_sha in timeout and check_name='Functional stateless tests (memory)' order by test_name\r\n\r\nWITH timeout AS\r\n    (\r\n        SELECT commit_sha\r\n        FROM checks\r\n        WHERE (check_name = 'Functional stateless tests (memory)') AND ((check_start_time >= '2021-07-07') AND (check_start_time <= '2021-10-07'))\r\n        GROUP BY commit_sha\r\n        HAVING count() < 10\r\n    )\r\nSELECT test_name\r\nFROM checks\r\nWHERE (commit_sha IN (timeout)) AND (check_name = 'Functional stateless tests (memory)')\r\nORDER BY test_name ASC\r\n...\r\n<data>\r\n...\r\n\r\n209 rows in set. Elapsed: 0.455 sec. Processed 5.64 million rows, 94.27 MB (12.39 million rows/s., 207.27 MB/s.)        <-- 209 rows as expected\r\n```\r\n\r\nQueries with logs: https://gist.githubusercontent.com/tavplubix/e8deba630bdeaa3d55a8c4a1b433d540/raw/21decf09c22f93e0e6813b24c9eef6e0165f352a/gistfile1.txt\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29895/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29893","id":1021067098,"node_id":"I_kwDOA5dJV8483D9a","number":29893,"title":"base/glibc-compatibility/musl/pow.c:291:35: runtime error: division by zero","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-08T12:55:22Z","updated_at":"2021-10-08T12:55:22Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"There is comment that x2 should not be zero and clang could incorrectly trigger division by 0 without barrier, but we can see that in clang-13 build with undefined sanitizer it is still triggered\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/blob/master/base/glibc-compatibility/musl/pow.c#L285-L292\r\n\r\n```\r\n../base/glibc-compatibility/musl/pow.c:291:35: runtime error: division by zero\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior ../base/glibc-compatibility/musl/pow.c:291:35 in\r\n[Inferior 1 (process 28251) exited with code 01]\r\n(gdb) run\r\nStarting program: /home/qoega/undefined-ch/output/binary/clickhouse --version\r\n[Thread debugging using libthread_db enabled]\r\nUsing host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\r\n\r\nBreakpoint 1, pow (x=<optimized out>, y=-3) at ../base/glibc-compatibility/musl/pow.c:291\r\n291\tin ../base/glibc-compatibility/musl/pow.c\r\n(gdb) bt full\r\n#0  pow (x=<optimized out>, y=-3) at ../base/glibc-compatibility/musl/pow.c:291\r\n        x2 = 0\r\n        sign_bias = 0\r\n        ix = 0\r\n        iy = 13837309855095848960\r\n        topx = 0\r\n        topy = <optimized out>\r\n        hi = <optimized out>\r\n        ylo = <optimized out>\r\n        yhi = <optimized out>\r\n        llo = <optimized out>\r\n        lo = <optimized out>\r\n        lhi = <optimized out>\r\n        elo = <optimized out>\r\n        ehi = <optimized out>\r\n#1  0x00000000171efd63 in _GLOBAL__sub_I_fdr_compile.cpp ()\r\nNo symbol table info available.\r\n#2  0x000000001e41c57d in __libc_csu_init ()\r\nNo symbol table info available.\r\n#3  0x00007ffff73d8b88 in __libc_start_main (main=0xcb8c180 <main(int, char**)>, argc=2, argv=0x7fffffffe2f8, init=0x1e41c530 <__libc_csu_init>, fini=<optimized out>, rtld_fini=<optimized out>, stack_end=0x7fffffffe2e8) at ../csu/libc-start.c:266\r\n        result = <optimized out>\r\n        unwind_buf = {cancel_jmp_buf = {{jmp_buf = {0, 140737341392376, 2097152, 12884901901, 0, 5, 2, 140737488347896}, mask_was_saved = -7408}}, priv = {pad = {0x7ffff779e630, 0x2, 0x7ffff7de38d3 <_dl_init+259>, 0x7ffff7dd1d70}, data = {prev = 0x7ffff779e630, cleanup = 0x2, canceltype = -136431405}}}\r\n        not_first_call = <optimized out>\r\n#4  0x000000000cb67eae in _start ()\r\nNo symbol table info available.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29893/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29892","id":1021029307,"node_id":"I_kwDOA5dJV84826u7","number":29892,"title":"multiIf vs If performance difference with 1 condition","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-08T12:13:44Z","updated_at":"2022-01-27T08:22:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the situation**\r\nmultiIf works 10 times slower than If with single condition.\r\n\r\n**How to reproduce**\r\nClickHouse version 21.10\r\n\r\n```\r\n\r\nSELECT count()\r\nFROM numbers_mt(1000000000)\r\nWHERE NOT ignore(multiIf(number = 0, NULL, toNullable(number)))\r\n\r\n┌────count()─┐\r\n│ 1000000000 │\r\n└────────────┘\r\n\r\n1 rows in set. Elapsed: 10.267 sec. Processed 1.00 billion rows, 8.00 GB (97.41 million rows/s., 779.25 MB/s.)\r\n\r\nSELECT count()\r\nFROM numbers_mt(1000000000)\r\nWHERE NOT ignore(If(number = 0, NULL, toNullable(number)))\r\n\r\n┌────count()─┐\r\n│ 1000000000 │\r\n└────────────┘\r\n\r\n1 rows in set. Elapsed: 1.010 sec. Processed 1.00 billion rows, 8.00 GB (990.63 million rows/s., 7.93 GB/s.)\r\n```\r\n\r\n**Expected performance**\r\nThe same performance for multiIf and if.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29892/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29885","id":1020985968,"node_id":"I_kwDOA5dJV8482wJw","number":29885,"title":"AggregationCommon.h:97:35: runtime error: downcast of address which does not point to an object of type 'const DB::ColumnVectorHelper'","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-08T11:20:43Z","updated_at":"2021-10-08T11:20:43Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/29804/90cc63aecd37ffe7a3f6497b462be55540bc70a5/fuzzer_ubsan/report.html#fail1\r\n\r\n```\r\n2021.10.08 01:36:26.272326 [ 86 ] {c4bfe9b4-b173-4034-9564-ef4b6140e81f} <Debug> executeQuery: (from [::1]:49682) SELECT inf, number FROM numbers(2) INTERSECT SELECT inf, number FROM numbers(0)\r\n2021.10.08 01:36:26.272526 [ 86 ] {c4bfe9b4-b173-4034-9564-ef4b6140e81f} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2021.10.08 01:36:26.272860 [ 86 ] {c4bfe9b4-b173-4034-9564-ef4b6140e81f} <Trace> ContextAccess (default): Access granted: CREATE TEMPORARY TABLE ON *.*\r\n2021.10.08 01:36:26.273178 [ 86 ] {c4bfe9b4-b173-4034-9564-ef4b6140e81f} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2021.10.08 01:36:26.273268 [ 86 ] {c4bfe9b4-b173-4034-9564-ef4b6140e81f} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n../src/Interpreters/AggregationCommon.h:97:35: runtime error: downcast of address 0x7f5d980148c0 which does not point to an object of type 'const DB::ColumnVectorHelper'\r\n0x7f5d980148c0: note: object is of type 'DB::ColumnConst'\r\n 00 00 00 00  90 7c c5 08 00 00 00 00  01 00 00 00 5d 7f 00 00  20 86 3d 9b 5f 7f 00 00  02 00 00 00\r\n              ^~~~~~~~~~~~~~~~~~~~~~~\r\n              vptr for 'DB::ColumnConst'\r\n2021.10.08 01:36:26.792315 [ 151 ] {} <Trace> SystemLog (system.query_log): Flushing system log, 1606 entries to flush up to offset 397019\r\n2021.10.08 01:36:26.799251 [ 87 ] {} <Trace> KeeperTCPHandler: Received heartbeat for session #1\r\n2021.10.08 01:36:26.876509 [ 151 ] {} <Debug> DiskLocal: Reserving 6.44 MiB on disk `default`, having unreserved 2.27 TiB.\r\n2021.10.08 01:36:26.894697 [ 151 ] {} <Trace> system.query_log (5b1fa2ed-6727-41c7-9b1f-a2ed672731c7): Renaming temporary part tmp_insert_202110_404_404_0 to 202110_404_404_0.\r\n2021.10.08 01:36:26.895609 [ 151 ] {} <Trace> SystemLog (system.query_log): Flushed system log up to offset 397019\r\n2021.10.08 01:36:27.000262 [ 293 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 5.60 GiB, peak 9.15 GiB, will set to 5.62 GiB (RSS), difference: 12.93 MiB\r\n2021.10.08 01:36:27.638920 [ 152 ] {} <Trace> SystemLog (system.query_thread_log): Flushing system log, 2593 entries to flush up to offset 574493\r\n2021.10.08 01:36:27.661463 [ 152 ] {} <Debug> DiskLocal: Reserving 2.20 MiB on disk `default`, having unreserved 2.27 TiB.\r\n2021.10.08 01:36:27.668568 [ 152 ] {} <Trace> system.query_thread_log (f13e431a-e259-444c-b13e-431ae259544c): Renaming temporary part tmp_insert_202110_404_404_0 to 202110_404_404_0.\r\n2021.10.08 01:36:27.669107 [ 152 ] {} <Trace> SystemLog (system.query_thread_log): Flushed system log up to offset 574493\r\n2021.10.08 01:36:28.000137 [ 293 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 5.61 GiB, peak 9.15 GiB, will set to 5.62 GiB (RSS), difference: 8.43 MiB\r\n    #0 0x197a7052 in void DB::fillFixedBatch<unsigned long, wide::integer<128ul, unsigned int> >(unsigned long, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::PODArray<wide::integer<128ul, unsigned int>, 4096ul, Allocator<false, false>, 15ul, 16ul>&, unsigned long&) (/workspace/clickhouse+0x197a7052)\r\n    #1 0x197a69c9 in void DB::packFixedBatch<wide::integer<128ul, unsigned int> >(unsigned long, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::PODArray<wide::integer<128ul, unsigned int>, 4096ul, Allocator<false, false>, 15ul, 16ul>&) (/workspace/clickhouse+0x197a69c9)\r\n    #2 0x1ce53e60 in DB::ColumnsHashing::HashMethodKeysFixed<wide::integer<128ul, unsigned int>, wide::integer<128ul, unsigned int>, void, false, false, true, false>::HashMethodKeysFixed(std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, std::__1::shared_ptr<DB::ColumnsHashing::HashMethodContext> const&) obj-x86_64-linux-gnu/../src/Common/ColumnsHashing.h:524:13\r\n    #3 0x1e03fdfc in unsigned long DB::IntersectOrExceptTransform::buildFilter<DB::SetMethodKeysFixed<HashSetTable<wide::integer<128ul, unsigned int>, HashTableCell<wide::integer<128ul, unsigned int>, UInt128HashCRC32, HashTableNoState>, UInt128HashCRC32, HashTableGrower<8ul>, Allocator<true, true> >, false> >(DB::SetMethodKeysFixed<HashSetTable<wide::integer<128ul, unsigned int>, HashTableCell<wide::integer<128ul, unsigned int>, UInt128HashCRC32, HashTableNoState>, UInt128HashCRC32, HashTableGrower<8ul>, Allocator<true, true> >, false>&, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul>&, unsigned long, DB::SetVariantsTemplate<DB::NonClearableSet>&) const obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:108:28\r\n    #4 0x1e036e8f in DB::IntersectOrExceptTransform::filter(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:184:13\r\n    #5 0x1e0357c4 in DB::IntersectOrExceptTransform::work() obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:86:9\r\n    #6 0x1dc9be98 in DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20\r\n    #7 0x1dc9bd86 in DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13\r\n    #8 0x1dc9bd86 in decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #9 0x1dc9a327 in std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16\r\n    #10 0x1dc9a327 in std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12\r\n    #11 0x1dc9a327 in DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17\r\n    #12 0x1dc9c769 in DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487:5\r\n    #13 0x1dc9c769 in DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:790:21\r\n    #14 0x1dc9c769 in decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1\r\n    #15 0x1dc9c642 in decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1\r\n    #16 0x1dc9c642 in decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1\r\n    #17 0x1dc9c642 in ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13\r\n    #18 0x1dc9c642 in decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #19 0xd3a7f19 in std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16\r\n    #20 0xd3a7f19 in std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12\r\n    #21 0xd3a7f19 in ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17\r\n    #22 0xd3aa921 in void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73\r\n    #23 0xd3aa921 in decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #24 0xd3aa921 in void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5\r\n    #25 0xd3aa921 in void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5\r\n    #26 0x7f63f7f42608 in start_thread /build/glibc-eX1tMB/glibc-2.31/nptl/pthread_create.c:477:8\r\n    #27 0x7f63f7e69292 in __clone /build/glibc-eX1tMB/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior ../src/Interpreters/AggregationCommon.h:97:35 in \r\n2021.10.08 01:36:29.204626 [ 40 ] {} <Trace> BaseDaemon: Received signal -3\r\n2021.10.08 01:36:29.204885 [ 604 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.10.08 01:36:29.205014 [ 604 ] {} <Fatal> BaseDaemon: (version 21.11.1.8340, build id: 883C21F92B1A606829076A906F2EA35CE47DE3B2) (from thread 359) (query_id: c4bfe9b4-b173-4034-9564-ef4b6140e81f) Received signal Unknown signal (-3)\r\n2021.10.08 01:36:29.205061 [ 604 ] {} <Fatal> BaseDaemon: Sanitizer trap.\r\n2021.10.08 01:36:29.205135 [ 604 ] {} <Fatal> BaseDaemon: Stack trace: 0xd37926a 0x1be0a271 0xd33aba6 0xd34c8d9 0x197a7053 0x197a69ca 0x1ce53e61 0x1e03fdfd 0x1e036e90 0x1e0357c5 0x1dc9be99 0x1dc9bd87 0x1dc9a328 0x1dc9c76a 0x1dc9c643 0xd3a7f1a 0xd3aa922 0x7f63f7f42609 0x7f63f7e69293\r\n2021.10.08 01:36:29.216792 [ 604 ] {} <Fatal> BaseDaemon: 0.1. inlined from ./obj-x86_64-linux-gnu/../src/Common/StackTrace.cpp:303: StackTrace::tryCapture()\r\n2021.10.08 01:36:29.216831 [ 604 ] {} <Fatal> BaseDaemon: 0. ../src/Common/StackTrace.cpp:264: StackTrace::StackTrace() @ 0xd37926a in /workspace/clickhouse\r\n2021.10.08 01:36:29.241333 [ 604 ] {} <Fatal> BaseDaemon: 1. ./obj-x86_64-linux-gnu/../base/daemon/BaseDaemon.cpp:400: sanitizerDeathCallback() @ 0x1be0a271 in /workspace/clickhouse\r\n2021.10.08 01:36:30.000175 [ 293 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 5.62 GiB, peak 9.15 GiB, will set to 5.74 GiB (RSS), difference: 128.75 MiB\r\n2021.10.08 01:36:30.481869 [ 604 ] {} <Fatal> BaseDaemon: 2. __sanitizer::Die() @ 0xd33aba6 in /workspace/clickhouse\r\n2021.10.08 01:36:31.000131 [ 293 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 5.74 GiB, peak 9.15 GiB, will set to 5.93 GiB (RSS), difference: 189.75 MiB\r\n2021.10.08 01:36:31.130606 [ 156 ] {} <Debug> DiskLocal: Reserving 4.39 MiB on disk `default`, having unreserved 2.27 TiB.\r\n2021.10.08 01:36:31.138206 [ 156 ] {} <Trace> system.text_log (167da525-3faa-4b51-967d-a5253faa7b51): Renaming temporary part tmp_insert_202110_443_443_0 to 202110_443_443_0.\r\n2021.10.08 01:36:31.563977 [ 157 ] {} <Trace> SystemLog (system.metric_log): Flushing system log, 8 entries to flush up to offset 3336\r\n2021.10.08 01:36:31.586845 [ 157 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.27 TiB.\r\n2021.10.08 01:36:31.593184 [ 157 ] {} <Trace> system.metric_log (4783fde8-527e-420c-8783-fde8527e920c): Renaming temporary part tmp_insert_202110_443_443_0 to 202110_443_443_0.\r\n2021.10.08 01:36:31.594805 [ 157 ] {} <Trace> SystemLog (system.metric_log): Flushed system log up to offset 3336\r\n2021.10.08 01:36:31.721917 [ 604 ] {} <Fatal> BaseDaemon: 3. ? @ 0xd34c8d9 in /workspace/clickhouse\r\n2021.10.08 01:36:32.000135 [ 293 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 5.92 GiB, peak 9.15 GiB, will set to 5.93 GiB (RSS), difference: 11.18 MiB\r\n2021.10.08 01:36:32.928205 [ 158 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushing system log, 9548 entries to flush up to offset 4550416\r\n2021.10.08 01:36:32.935541 [ 158 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.27 TiB.\r\n2021.10.08 01:36:32.937504 [ 158 ] {} <Trace> system.asynchronous_metric_log (e874de4b-d37a-49dd-a874-de4bd37a39dd): Renaming temporary part tmp_insert_202110_476_476_0 to 202110_476_476_0.\r\n2021.10.08 01:36:32.937757 [ 158 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushed system log up to offset 4550416\r\n2021.10.08 01:36:32.954677 [ 604 ] {} <Fatal> BaseDaemon: 4. void DB::fillFixedBatch<unsigned long, wide::integer<128ul, unsigned int> >(unsigned long, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::PODArray<wide::integer<128ul, unsigned int>, 4096ul, Allocator<false, false>, 15ul, 16ul>&, unsigned long&) @ 0x197a7053 in /workspace/clickhouse\r\n2021.10.08 01:36:34.196806 [ 604 ] {} <Fatal> BaseDaemon: 5. void DB::packFixedBatch<wide::integer<128ul, unsigned int> >(unsigned long, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, DB::PODArray<wide::integer<128ul, unsigned int>, 4096ul, Allocator<false, false>, 15ul, 16ul>&) @ 0x197a69ca in /workspace/clickhouse\r\n2021.10.08 01:36:34.257107 [ 604 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/ColumnsHashing.h:524: DB::ColumnsHashing::HashMethodKeysFixed<wide::integer<128ul, unsigned int>, wide::integer<128ul, unsigned int>, void, false, false, true, false>::HashMethodKeysFixed(std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, std::__1::shared_ptr<DB::ColumnsHashing::HashMethodContext> const&) @ 0x1ce53e61 in /workspace/clickhouse\r\n2021.10.08 01:36:34.290624 [ 604 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:0: unsigned long DB::IntersectOrExceptTransform::buildFilter<DB::SetMethodKeysFixed<HashSetTable<wide::integer<128ul, unsigned int>, HashTableCell<wide::integer<128ul, unsigned int>, UInt128HashCRC32, HashTableNoState>, UInt128HashCRC32, HashTableGrower<8ul>, Allocator<true, true> >, false> >(DB::SetMethodKeysFixed<HashSetTable<wide::integer<128ul, unsigned int>, HashTableCell<wide::integer<128ul, unsigned int>, UInt128HashCRC32, HashTableNoState>, UInt128HashCRC32, HashTableGrower<8ul>, Allocator<true, true> >, false>&, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul>&, unsigned long, DB::SetVariantsTemplate<DB::NonClearableSet>&) const @ 0x1e03fdfd in /workspace/clickhouse\r\n2021.10.08 01:36:34.318954 [ 604 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:184: DB::IntersectOrExceptTransform::filter(DB::Chunk&) @ 0x1e036e90 in /workspace/clickhouse\r\n2021.10.08 01:36:34.347195 [ 604 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/IntersectOrExceptTransform.cpp:87: DB::IntersectOrExceptTransform::work() @ 0x1e0357c5 in /workspace/clickhouse\r\n2021.10.08 01:36:34.384868 [ 604 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:96: DB::executeJob(DB::IProcessor*) @ 0x1dc9be99 in /workspace/clickhouse\r\n2021.10.08 01:36:34.421259 [ 604 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:108: operator()\r\n2021.10.08 01:36:34.421298 [ 604 ] {} <Fatal> BaseDaemon: 11. ../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x1dc9bd87 in /workspace/clickhouse\r\n2021.10.08 01:36:34.453360 [ 604 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:606: DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1dc9a328 in /workspace/clickhouse\r\n2021.10.08 01:36:34.490540 [ 604 ] {} <Fatal> BaseDaemon: 13.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2851: std::__1::shared_ptr<DB::ThreadGroupStatus>::operator bool() const\r\n2021.10.08 01:36:34.490570 [ 604 ] {} <Fatal> BaseDaemon: 13.2. inlined from ../src/Processors/Executors/PipelineExecutor.cpp:783: operator()\r\n2021.10.08 01:36:34.490595 [ 604 ] {} <Fatal> BaseDaemon: 13.3. inlined from ../base/base/../base/scope_guard.h:94: basic_scope_guard<DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const::'lambda'()>::invoke()\r\n2021.10.08 01:36:34.490617 [ 604 ] {} <Fatal> BaseDaemon: 13.4. inlined from ../base/base/../base/scope_guard.h:44: ~basic_scope_guard\r\n2021.10.08 01:36:34.490639 [ 604 ] {} <Fatal> BaseDaemon: 13.5. inlined from ../src/Processors/Executors/PipelineExecutor.cpp:798: operator()\r\n2021.10.08 01:36:34.490660 [ 604 ] {} <Fatal> BaseDaemon: 13. ../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) @ 0x1dc9c76a in /workspace/clickhouse\r\n2021.10.08 01:36:34.527589 [ 604 ] {} <Fatal> BaseDaemon: 14.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:0: operator()\r\n2021.10.08 01:36:34.527646 [ 604 ] {} <Fatal> BaseDaemon: 14. ../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) @ 0x1dc9c643 in /workspace/clickhouse\r\n2021.10.08 01:36:34.542557 [ 604 ] {} <Fatal> BaseDaemon: 15.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2533: std::__1::function<void ()>::operator=(std::nullptr_t)\r\n2021.10.08 01:36:34.542596 [ 604 ] {} <Fatal> BaseDaemon: 15. ../src/Common/ThreadPool.cpp:277: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0xd3a7f1a in /workspace/clickhouse\r\n2021.10.08 01:36:34.559726 [ 604 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:0: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0xd3aa922 in /workspace/clickhouse\r\n2021.10.08 01:36:34.559793 [ 604 ] {} <Fatal> BaseDaemon: 17. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.10.08 01:36:34.559838 [ 604 ] {} <Fatal> BaseDaemon: 18. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.08 01:36:34.922434 [ 604 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: 1C26B2373B662DEE1203345C32456C8B. There is no information about the reference checksum.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29885/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29884","id":1020929671,"node_id":"I_kwDOA5dJV8482iaH","number":29884,"title":"Big mutation count stops updates?","user":{"login":"Grian","id":65714,"node_id":"MDQ6VXNlcjY1NzE0","avatar_url":"https://avatars.githubusercontent.com/u/65714?v=4","gravatar_id":"","url":"https://api.github.com/users/Grian","html_url":"https://github.com/Grian","followers_url":"https://api.github.com/users/Grian/followers","following_url":"https://api.github.com/users/Grian/following{/other_user}","gists_url":"https://api.github.com/users/Grian/gists{/gist_id}","starred_url":"https://api.github.com/users/Grian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Grian/subscriptions","organizations_url":"https://api.github.com/users/Grian/orgs","repos_url":"https://api.github.com/users/Grian/repos","events_url":"https://api.github.com/users/Grian/events{/privacy}","received_events_url":"https://api.github.com/users/Grian/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-08T10:12:19Z","updated_at":"2021-10-08T10:12:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have seen if table mutation count is greater than above 100 - 1000 (aprox.) they suddenly stop progressing.\r\n\r\nAnd this is very inconvenient.\r\n\r\nWhat is happens?\r\n\r\nIs there way to change this behavior?\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29884/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29883","id":1020877336,"node_id":"I_kwDOA5dJV8482VoY","number":29883,"title":"GraphiteMergeTree table cannot be populated on 21.*. Code: 49.","user":{"login":"zvonand","id":32552679,"node_id":"MDQ6VXNlcjMyNTUyNjc5","avatar_url":"https://avatars.githubusercontent.com/u/32552679?v=4","gravatar_id":"","url":"https://api.github.com/users/zvonand","html_url":"https://github.com/zvonand","followers_url":"https://api.github.com/users/zvonand/followers","following_url":"https://api.github.com/users/zvonand/following{/other_user}","gists_url":"https://api.github.com/users/zvonand/gists{/gist_id}","starred_url":"https://api.github.com/users/zvonand/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zvonand/subscriptions","organizations_url":"https://api.github.com/users/zvonand/orgs","repos_url":"https://api.github.com/users/zvonand/repos","events_url":"https://api.github.com/users/zvonand/events{/privacy}","received_events_url":"https://api.github.com/users/zvonand/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":1575092433,"node_id":"MDU6TGFiZWwxNTc1MDkyNDMz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-graphite","name":"comp-graphite","color":"b5bcff","default":false,"description":"GraphiteMergeTree engine"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":9,"created_at":"2021-10-08T09:15:14Z","updated_at":"2021-10-15T16:57:06Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When trying to populate a newly created GraphiteMergeTree table with some data, I catch an exception.\r\n\r\n```sql\r\nCREATE TABLE default.ttt\r\n(\r\n    `C1` Int8,\r\n    `Sign` Int8,\r\n    `Version` UInt8,\r\n    `Path` String,\r\n    `Time` DateTime,\r\n    `Value` Int8\r\n)\r\nENGINE = GraphiteMergeTree('graphite_rollup_params')\r\nORDER BY tuple()\r\n```\r\nTable is created and it shows up in `SHOW TABLES`. Then, I try to insert some data:\r\n\r\n```sql\r\ntietokone :) INSERT INTO ttt (*) VALUES (1, 1, 2, 'qwew', '2021-12-12 01:01:02', 2)\r\n\r\nINSERT INTO ttt (*) VALUES\r\n\r\nReceived exception from server (version 21.9.4):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Invalid number of rows in Chunk column Int8 position 5: expected 1, got 8. (LOGICAL_ERROR)\r\n```\r\n\r\n`graphite_rollup_params`:\r\n```xml\r\n<yandex>\r\n    <graphite_rollup_params>\r\n        <version_column_name>Version</version_column_name>\r\n        <pattern>\r\n            <regexp>click_cost</regexp>\r\n            <function>any</function>\r\n            <retention>\r\n                <age>0</age>\r\n                <precision>5</precision>\r\n            </retention>\r\n            <retention>\r\n                <age>86400</age>\r\n                <precision>60</precision>\r\n            </retention>\r\n        </pattern>\r\n        <default>\r\n            <function>max</function>\r\n            <retention>\r\n                <age>0</age>\r\n                <precision>60</precision>\r\n            </retention>\r\n            <retention>\r\n                <age>3600</age>\r\n                <precision>300</precision>\r\n            </retention>\r\n            <retention>\r\n                <age>86400</age>\r\n                <precision>3600</precision>\r\n            </retention>\r\n        </default>\r\n    </graphite_rollup_params>\r\n</yandex>\r\n```\r\n\r\nThis happens on `21.3`, `21.8`, `21.9`\r\nHowever, `20.3`, `20.8` work as expected","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29883/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29880","id":1020763050,"node_id":"I_kwDOA5dJV84815uq","number":29880,"title":"ClickHouse connection strings","user":{"login":"olli2017","id":29354173,"node_id":"MDQ6VXNlcjI5MzU0MTcz","avatar_url":"https://avatars.githubusercontent.com/u/29354173?v=4","gravatar_id":"","url":"https://api.github.com/users/olli2017","html_url":"https://github.com/olli2017","followers_url":"https://api.github.com/users/olli2017/followers","following_url":"https://api.github.com/users/olli2017/following{/other_user}","gists_url":"https://api.github.com/users/olli2017/gists{/gist_id}","starred_url":"https://api.github.com/users/olli2017/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/olli2017/subscriptions","organizations_url":"https://api.github.com/users/olli2017/orgs","repos_url":"https://api.github.com/users/olli2017/repos","events_url":"https://api.github.com/users/olli2017/events{/privacy}","received_events_url":"https://api.github.com/users/olli2017/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-08T06:58:38Z","updated_at":"2021-11-01T15:58:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nNow you can connect to ClickHouse by specifiying separately host, port, username, password and other parameters.\r\n_clickhouse-client --host=... --port=... --user=... --password=..._\r\n\r\nIt is already possible to specify the connection string as a URI in many databases. Example from PostgreSQL\r\n_postgresql://localhost:5433/my_database_\r\n\r\nIt is simple, convenient and frequently used. You can just copy the link and connect.\r\n\r\n**Describe the solution you'd like**\r\n\r\nI want to achieve next general form of the connection URL:\r\n\r\n```\r\nclickhouse://[userspec@][hostspec][/dbname][?paramspec]\r\nwhere userspec is:\r\nuser[:password]\r\nwhere hostspec is:\r\n[host][:port][,...]\r\nand paramspec is:\r\nname=value[&...]\r\n```\r\nEx: `clickhouse://login:password@host:port/database`\r\nThere are many params for connecting to ClickHouse. Therefore last [?paramspec] is necessary.\r\nCredentials will be verified after connection string accepted if it is not specified.\r\n\r\nI propose this solution because it is convenient and general way. It is similar on example from PostgreSQL(details below).\r\n\r\n**Describe alternatives you've considered**\r\n\r\nOther implementations are similar:\r\n1) **PostgreSql**\r\n_postgresql://[userspec@][hostspec][/dbname][?paramspec]_\r\n```\r\nwhere userspec is:\r\nuser[:password]\r\nand hostspec is:\r\n[host][:port][,...]\r\nand paramspec is:\r\nname=value[&...]\r\n```\r\nThe following examples illustrate valid URI syntax:\r\n```\r\npostgresql://\r\npostgresql://localhost\r\npostgresql://localhost:5433\r\npostgresql://localhost/mydb\r\npostgresql://user@localhost\r\npostgresql://user:secret@localhost\r\npostgresql://other@localhost/otherdb?connect_timeout=10&application_name=myapp\r\npostgresql://host1:123,host2:456/somedb?target_session_attrs=any&application_name=myapp\r\n```\r\nhttps://www.postgresql.org/docs/current/libpq-connect.html#LIBPQ-CONNSTRING\r\n\r\n2) **JDBC driver(SQL Server)**\r\n_jdbc:sqlserver://[serverName[\\instanceName][:portNumber]][;property=value[;property=value]]_\r\n```\r\nwhere:\r\n- jdbc:sqlserver:// (Required) is known as the subprotocol and is constant.\r\n- serverName (Optional) is the address of the server to connect to. This address can be a DNS or IP address, or it can be localhost or 127.0.0.1 for the local computer. If not specified in the connection URL, the server name must be specified in the properties collection.\r\n- instanceName (Optional) is the instance to connect to on serverName. If not specified, a connection to the default instance is made.\r\n- portNumber (Optional) is the port to connect to on serverName. The default is 1433. If you're using the default, you don't have to specify the port, nor its preceding ':', in the URL.\r\n- property (Optional) is one or more option connection properties. For more information, see Setting the connection properties. Any property from the list can be specified. Properties can only be delimited by using the semicolon (';'), and they can't be duplicated.\r\n```\r\nExamples:\r\n```\r\njdbc:sqlserver://localhost;user=MyUserName;password=*****;\r\njdbc:sqlserver://localhost:1433;databaseName=AdventureWorks;integratedSecurity=true;\r\n```\r\nhttps://docs.microsoft.com/en-us/sql/connect/jdbc/building-the-connection-url?view=sql-server-ver15\r\n\r\n3) **ODBC driver(SQL Server 2000)**\r\nDriver={SQL Server};Server=myServerAddress;Database=myDataBase;Uid=myUsername;Pwd=myPassword;\r\nhttps://www.connectionstrings.com/microsoft-sql-server-odbc-driver/\r\n\r\n4) **JDBC Oracle**\r\n\r\nConnect to Oracle Database SID. In some older versions of the Oracle database, the database is defined as a SID\r\n```\r\njdbc:oracle:thin:[<user>/<password>]@<host>[:<port>]:<SID>\r\nEx: jdbc:oracle:thin:@myoracle.db.server:1521:my_sid\r\n```\r\n\r\nConnect to Oracle Database Service Name\r\n```\r\njdbc:oracle:thin:[<user>/<password>]@//<host>[:<port>]/<service>\r\nEx: jdbc:oracle:thin:@//myoracle.db.server:1521/my_servicename\r\n```\r\nhttps://www.baeldung.com/java-jdbc-url-format\r\n\r\n5) **JDBC MySQL**\r\n_protocol//[hosts][/database][?properties]_\r\n```\r\njdbc:mysql://mysql.db.server:3306/my_database?useSSL=false&serverTimezone=UTC\r\nprotocol – jdbc:mysql:  // specific param for mysql. There are a lot of values.\r\nhost – mysql.db.server:3306\r\ndatabase – my_database\r\nproperties – useSSL=false&serverTimezone=UTC\r\n```\r\nhttps://www.baeldung.com/java-jdbc-url-format\r\n\r\nAnother examples for JDBC provided in screen:\r\n![image](https://user-images.githubusercontent.com/29354173/136739663-c77a10d2-390a-4c89-8fa2-a4d25cdd89a1.png)\r\nhttps://www.tutorialspoint.com/jdbc/jdbc-db-connections.htm\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29880/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29875","id":1020560542,"node_id":"I_kwDOA5dJV8481ISe","number":29875,"title":"SQL query parsing functions","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-08T00:13:34Z","updated_at":"2021-11-05T18:51:56Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nQuery log analysis (for clickhouse SQL dialect first)\r\n\r\nAnalysis of query logs for other SQL databases later?\r\n\r\n**Describe the solution you'd like**\r\n\r\nparseSQL function\r\n\r\n```\r\nparseSQL('SELECT 1', 'where')\r\nparseSQL('SELECT 1', 'group by')\r\nparseSQL('SELECT 1', 'columns')\r\nparseSQL('SELECT 1', 'tables')\r\n```\r\n\r\nOr it will return some complex structure (JSON?, map?)\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29875/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29853","id":1020290846,"node_id":"I_kwDOA5dJV8480Gce","number":29853,"title":"Logical error: Bad cast from type DB::ColumnVector<long> to DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > >","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-07T17:30:58Z","updated_at":"2021-11-19T12:46:32Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/4bc14dedfb5df5f66ddc10e867852f5ada461892/fuzzer_debug/report.html#fail1\r\n\r\n```\r\n2021.10.07 16:19:14.112264 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Debug> executeQuery: (from [::1]:45310) SELECT if(y = 9223372036854775807, -9223372036854775807, intDiv(intDiv(65537, CAST(NULL, 'Nullable(UInt32)')), a)) FROM nullable_division\r\n2021.10.07 16:19:14.116948 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Trace> ContextAccess (default): Access granted: SELECT(y, a) ON default.nullable_division\r\n2021.10.07 16:19:14.117428 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2021.10.07 16:19:14.122410 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Debug> default.nullable_division (723698c3-599c-4403-b236-98c3599c2403) (SelectExecutor): Key condition: unknown\r\n2021.10.07 16:19:14.122681 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Debug> default.nullable_division (723698c3-599c-4403-b236-98c3599c2403) (SelectExecutor): Selected 1/1 parts by partition key, 1 parts by primary key, 1/1 marks by primary key, 1 marks to read from 1 ranges\r\n2021.10.07 16:19:14.123064 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Debug> MergeTreeInOrderSelectProcessor: Reading 1 ranges in order from part all_1_1_0, approx. 3 rows starting from 0\r\n2021.10.07 16:19:14.127749 [ 337 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Trace> PipelineExecutor: Thread finished. Total time: 0.001412046 sec. Execution time: 0.001184382 sec. Processing time: 0.000212721 sec. Wait time: 1.4943e-05 sec.\r\n2021.10.07 16:19:14.127983 [ 140 ] {c132e1da-6d16-404a-a5d0-da5f6d845377} <Fatal> : Logical error: 'Bad cast from type DB::ColumnVector<long> to DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > >'.\r\n2021.10.07 16:19:14.128120 [ 187 ] {} <Trace> system.text_log (b17317ea-8f09-4dfd-b173-17ea8f09fdfd): Renaming temporary part tmp_insert_202110_173_173_0 to 202110_173_173_0.\r\n2021.10.07 16:19:14.128799 [ 42 ] {} <Trace> BaseDaemon: Received signal 6\r\n2021.10.07 16:19:14.129633 [ 355 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.10.07 16:19:14.129939 [ 355 ] {} <Fatal> BaseDaemon: (version 21.11.1.8334 (official build), build id: 2F18DAB2553A20991FE2347A7B974DC855F8D456) (from thread 140) (query_id: c132e1da-6d16-404a-a5d0-da5f6d845377) Received signal Aborted (6)\r\n2021.10.07 16:19:14.130198 [ 355 ] {} <Fatal> BaseDaemon: \r\n2021.10.07 16:19:14.130514 [ 355 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f1773b5318b 0x7f1773b32859 0x14e2d238 0x14e2d342 0x1d0cfd24 0x20eb5e45 0x20ea3dc2 0x20ec96eb 0x21a311a6 0x21a30f58 0x22907c30 0x229067d9 0x22900521 0x2290dcc5 0x26d50c39 0x26d51448 0x26e9f374 0x26e9be5a 0x26e9ac3c 0x7f1773d19609 0x7f1773c2f293\r\n2021.10.07 16:19:14.130898 [ 355 ] {} <Fatal> BaseDaemon: 4. gsignal @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.07 16:19:14.131052 [ 355 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.07 16:19:14.223800 [ 355 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:53: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x14e2d238 in /workspace/clickhouse\r\n2021.10.07 16:19:14.303362 [ 355 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:60: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x14e2d342 in /workspace/clickhouse\r\n2021.10.07 16:19:15.000292 [ 324 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 830.95 MiB, peak 1.15 GiB, will set to 880.95 MiB (RSS), difference: 50.00 MiB\r\n2021.10.07 16:19:15.019555 [ 355 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Common/typeid_cast.h:40: std::__1::enable_if<std::is_reference_v<DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > > const&>, DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > > const&>::type typeid_cast<DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > > const&, DB::IColumn const>(DB::IColumn const&) @ 0x1d0cfd24 in /workspace/clickhouse\r\n2021.10.07 16:19:15.127565 [ 355 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/DataTypes/Serializations/SerializationDecimalBase.cpp:31: DB::SerializationDecimalBase<DB::Decimal<wide::integer<128ul, int> > >::serializeBinaryBulk(DB::IColumn const&, DB::WriteBuffer&, unsigned long, unsigned long) const @ 0x20eb5e45 in /workspace/clickhouse\r\n2021.10.07 16:19:15.212212 [ 355 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/DataTypes/Serializations/ISerialization.cpp:84: DB::ISerialization::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::ISerialization::SerializeBinaryBulkSettings&, std::__1::shared_ptr<DB::ISerialization::SerializeBinaryBulkState>&) const @ 0x20ea3dc2 in /workspace/clickhouse\r\n2021.10.07 16:19:15.245060 [ 136 ] {} <Trace> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Found 6 old parts to remove.\r\n2021.10.07 16:19:15.245207 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_1_106_21\r\n2021.10.07 16:19:15.246099 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_107_107_0\r\n2021.10.07 16:19:15.246752 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_108_108_0\r\n2021.10.07 16:19:15.247412 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_109_109_0\r\n2021.10.07 16:19:15.248112 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_110_110_0\r\n2021.10.07 16:19:15.248775 [ 136 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Removing part from filesystem 202110_111_111_0\r\n2021.10.07 16:19:15.299729 [ 355 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/DataTypes/Serializations/SerializationNullable.cpp:81: DB::SerializationNullable::serializeBinaryBulkWithMultipleStreams(DB::IColumn const&, unsigned long, unsigned long, DB::ISerialization::SerializeBinaryBulkSettings&, std::__1::shared_ptr<DB::ISerialization::SerializeBinaryBulkState>&) const @ 0x20ec96eb in /workspace/clickhouse\r\n2021.10.07 16:19:15.410687 [ 355 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/DataStreams/NativeBlockOutputStream.cpp:60: DB::writeData(DB::IDataType const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, DB::WriteBuffer&, unsigned long, unsigned long) @ 0x21a311a6 in /workspace/clickhouse\r\n2021.10.07 16:19:15.514242 [ 355 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/DataStreams/NativeBlockOutputStream.cpp:126: DB::NativeBlockOutputStream::write(DB::Block const&) @ 0x21a30f58 in /workspace/clickhouse\r\n2021.10.07 16:19:15.840712 [ 355 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1536: DB::TCPHandler::sendData(DB::Block const&) @ 0x22907c30 in /workspace/clickhouse\r\n2021.10.07 16:19:15.914500 [ 186 ] {} <Trace> SystemLog (system.trace_log): Flushing system log, 54 entries to flush up to offset 4707\r\n2021.10.07 16:19:15.925243 [ 186 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:15.929331 [ 186 ] {} <Trace> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Renaming temporary part tmp_insert_202110_175_175_0 to 202110_175_175_0.\r\n2021.10.07 16:19:15.929907 [ 241 ] {} <Debug> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3) (MergerMutator): Selected 6 parts from 202110_1_170_34 to 202110_175_175_0\r\n2021.10.07 16:19:15.930095 [ 241 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:15.930582 [ 110 ] {} <Debug> MergeTask::PrepareStage: Merging 6 parts: from 202110_1_170_34 to 202110_175_175_0 into Compact\r\n2021.10.07 16:19:15.930752 [ 186 ] {} <Trace> SystemLog (system.trace_log): Flushed system log up to offset 4707\r\n2021.10.07 16:19:15.931159 [ 110 ] {} <Debug> MergeTask::PrepareStage: Selected MergeAlgorithm: Horizontal\r\n2021.10.07 16:19:15.931616 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_1_170_34, total 4519 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.933178 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_171_171_0, total 16 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.934629 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_172_172_0, total 70 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.936065 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_173_173_0, total 11 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.937473 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_174_174_0, total 37 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.938885 [ 110 ] {} <Debug> MergeTreeSequentialSource: Reading 2 marks from part 202110_175_175_0, total 54 rows starting from the beginning of the part\r\n2021.10.07 16:19:15.967201 [ 110 ] {} <Debug> MergeTask::MergeProjectionsStage: Merge sorted 4707 rows, containing 10 columns (10 merged, 0 gathered) in 0.036866307 sec., 127677.55663728401 rows/sec., 45.78 MiB/sec.\r\n2021.10.07 16:19:15.985866 [ 110 ] {} <Trace> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3): Renaming temporary part tmp_merge_202110_1_175_35 to 202110_1_175_35.\r\n2021.10.07 16:19:15.986285 [ 110 ] {} <Trace> system.trace_log (e49b8599-32df-4af3-a49b-859932dfeaf3) (MergerMutator): Merged 6 parts: from 202110_1_170_34 to 202110_175_175_0\r\n2021.10.07 16:19:15.986957 [ 110 ] {} <Debug> MemoryTracker: Peak memory usage Mutate/Merge: 4.23 MiB.\r\n2021.10.07 16:19:16.000240 [ 324 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 880.81 MiB, peak 1.15 GiB, will set to 928.14 MiB (RSS), difference: 47.32 MiB\r\n2021.10.07 16:19:16.165166 [ 355 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:675: DB::TCPHandler::processOrdinaryQueryWithProcessors() @ 0x229067d9 in /workspace/clickhouse\r\n2021.10.07 16:19:16.440359 [ 355 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:315: DB::TCPHandler::runImpl() @ 0x22900521 in /workspace/clickhouse\r\n2021.10.07 16:19:16.767922 [ 355 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1653: DB::TCPHandler::run() @ 0x2290dcc5 in /workspace/clickhouse\r\n2021.10.07 16:19:16.844097 [ 355 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43: Poco::Net::TCPServerConnection::start() @ 0x26d50c39 in /workspace/clickhouse\r\n2021.10.07 16:19:16.938798 [ 355 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115: Poco::Net::TCPServerDispatcher::run() @ 0x26d51448 in /workspace/clickhouse\r\n2021.10.07 16:19:16.971916 [ 188 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushing system log, 9149 entries to flush up to offset 1720142\r\n2021.10.07 16:19:17.000289 [ 324 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 928.14 MiB, peak 1.15 GiB, will set to 934.50 MiB (RSS), difference: 6.36 MiB\r\n2021.10.07 16:19:17.002267 [ 188 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:17.009980 [ 188 ] {} <Trace> system.asynchronous_metric_log (1b28115b-a3cb-462b-9b28-115ba3cbd62b): Renaming temporary part tmp_insert_202110_187_187_0 to 202110_187_187_0.\r\n2021.10.07 16:19:17.011308 [ 188 ] {} <Trace> SystemLog (system.asynchronous_metric_log): Flushed system log up to offset 1720142\r\n2021.10.07 16:19:17.021799 [ 181 ] {} <Trace> SystemLog (system.query_log): Flushing system log, 93 entries to flush up to offset 58107\r\n2021.10.07 16:19:17.040896 [ 355 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199: Poco::PooledThread::run() @ 0x26e9f374 in /workspace/clickhouse\r\n2021.10.07 16:19:17.123397 [ 181 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:17.142287 [ 355 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x26e9be5a in /workspace/clickhouse\r\n2021.10.07 16:19:17.147807 [ 181 ] {} <Trace> system.query_log (821b274d-7d15-4b8d-821b-274d7d159b8d): Renaming temporary part tmp_insert_202110_172_172_0 to 202110_172_172_0.\r\n2021.10.07 16:19:17.153773 [ 181 ] {} <Trace> SystemLog (system.query_log): Flushed system log up to offset 58107\r\n2021.10.07 16:19:17.240624 [ 355 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345: Poco::ThreadImpl::runnableEntry(void*) @ 0x26e9ac3c in /workspace/clickhouse\r\n2021.10.07 16:19:17.240937 [ 355 ] {} <Fatal> BaseDaemon: 23. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.10.07 16:19:17.241312 [ 355 ] {} <Fatal> BaseDaemon: 24. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.07 16:19:18.000278 [ 324 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 934.48 MiB, peak 1.15 GiB, will set to 937.63 MiB (RSS), difference: 3.15 MiB\r\n2021.10.07 16:19:18.174505 [ 189 ] {} <Trace> SystemLog (system.metric_log): Flushing system log, 8 entries to flush up to offset 1319\r\n2021.10.07 16:19:18.379454 [ 183 ] {} <Trace> SystemLog (system.query_thread_log): Flushing system log, 93 entries to flush up to offset 58097\r\n2021.10.07 16:19:18.414701 [ 183 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:18.425829 [ 183 ] {} <Trace> system.query_thread_log (871ad504-1213-40f0-871a-d504121300f0): Renaming temporary part tmp_insert_202110_174_174_0 to 202110_174_174_0.\r\n2021.10.07 16:19:18.429512 [ 183 ] {} <Trace> SystemLog (system.query_thread_log): Flushed system log up to offset 58097\r\n2021.10.07 16:19:18.443035 [ 189 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.24 TiB.\r\n2021.10.07 16:19:18.516220 [ 189 ] {} <Trace> system.metric_log (937eedd3-67b2-4238-937e-edd367b21238): Renaming temporary part tmp_insert_202110_167_167_0 to 202110_167_167_0.\r\n2021.10.07 16:19:18.550648 [ 189 ] {} <Trace> SystemLog (system.metric_log): Flushed system log up to offset 1319\r\n2021.10.07 16:19:18.643540 [ 355 ] {} <Fatal> BaseDaemon: Checksum of the binary: BE878A16B53A0B863843242008A42124, integrity check passed.\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29853/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29852","id":1020288070,"node_id":"I_kwDOA5dJV8480FxG","number":29852,"title":"MergeTreeSink.cpp:22:24: runtime error: member access within address which does not point to an object of type 'DB::StorageMergeTree'","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-07T17:27:39Z","updated_at":"2021-12-17T06:01:30Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/185d58a7cc4b0039459a0eb8c61fa67effb3a285/stress_test_(undefined).html#fail1\r\n\r\n```\r\n../src/Storages/MergeTree/MergeTreeSink.cpp:22:24: runtime error: member access within address 0x7f915450ffd0 which does not point to an object of type 'DB::StorageMergeTree'\r\n0x7f915450ffd0: note: object has invalid vptr\r\n 00 00 00 00  2f 63 6c 69 63 6b 68 6f  75 73 65 2f 74 61 62 6c  65 73 2f 30 31 33 39 36  5f 69 6e 61\r\n              ^~~~~~~~~~~~~~~~~~~~~~~\r\n              invalid vptr\r\n    #0 0x1da5ad64 in DB::MergeTreeSink::consume(DB::Chunk) obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeSink.cpp:22:32\r\n    #1 0x1dfb4ad3 in DB::SinkToStorage::transform(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Sinks/SinkToStorage.cpp:18:5\r\n    #2 0x1df499f2 in std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16\r\n    #3 0x1df499f2 in std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12\r\n    #4 0x1df499f2 in DB::runStep(std::__1::function<void ()>, DB::ThreadStatus*, std::__1::atomic<unsigned long>*) obj-x86_64-linux-gnu/../src/Processors/Transforms/ExceptionKeepingTransform.cpp:102:9\r\n    #5 0x1df48ffa in DB::ExceptionKeepingTransform::work() obj-x86_64-linux-gnu/../src/Processors/Transforms/ExceptionKeepingTransform.cpp:135:30\r\n    #6 0x1dc8c838 in DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20\r\n    #7 0x1dc8c726 in DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13\r\n    #8 0x1dc8c726 in decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #9 0x1dc8acc7 in std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16\r\n    #10 0x1dc8acc7 in std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12\r\n    #11 0x1dc8acc7 in DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17\r\n    #12 0x1dc8d109 in DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487:5\r\n    #13 0x1dc8d109 in DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:790:21\r\n    #14 0x1dc8d109 in decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1\r\n    #15 0x1dc8cfe2 in decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1\r\n    #16 0x1dc8cfe2 in decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1\r\n    #17 0x1dc8cfe2 in ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13\r\n    #18 0x1dc8cfe2 in decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #19 0xd399d99 in std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16\r\n    #20 0xd399d99 in std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12\r\n    #21 0xd399d99 in ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17\r\n    #22 0xd39c7a1 in void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73\r\n    #23 0xd39c7a1 in decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1\r\n    #24 0xd39c7a1 in void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5\r\n    #25 0xd39c7a1 in void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5\r\n    #26 0x7f9508be6608 in start_thread (/lib/x86_64-linux-gnu/libpthread.so.0+0x9608)\r\n    #27 0x7f9508b0d292 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x122292)\r\n\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior ../src/Storages/MergeTree/MergeTreeSink.cpp:22:24 in \r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29852/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29838","id":1019996591,"node_id":"I_kwDOA5dJV848y-mv","number":29838,"title":"Incorrect join with Merge engine","user":{"login":"orloffv","id":504986,"node_id":"MDQ6VXNlcjUwNDk4Ng==","avatar_url":"https://avatars.githubusercontent.com/u/504986?v=4","gravatar_id":"","url":"https://api.github.com/users/orloffv","html_url":"https://github.com/orloffv","followers_url":"https://api.github.com/users/orloffv/followers","following_url":"https://api.github.com/users/orloffv/following{/other_user}","gists_url":"https://api.github.com/users/orloffv/gists{/gist_id}","starred_url":"https://api.github.com/users/orloffv/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/orloffv/subscriptions","organizations_url":"https://api.github.com/users/orloffv/orgs","repos_url":"https://api.github.com/users/orloffv/repos","events_url":"https://api.github.com/users/orloffv/events{/privacy}","received_events_url":"https://api.github.com/users/orloffv/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-07T12:39:21Z","updated_at":"2021-10-07T13:31:04Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**How to reproduce:**\r\n\r\n```\r\n// first table with distributed\r\nCREATE TABLE first_table_lr on cluster '{cluster}'\r\n(\r\n    id String,\r\n    id2 String\r\n) ENGINE = ReplicatedMergeTree()\r\nORDER BY id;\r\n\r\n\r\nCREATE TABLE first_table on cluster '{cluster}'\r\n(\r\n    id String,\r\n    id2 String\r\n)\r\nENGINE = Distributed('{cluster}', 'default', 'first_table_lr');\r\n\r\n// second table with distributed\r\nCREATE TABLE second_table_lr on cluster '{cluster}'\r\n(\r\n    id String,\r\n    id2 String\r\n) ENGINE = MergeTree()\r\nORDER BY id;\r\n\r\nCREATE TABLE second_table on cluster '{cluster}'\r\n(\r\n    id String,\r\n    id2 String\r\n)\r\nENGINE = Distributed('{cluster}', 'default', 'second_table_lr');\r\n\r\nINSERT INTO first_table VALUES ('1', '2'), ('3', '4');\r\nINSERT INTO second_table VALUES ('1', '2'), ('3', '4');\r\n\r\n// Add merge engine table\r\nCREATE TABLE two_tables\r\n(\r\n    id String,\r\n    id2 String\r\n)\r\nENGINE = Merge('default', '^(first_table)$')\r\n```\r\n\r\nCorrect result\r\n`count() 2`\r\n\r\n```\r\nSELECT \r\n    count()\r\nFROM first_table as s\r\nGLOBAL ANY JOIN second_table as f USING (id)\r\nWHERE \r\n    f.id2 GLOBAL IN (\r\n        SELECT\r\n            id2\r\n        FROM second_table\r\n        GROUP BY id2\r\n    );  \r\n```\r\n\r\nBut if i use merge engine table(**two_tables**)\r\nResult is not correct\r\n`count() 0`\r\n\r\n```\r\nSELECT \r\n    count()\r\nFROM two_tables as s\r\nGLOBAL ANY JOIN second_table as f USING (id)\r\nWHERE \r\n    f.id2 GLOBAL IN (\r\n        SELECT\r\n            id2\r\n        FROM second_table\r\n        GROUP BY id2\r\n    ); \r\n```\r\n\r\nI reproduce it on 21.9.4.35, 21.8.7.22 and 21.3.17.2 versions\r\nOn version 19.13.1.11 everything works correctly","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29838/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29831","id":1019882625,"node_id":"I_kwDOA5dJV848yiyB","number":29831,"title":"Different decimal scale rules for functions If and multiIf","user":{"login":"Algunenano","id":664253,"node_id":"MDQ6VXNlcjY2NDI1Mw==","avatar_url":"https://avatars.githubusercontent.com/u/664253?v=4","gravatar_id":"","url":"https://api.github.com/users/Algunenano","html_url":"https://github.com/Algunenano","followers_url":"https://api.github.com/users/Algunenano/followers","following_url":"https://api.github.com/users/Algunenano/following{/other_user}","gists_url":"https://api.github.com/users/Algunenano/gists{/gist_id}","starred_url":"https://api.github.com/users/Algunenano/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Algunenano/subscriptions","organizations_url":"https://api.github.com/users/Algunenano/orgs","repos_url":"https://api.github.com/users/Algunenano/repos","events_url":"https://api.github.com/users/Algunenano/events{/privacy}","received_events_url":"https://api.github.com/users/Algunenano/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-07T10:31:48Z","updated_at":"2021-11-30T13:12:52Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\n\r\n`If` raises an exception if the 2 branches have different decimal scales:\r\n\r\n```\r\nSELECT If(number > 3, CAST('1', 'Decimal64(0)'), CAST('1', 'Decimal64(3)'))\r\nFROM system.numbers\r\nLIMIT 1\r\n\r\n Received exception from server (version 21.11.1):\r\nCode: 48. DB::Exception: Received from localhost:9000. DB::Exception: Conditional functions with different Decimal scales. (NOT_IMPLEMENTED)\r\n```\r\n\r\n`multiIf` does not:\r\n\r\n```\r\nSELECT multiIf(number > 3, CAST('1', 'Decimal64(0)'), CAST('1', 'Decimal64(3)'))\r\nFROM system.numbers\r\nLIMIT 1\r\n\r\nQuery id: e4319209-929b-4a10-b03c-b1f8d077d591\r\n\r\n┌─multiIf(greater(number, 3), CAST('1', 'Decimal64(0)'), CAST('1', 'Decimal64(3)'))─┐\r\n│                                                                                 1 │\r\n└───────────────────────────────────────────────────────────────────────────────────┘\r\n```\r\n\r\n\r\nThis means that multiIf might overflow due to the implicit cast:\r\n\r\n```\r\nSELECT multiIf(number > 3, CAST('999999999', 'Decimal32(0)'), CAST('1', 'Decimal32(3)'))\r\nFROM system.numbers\r\nLIMIT 1\r\n\r\nQuery id: 67991218-d99b-430e-aaf2-b7f28b25f7ea\r\n\r\n\r\n0 rows in set. Elapsed: 0.049 sec. \r\n\r\nReceived exception:\r\nCode: 407. DB::Exception: Decimal convert overflow. (DECIMAL_OVERFLOW)\r\n\r\n```\r\n\r\n\r\n**Expected behavior**\r\n\r\nFirst, it should be consistent across both functions; and secondly it should work as other numeric types: if there is a supertype that can hold both it should use that, and if not then throw an error. See the interaction with integers for example:\r\n\r\n```\r\nSELECT multiIf(number > 3, CAST('0', 'Int64'), CAST('0', 'UInt64'))\r\nFROM system.numbers\r\nLIMIT 1\r\n\r\nQuery id: 5c742432-af74-4270-a8f4-5739344a1f36\r\n\r\n\r\n0 rows in set. Elapsed: 0.026 sec. \r\n\r\nReceived exception from server (version 21.11.1):\r\nCode: 386. DB::Exception: Received from localhost:9000. DB::Exception: There is no supertype for types Int64, UInt64 because some of them are signed integers and some are unsigned integers, but there is no signed integer type, that can exactly represent all required unsigned integer values: While processing multiIf(number > 3, CAST('0', 'Int64'), CAST('0', 'UInt64')). (NO_COMMON_TYPE)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29831/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29822","id":1019484027,"node_id":"I_kwDOA5dJV848xBd7","number":29822,"title":"XML as input format","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-07T00:23:32Z","updated_at":"2022-01-28T13:33:05Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\n1. Stack Exchange archives: https://archive.org/details/stackexchange\r\n2. Wikipedia dumps.\r\n3. OpenStreetMap dumps.\r\n\r\n**Describe the solution you'd like**\r\n\r\nAdd support for `XML` as input format.\r\n\r\nThe user should specify one of the three flavours with a setting:\r\n\r\n1. `elements`:\r\n```\r\n<table>\r\n    <row>\r\n        <column1_name>value</column1_name>\r\n        <column2_name>value</column2_name>\r\n        ...\r\n    </row>\r\n    ...\r\n</table>\r\n```\r\n\r\n2. `attributes`\r\n```\r\n<table>\r\n    <row column1_name=\"value\" column2_name=\"value\" ... />\r\n    ...\r\n</table>\r\n```\r\n\r\n3. `cells`\r\n```\r\n<table>\r\n    <row>\r\n        <cell name=\"column1_name\">value</cell>\r\n        <cell name=\"column2_name\">value</cell>\r\n        ...\r\n    </row>\r\n    ...\r\n</table>\r\n```\r\n\r\nand the settings with the path to the table (like `/table`),\r\nthe name of row element,\r\nthe name of the cell element and the name of the attribute with column name (for `cells` variant).\r\n\r\nThe format should not use full-featured XML parser, but should support:\r\n- decoding of XML entities;\r\n- optional BOM at the beginning of the file, including UTF-8 BOM;\r\n- UTF16 and UTF-32 BE/LE encodings;\r\n- CDATA;\r\n- attributes in single and double quotes;\r\n- self-closing tags or separate closing tags;\r\n- skipping XML header and DOCTYPE;\r\n- processing of invalid (unescaped) charaters;\r\n\r\n**Additional context**\r\n\r\nLet these settings also control XML output format (that we already have).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29822/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29821","id":1019368725,"node_id":"I_kwDOA5dJV848wlUV","number":29821,"title":"error in compression paramters fails inserts.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-06T22:26:24Z","updated_at":"2021-10-06T22:26:45Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Let's add an excessive new line in `<min_part_size>`\r\n\r\n```\r\n    <compression>\r\n        <case>\r\n                <min_part_size>100000000\r\n                </min_part_size>\r\n            <method>zstd</method>\r\n            <level>1</level>\r\n        </case>\r\n    </compression>\r\n```\r\n\r\n/etc/init.d/clickhouse-server restart  \r\n\r\n```sql\r\ncreate table xx(A Int64) Engine=MergeTree order by tuple();\r\ninsert into xx select 1;\r\n\r\nReceived exception from server (version 21.9.1):\r\nCode: 1000. DB::Exception: Received from localhost:9000. DB::Exception: \r\nSyntax error: Not a valid unsigned integer: 100000000\r\n\t\t. (POCO_EXCEPTION)\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29821/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29809","id":1018415419,"node_id":"I_kwDOA5dJV848s8k7","number":29809,"title":"MergeTreeDataMergerMutator::selectAllPartsToMergeWithinPartition","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-06T14:13:14Z","updated_at":"2021-10-06T14:13:14Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"```\r\n../src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp:393:26: runtime error: 1.84467e+19 is outside the range of representable values of type 'unsigned long'\r\n    #0 0x1c257b20 in DB::MergeTreeDataMergerMutator::selectAllPartsToMergeWithinPartition(std::__1::shared_ptr<DB::FutureMergedMutatedPart>, unsigned long&, std::__1::function<bool (std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool) obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp:393:26\r\n    #1 0x1c473d10 in DB::StorageMergeTree::selectPartsToMerge(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, std::__1::unique_lock<std::__1::mutex>&, bool, DB::SelectPartsDecision*) obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:746:46\r\n    #2 0x1c475119 in DB::StorageMergeTree::merge(bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool) obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:820:30\r\n    #3 0x1c47a82a in DB::StorageMergeTree::optimize(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::IAST> const&, bool, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::Context const>) obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:1170:18\r\n    #4 0x1b4a030b in DB::InterpreterOptimizeQuery::execute() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterOptimizeQuery.cpp:72:12\r\n    #5 0x1ba0f310 in DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:635:32\r\n    #6 0x1ba0c929 in DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:950:30\r\n    #7 0x1c60e0b2 in DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:292:24\r\n    #8 0x1c6271b5 in DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1641:9\r\n    #9 0x1d3cf78b in Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3\r\n    #10 0x1d3cfc24 in Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20\r\n    #11 0x1d4c01a6 in Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14\r\n    #12 0x1d4bda6b in Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27\r\n    #13 0x7f987f95f608 in start_thread (/lib/x86_64-linux-gnu/libpthread.so.0+0x9608)\r\n    #14 0x7f987f886292 in __clone (/lib/x86_64-linux-gnu/libc.so.6+0x122292)\r\n\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior ../src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp:393:26\r\n```\r\n\r\n```\r\n2021.10.06 17:05:24.551032 [ 208 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.10.06 17:05:24.551206 [ 208 ] {} <Fatal> BaseDaemon: (version 21.11.1.1, build id: 08ED26ACFD1B086D5E26FB8818BB508EF7FB912D) (from thread 55) (query_id: c9da8656-e8e5-4a86-b679-63cbf20133ba) Received signal Unknown signal (-3)\r\n2021.10.06 17:05:24.551243 [ 208 ] {} <Fatal> BaseDaemon: Sanitizer trap.\r\n2021.10.06 17:05:24.551302 [ 208 ] {} <Fatal> BaseDaemon: Stack trace: 0xc824f6a 0x1a8206d1 0xc7e68a6 0xc7f54af 0x1c257b21 0x1c473d11 0x1c47511a 0x1c47a82b 0x1b4a030c 0x1ba0f311 0x1ba0c92a 0x1c60e0b3 0x1c6271b6 0x1d3cf78c 0x1d3cfc25 0x1d4c01a7 0x1d4bda6c 0x7f987f95f609 0x7f987f886293\r\n2021.10.06 17:05:24.607103 [ 208 ] {} <Fatal> BaseDaemon: 0.1. inlined from ./obj-x86_64-linux-gnu/../src/Common/StackTrace.cpp:303: StackTrace::tryCapture()\r\n2021.10.06 17:05:24.607154 [ 208 ] {} <Fatal> BaseDaemon: 0. ../src/Common/StackTrace.cpp:264: StackTrace::StackTrace() @ 0xc824f6a in /usr/bin/clickhouse\r\n2021.10.06 17:05:24.666044 [ 208 ] {} <Fatal> BaseDaemon: 1. ./obj-x86_64-linux-gnu/../base/daemon/BaseDaemon.cpp:400: sanitizerDeathCallback() @ 0x1a8206d1 in /usr/bin/clickhouse\r\n2021.10.06 17:05:26.143256 [ 208 ] {} <Fatal> BaseDaemon: 2. __sanitizer::Die() @ 0xc7e68a6 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.086677 [ 208 ] {} <Fatal> BaseDaemon: 3. ? @ 0xc7f54af in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.161882 [ 208 ] {} <Fatal> BaseDaemon: 4. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeDataMergerMutator.cpp:0: DB::MergeTreeDataMergerMutator::selectAllPartsToMergeWithinPartition(std::__1::shared_ptr<DB::FutureMergedMutatedPart>, unsigned long&, std::__1::function<bool (std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::shared_ptr<DB::IMergeTreeDataPart const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*)> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool) @ 0x1c257b21 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.243601 [ 208 ] {} <Fatal> BaseDaemon: 5. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:746: DB::StorageMergeTree::selectPartsToMerge(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>&, std::__1::unique_lock<std::__1::mutex>&, bool, DB::SelectPartsDecision*) @ 0x1c473d11 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.298018 [ 208 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:0: DB::StorageMergeTree::merge(bool, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >*, bool) @ 0x1c47511a in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.374069 [ 208 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:1170: DB::StorageMergeTree::optimize(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, std::__1::shared_ptr<DB::IAST> const&, bool, bool, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::Context const>) @ 0x1c47a82b in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.435810 [ 208 ] {} <Fatal> BaseDaemon: 8.1. inlined from ./obj-x86_64-linux-gnu/../src/DataStreams/BlockIO.h:14: BlockIO\r\n2021.10.06 17:05:27.435878 [ 208 ] {} <Fatal> BaseDaemon: 8. ../src/Interpreters/InterpreterOptimizeQuery.cpp:74: DB::InterpreterOptimizeQuery::execute() @ 0x1b4a030c in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.494310 [ 208 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:0: DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x1ba0f311 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.553665 [ 208 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:950: DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x1ba0c92a in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.599928 [ 208 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:292: DB::TCPHandler::runImpl() @ 0x1c60e0b3 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.646965 [ 208 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1643: DB::TCPHandler::run() @ 0x1c6271b6 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.650629 [ 208 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x1d3cf78c in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.656948 [ 208 ] {} <Fatal> BaseDaemon: 14.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1397: std::__1::default_delete<Poco::Net::TCPServerConnection>::operator()(Poco::Net::TCPServerConnection*) const\r\n2021.10.06 17:05:27.657003 [ 208 ] {} <Fatal> BaseDaemon: 14.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<Poco::Net::TCPServerConnection, std::__1::default_delete<Poco::Net::TCPServerConnection> >::reset(Poco::Net::TCPServerConnection*)\r\n2021.10.06 17:05:27.657042 [ 208 ] {} <Fatal> BaseDaemon: 14.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n2021.10.06 17:05:27.657086 [ 208 ] {} <Fatal> BaseDaemon: 14. ../contrib/poco/Net/src/TCPServerDispatcher.cpp:116: Poco::Net::TCPServerDispatcher::run() @ 0x1d3cfc25 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.664822 [ 208 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:213: Poco::PooledThread::run() @ 0x1d4c01a7 in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.673266 [ 208 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::assign(Poco::Runnable*)\r\n2021.10.06 17:05:27.673335 [ 208 ] {} <Fatal> BaseDaemon: 16.2. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:208: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::operator=(Poco::Runnable*)\r\n2021.10.06 17:05:27.673385 [ 208 ] {} <Fatal> BaseDaemon: 16. ../contrib/poco/Foundation/src/Thread_POSIX.cpp:360: Poco::ThreadImpl::runnableEntry(void*) @ 0x1d4bda6c in /usr/bin/clickhouse\r\n2021.10.06 17:05:27.673474 [ 208 ] {} <Fatal> BaseDaemon: 17. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.10.06 17:05:27.673539 [ 208 ] {} <Fatal> BaseDaemon: 18. __clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.06 17:05:27.942158 [ 208 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: 2C40522105E21EC1B84BBAB0443014C0. There is no information about the reference checksum.\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29809/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29808","id":1018410982,"node_id":"I_kwDOA5dJV848s7fm","number":29808,"title":"Drop table & Drop database keeps an old UUID reference in zookeeper","user":{"login":"unkn0wnnnnnn","id":92038043,"node_id":"U_kgDOBXxjmw","avatar_url":"https://avatars.githubusercontent.com/u/92038043?v=4","gravatar_id":"","url":"https://api.github.com/users/unkn0wnnnnnn","html_url":"https://github.com/unkn0wnnnnnn","followers_url":"https://api.github.com/users/unkn0wnnnnnn/followers","following_url":"https://api.github.com/users/unkn0wnnnnnn/following{/other_user}","gists_url":"https://api.github.com/users/unkn0wnnnnnn/gists{/gist_id}","starred_url":"https://api.github.com/users/unkn0wnnnnnn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/unkn0wnnnnnn/subscriptions","organizations_url":"https://api.github.com/users/unkn0wnnnnnn/orgs","repos_url":"https://api.github.com/users/unkn0wnnnnnn/repos","events_url":"https://api.github.com/users/unkn0wnnnnnn/events{/privacy}","received_events_url":"https://api.github.com/users/unkn0wnnnnnn/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":11,"created_at":"2021-10-06T14:10:58Z","updated_at":"2021-10-06T18:40:34Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nCREATE TABLE IF NOT EXISTS db.tablename ON CLUSTER\r\nENGINE = ReplicatedMergeTree\r\n\r\nCREATE TABLE IF NOT EXISTS db.tablename_2 ON CLUSTER\r\nENGINE = Distributed(clustername, dbname, tablename, shardingkey)\r\n\r\nDROP TABLE db.tablename ON CLUSTER clustername SYNC\r\nDROP TABLE db.tablename_2 ON CLUSTER clustername SYNC\r\nDROP DATABASE db ON CLUSTER clustername SYNC\r\n\r\nThe table drops, but zookeeper keeps a stale reference to the old tables UUID,\r\n\r\nSELECT * FROM system.zookeeper where path = '/clickhouse/tables/'\r\n\r\nalways shows the previous UUID that the macro used to create the zookeeper reference to the previous table\r\n\r\nThis was an issue when I was manually specifying parameters on the replicated tables constructor, saying that the replica already exists even though it did not because I deleted the tables and database\r\n\r\n**How to reproduce**\r\nCREATE TABLE IF NOT EXISTS db.tablename ON CLUSTER\r\nENGINE = ReplicatedMergeTree\r\n\r\nCREATE TABLE IF NOT EXISTS db.tablename_2 ON CLUSTER\r\nENGINE = Distributed(clustername, dbname, tablename, shardingkey)\r\n\r\nDROP TABLE db.tablename ON CLUSTER clustername SYNC\r\nDROP TABLE db.tablename_2 ON CLUSTER clustername SYNC\r\nDROP DATABASE db ON CLUSTER clustername SYNC\r\n\r\n\r\n**Expected behavior**\r\nDROP TABLE db.tablename ON CLUSTER clustername SYNC\r\nDROP TABLE db.tablename_2 ON CLUSTER clustername SYNC\r\nDROP DATABASE db ON CLUSTER clustername SYNC\r\n\r\nshould remove the reference in the zookeeper but it does not.\r\n\r\nSELECT * FROM system.zookeeper where path = '/clickhouse/tables/'\r\nalways shows the previous tables UUID\r\n\r\nI've attempted to manually remove the reference but the database throws an error saying zookeeper does not support mutations\r\n\r\n**Error message and/or stacktrace**\r\nN/A\r\n\r\n**Additional context**\r\nI'm not sure if this effects performance or operation, but it does keep a stale reference in system.zookeeper to non existent databases\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29808/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29789","id":1016986130,"node_id":"I_kwDOA5dJV848nfoS","number":29789,"title":"DDL/alter queries should respond with valid JSON ","user":{"login":"genzgd","id":625882,"node_id":"MDQ6VXNlcjYyNTg4Mg==","avatar_url":"https://avatars.githubusercontent.com/u/625882?v=4","gravatar_id":"","url":"https://api.github.com/users/genzgd","html_url":"https://github.com/genzgd","followers_url":"https://api.github.com/users/genzgd/followers","following_url":"https://api.github.com/users/genzgd/following{/other_user}","gists_url":"https://api.github.com/users/genzgd/gists{/gist_id}","starred_url":"https://api.github.com/users/genzgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/genzgd/subscriptions","organizations_url":"https://api.github.com/users/genzgd/orgs","repos_url":"https://api.github.com/users/genzgd/repos","events_url":"https://api.github.com/users/genzgd/events{/privacy}","received_events_url":"https://api.github.com/users/genzgd/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401254412,"node_id":"MDU6TGFiZWwxNDAxMjU0NDEy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dddl","name":"comp-dddl","color":"b5bcff","default":false,"description":"Distributed DDL feature"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-05T22:41:55Z","updated_at":"2021-10-05T23:21:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We have a simple UI for various ClickHouse maintenance tasks that allows submitting simple queries and expects a standard JSON query response (meta, data, rows, statistics).  We would like to extend this and other tools to allow submitting basic DDL and alter table queries via HTTP.  Adding \"FORMAT JSON\" to such queries works, but the response is still in plain text, unless the query is \"ON CLUSTER\" and no errors are returned.\r\n\r\nIt would useful if such queries with \"FORMAT JSON\" always returned valid JSON.  A single JSON property of \"result\", or possibly array of \"errors\" or \"messages\" could be included in the response, as well the array results from the ON CLUSTER operation.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29789/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/29778","id":1016363581,"node_id":"I_kwDOA5dJV848lHo9","number":29778,"title":"Missing merged parts are checked regularly, missing blocks","user":{"login":"jzombi","id":1073189,"node_id":"MDQ6VXNlcjEwNzMxODk=","avatar_url":"https://avatars.githubusercontent.com/u/1073189?v=4","gravatar_id":"","url":"https://api.github.com/users/jzombi","html_url":"https://github.com/jzombi","followers_url":"https://api.github.com/users/jzombi/followers","following_url":"https://api.github.com/users/jzombi/following{/other_user}","gists_url":"https://api.github.com/users/jzombi/gists{/gist_id}","starred_url":"https://api.github.com/users/jzombi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jzombi/subscriptions","organizations_url":"https://api.github.com/users/jzombi/orgs","repos_url":"https://api.github.com/users/jzombi/repos","events_url":"https://api.github.com/users/jzombi/events{/privacy}","received_events_url":"https://api.github.com/users/jzombi/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-05T13:56:20Z","updated_at":"2022-01-18T16:24:11Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"We have a ReplicatedMergeTree table with 2 replicas. The error log `clickhouse-server.err.log` is full of warnings like:\r\n\r\n```\r\nChecking part [...]\r\nChecking if anyone has a part [...] or covering part.\r\nFound parts with the same min block and with the same max block as the missing part [...]. Hoping that it will eventually appear as a result of a merge.\r\n```\r\n\r\nWe could identify the problematic parts, from the log files and based on this comment: https://github.com/ClickHouse/ClickHouse/issues/10368#issuecomment-616310015, but in our case the type field is `'GET_PART'`, not `'MERGE_PARTS'`.\r\n\r\nWe've checked whether min_block_number, max_block_number ranges of existing parts with the same partition_id covers the range of the missing \"merged\" part's min_block_number, max_block_number. It turned out that all missing \"merged\" part but one are covered by existing parts. The missing part that is not fully covered has the same block numbers missing in both replicas.\r\n\r\nThe missing \"merged\" part: `b762d3709bf376c5f06287ec8e9d2665_0_822_4`\r\nParts found in the file system / system.parts table:\r\n```\r\nb762d3709bf376c5f06287ec8e9d2665_0_198_3\r\nb762d3709bf376c5f06287ec8e9d2665_199_233_2\r\n[...]\r\nb762d3709bf376c5f06287ec8e9d2665_679_708_2\r\nb762d3709bf376c5f06287ec8e9d2665_742_775_2\r\n[...]\r\n```\r\nMissing block numbers: 709-741\r\n\r\nMy questions regarding the situation:\r\n1. Is it ok to delete the queue nodes in ZooKeeper to avoid getting the warnings, even when `type='GET_PART'`?\r\n2. Are the block numbers expected to be continuously assigned? Does the missing block numbers indicate data loss?\r\n3. Can the missing block numbers cause any trouble in the future, eg. in later merges, etc.? (Apart from the possibly already lost data,) I couldn't find any other warnings/errors in the log files for the affected partition_id, it seems as if ClickHouse checks only the minimum and maximum block numbers.\r\n\r\nThanks!","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778/reactions","total_count":4,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":4,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/29778/timeline","performed_via_github_app":null}]

[{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31533","id":1058463384,"node_id":"I_kwDOA5dJV84_Ft6Y","number":31533,"title":"select Array type column showing error `Cannot read all array values`","user":{"login":"gfunc","id":20113306,"node_id":"MDQ6VXNlcjIwMTEzMzA2","avatar_url":"https://avatars.githubusercontent.com/u/20113306?v=4","gravatar_id":"","url":"https://api.github.com/users/gfunc","html_url":"https://github.com/gfunc","followers_url":"https://api.github.com/users/gfunc/followers","following_url":"https://api.github.com/users/gfunc/following{/other_user}","gists_url":"https://api.github.com/users/gfunc/gists{/gist_id}","starred_url":"https://api.github.com/users/gfunc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gfunc/subscriptions","organizations_url":"https://api.github.com/users/gfunc/orgs","repos_url":"https://api.github.com/users/gfunc/repos","events_url":"https://api.github.com/users/gfunc/events{/privacy}","received_events_url":"https://api.github.com/users/gfunc/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-11-19T12:12:16Z","updated_at":"2022-01-21T11:08:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hey guys.\r\n\r\nafter upgrading from 21.10.x to 21.11 version, I started to experience below error:\r\n ```\r\n  Code: 33.\r\n  DB::Exception: Cannot read all array values: read just 48525 of 49000: (while reading column data): (while reading from part /var/lib/clickhouse/store/8e0/8e0d4e9a-e29f-497d-8e0d-4e9ae29fe97d/5cef2ab042e0ee48578729a50baa7390_105_3295_719/ from mark 5 with max_rows_to_read = 1225): While executing MergeTreeThread. Stack trace:\r\n  \r\n  0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9b605d4 in /usr/bin/clickhouse\r\n  1. DB::SerializationArray::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::ISerialization::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::ISerialization::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const @ 0x11cd9fe2 in /usr/bin/clickhouse\r\n  2. DB::SerializationArray::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::ISerialization::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::ISerialization::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const @ 0x11cd9a00 in /usr/bin/clickhouse\r\n  3. DB::MergeTreeReaderWide::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, bool, unsigned long, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&, bool) @ 0x12e84a02 in /usr/bin/clickhouse\r\n  4. DB::MergeTreeReaderWide::readRows(unsigned long, unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&) @ 0x12e837d9 in /usr/bin/clickhouse\r\n  5. DB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&) @ 0x133b29ce in /usr/bin/clickhouse\r\n  6. DB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&) @ 0x133b6a79 in /usr/bin/clickhouse\r\n  7. DB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&) @ 0x133b5ad3 in /usr/bin/clickhouse\r\n  8. DB::MergeTreeBaseSelectProcessor::readFromPartImpl() @ 0x133ae1ba in /usr/bin/clickhouse\r\n  9. DB::MergeTreeBaseSelectProcessor::readFromPart() @ 0x133af40d in /usr/bin/clickhouse\r\n  10. DB::MergeTreeBaseSelectProcessor::generate() @ 0x133adaab in /usr/bin/clickhouse\r\n  11. DB::ISource::tryGenerate() @ 0x13114d95 in /usr/bin/clickhouse\r\n  12. DB::ISource::work() @ 0x1311495a in /usr/bin/clickhouse\r\n  13. DB::SourceWithProgress::work() @ 0x13320742 in /usr/bin/clickhouse\r\n  14. ? @ 0x131301fb in /usr/bin/clickhouse\r\n  15. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x1312c1b1 in /usr/bin/clickhouse\r\n  16. ? @ 0x13132185 in /usr/bin/clickhouse\r\n  17. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x9ba2697 in /usr/bin/clickhouse\r\n  18. ? @ 0x9ba609d in /usr/bin/clickhouse\r\n  19. start_thread @ 0x7ea5 in /usr/lib64/libpthread-2.17.so\r\n  20. clone @ 0xfe9fd in /usr/lib64/libc-2.17.so\r\n```\r\n\r\nsome background:\r\n1.  a cluster of 3,  all upgraded to version `21.11.4.14` \r\n2. data integrated using Kafka engine and materialized view. \r\n3. Kafka setting as follows:\r\n```SQL\r\nCREATE TABLE default.kafka_stream on cluster bi\r\n(\r\n    \"data\"      Array(Map(String,Nullable(String))),\r\n    \"database\"  String,\r\n    \"es\"        Int64,\r\n    \"id\"        Int64,\r\n    \"isDdl\"     boolean,\r\n    \"mysqlType\" Map(String, String),\r\n    \"pkNames\"   Array(String),\r\n    \"sql\"       String,\r\n    \"sqlType\"   Map(String, Int64),\r\n    \"table\"     String,\r\n    \"ts\"        Int64,\r\n    \"type\"      String\r\n) ENGINE = Kafka()\r\nSETTINGS\r\n    kafka_broker_list = 'my_brokers',\r\n    kafka_topic_list = 'my_topic',\r\n    kafka_group_name = 'bi_group',\r\n    kafka_flush_interval_ms = 3500,\r\n    kafka_num_consumers = 1,\r\n    kafka_format = 'JSONEachRow',\r\n    kafka_row_delimiter = '\\n';\r\n```\r\n4. a local table and a distributed table created for storage and query, the distributed table is using `halfMD5(ts)` as sharding key\r\n5. a materialized view created \"to distributed table\"\r\n\r\nfirst, I thought it was a problem with the setting since there was a mention of `max_rows_to_read`, but setting `max_rows_to_read=0` did not solve the problem.\r\n\r\nThen I start to notice that this problem was related to the where clause since the select query would work if I shrink the time range (which is the time data processed by Kafka, in this case `_timestamp`) I was querying. And further, since the error message mentioned \"array\", I tried to select columns  of type `array` only, and found that the problem is with column `data`\r\n\r\nSo it is probably a problem with the data itself. \r\n\r\nAnd my question is:\r\n1. if the data was corrupted or was not of array type, how did the materialized view successfully insert the data into the merge tree table?\r\n2. is there a way to bypass this error and only select the correct records?\r\n\r\nthanks in advance\r\n\r\n ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31533/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31531","id":1058372075,"node_id":"I_kwDOA5dJV84_FXnr","number":31531,"title":"Assertion in jemalloc: `nmalloc >= ndalloc`","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1507860028,"node_id":"MDU6TGFiZWwxNTA3ODYwMDI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-repro","name":"st-need-repro","color":"e5b890","default":false,"description":"We were not able to reproduce the problem, please help us."},{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2021-11-19T10:23:57Z","updated_at":"2022-01-10T09:31:16Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://s3.amazonaws.com/clickhouse-test-reports/31391/a13ed01c05b90edd9408fe33b80c6075fcc571cd/stress_test__debug__actions_.html\r\n\r\nhttps://github.com/ClickHouse-Extras/jemalloc/blob/bc0998a9052957584b6944b6f43fffe0648f603e/src/arena.c#L165\r\n\r\n```\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.881299 [ 182492 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.916694 [ 182492 ] {} <Fatal> BaseDaemon: (version 21.12.1.0, build id: 27076C421FC81871AC47D934B5E9C8EE9EA342E4) (from thread 780) (no query) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.919918 [ 182492 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.921775 [ 182492 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fce02b4718b 0x7fce02b26859 0x2698a37c 0x269c6916 0x269c60c7 0x269c534b 0x269c838d 0x269c42da 0x2697bfe8 0x2145a237 0x2145916b 0x21459fda 0x2145bd98 0x2145bd5d 0x2145bd01 0x2145bc12 0x2145baf6 0x2145b9bd 0x2145b97d 0x2145b955 0x2145b920 0x150e8d46 0x150e7e55 0x15113eaf 0x1511b064 0x1511afdd 0x1511af05 0x1511a842 0x7fce02d29609 0x7fce02c23293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.976837 [ 182492 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:30.979971 [ 182492 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:31.248659 [ 182492 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/arena.c:165: arena_stats_merge @ 0x2698a37c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:31.554431 [ 182492 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/ctl.c:806: ctl_arena_stats_amerge @ 0x269c6916 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:31.857445 [ 182492 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/ctl.c:1000: ctl_arena_refresh @ 0x269c60c7 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:32.120126 [ 182492 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/ctl.c:1068: ctl_refresh @ 0x269c534b in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:32.309890 [ 182492 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/ctl.c:1626: epoch_ctl @ 0x269c838d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:32.504159 [ 182492 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/ctl.c:1313: ctl_byname @ 0x269c42da in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:32.886260 [ 182492 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/jemalloc/src/jemalloc.c:3645: mallctl @ 0x2697bfe8 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:34.336133 [ 182492 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/AsynchronousMetrics.cpp:342: DB::updateJemallocEpoch() @ 0x2145a237 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:35.384737 [ 182492 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/AsynchronousMetrics.cpp:1407: DB::AsynchronousMetrics::update(std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000l> > >) @ 0x2145916b in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:36.075106 [ 182492 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Interpreters/AsynchronousMetrics.cpp:311: DB::AsynchronousMetrics::run() @ 0x21459fda in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:36.637913 [ 182492 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Interpreters/AsynchronousMetrics.cpp:238: DB::AsynchronousMetrics::start()::$_0::operator()() const @ 0x2145bd98 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:37.593059 [ 182492 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::AsynchronousMetrics::start()::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::AsynchronousMetrics::start()::$_0&>(DB::AsynchronousMetrics::start()::$_0&) @ 0x2145bd5d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:38.414669 [ 182492 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<DB::AsynchronousMetrics::start()::$_0&, std::__1::tuple<>&>(DB::AsynchronousMetrics::start()::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) @ 0x2145bd01 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:39.452650 [ 182492 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<DB::AsynchronousMetrics::start()::$_0&, std::__1::tuple<>&>(DB::AsynchronousMetrics::start()::$_0&, std::__1::tuple<>&) @ 0x2145bc12 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:40.826306 [ 182492 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188: ThreadFromGlobalPool::ThreadFromGlobalPool<DB::AsynchronousMetrics::start()::$_0>(DB::AsynchronousMetrics::start()::$_0&&)::'lambda'()::operator()() @ 0x2145baf6 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:41.825878 [ 182492 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::AsynchronousMetrics::start()::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::AsynchronousMetrics::start()::$_0>(DB::AsynchronousMetrics::start()::$_0&&)::'lambda'()&>(DB::AsynchronousMetrics::start()::$_0&&) @ 0x2145b9bd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:42.702096 [ 182492 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::AsynchronousMetrics::start()::$_0>(DB::AsynchronousMetrics::start()::$_0&&)::'lambda'()&>(DB::AsynchronousMetrics::start()::$_0&&...) @ 0x2145b97d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:43.330054 [ 182492 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::AsynchronousMetrics::start()::$_0>(DB::AsynchronousMetrics::start()::$_0&&)::'lambda'(), void ()>::operator()() @ 0x2145b955 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:44.207351 [ 182492 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::AsynchronousMetrics::start()::$_0>(DB::AsynchronousMetrics::start()::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x2145b920 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:44.541945 [ 182492 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x150e8d46 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:44.722699 [ 182492 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x150e7e55 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:45.070753 [ 182492 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x15113eaf in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:45.274788 [ 182492 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139: void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const @ 0x1511b064 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:45.500236 [ 182492 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()&&...) @ 0x1511afdd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:45.948870 [ 182492 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:281: void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) @ 0x1511af05 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:46.284778 [ 182492 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291: void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) @ 0x1511a842 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:46.287244 [ 182492 ] {} <Fatal> BaseDaemon: 32. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:46.291034 [ 182492 ] {} <Fatal> BaseDaemon: 33. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:48.654266 [ 182492 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: A43D023B9A11845F84E6BFB6086E070C. There is no information about the reference checksum.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.11.18 22:29:51.225744 [ 496 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31531/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31530","id":1058370091,"node_id":"I_kwDOA5dJV84_FXIr","number":31530,"title":"Perf test: Arrays passed to arrayMap must have equal size","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-19T10:21:39Z","updated_at":"2021-11-19T10:21:39Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Sometimes Performance test fails to build a report due to\r\n```\r\nCode: 190. DB::Exception: Arrays passed to arrayMap must have equal size: while executing 'FUNCTION arrayMap(__lambda_6 :: 6, arrayElement(medians_by_version, 1) : 8, arrayElement(medians_by_version, 2) :: 5) -> arrayMap(lambda(tuple(x, y), floor(divide(minus(y, x), x), 3)), arrayElement(medians_by_version, 1), arrayElement(medians_by_version, 2)) Array(Float64) : 9'. (SIZES_OF_ARRAYS_DOESNT_MATCH)\r\n```\r\nEither the following query\r\nhttps://github.com/ClickHouse/ClickHouse/blob/08148e062f1c7ccc84c2e6d3cbd4f34ed3cb0faf/docker/test/performance-comparison/eqmed.sql#L5-L8\r\ndoes not handle some corner case correctly or there's a bug somewhere in sh scripts that generate data for this query, it's hard to tell.\r\n\r\nhttps://clickhouse-test-reports.s3.yandex.net/31391/a13ed01c05b90edd9408fe33b80c6075fcc571cd/performance_comparison/report.html#fail1","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31530/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31502","id":1057088094,"node_id":"I_kwDOA5dJV84_AeJe","number":31502,"title":"Context has expired: while pushing to view","user":{"login":"zealjoanna","id":21325163,"node_id":"MDQ6VXNlcjIxMzI1MTYz","avatar_url":"https://avatars.githubusercontent.com/u/21325163?v=4","gravatar_id":"","url":"https://api.github.com/users/zealjoanna","html_url":"https://github.com/zealjoanna","followers_url":"https://api.github.com/users/zealjoanna/followers","following_url":"https://api.github.com/users/zealjoanna/following{/other_user}","gists_url":"https://api.github.com/users/zealjoanna/gists{/gist_id}","starred_url":"https://api.github.com/users/zealjoanna/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zealjoanna/subscriptions","organizations_url":"https://api.github.com/users/zealjoanna/orgs","repos_url":"https://api.github.com/users/zealjoanna/repos","events_url":"https://api.github.com/users/zealjoanna/events{/privacy}","received_events_url":"https://api.github.com/users/zealjoanna/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-18T08:46:29Z","updated_at":"2021-11-18T12:20:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> Make sure to check documentation https://clickhouse.yandex/docs/en/ first. If the question is concise and probably has a short answer, asking it in Telegram chat https://telegram.me/clickhouse_en is probably the fastest way to find the answer. For more complicated questions, consider asking them on StackOverflow with \"clickhouse\" tag https://stackoverflow.com/questions/tagged/clickhouse \r\n\r\n> If you still prefer GitHub issues, remove all this text and ask your question here.\r\n after upgrading the program to v21.11.3.1-stable    \r\nevery thing seems ,ok  but when excute a insert query then i get an error\r\n\r\nClickHouse exception, code: 1002, host: localhost, port: 8123; Code: 49. DB::Exception: Context has expired: while pushing to view TEST.pipeline_test_merge_to_test_set (8b3cb0d0-dc28-4355-8b3c-b0d0dc28e355). (LOGICAL_ERROR) (version 21.11.3.1)\r\n\r\nmy test sql is :\r\n\r\n\r\ncreate database TEST\r\n\r\ncreate table TEST.test_merge\r\n(\r\ntext String\r\n)\r\nengine =MergeTree()\r\nORDER BY text\r\n\r\n\r\n\r\ncreate table TEST.test_set\r\n(\r\ntext String\r\n)\r\nengine =Set()\r\n\r\n CREATE MATERIALIZED VIEW TEST.pipeline_test_merge_to_test_set\r\n to TEST.test_set\r\n AS\r\n SELECT * from TEST.test_merge\r\n where text!='hello'\r\n\r\n \r\ni  excute  two insert query and one is ok  the other error\r\n\r\n insert into TEST.test_merge \r\n values ('hello1')\r\n \r\n insert into TEST.test_merge \r\n values ('hello')\r\n \r\n \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31502/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31497","id":1056951246,"node_id":"I_kwDOA5dJV84-_8vO","number":31497,"title":"Extended temporary tables (RFC).","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-11-18T05:17:37Z","updated_at":"2022-01-20T19:16:50Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"1. Allow to create TEMPORARY tables with arbitrary ENGINE.\r\nThese tables will be automatically removed on session close, similarly to how regular tables are removed in Atomic database after being dropped.\r\n\r\n2. Allow to create TEMPORARY databases. All tables created inside a temporary database are also temporary.\r\n\r\n3. Allow to create tables and databases with limited lifetime. They will be automatically removed after specified time interval after last usage. It can be defined with some syntax like `DROP AFTER ...`, not to be confused with temporary tables and tables' data TTL. These tables are visible to all clients.\r\n\r\n**Additional context**\r\n\r\nCan we make tables with limited lifetime available only for user who created them - with appropriate setting of RBAC rules?\r\n\r\nSee also #31496","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":1,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31497/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31484","id":1056501686,"node_id":"PR_kwDOA5dJV84urBBz","number":31484,"title":"Implement ssl x509 certificate authentication","user":{"login":"eungenue","id":66750374,"node_id":"MDQ6VXNlcjY2NzUwMzc0","avatar_url":"https://avatars.githubusercontent.com/u/66750374?v=4","gravatar_id":"","url":"https://api.github.com/users/eungenue","html_url":"https://github.com/eungenue","followers_url":"https://api.github.com/users/eungenue/followers","following_url":"https://api.github.com/users/eungenue/following{/other_user}","gists_url":"https://api.github.com/users/eungenue/gists{/gist_id}","starred_url":"https://api.github.com/users/eungenue/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/eungenue/subscriptions","organizations_url":"https://api.github.com/users/eungenue/orgs","repos_url":"https://api.github.com/users/eungenue/repos","events_url":"https://api.github.com/users/eungenue/events{/privacy}","received_events_url":"https://api.github.com/users/eungenue/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false},"assignees":[{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-11-17T19:02:03Z","updated_at":"2022-01-21T16:36:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/31484","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31484","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/31484.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/31484.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nSupport authentication of users connected via SSL by their X.509 certificate\r\n\r\nDetailed description / Documentation draft:\r\nWhen user connects to ClickHouse via SSL it is possible to force user certificate validation. In this case the SSL library authenticates the users and only users with valid SSL certificates will be allowed to connect. Connections from users with invalid certificates will be dropped by the SSL library. It seems convenient to rely on SSL authentication in addition to existing authentication methods (password/LDAP/etc). Connection certificate can be matched to a specific user using certificate's CommonName field, which allows transparent certificate renewal and multiple certificate usage for a particular user. In addition, certificate renewal/revoking along with SSL trust chains is a well-known area with a number of solutions instantly available.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31484/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31480","id":1056420966,"node_id":"I_kwDOA5dJV84-97Rm","number":31480,"title":"TABLE OVERRIDE mechanism for Materialized databases","user":{"login":"stigsb","id":169939,"node_id":"MDQ6VXNlcjE2OTkzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/169939?v=4","gravatar_id":"","url":"https://api.github.com/users/stigsb","html_url":"https://github.com/stigsb","followers_url":"https://api.github.com/users/stigsb/followers","following_url":"https://api.github.com/users/stigsb/following{/other_user}","gists_url":"https://api.github.com/users/stigsb/gists{/gist_id}","starred_url":"https://api.github.com/users/stigsb/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/stigsb/subscriptions","organizations_url":"https://api.github.com/users/stigsb/orgs","repos_url":"https://api.github.com/users/stigsb/repos","events_url":"https://api.github.com/users/stigsb/events{/privacy}","received_events_url":"https://api.github.com/users/stigsb/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-17T17:50:59Z","updated_at":"2022-01-11T06:40:09Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"A general challenge with the Materialized* database engines is that DDL conversion sometimes result in very generic but very suboptimal schemas on the ClickHouse side. If users could supply a bit more direction based on knowing the application, the translated ClickHouse DDL could become more efficient.\r\n\r\nTo address this, I propose adding a mechanism to `CREATE DATABASE` for overlaying changes to columns (including indexes, constraints and projections) and storage (partition by, order by, sample by, etc.).  We have a working implementation of this in our internal fork, but I think it'd be good with an RFC before throwing in a pull request.\r\n\r\n## Suggested Syntax\r\n\r\n```\r\nCREATE DATABASE test ENGINE=MaterializeMySQL('127.0.0.1:3306', 'test', 'root', 'clickhouse')\r\n    TABLE OVERRIDE t1 (\r\n        COLUMNS (\r\n            -- sparse column list, replacing existing ones or adding new ones\r\n            timestamp DateTime CODEC(DoubleDelta, Default)\r\n            PROJECTION ...,\r\n            CONSTRAINT ...,\r\n            INDEX ...\r\n        )\r\n        -- storage parameters:\r\n        ORDER BY expr\r\n        PRIMARY KEY expr\r\n        PARTITION BY expr\r\n        SAMPLE BY expr\r\n        TTL ...\r\n    ),\r\n    -- multiple tables can be overridden\r\n    TABLE OVERRIDE t2 (\r\n        PARTITION BY tuple(id % 10, toYYYY(created))\r\n    )\r\n```\r\n\r\n## ALTERing overrides\r\n\r\nCurrently table overrides would have to be added before creating the database. To support schema evolution, it should be possible to add/modify/drop table overrides. For this, `ALTER DATABASE` queries are needed:\r\n\r\n```\r\nALTER DATABASE test ADD TABLE OVERRIDE ..., DROP TABLE OVERRIDE ..., MODIFY TABLE OVERRIDE ...\r\n```\r\n\r\nSome of these changes are impossible (or impractical) to implement dynamically. It's fairly straightforward to handle ADD TABLE OVERRIDE for columns by running ALTER TABLE sub-queries. But for DROP/MODIFY TABLE OVERRIDE it's harder to construct such ALTER TABLE queries, because we no longer have the original schema. It could be possible to address this by storing the converted CREATE query (before applying overrides) in COMMENT or a new database property, but I'm not planning to address that in the coming PR.\r\n\r\nADD/DROP/MODIFY TABLE OVERRIDE queries that can not be applied dynamically can drop and re-create the database, if a new setting (for example `drop_and_recreate_database_if_needed`) is true. Otherwise, they will give an error:\r\n\r\n```\r\nALTER DATABASE test DROP TABLE OVERRIDE t1 SETTINGS drop_and_recreate_database_if_needed=1\r\n```\r\n\r\nIt should be possible to add a table override for a table that does not yet exist in the source database (MySQL)\r\n\r\n## Task List\r\n\r\n* [x] Implement CREATE TABLE TABLE OVERRIDE (apply to CREATE queries) - #32325\r\n* [ ] Implement ALTER DATABASE {ADD | DROP | MODIFY} TABLE OVERRIDE\r\n* [x] Implement EXPLAIN TABLE OVERRIDE for extra validation (to prevent overriding PARTITION BY with nullable columns etc.) - #32836\r\n* [x] Add support for MaterializedPostgreSQL - #32749\r\n* [ ] Apply overrides to ALTER queries\r\n* [ ] Handle ALTER TABLE RENAME COLUMN\r\n* [ ] Run validations provided by EXPLAIN TABLE OVERRIDE from CREATE interpreter\r\n* [ ] Support EXPLAIN TABLE OVERRIDE for PostgreSQL\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31480/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31478","id":1056331620,"node_id":"I_kwDOA5dJV84-9ldk","number":31478,"title":"clickhouse don't graceful shutdown on systemctl stop clickhouse-server but no any errors in logs","user":{"login":"deepdivenow","id":27303815,"node_id":"MDQ6VXNlcjI3MzAzODE1","avatar_url":"https://avatars.githubusercontent.com/u/27303815?v=4","gravatar_id":"","url":"https://api.github.com/users/deepdivenow","html_url":"https://github.com/deepdivenow","followers_url":"https://api.github.com/users/deepdivenow/followers","following_url":"https://api.github.com/users/deepdivenow/following{/other_user}","gists_url":"https://api.github.com/users/deepdivenow/gists{/gist_id}","starred_url":"https://api.github.com/users/deepdivenow/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/deepdivenow/subscriptions","organizations_url":"https://api.github.com/users/deepdivenow/orgs","repos_url":"https://api.github.com/users/deepdivenow/repos","events_url":"https://api.github.com/users/deepdivenow/events{/privacy}","received_events_url":"https://api.github.com/users/deepdivenow/received_events","type":"User","site_admin":false},"labels":[{"id":1006992822,"node_id":"MDU6TGFiZWwxMDA2OTkyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/operations","name":"operations","color":"7fe8ba","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-11-17T16:30:17Z","updated_at":"2021-11-18T05:55:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> You have to provide the following information whenever possible.\r\nClickHouse 21.10.2.15\r\nsystemctl restart clickhouse-server\r\n\r\n\r\n**How to reproduce**\r\n\r\nsystemctl restart clickhouse-server\r\n\r\n> A clear and concise description of what you expected to happen.\r\nmain.log:\r\n2021.11.17 16:38:36.000118 [ 16805 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 3.66 GiB, peak 3.71 GiB, will set to 3.65 GiB (RSS), difference: -3.99 MiB\r\n2021.11.17 16:38:36.714041 [ 16651 ] {} <Information> Application: Closed connections. But 1 remain. Tip: To increase wait time add to config: <shutdown_wait_unfinished>60</shutdown_wait_unfinished>\r\n2021.11.17 16:38:36.714103 [ 16651 ] {} <Information> Application: Will shutdown forcefully.\r\n2021.11.17 16:38:36.867171 [ 16640 ] {} <Information> Application: Child process exited normally with code 0.\r\n2021.11.17 16:39:25.043803 [ 18271 ] {} <Information> Application: Will watch for the process with pid 18283\r\n2021.11.17 16:39:25.044084 [ 18283 ] {} <Information> Application: Forked a child process to watch\r\n2021.11.17 16:39:25.044656 [ 18283 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2021.11.17 16:39:25.044818 [ 18283 ] {} <Trace> Pipe: Pipe capacity is 1.00 MiB\r\n2021.11.17 16:39:25.095628 [ 18283 ] {} <Information> : Starting ClickHouse 21.10.2.15 with revision 54455, build id: 6699B86599A2121E78E0D42DD67791ABD9AE5265, PID 18283\r\n2021.11.17 16:39:25.095741 [ 18283 ] {} <Information> Application: starting up\r\n2021.11.17 16:39:25.095766 [ 18283 ] {} <Information> Application: OS name: Linux, version: 4.15.0-162-generic, architecture: x86_64\r\n2021.11.17 16:39:25.215198 [ 18283 ] {} <Information> Application: Calculated checksum of the binary: 902F2F159F07FEEC02834AEF44CBFB70, integrity check passed.\r\n2021.11.17 16:39:25.215301 [ 18283 ] {} <Information> StatusFile: Status file /srv/clickhouse/server/status already exists - unclean restart. Contents:\r\nPID: 16651\r\nStarted at: 2021-11-17 16:31:13\r\nRevision: 54455\r\n\r\n\r\n> Add any other context about the problem here.\r\nWhere i can found description about clickhouse-server shutdown process.\r\nWhy it unclean restart but no any error messages about this.\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31478/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31473","id":1056070411,"node_id":"I_kwDOA5dJV84-8lsL","number":31473,"title":"Publish ClickHouse as an official image on docker-hub","user":{"login":"Felixoid","id":3025537,"node_id":"MDQ6VXNlcjMwMjU1Mzc=","avatar_url":"https://avatars.githubusercontent.com/u/3025537?v=4","gravatar_id":"","url":"https://api.github.com/users/Felixoid","html_url":"https://github.com/Felixoid","followers_url":"https://api.github.com/users/Felixoid/followers","following_url":"https://api.github.com/users/Felixoid/following{/other_user}","gists_url":"https://api.github.com/users/Felixoid/gists{/gist_id}","starred_url":"https://api.github.com/users/Felixoid/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Felixoid/subscriptions","organizations_url":"https://api.github.com/users/Felixoid/orgs","repos_url":"https://api.github.com/users/Felixoid/repos","events_url":"https://api.github.com/users/Felixoid/events{/privacy}","received_events_url":"https://api.github.com/users/Felixoid/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-17T12:25:26Z","updated_at":"2021-11-17T12:25:26Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the issue**  \r\nIt will be more than convenient to have a way to use `docker pull clickhouse && docker run clickhouse`  \r\nhttps://github.com/docker-library/official-images\r\n\r\n**Additional context**\r\nTo achieve it, the images should be updated in two repositories:\r\n\r\n- https://github.com/docker-library/docs\r\n- https://github.com/docker-library/official-images\r\n\r\ne.g.: https://github.com/docker-library/docs/tree/master/debian and https://github.com/docker-library/official-images/blob/master/library/debian","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31473/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31469","id":1055944571,"node_id":"I_kwDOA5dJV84-8G97","number":31469,"title":"MaterializedMysql не корректное преобразование Date при  Date<1970","user":{"login":"bollustrado","id":4647369,"node_id":"MDQ6VXNlcjQ2NDczNjk=","avatar_url":"https://avatars.githubusercontent.com/u/4647369?v=4","gravatar_id":"","url":"https://api.github.com/users/bollustrado","html_url":"https://github.com/bollustrado","followers_url":"https://api.github.com/users/bollustrado/followers","following_url":"https://api.github.com/users/bollustrado/following{/other_user}","gists_url":"https://api.github.com/users/bollustrado/gists{/gist_id}","starred_url":"https://api.github.com/users/bollustrado/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bollustrado/subscriptions","organizations_url":"https://api.github.com/users/bollustrado/orgs","repos_url":"https://api.github.com/users/bollustrado/repos","events_url":"https://api.github.com/users/bollustrado/events{/privacy}","received_events_url":"https://api.github.com/users/bollustrado/received_events","type":"User","site_admin":false},"labels":[{"id":2121263626,"node_id":"MDU6TGFiZWwyMTIxMjYzNjI2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mysql","name":"comp-mysql","color":"b5bcff","default":false,"description":""},{"id":2673090580,"node_id":"MDU6TGFiZWwyNjczMDkwNTgw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug%20experimental","name":"bug experimental","color":"E99695","default":false,"description":"Bug in the feature that should not be used in production"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-17T10:13:27Z","updated_at":"2021-11-29T17:37:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Добрый день.\r\nИмеется ClickHouse server version 21.8.5 revision 54449 и  mysql таблица\r\n\r\nCREATE TABLE `customer` (\r\n  `id` int NOT NULL AUTO_INCREMENT,\r\n  `birthday` date DEFAULT NULL\r\n) ENGINE=InnoDB AUTO_INCREMENT=18621567 DEFAULT CHARSET=utf8;\r\n\r\nЕсли значения birthday < 1970 года, в ClicHouse они не корректно отображаются. Например\r\n\r\nMysql\r\n1962-01-24\r\n1964-10-16\r\n1960-01-24\r\n1970-10-16\r\n1965-01-24\r\n1975-10-16\r\n\r\nClickHouse\r\n2144-06-30\r\n2144-03-22\r\n2141-06-30\r\n1975-10-16\r\n1970-10-16\r\n2139-06-30","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31469/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31468","id":1055908385,"node_id":"I_kwDOA5dJV84-7-Ih","number":31468,"title":"close rpc call in native protocol","user":{"login":"thedolphin","id":1102063,"node_id":"MDQ6VXNlcjExMDIwNjM=","avatar_url":"https://avatars.githubusercontent.com/u/1102063?v=4","gravatar_id":"","url":"https://api.github.com/users/thedolphin","html_url":"https://github.com/thedolphin","followers_url":"https://api.github.com/users/thedolphin/followers","following_url":"https://api.github.com/users/thedolphin/following{/other_user}","gists_url":"https://api.github.com/users/thedolphin/gists{/gist_id}","starred_url":"https://api.github.com/users/thedolphin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/thedolphin/subscriptions","organizations_url":"https://api.github.com/users/thedolphin/orgs","repos_url":"https://api.github.com/users/thedolphin/repos","events_url":"https://api.github.com/users/thedolphin/events{/privacy}","received_events_url":"https://api.github.com/users/thedolphin/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-17T09:37:14Z","updated_at":"2021-11-18T05:08:13Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Frequent connections are odd, but used in real life.\r\nClosing connection from client side leaves client socket in TIME_WAIT state, which in heavy load cases leads to outgoing port pool exhaust.\r\n\r\nAdding \"close\" rpc call to protocol for asking server to close connection will be useful in this case.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31468/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31466","id":1055840944,"node_id":"I_kwDOA5dJV84-7tqw","number":31466,"title":"Debuginfo is not always used in CentOS","user":{"login":"ilejn","id":360758,"node_id":"MDQ6VXNlcjM2MDc1OA==","avatar_url":"https://avatars.githubusercontent.com/u/360758?v=4","gravatar_id":"","url":"https://api.github.com/users/ilejn","html_url":"https://github.com/ilejn","followers_url":"https://api.github.com/users/ilejn/followers","following_url":"https://api.github.com/users/ilejn/following{/other_user}","gists_url":"https://api.github.com/users/ilejn/gists{/gist_id}","starred_url":"https://api.github.com/users/ilejn/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ilejn/subscriptions","organizations_url":"https://api.github.com/users/ilejn/orgs","repos_url":"https://api.github.com/users/ilejn/repos","events_url":"https://api.github.com/users/ilejn/events{/privacy}","received_events_url":"https://api.github.com/users/ilejn/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-17T08:21:57Z","updated_at":"2021-11-18T05:35:14Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"If ClickHouse RPMs are built by the native rpm toolchain debuginfo is not suitable for backtraces and reflection, while third party tools (e.g. gdb or readelf -n) do not have any issues.\r\n\r\nThis is happening because ClickHouse uses Program Header (ELF table) to obtain Build ID to validate debuginfo against executable. This approach does not seem reliable, it is better to use Section Header.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31466/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31459","id":1055641292,"node_id":"I_kwDOA5dJV84-687M","number":31459,"title":"Not find column in block  when using the union all","user":{"login":"jztian","id":31120351,"node_id":"MDQ6VXNlcjMxMTIwMzUx","avatar_url":"https://avatars.githubusercontent.com/u/31120351?v=4","gravatar_id":"","url":"https://api.github.com/users/jztian","html_url":"https://github.com/jztian","followers_url":"https://api.github.com/users/jztian/followers","following_url":"https://api.github.com/users/jztian/following{/other_user}","gists_url":"https://api.github.com/users/jztian/gists{/gist_id}","starred_url":"https://api.github.com/users/jztian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jztian/subscriptions","organizations_url":"https://api.github.com/users/jztian/orgs","repos_url":"https://api.github.com/users/jztian/repos","events_url":"https://api.github.com/users/jztian/events{/privacy}","received_events_url":"https://api.github.com/users/jztian/received_events","type":"User","site_admin":false},"labels":[{"id":1507860028,"node_id":"MDU6TGFiZWwxNTA3ODYwMDI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-repro","name":"st-need-repro","color":"e5b890","default":false,"description":"We were not able to reproduce the problem, please help us."},{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-17T02:43:44Z","updated_at":"2021-12-01T11:33:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> You have to provide the following information whenever possible.\r\n\r\nwhen I use the ReplicatedMergeTree engine and query with union all, It's reported with Not find column in block \r\n![image](https://user-images.githubusercontent.com/31120351/142110458-5a392980-ece4-4e31-95af-5e797551ce7d.png)\r\n\r\nIf I use the Mergetree engine without replicated, this problem will not occur\r\nso, this temporary field cannot be used in conjunction with union all in an engine with a replica\r\n\r\nsql:\r\nSELECT\r\n  toDate(o.time) AS xData,\r\n  COUNT(o.distinct_id) AS value,\r\n  ou.work_status AS yData\r\nFROM\r\n  onelog o\r\n  LEFT JOIN (\r\n    SELECT\r\n      user_code,\r\n      CASE\r\n        job_status\r\n        WHEN 1 THEN '在职'\r\n        ELSE '其他'\r\n      END AS work_status\r\n    FROM\r\n      onelog_user\r\n    UNION ALL\r\n    SELECT\r\n      employee_code AS user_code,\r\n      CASE\r\n        job_status\r\n        WHEN 1 THEN '在职'\r\n        ELSE '其他'\r\n      END AS work_status\r\n    FROM\r\n      onelog_user\r\n  ) ou ON ou.user_code = o.distinct_id\r\nGROUP BY\r\n  toDate(o.time),\r\n  ou.work_status\r\nORDER BY\r\n  toDate(o.time)\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\nClickHouse server version 21.9.2.17 (official build).\r\n  ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31459/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31452","id":1054858238,"node_id":"I_kwDOA5dJV84-39v-","number":31452,"title":"Server triggers timeouts after few minutes even when I set the timeout to be 8 hours","user":{"login":"maciejblaz","id":72199669,"node_id":"MDQ6VXNlcjcyMTk5NjY5","avatar_url":"https://avatars.githubusercontent.com/u/72199669?v=4","gravatar_id":"","url":"https://api.github.com/users/maciejblaz","html_url":"https://github.com/maciejblaz","followers_url":"https://api.github.com/users/maciejblaz/followers","following_url":"https://api.github.com/users/maciejblaz/following{/other_user}","gists_url":"https://api.github.com/users/maciejblaz/gists{/gist_id}","starred_url":"https://api.github.com/users/maciejblaz/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/maciejblaz/subscriptions","organizations_url":"https://api.github.com/users/maciejblaz/orgs","repos_url":"https://api.github.com/users/maciejblaz/repos","events_url":"https://api.github.com/users/maciejblaz/events{/privacy}","received_events_url":"https://api.github.com/users/maciejblaz/received_events","type":"User","site_admin":false},"labels":[{"id":845267693,"node_id":"MDU6TGFiZWw4NDUyNjc2OTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-info","name":"st-need-info","color":"e5b890","default":false,"description":"We need extra data to continue"},{"id":1507860028,"node_id":"MDU6TGFiZWwxNTA3ODYwMDI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-need-repro","name":"st-need-repro","color":"e5b890","default":false,"description":"We were not able to reproduce the problem, please help us."},{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-16T13:05:26Z","updated_at":"2021-11-29T06:40:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> You have to provide the following information whenever possible.\r\n\r\nI have one clickhouse server running in kubernetes (no cluster). I'm running 5-10 queries from client vm using clickhouse driver for python. Result set of each query is iterated lazily, using \"execute_iter\" and I jump between the result sets. At some point some of the queries start to trigger \"Timeout exceeded while writing to socket\". I tried to increase the timeouts (just to see if it helps) and upgraded clickhouse server to the latest version, as the timeout message is more descriptive in the latest version. Now I'm getting:\r\n\"<Error> executeQuery: Code: 209. DB::NetException: Timeout exceeded while writing to socket ([::ffff:<ip_address>]:46084, **30000000** ms). (SOCKET_TIMEOUT) (version 21.11.3.6 (official build))\"\r\nBut the problem is that whole the process fails after approx 15-30 min. While the timeout is set to over 8h (30000sec). \r\n\r\nThe issue happens only when I go through HA proxy. When I connect directly then everything works fine. But there is something wrong with the timeout itself. It should not time out for 30000sec (8h+) while it times out after few min.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nI'm running latest 21.11 version:\r\n┌─version()─┐\r\n│ 21.11.3.6 │\r\n└───────────┘\r\n\r\n**Enable crash reporting**\r\n\r\nThe server doesn't crash\r\n\r\n**How to reproduce**\r\n\r\nThat is not easy as it fails randomly in random places after random time. I run simple queries (select of multiple columns from one table). But there are multiple queries being run at a time and I switch between result sets in one thread. I have HA proxy between client and server.\r\n\r\n**Expected behavior**\r\n\r\nTime out settings should be respected.\r\n\r\n**Error message and/or stacktrace**\r\n\r\n<Error> executeQuery: Code: 209. DB::NetException: Timeout exceeded while writing to socket ([::ffff:<ip_address>]:46084, **30000000** ms). (SOCKET_TIMEOUT) (version 21.11.3.6 (official build))\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31452/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31447","id":1054795624,"node_id":"I_kwDOA5dJV84-3udo","number":31447,"title":"NULL mode for default values","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-16T11:59:56Z","updated_at":"2021-11-17T06:33:51Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nParsing values without storing raw data.\r\n\r\n**Describe the solution you'd like**\r\n\r\nIt's possible to define some column as NULL, so it's possible to insert data in it, use it for DEFAULT value calculation for other columns but this column will not store any data in it. This column will also not appear in SELECT *\r\n\r\n**Describe alternatives you've considered**\r\n\r\nNull table engine for such parsing, but it's more complex to have 2 tables and 1 MV.\r\n\r\nhttps://kb.altinity.com/altinity-kb-schema-design/altinity-kb-jsonasstring-and-mat.-view-as-json-parser/\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31447/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31446","id":1054738868,"node_id":"I_kwDOA5dJV84-3gm0","number":31446,"title":"Missing mutations when max_expanded_ast_elements is small and alter table update is large","user":{"login":"zhangjmruc","id":66244986,"node_id":"MDQ6VXNlcjY2MjQ0OTg2","avatar_url":"https://avatars.githubusercontent.com/u/66244986?v=4","gravatar_id":"","url":"https://api.github.com/users/zhangjmruc","html_url":"https://github.com/zhangjmruc","followers_url":"https://api.github.com/users/zhangjmruc/followers","following_url":"https://api.github.com/users/zhangjmruc/following{/other_user}","gists_url":"https://api.github.com/users/zhangjmruc/gists{/gist_id}","starred_url":"https://api.github.com/users/zhangjmruc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhangjmruc/subscriptions","organizations_url":"https://api.github.com/users/zhangjmruc/orgs","repos_url":"https://api.github.com/users/zhangjmruc/repos","events_url":"https://api.github.com/users/zhangjmruc/events{/privacy}","received_events_url":"https://api.github.com/users/zhangjmruc/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-16T10:59:12Z","updated_at":"2021-11-16T10:59:12Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe what's wrong**\r\nI set the max_expanded_ast_elements setting to 200 and stop background merges on a MergeTree table. Then I summit 2 alter table update stmts with about 45 updates. Just make sure that the first  mutation commands have ast elements larger than 200. \r\nAfter merges are started, all the data parts have the second mutation version, but without updates of it.\r\n\r\n**How to reproduce**\r\n\r\n* Which ClickHouse server version to use\r\nCH 21.3.10.1\r\n\r\n* Non-default settings, if any\r\n<max_expanded_ast_elements>200</max_expanded_ast_elements>\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\ncreate table t (a int, b int, c int, d date, e String, f String, g String) engine=MergeTree order by a partition by d;\r\n\r\n* Sample data for all these tables, use [clickhouse-obfuscator]\r\ninsert into t values(1,1,1,'2021-11-16','a','a','a'),(2,2,2,'2021-11-16','b','b','b'),(3,3,3,'2021-11-16','c','c','c'),(4,4,4,'2021-11-16','4','4','4');\r\ninsert into t select a+4,b+4,c+4,d-1,'5','5','5' from t;\r\ninsert into t select a+8,b+8,c+8,d-2,'6','6','6' from t;\r\ninsert into t select a+16,b+16,c+16,d-4,'7','7','7' from t;\r\ninsert into t select a+32,b+32,c+32,d-4,'8','8','8' from t;\r\n\r\n* Queries to run that lead to unexpected result\r\nsystem stop merges t;\r\n\r\nalter table t update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=1, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=2, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=3, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=4,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=5,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=6, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=7, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=8, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=9,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=10,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=11, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=12, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=13, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=14,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=15,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=16, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=17, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=18, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=19,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=20,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=21, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=22, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=23, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=24,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=25,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=26, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=27, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=28, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=29,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=30,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=31, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=32, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=33, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=34,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=35,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=36, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=37, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=38, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=39,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=40,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=41, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=42, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=43, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=44,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_1',f=f||'mute_1',g=g||'mute_1' where a=45;\r\n\r\nalter table t update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=1, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=2, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=3, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=4,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=5,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=6, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=7, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=8, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=9,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=10,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=11, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=12, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=13, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=14,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=15,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=16, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=17, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=18, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=19,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=20,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=21, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=22, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=23, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=24,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=25,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=26, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=27, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=28, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=29,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=30,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=31, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=32, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=33, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=34,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=35,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=36, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=37, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=38, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=39,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=40,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=41, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=42, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=43, update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=44,update b=multiIf(b>0,1,b=0,0,-1),c=multiIf(b>0,1,b=0,0,-1),e=e||'mute_2',f=f||'mute_2',g=g||'mute_2' where a=45;\r\n\r\nsystem start merges t;\r\n\r\n-- make sure mutations are done\r\nUbuntu :) select distinct mutation_id,is_done, table, database from system.mutations where is_done=1;\r\n┌─mutation_id─────┬─is_done─┬─table─┬─database─┐\r\n│ mutation_17.txt │       1 │ t     │ default  │\r\n│ mutation_18.txt │       1 │ t     │ default  │\r\n└─────────────────┴─────────┴───────┴──────────┘\r\n--- wrong result \r\nUbuntu :) select a,f from t where a =1;\r\n┌─a─┬─f───────┐\r\n│ 1 │ amute_1 │\r\n└───┴─────────┘\r\n\r\n**Expected behavior**\r\n\r\nThe content of column f contains \"mute_2\" for a in range of [1, 45]\r\n\r\n**Additional context**\r\nFrom the code logic in StorageMergeTree::selectPartsToMutate(), we will break from the for loop  of mutations when max_ast_elements is exceeded, but for the future part, we assign the final mutation version to it. This is the root cause of missing update.\r\n\r\nStorageMergeTree::selectPartsToMutate()\r\n...\r\n    size_t max_ast_elements = getContext()->getSettingsRef().max_expanded_ast_elements;\r\n...\r\n        size_t current_ast_elements = 0;\r\n        for (auto it = mutations_begin_it; it != mutations_end_it; ++it)\r\n        {\r\n...\r\n\r\n            if (current_ast_elements + commands_size >= max_ast_elements)\r\n                break; >>>> Maybe we should save the last collected mutation, for this early break case?\r\n\r\n            current_ast_elements += commands_size;\r\n            commands.insert(commands.end(), it->second.commands.begin(), it->second.commands.end());\r\n        }\r\n\r\n        if (!commands.empty())\r\n        {\r\n            auto new_part_info = part->info;\r\n            new_part_info.mutation = **current_mutations_by_version.rbegin()->first;** <<<< This is wrong when we break from above if\r\n...","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31446/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31423","id":1053620742,"node_id":"I_kwDOA5dJV84-zPoG","number":31423,"title":"Data race in `arrow::internal::ThreadPool::ProtectAgainstFork()`","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-15T12:39:52Z","updated_at":"2022-01-17T14:29:30Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/31292/8ffb46b037b65123d06e7994581f5d93a28bb64a/stress_test_(thread).html#fail1\r\n\r\n```\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=495)\r\n  Read of size 4 at 0x7b0c0000cef4 by thread T350:\r\n    #0 arrow::internal::ThreadPool::ProtectAgainstFork() <null> (clickhouse+0x1b5d42b0)\r\n    #1 arrow::internal::ThreadPool::SpawnReal(arrow::internal::TaskHints, arrow::internal::FnOnce<void ()>, arrow::StopToken, arrow::internal::FnOnce<void (arrow::Status const&)>&&) <null> (clickhouse+0x1b5d4e2a)\r\n    #2 arrow::io::RandomAccessFile::ReadAsync(arrow::io::IOContext const&, long, long) <null> (clickhouse+0x1b50a41c)\r\n    #3 arrow::io::RandomAccessFile::ReadAsync(long, long) <null> (clickhouse+0x1b50a86c)\r\n    #4 arrow::ipc::RecordBatchFileReaderImpl::ReadFooterAsync(arrow::internal::Executor*) <null> (clickhouse+0x1b49fc9f)\r\n    #5 arrow::ipc::RecordBatchFileReaderImpl::ReadFooter() <null> (clickhouse+0x1b49f929)\r\n    #6 arrow::ipc::RecordBatchFileReaderImpl::Open(arrow::io::RandomAccessFile*, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b48f570)\r\n    #7 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b47f79e)\r\n    #8 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b47f5da)\r\n    #9 DB::ArrowBlockInputFormat::prepareReader() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:97:35 (clickhouse+0x16f4ee7b)\r\n    #10 DB::ArrowBlockInputFormat::generate() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:47:13 (clickhouse+0x16f4e75e)\r\n    #11 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16f10494)\r\n    #12 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16f0ff52)\r\n    #13 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:45:20 (clickhouse+0x16f3878c)\r\n    #14 DB::ExecutionThreadContext::executeTask() obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:63:9 (clickhouse+0x16f3878c)\r\n    #15 DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:187:26 (clickhouse+0x16f28740)\r\n    #16 DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:112:5 (clickhouse+0x16f28383)\r\n    #17 DB::PullingPipelineExecutor::pull(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingPipelineExecutor.cpp:47:20 (clickhouse+0x16f3dbea)\r\n    #18 DB::StorageFileSource::generate() obj-x86_64-linux-gnu/../src/Storages/StorageFile.cpp:422:25 (clickhouse+0x166f8002)\r\n    #19 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16f10494)\r\n    #20 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16f0ff52)\r\n    #21 DB::SourceWithProgress::work() obj-x86_64-linux-gnu/../src/Processors/Sources/SourceWithProgress.cpp:65:30 (clickhouse+0x1719b269)\r\n    #22 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:45:20 (clickhouse+0x16f3878c)\r\n    #23 DB::ExecutionThreadContext::executeTask() obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:63:9 (clickhouse+0x16f3878c)\r\n    #24 DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:187:26 (clickhouse+0x16f28740)\r\n    #25 DB::PipelineExecutor::executeSingleThread(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:152:5 (clickhouse+0x16f27e63)\r\n    #26 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:300:9 (clickhouse+0x16f27e63)\r\n    #27 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:81:9 (clickhouse+0x16f27773)\r\n    #28 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16f3ca59)\r\n    #29 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16f3ca59)\r\n    #30 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16f3ca59)\r\n    #31 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16f3ca59)\r\n    #32 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16f3ca59)\r\n    #33 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16f3ca59)\r\n    #34 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16f3ca59)\r\n    #35 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16f3ca59)\r\n    #36 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16f3ca59)\r\n    #37 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16f3ca59)\r\n    #38 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9c2e06d)\r\n    #39 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9c2e06d)\r\n    #40 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9c2e06d)\r\n    #41 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9c31830)\r\n    #42 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9c31830)\r\n    #43 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9c31830)\r\n    #44 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9c31830)\r\n\r\n  Previous write of size 4 at 0x7b0c0000cef4 by thread T755:\r\n    #0 arrow::internal::ThreadPool::ProtectAgainstFork() <null> (clickhouse+0x1b5d43cd)\r\n    #1 arrow::internal::ThreadPool::SpawnReal(arrow::internal::TaskHints, arrow::internal::FnOnce<void ()>, arrow::StopToken, arrow::internal::FnOnce<void (arrow::Status const&)>&&) <null> (clickhouse+0x1b5d4e2a)\r\n    #2 arrow::io::RandomAccessFile::ReadAsync(arrow::io::IOContext const&, long, long) <null> (clickhouse+0x1b50a41c)\r\n    #3 arrow::io::RandomAccessFile::ReadAsync(long, long) <null> (clickhouse+0x1b50a86c)\r\n    #4 arrow::ipc::RecordBatchFileReaderImpl::ReadFooterAsync(arrow::internal::Executor*) <null> (clickhouse+0x1b49fc9f)\r\n    #5 arrow::ipc::RecordBatchFileReaderImpl::ReadFooter() <null> (clickhouse+0x1b49f929)\r\n    #6 arrow::ipc::RecordBatchFileReaderImpl::Open(arrow::io::RandomAccessFile*, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b48f570)\r\n    #7 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, long, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b47f79e)\r\n    #8 arrow::ipc::RecordBatchFileReader::Open(std::__1::shared_ptr<arrow::io::RandomAccessFile> const&, arrow::ipc::IpcReadOptions const&) <null> (clickhouse+0x1b47f5da)\r\n    #9 DB::ArrowBlockInputFormat::prepareReader() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:97:35 (clickhouse+0x16f4ee7b)\r\n    #10 DB::ArrowBlockInputFormat::generate() obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ArrowBlockInputFormat.cpp:47:13 (clickhouse+0x16f4e75e)\r\n    #11 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16f10494)\r\n    #12 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16f0ff52)\r\n    #13 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:45:20 (clickhouse+0x16f3878c)\r\n    #14 DB::ExecutionThreadContext::executeTask() obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:63:9 (clickhouse+0x16f3878c)\r\n    #15 DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:187:26 (clickhouse+0x16f28740)\r\n    #16 DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:112:5 (clickhouse+0x16f28383)\r\n    #17 DB::PullingPipelineExecutor::pull(DB::Chunk&) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingPipelineExecutor.cpp:47:20 (clickhouse+0x16f3dbea)\r\n    #18 DB::StorageFileSource::generate() obj-x86_64-linux-gnu/../src/Storages/StorageFile.cpp:422:25 (clickhouse+0x166f8002)\r\n    #19 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x16f10494)\r\n    #20 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x16f0ff52)\r\n    #21 DB::SourceWithProgress::work() obj-x86_64-linux-gnu/../src/Processors/Sources/SourceWithProgress.cpp:65:30 (clickhouse+0x1719b269)\r\n    #22 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:45:20 (clickhouse+0x16f3878c)\r\n    #23 DB::ExecutionThreadContext::executeTask() obj-x86_64-linux-gnu/../src/Processors/Executors/ExecutionThreadContext.cpp:63:9 (clickhouse+0x16f3878c)\r\n    #24 DB::PipelineExecutor::executeStepImpl(unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:187:26 (clickhouse+0x16f28740)\r\n    #25 DB::PipelineExecutor::executeSingleThread(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:152:5 (clickhouse+0x16f27e63)\r\n    #26 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:300:9 (clickhouse+0x16f27e63)\r\n    #27 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:81:9 (clickhouse+0x16f27773)\r\n    #28 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16f3ca59)\r\n    #29 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16f3ca59)\r\n    #30 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16f3ca59)\r\n    #31 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16f3ca59)\r\n    #32 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16f3ca59)\r\n    #33 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16f3ca59)\r\n    #34 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16f3ca59)\r\n    #35 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16f3ca59)\r\n    #36 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16f3ca59)\r\n    #37 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16f3ca59)\r\n    #38 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9c2e06d)\r\n    #39 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9c2e06d)\r\n    #40 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9c2e06d)\r\n    #41 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9c31830)\r\n    #42 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9c31830)\r\n    #43 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9c31830)\r\n    #44 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9c31830)\r\n\r\n  Location is heap block of size 40 at 0x7b0c0000ced0 allocated by main thread:\r\n    #0 operator new(unsigned long) <null> (clickhouse+0x9bbae18)\r\n    #1 arrow::internal::ThreadPool::Make(int) <null> (clickhouse+0x1b5d51e8)\r\n    #2 arrow::internal::ThreadPool::MakeEternal(int) <null> (clickhouse+0x1b5d5467)\r\n    #3 arrow::io::internal::MakeIOThreadPool() interfaces.cc (clickhouse+0x1b50b181)\r\n    #4 arrow::io::IOContext::IOContext(arrow::MemoryPool*, arrow::StopToken) <null> (clickhouse+0x1b508709)\r\n    #5 _GLOBAL__sub_I_interfaces.cc interfaces.cc (clickhouse+0x1b510c94)\r\n    #6 __libc_csu_init <null> (clickhouse+0x1cfbcc3c)\r\n\r\n  Thread T350 'QueryPipelineEx' (tid=1232, running) created by thread T296 at:\r\n    #0 pthread_create <null> (clickhouse+0x9b460dd)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x9c312a6)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'(), void>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x9c312a6)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9c2c6d4)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:171:5 (clickhouse+0x9c2d007)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:169:38 (clickhouse+0x16f3b4b8)\r\n    #6 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:115:24 (clickhouse+0x16f3b4b8)\r\n    #7 DB::PullingAsyncPipelineExecutor::pull(DB::Block&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:152:10 (clickhouse+0x16f3bd46)\r\n    #8 DB::TCPHandler::processOrdinaryQueryWithProcessors() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:668:25 (clickhouse+0x16ebe99b)\r\n    #9 DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:331:17 (clickhouse+0x16eb8179)\r\n    #10 DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1844:9 (clickhouse+0x16ecc2e7)\r\n    #11 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a305b02)\r\n    #12 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a306312)\r\n    #13 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a48a255)\r\n    #14 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a48842f)\r\n    #15 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a486b07)\r\n\r\n  Thread T755 'QueryPipelineEx' (tid=2825, running) created by thread T456 at:\r\n    #0 pthread_create <null> (clickhouse+0x9b460dd)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x9c312a6)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'(), void>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x9c312a6)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9c2c6d4)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:171:5 (clickhouse+0x9c2d007)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:169:38 (clickhouse+0x16f29be7)\r\n    #6 void std::__1::allocator<ThreadFromGlobalPool>::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886:28 (clickhouse+0x16f29be7)\r\n    #7 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::__construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(std::__1::integral_constant<bool, true>, std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:519:21 (clickhouse+0x16f29be7)\r\n    #8 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:481:14 (clickhouse+0x16f29be7)\r\n    #9 void std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::__construct_one_at_end<DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:926:5 (clickhouse+0x16f279ad)\r\n    #10 ThreadFromGlobalPool& std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::emplace_back<DB::PipelineExecutor::executeImpl(unsigned long)::$_1>(DB::PipelineExecutor::executeImpl(unsigned long)::$_1&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1684:9 (clickhouse+0x16f279ad)\r\n    #11 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:266:21 (clickhouse+0x16f279ad)\r\n    #12 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:81:9 (clickhouse+0x16f27773)\r\n    #13 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85:24 (clickhouse+0x16f3ca59)\r\n    #14 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:112:13 (clickhouse+0x16f3ca59)\r\n    #15 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16f3ca59)\r\n    #16 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16f3ca59)\r\n    #17 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16f3ca59)\r\n    #18 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16f3ca59)\r\n    #19 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16f3ca59)\r\n    #20 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16f3ca59)\r\n    #21 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16f3ca59)\r\n    #22 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16f3ca59)\r\n    #23 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9c2e06d)\r\n    #24 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9c2e06d)\r\n    #25 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9c2e06d)\r\n    #26 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9c31830)\r\n    #27 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9c31830)\r\n    #28 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9c31830)\r\n    #29 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9c31830)\r\n\r\nSUMMARY: ThreadSanitizer: data race (/usr/bin/clickhouse+0x1b5d42b0) in arrow::internal::ThreadPool::ProtectAgainstFork()\r\n==================\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31423/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31411","id":1053350641,"node_id":"I_kwDOA5dJV84-yNrx","number":31411,"title":"PREWHERE for queries with FINAL","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-15T08:26:14Z","updated_at":"2022-01-26T08:42:54Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Currently, PREWHERE is not too useful for Replacing / Collapsing tables with FINAL\r\nSee https://github.com/ClickHouse/ClickHouse/issues/23702  https://github.com/ClickHouse/ClickHouse/issues/8684 https://github.com/ClickHouse/ClickHouse/pull/24433, etc. \r\n\r\nAt the same time one of the obvious optimizations here is very similar to PREWHERE behavior:\r\n\r\n```sql\r\nSELECT *\r\nFROM replacing_mt_table FINAL\r\nWHERE (pk_col1, pk_col2) IN\r\n(\r\n    SELECT\r\n       pk_col1,\r\n       pk_col2\r\n    FROM replacing_mt_table\r\n    PREWHERE non_pk_col='val'\r\n)\r\nAND non_pk_col='val'\r\n```\r\n\r\nI think it should be possible to make something similar to that rewrite during PREWHERE itself.\r\nI.e. during the filtering of the non_pk_col 'expand' the matching ranges to a full (wider) range of those PK values.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31411/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31401","id":1052807512,"node_id":"I_kwDOA5dJV84-wJFY","number":31401,"title":" Sizes of sort description and actions are mismatched. PROJECTION + ReadInOrder","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""},{"id":3045757785,"node_id":"MDU6TGFiZWwzMDQ1NzU3Nzg1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-projections","name":"comp-projections","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"assignees":[{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-11-14T00:10:48Z","updated_at":"2021-11-14T02:08:21Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe what's wrong**\r\n\r\nQuery doesn't work with projection and aggregation_in_order optimization.\r\n\r\n**Does it reproduce on recent release?**\r\n\r\nClickHouse version 21.12\r\n\r\n**How to reproduce**\r\n\r\n```\r\nCREATE TABLE xxx_tbl\r\n(\r\n    `key` UInt32,\r\n    `ts` DateTime,\r\n    `value` UInt32,\r\n    PROJECTION aaaa\r\n    (\r\n        SELECT\r\n            ts,\r\n            key,\r\n            value\r\n        ORDER BY (ts, key)\r\n    )\r\n)\r\nENGINE = MergeTree\r\nORDER BY (key, ts);\r\n\r\nINSERT INTO xxx_tbl SELECT\r\n    1,\r\n    now() + number,\r\n    number\r\nFROM numbers(10000000);\r\n\r\n set allow_experimental_projection_optimization=1, optimize_aggregation_in_order=1;\r\n\r\nSELECT toStartOfHour(ts) AS a\r\nFROM xxx_tbl\r\nWHERE ts > '2021-12-06 22:00:00'\r\nGROUP BY a\r\nLIMIT 5;\r\n\r\n\r\n0 rows in set. Elapsed: 0.010 sec.\r\n\r\nReceived exception from server (version 21.12.1):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: Sizes of sort description and actions are mismatched. (LOGICAL_ERROR)\r\n```\r\n\r\n**Expected behavior**\r\n\r\nQuery works","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31401/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31388","id":1052655944,"node_id":"I_kwDOA5dJV84-vkFI","number":31388,"title":"Which way is better to etl data of mysql to ClickHosue?","user":{"login":"7omsxx","id":45144402,"node_id":"MDQ6VXNlcjQ1MTQ0NDAy","avatar_url":"https://avatars.githubusercontent.com/u/45144402?v=4","gravatar_id":"","url":"https://api.github.com/users/7omsxx","html_url":"https://github.com/7omsxx","followers_url":"https://api.github.com/users/7omsxx/followers","following_url":"https://api.github.com/users/7omsxx/following{/other_user}","gists_url":"https://api.github.com/users/7omsxx/gists{/gist_id}","starred_url":"https://api.github.com/users/7omsxx/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/7omsxx/subscriptions","organizations_url":"https://api.github.com/users/7omsxx/orgs","repos_url":"https://api.github.com/users/7omsxx/repos","events_url":"https://api.github.com/users/7omsxx/events{/privacy}","received_events_url":"https://api.github.com/users/7omsxx/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-13T11:04:39Z","updated_at":"2021-11-13T15:29:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I'm confused. I don't know which tool to choose to synchronize data to ClickHosue？I was inclined to use MaterializedMysql but it is Experimental stage and Rarely updated. I hope experienced people can recommend your scheme. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31388/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31367","id":1052413750,"node_id":"I_kwDOA5dJV84-uo82","number":31367,"title":"max_insert_block_size=0 makes an insert to insert 0 rows","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-12T21:16:35Z","updated_at":"2021-11-12T22:15:30Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```bash\r\ndrop table if exists trg;\r\ncreate table trg(A Int64, S String) Engine=MergeTree order by A;\r\n\r\nclickhouse-client -q \\\r\n      'select toInt64(number) A, toString(number) S from numbers(1000) format TSV' > t.tsv\r\n\r\nclickhouse-client --max_insert_block_size=0  -q 'insert into trg format TSV' <t.tsv\r\n\r\nselect count() from trg;\r\n┌─count()─┐\r\n│       0 │\r\n└─────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31367/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31363","id":1052196800,"node_id":"I_kwDOA5dJV84-tz_A","number":31363,"title":"Allow to configure FORMAT Template with string in SETTINGS","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":3086255531,"node_id":"MDU6TGFiZWwzMDg2MjU1NTMx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/warmup%20task","name":"warmup task","color":"FBCA04","default":false,"description":"The task for new ClickHouse team members. Low risk, moderate complexity, no urgency."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-12T17:24:49Z","updated_at":"2021-11-12T20:36:54Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nSometimes it is too hard or impossible to deploy format output configuration for template format to directory on all nodes in cluster. Sometimes format is very specific and trivial that it is easier to add it to query.\r\n\r\nCurrently you need to place it to specific directory and pass path to file with template.\r\nhttps://github.com/ClickHouse/ClickHouse/blob/a16eda68dc54ff0c3be7362dfec02061074f6e04/tests/queries/0_stateless/00938_template_input_format.sh#L46-L51\r\n\r\n**Describe the solution you'd like**\r\nSomething like this\r\n``` sql\r\nSELECT\r\n    cluster,\r\n    shard_num,\r\n    replica_num\r\nFROM system.clusters\r\nWHERE host_name = fqdn()\r\nSETTINGS format_schema_rows_template = '|| ${cluster:Quoted} | ${shard_num:Quoted} | ${replica_num:Quoted}||', format_schema_rows_between_delimiter = '\\n'\r\n```\r\n\r\nWe add a separate setting and check that only one is set (filename or string). \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31363/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31361","id":1052120443,"node_id":"I_kwDOA5dJV84-thV7","number":31361,"title":"Logical error: Invalid Field get from type Decimal64 to type Decimal128","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-12T16:04:22Z","updated_at":"2021-11-12T16:04:22Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/7a615a29f4bc7c62c43ef8cff59040c717bde975/fuzzer_debug/report.html#fail1\r\n\r\n```\r\n2021.11.12 13:31:58.791272 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Debug> executeQuery: (from [::1]:35760) SELECT [9223372036854775806, 1048575], [], sumMap(val, [toDateTime64([CAST(1., 'Decimal(10,2)'), CAST(10.000100135803223, 'Decimal(10,2)')], NULL), CAST(-0., 'Decimal(10,2)')], cnt) FROM (SELECT toDateTime64('0.0000001023', [1025, 256], '102.5', NULL), [NULL], [CAST('a', 'FixedString(1)'), CAST('', 'FixedString(1)')] AS val, [1024, 100] AS cnt)\r\n2021.11.12 13:31:58.796081 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> ContextAccess (default): Access granted: SELECT(dummy) ON system.one\r\n2021.11.12 13:31:58.800478 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> ContextAccess (default): Access granted: SELECT(dummy) ON system.one\r\n2021.11.12 13:31:58.813484 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> ContextAccess (default): Access granted: SELECT(dummy) ON system.one\r\n2021.11.12 13:31:58.814235 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2021.11.12 13:31:58.814659 [ 156 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n2021.11.12 13:31:58.820864 [ 381 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> AggregatingTransform: Aggregating\r\n2021.11.12 13:31:58.820937 [ 381 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> Aggregator: Aggregation method: without_key\r\n2021.11.12 13:31:58.821240 [ 381 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Debug> AggregatingTransform: Aggregated. 1 to 1 rows (from 80.00 B) in 0.003779052 sec. (264.617 rows/sec., 20.67 KiB/sec.)\r\n2021.11.12 13:31:58.821317 [ 381 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Trace> Aggregator: Merging aggregated data\r\n2021.11.12 13:31:58.822096 [ 381 ] {cc111396-5fa8-43a5-b527-58c27c537cc9} <Fatal> : Logical error: 'Invalid Field get from type Decimal64 to type Decimal128'.\r\n\r\n2021.11.12 13:31:58.823780 [ 384 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.11.12 13:31:58.824116 [ 384 ] {} <Fatal> BaseDaemon: (version 21.12.1.8724 (official build), build id: 31793282E8B4A3C8206EAE739D382EC943FC7C1E) (from thread 381) (query_id: cc111396-5fa8-43a5-b527-58c27c537cc9) Received signal Aborted (6)\r\n2021.11.12 13:31:58.824577 [ 384 ] {} <Fatal> BaseDaemon: \r\n2021.11.12 13:31:58.824976 [ 384 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fed3aa4d18b 0x7fed3aa2c859 0x15094bb8 0x15094cc2 0x150ff2e6 0x16e54c5a 0x18b6e21d 0x2218fbd5 0x2218fba5 0x16ea8efb 0x21d3d7a4 0x21b4d8e3 0x21b3df46 0x21b3d6c9 0x2310b336 0x23107dbf 0x22de2af9 0x22de2a5f 0x22de29fd 0x22de29bd 0x22de2995 0x22de295d 0x150e1886 0x150e0915 0x22de1405 0x22de1d85 0x22ddfceb 0x22ddefb3 0x22dffa01 0x22dff920 0x22dff89d 0x22dff841 0x22dff752 0x22dff63b 0x22dff4fd 0x22dff4bd 0x22dff495 0x22dff460 0x150e1886 0x150e0915 0x1510c96f\r\n2021.11.12 13:31:58.825433 [ 384 ] {} <Fatal> BaseDaemon: 4. gsignal @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.11.12 13:31:58.825623 [ 384 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.11.12 13:31:58.913977 [ 384 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:51: DB::handle_error_code(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool, std::__1::vector<void*, std::__1::allocator<void*> > const&) @ 0x15094bb8 in /workspace/clickhouse\r\n2021.11.12 13:31:58.986521 [ 384 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Common/Exception.cpp:58: DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x15094cc2 in /workspace/clickhouse\r\n2021.11.12 13:31:59.075298 [ 384 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Common/Exception.h:40: DB::Exception::Exception<DB::Field::Types::Which&, DB::Field::Types::Which const&>(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Field::Types::Which&, DB::Field::Types::Which const&) @ 0x150ff2e6 in /workspace/clickhouse\r\n2021.11.12 13:31:59.913873 [ 384 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Core/Field.h:785: DB::NearestFieldTypeImpl<std::__1::decay<DB::Decimal<wide::integer<128ul, int> > >::type, void>::Type& DB::Field::get<DB::Decimal<wide::integer<128ul, int> > >() @ 0x16e54c5a in /workspace/clickhouse\r\n2021.11.12 13:32:00.633409 [ 384 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Core/Field.h:404: auto const& DB::Field::get<DB::Decimal<wide::integer<128ul, int> > >() const @ 0x18b6e21d in /workspace/clickhouse\r\n2021.11.12 13:32:00.873940 [ 384 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Core/Field.h:819: DB::Decimal<wide::integer<128ul, int> > DB::get<DB::Decimal<wide::integer<128ul, int> > >(DB::Field const&) @ 0x2218fbd5 in /workspace/clickhouse\r\n2021.11.12 13:32:01.065464 [ 384 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Columns/ColumnDecimal.h:111: DB::ColumnDecimal<DB::Decimal<wide::integer<128ul, int> > >::insert(DB::Field const&) @ 0x2218fba5 in /workspace/clickhouse\r\n2021.11.12 13:32:02.049082 [ 384 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/AggregateFunctions/AggregateFunctionSumMap.h:353: DB::AggregateFunctionMapBase<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::AggregateFunctionSumMap<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, false, false>, DB::FieldVisitorSum, false, false, true>::insertResultInto(char*, DB::IColumn&, DB::Arena*) const @ 0x16ea8efb in /workspace/clickhouse\r\n2021.11.12 13:32:03.175084 [ 384 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/Aggregator.cpp:1332: void DB::Aggregator::insertAggregatesIntoColumns<char*>(char*&, std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, DB::Arena*) const @ 0x21d3d7a4 in /workspace/clickhouse\r\n2021.11.12 13:32:04.284175 [ 384 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Interpreters/Aggregator.cpp:0: DB::Aggregator::prepareBlockAndFillWithoutKey(DB::AggregatedDataVariants&, bool, bool) const::$_1::operator()(std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, std::__1::vector<DB::PODArray<char*, 4096ul, Allocator<false, false>, 15ul, 16ul>*, std::__1::allocator<DB::PODArray<char*, 4096ul, Allocator<false, false>, 15ul, 16ul>*> >&, std::__1::vector<COW<DB::IColumn>::mutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::mutable_ptr<DB::IColumn> > >&, bool) const @ 0x21b4d8e3 in /workspace/clickhouse\r\n2021.11.12 13:32:04.971068 [ 384 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Interpreters/Aggregator.cpp:1576: DB::Block DB::Aggregator::prepareBlockAndFill<DB::Aggregator::prepareBlockAndFillWithoutKey(DB::AggregatedDataVariants&, bool, bool) const::$_1&>(DB::AggregatedDataVariants&, bool, unsigned long, DB::Aggregator::prepareBlockAndFillWithoutKey(DB::AggregatedDataVariants&, bool, bool) const::$_1&) const @ 0x21b3df46 in /workspace/clickhouse\r\n2021.11.12 13:32:05.658381 [ 384 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Interpreters/Aggregator.cpp:1678: DB::Aggregator::prepareBlockAndFillWithoutKey(DB::AggregatedDataVariants&, bool, bool) const @ 0x21b3d6c9 in /workspace/clickhouse\r\n2021.11.12 13:32:05.969102 [ 384 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/AggregatingTransform.cpp:339: DB::ConvertingAggregatedToChunksTransform::initialize() @ 0x2310b336 in /workspace/clickhouse\r\n2021.11.12 13:32:06.280675 [ 384 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Processors/Transforms/AggregatingTransform.cpp:174: DB::ConvertingAggregatedToChunksTransform::work() @ 0x23107dbf in /workspace/clickhouse\r\n2021.11.12 13:32:06.510948 [ 384 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88: DB::executeJob(DB::IProcessor*) @ 0x22de2af9 in /workspace/clickhouse\r\n2021.11.12 13:32:06.723554 [ 384 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105: DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const @ 0x22de2a5f in /workspace/clickhouse\r\n2021.11.12 13:32:06.950810 [ 384 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x22de29fd in /workspace/clickhouse\r\n2021.11.12 13:32:07.158747 [ 384 ] {} <Fatal> BaseDaemon: 23. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) @ 0x22de29bd in /workspace/clickhouse\r\n2021.11.12 13:32:07.361668 [ 384 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() @ 0x22de2995 in /workspace/clickhouse\r\n2021.11.12 13:32:07.565669 [ 384 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x22de295d in /workspace/clickhouse\r\n2021.11.12 13:32:07.616187 [ 384 ] {} <Fatal> BaseDaemon: 26. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x150e1886 in /workspace/clickhouse\r\n2021.11.12 13:32:07.665368 [ 384 ] {} <Fatal> BaseDaemon: 27. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x150e0915 in /workspace/clickhouse\r\n2021.11.12 13:32:07.857795 [ 384 ] {} <Fatal> BaseDaemon: 28. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:602: DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x22de1405 in /workspace/clickhouse\r\n2021.11.12 13:32:08.049969 [ 384 ] {} <Fatal> BaseDaemon: 29. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487: DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) @ 0x22de1d85 in /workspace/clickhouse\r\n2021.11.12 13:32:08.220967 [ 384 ] {} <Fatal> BaseDaemon: 30. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:826: DB::PipelineExecutor::executeImpl(unsigned long) @ 0x22ddfceb in /workspace/clickhouse\r\n2021.11.12 13:32:08.427795 [ 384 ] {} <Fatal> BaseDaemon: 31. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:407: DB::PipelineExecutor::execute(unsigned long) @ 0x22ddefb3 in /workspace/clickhouse\r\n2021.11.12 13:32:08.568357 [ 384 ] {} <Fatal> BaseDaemon: 32. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:85: DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) @ 0x22dffa01 in /workspace/clickhouse\r\n2021.11.12 13:32:08.707437 [ 384 ] {} <Fatal> BaseDaemon: 33. ./obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:113: DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const @ 0x22dff920 in /workspace/clickhouse\r\n2021.11.12 13:32:08.847330 [ 384 ] {} <Fatal> BaseDaemon: 34. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682: decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) @ 0x22dff89d in /workspace/clickhouse\r\n2021.11.12 13:32:08.988025 [ 384 ] {} <Fatal> BaseDaemon: 35. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415: decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) @ 0x22dff841 in /workspace/clickhouse\r\n2021.11.12 13:32:09.127436 [ 384 ] {} <Fatal> BaseDaemon: 36. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424: decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) @ 0x22dff752 in /workspace/clickhouse\r\n2021.11.12 13:32:09.251298 [ 384 ] {} <Fatal> BaseDaemon: 37. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188: ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() @ 0x22dff63b in /workspace/clickhouse\r\n2021.11.12 13:32:09.391272 [ 384 ] {} <Fatal> BaseDaemon: 38. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676: decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) @ 0x22dff4fd in /workspace/clickhouse\r\n2021.11.12 13:32:09.531044 [ 384 ] {} <Fatal> BaseDaemon: 39. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:349: void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&...) @ 0x22dff4bd in /workspace/clickhouse\r\n2021.11.12 13:32:09.669984 [ 384 ] {} <Fatal> BaseDaemon: 40. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608: std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() @ 0x22dff495 in /workspace/clickhouse\r\n2021.11.12 13:32:09.808585 [ 384 ] {} <Fatal> BaseDaemon: 41. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089: void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) @ 0x22dff460 in /workspace/clickhouse\r\n2021.11.12 13:32:09.858183 [ 384 ] {} <Fatal> BaseDaemon: 42. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221: std::__1::__function::__policy_func<void ()>::operator()() const @ 0x150e1886 in /workspace/clickhouse\r\n2021.11.12 13:32:09.905166 [ 384 ] {} <Fatal> BaseDaemon: 43. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560: std::__1::function<void ()>::operator()() const @ 0x150e0915 in /workspace/clickhouse\r\n2021.11.12 13:32:09.971151 [ 384 ] {} <Fatal> BaseDaemon: 44. ./obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274: ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x1510c96f in /workspace/clickhouse\r\n2021.11.12 13:32:11.307841 [ 384 ] {} <Fatal> BaseDaemon: Checksum of the binary: 1E8B119750F0B37B2A4085F3613DABCC, integrity check passed.\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31361/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31336","id":1051777807,"node_id":"I_kwDOA5dJV84-sNsP","number":31336,"title":"ALTER TABLE UPDATE with transform and subquery produce 'Empty arrays are illegal in function transform'","user":{"login":"Grian","id":65714,"node_id":"MDQ6VXNlcjY1NzE0","avatar_url":"https://avatars.githubusercontent.com/u/65714?v=4","gravatar_id":"","url":"https://api.github.com/users/Grian","html_url":"https://github.com/Grian","followers_url":"https://api.github.com/users/Grian/followers","following_url":"https://api.github.com/users/Grian/following{/other_user}","gists_url":"https://api.github.com/users/Grian/gists{/gist_id}","starred_url":"https://api.github.com/users/Grian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Grian/subscriptions","organizations_url":"https://api.github.com/users/Grian/orgs","repos_url":"https://api.github.com/users/Grian/repos","events_url":"https://api.github.com/users/Grian/events{/privacy}","received_events_url":"https://api.github.com/users/Grian/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-12T09:51:07Z","updated_at":"2021-11-12T10:51:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Looks as produced in recent version ClickHouse,\r\n\r\n```\r\nClickHouse client version 21.11.3.6 (official build).\r\nConnected to ClickHouse server version 21.11.3 revision 54450.\r\n```\r\n\r\nQueries:\r\n```\r\nDROP TABLE IF EXISTS test_xy;\r\nDROP TABLE IF EXISTS updates;\r\nCREATE TABLE test_xy\r\n(\r\n    `x` Int32,\r\n    `y` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY x;\r\n\r\nCREATE TABLE updates\r\n(\r\n    `x` Int32,\r\n    `y` String\r\n)\r\nENGINE = MergeTree\r\nORDER BY x;\r\n\r\nINSERT INTO test_xy(x, y) VALUES (1, 'a1'), (2, 'a2'), (3, 'a3');\r\nINSERT INTO updates(x, y) VALUES  (2, 'b2'), (3, 'b3');\r\n\r\nSELECT x, y,\r\n    transform(x,\r\n        (select groupArray(x) from (select x, y from updates order by x) t1),\r\n        (select groupArray(y) from (select x, y from updates order by x) t2),\r\n        y)\r\nFROM test_xy\r\nWHERE 1;\r\n\r\nALTER table test_xy\r\n    UPDATE\r\n    y =  transform(x,\r\n        (select groupArray(x) from (select x, y from updates order by x) t1),\r\n        (select groupArray(y) from (select x, y from updates order by x) t2),\r\n        y)\r\n    WHERE 1;\r\n```\r\n\r\nResult:\r\n\r\n```\r\nReceived exception from server (version 21.11.3):\r\nCode: 36. DB::Exception: Received from 127.1:9000. DB::Exception: Empty arrays are illegal in function transform. (BAD_ARGUMENTS)\r\n(query: ALTER table test_xy\r\n    UPDATE \r\n    y =  transform(x,\r\n        (select groupArray(x) from (select x, y from updates order by x) t1),\r\n        (select groupArray(y) from (select x, y from updates order by x) t2),\r\n        y)\r\n    WHERE 1;)\r\n```\r\n\r\nUnexpected to see here error message.\r\n\r\nContext:\r\nOn older ClickHouse\r\n(Connected to ClickHouse server version 21.10.2 revision 54449)\r\n\r\nthis query simply hangs, maybe because some errors in configuration at /etc/clickhouse-server/users.d/default-password.xml ...\r\n\r\n```\r\n---  ClickHouse server version 21.10.2 revision 54449\r\n--- passed 10 minutes after query run\r\nselect is_done, table, count(), min(create_time), max(create_time), now() from system.mutations where 1 group by table, is_done;\r\n```\r\n```\r\n┌─is_done─┬─table───┬─count()─┬────min(create_time)─┬────max(create_time)─┬───────────────now()─┐\r\n│       0 │ test_xy │       1 │ 2021-11-12 13:37:32 │ 2021-11-12 13:37:32 │ 2021-11-12 13:47:44 │\r\n└─────────┴─────────┴─────────┴─────────────────────┴─────────────────────┴─────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31336/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31335","id":1051768639,"node_id":"I_kwDOA5dJV84-sLc_","number":31335,"title":"groupBitmapOr result incorrect","user":{"login":"matthew-lei","id":68599927,"node_id":"MDQ6VXNlcjY4NTk5OTI3","avatar_url":"https://avatars.githubusercontent.com/u/68599927?v=4","gravatar_id":"","url":"https://api.github.com/users/matthew-lei","html_url":"https://github.com/matthew-lei","followers_url":"https://api.github.com/users/matthew-lei/followers","following_url":"https://api.github.com/users/matthew-lei/following{/other_user}","gists_url":"https://api.github.com/users/matthew-lei/gists{/gist_id}","starred_url":"https://api.github.com/users/matthew-lei/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/matthew-lei/subscriptions","organizations_url":"https://api.github.com/users/matthew-lei/orgs","repos_url":"https://api.github.com/users/matthew-lei/repos","events_url":"https://api.github.com/users/matthew-lei/events{/privacy}","received_events_url":"https://api.github.com/users/matthew-lei/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-12T09:40:37Z","updated_at":"2021-11-15T03:09:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\ngroupBitmapOr result 0 but bitmapCardinality record result > 0 in same condition \r\n\r\n**How to reproduce**\r\n```sql\r\nCREATE TABLE ucube.ucube_source_level_tag_uv2 on cluster ucube_ck_cluster\r\n(\r\n    `partition_time` Int64,\r\n    `source` Int64,\r\n    `level` Int64,\r\n    `tag_id` Int64,\r\n    `dimension` Int64,\r\n    `name` String,\r\n    `full_id` String,\r\n    `user_set` Array(UInt32),\r\n    `user_uv` AggregateFunction(groupBitmap, UInt32) MATERIALIZED bitmapBuild(user_set) COMMENT 'userid bitmap'\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY partition_time\r\nPRIMARY KEY (source, level, tag_id, dimension, full_id)\r\nORDER BY (source, level, tag_id, dimension, full_id)\r\nSETTINGS index_granularity = 8192;\r\n-- create distribute table\r\nCREATE TABLE ucube.ucube_source_level_tag_uv2_view on cluster ucube_ck_cluster AS ucube.ucube_source_level_tag_uv2 ENGINE=Distributed(ucube_ck_cluster, ucube, ucube_source_level_tag_uv2, rand());\r\n```\r\nthen insert into tables 20 000 000 records and each record user_set.length < 10000\r\n1. calculate each array cardinality\r\n```sql\r\nselect bitmapCardinality(user_uv) from ucube_source_level_tag_uv2_view where full_id = '7_61_146';\r\n```\r\n136 rows in set and each result > 0\r\n![image](https://user-images.githubusercontent.com/68599927/141444906-0590cf0f-add1-49da-b56f-8f10849c7bef.png)\r\n2. But when i calculate all array cardinality by groupBitmapOr in same condition, it returns 0\r\n```sql\r\nselect groupBitmapOr(user_uv) from ucube_source_level_tag_uv2_view where full_id = '7_61_146';\r\n```\r\n![image](https://user-images.githubusercontent.com/68599927/141445109-2b90f881-4cc4-4ad9-831e-07c16bf2ec67.png)\r\n* Which ClickHouse server version to use\r\n21.9.2.17\r\n\r\n**Expected behavior**\r\ngroupBitmapOr result may approximate to Sum(bitmapCardinality), because element in each record `user_set` array is different in the condition\r\n\r\n**Error message and/or stacktrace**\r\nNo Error\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31335/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31332","id":1051652686,"node_id":"I_kwDOA5dJV84-rvJO","number":31332,"title":"Can clickhouse support Bit-Sliced Index  for better numeric data skipping","user":{"login":"lingtaolf","id":87509571,"node_id":"MDQ6VXNlcjg3NTA5NTcx","avatar_url":"https://avatars.githubusercontent.com/u/87509571?v=4","gravatar_id":"","url":"https://api.github.com/users/lingtaolf","html_url":"https://github.com/lingtaolf","followers_url":"https://api.github.com/users/lingtaolf/followers","following_url":"https://api.github.com/users/lingtaolf/following{/other_user}","gists_url":"https://api.github.com/users/lingtaolf/gists{/gist_id}","starred_url":"https://api.github.com/users/lingtaolf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lingtaolf/subscriptions","organizations_url":"https://api.github.com/users/lingtaolf/orgs","repos_url":"https://api.github.com/users/lingtaolf/repos","events_url":"https://api.github.com/users/lingtaolf/events{/privacy}","received_events_url":"https://api.github.com/users/lingtaolf/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"lingtaolf","id":87509571,"node_id":"MDQ6VXNlcjg3NTA5NTcx","avatar_url":"https://avatars.githubusercontent.com/u/87509571?v=4","gravatar_id":"","url":"https://api.github.com/users/lingtaolf","html_url":"https://github.com/lingtaolf","followers_url":"https://api.github.com/users/lingtaolf/followers","following_url":"https://api.github.com/users/lingtaolf/following{/other_user}","gists_url":"https://api.github.com/users/lingtaolf/gists{/gist_id}","starred_url":"https://api.github.com/users/lingtaolf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lingtaolf/subscriptions","organizations_url":"https://api.github.com/users/lingtaolf/orgs","repos_url":"https://api.github.com/users/lingtaolf/repos","events_url":"https://api.github.com/users/lingtaolf/events{/privacy}","received_events_url":"https://api.github.com/users/lingtaolf/received_events","type":"User","site_admin":false},"assignees":[{"login":"lingtaolf","id":87509571,"node_id":"MDQ6VXNlcjg3NTA5NTcx","avatar_url":"https://avatars.githubusercontent.com/u/87509571?v=4","gravatar_id":"","url":"https://api.github.com/users/lingtaolf","html_url":"https://github.com/lingtaolf","followers_url":"https://api.github.com/users/lingtaolf/followers","following_url":"https://api.github.com/users/lingtaolf/following{/other_user}","gists_url":"https://api.github.com/users/lingtaolf/gists{/gist_id}","starred_url":"https://api.github.com/users/lingtaolf/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lingtaolf/subscriptions","organizations_url":"https://api.github.com/users/lingtaolf/orgs","repos_url":"https://api.github.com/users/lingtaolf/repos","events_url":"https://api.github.com/users/lingtaolf/events{/privacy}","received_events_url":"https://api.github.com/users/lingtaolf/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-11-12T07:13:31Z","updated_at":"2021-11-15T07:46:39Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Clickhouse `minmax` index can't skip data well while records stored unordered. For example, the minimum and maximum value always appear  every `GRANULARITY` lines, in this scenario, clickhouse will not skip any data when you use the index key to filter data.    \r\n  \r\nAs far as I know, `Bit-Sliced Index` is the index which can use bitmap to locate numeric data. So I think clickhouse can use this index for better numeric data skipping. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31332/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31307","id":1051189411,"node_id":"I_kwDOA5dJV84-p-Cj","number":31307,"title":"KafkaEngine: Null values are treated as default","user":{"login":"ADovgalyuk","id":56922689,"node_id":"MDQ6VXNlcjU2OTIyNjg5","avatar_url":"https://avatars.githubusercontent.com/u/56922689?v=4","gravatar_id":"","url":"https://api.github.com/users/ADovgalyuk","html_url":"https://github.com/ADovgalyuk","followers_url":"https://api.github.com/users/ADovgalyuk/followers","following_url":"https://api.github.com/users/ADovgalyuk/following{/other_user}","gists_url":"https://api.github.com/users/ADovgalyuk/gists{/gist_id}","starred_url":"https://api.github.com/users/ADovgalyuk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ADovgalyuk/subscriptions","organizations_url":"https://api.github.com/users/ADovgalyuk/orgs","repos_url":"https://api.github.com/users/ADovgalyuk/repos","events_url":"https://api.github.com/users/ADovgalyuk/events{/privacy}","received_events_url":"https://api.github.com/users/ADovgalyuk/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-11T17:08:47Z","updated_at":"2021-12-21T12:41:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Clickhouse version:\r\n```\r\n[root@in_monitoring bin]# rpm -qa | grep clickhouse\r\nclickhouse-server-21.11.3.6-2.noarch\r\ngraphite-clickhouse-0.11.7-1.x86_64\r\nclickhouse-common-static-21.11.3.6-2.x86_64\r\nclickhouse-client-21.11.3.6-2.noarch\r\n```\r\nKafka_engine_table:\r\n```\r\nCREATE TABLE statistics.test_null_kafka\r\n(\r\n    `Status` Nullable(Int16),\r\n    `Cause` Nullable(Int16)\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = '192.168.126.251:9092', kafka_topic_list = 'test_null_kafka', kafka_group_name = 'clickhouse_prod', kafka_format = 'JSONEachRow'\r\n```\r\nData in kafka itself:\r\n```\r\n[root@in_monitoring bin]# ./kafka-console-producer.sh --broker-list 192.168.126.251:9092 --topic test_null_kafka\r\n>{\"Status\":\"-1\",\"Cause\":\"\"} \r\n```\r\nRequest to get values from kafka_table. I know that's incorrect thing to do, just in testing purposes only:\r\n```\r\nlocalhost :) select * from test_null_kafka;\r\n\r\nSELECT *\r\nFROM test_null_kafka\r\n\r\nQuery id: 46307158-cff1-496f-96f5-30bd9f91afce\r\n\r\n┌─Status─┬─Cause─┐\r\n│     -1 │     0 │\r\n└────────┴───────┘\r\n```\r\n\r\nWhat was expected to have in kafka table\r\n```\r\n┌─Status─┬─Cause─┐\r\n│     -1 │  NULL │\r\n└────────┴───────┘\r\n```\r\n\r\nThere are couple setting:\r\n1. **input_format_null_as_default**\r\nBut according to description \"If column type is nullable, then NULL values are inserted as is, regardless of this setting.\"\r\n\r\n2. **insert_null_as_default**\r\nBut again: \"If column type is nullable, then NULL values are inserted as is, regardless of this setting.\"\r\n\r\nI'm not sure is it a bug or some configuration tweaks needed, which are not obvious. \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31307/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31298","id":1050996077,"node_id":"I_kwDOA5dJV84-pO1t","number":31298,"title":"DB::Exception: WRITE locking attempt on \"xxx\" has timed out! (120000ms) Possible deadlock avoided","user":{"login":"orloffv","id":504986,"node_id":"MDQ6VXNlcjUwNDk4Ng==","avatar_url":"https://avatars.githubusercontent.com/u/504986?v=4","gravatar_id":"","url":"https://api.github.com/users/orloffv","html_url":"https://github.com/orloffv","followers_url":"https://api.github.com/users/orloffv/followers","following_url":"https://api.github.com/users/orloffv/following{/other_user}","gists_url":"https://api.github.com/users/orloffv/gists{/gist_id}","starred_url":"https://api.github.com/users/orloffv/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/orloffv/subscriptions","organizations_url":"https://api.github.com/users/orloffv/orgs","repos_url":"https://api.github.com/users/orloffv/repos","events_url":"https://api.github.com/users/orloffv/events{/privacy}","received_events_url":"https://api.github.com/users/orloffv/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false},"assignees":[{"login":"nikitamikhaylov","id":25705399,"node_id":"MDQ6VXNlcjI1NzA1Mzk5","avatar_url":"https://avatars.githubusercontent.com/u/25705399?v=4","gravatar_id":"","url":"https://api.github.com/users/nikitamikhaylov","html_url":"https://github.com/nikitamikhaylov","followers_url":"https://api.github.com/users/nikitamikhaylov/followers","following_url":"https://api.github.com/users/nikitamikhaylov/following{/other_user}","gists_url":"https://api.github.com/users/nikitamikhaylov/gists{/gist_id}","starred_url":"https://api.github.com/users/nikitamikhaylov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nikitamikhaylov/subscriptions","organizations_url":"https://api.github.com/users/nikitamikhaylov/orgs","repos_url":"https://api.github.com/users/nikitamikhaylov/repos","events_url":"https://api.github.com/users/nikitamikhaylov/events{/privacy}","received_events_url":"https://api.github.com/users/nikitamikhaylov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-11-11T13:45:43Z","updated_at":"2021-11-11T14:58:16Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Clickhouse server logs: https://nda.ya.ru/t/qZVim3JC4TQAcb \r\nClickhouse copier logs: https://nda.ya.ru/t/qhV9wwz14TQ9wX\r\nWhen I try to copy my old cluster to new cluster\r\nThree hosts in cluster, two hosts work correctly, but one host send errors.\r\nIn clickhouse copier log i see\r\n`\r\n2021.11.10 20:54:49.397379 [ 279995 ] {} <Warning> Application: An error occurred while processing query : \r\n  ALTER TABLE xxx ATTACH PARTITION 20211106 FROM xxx_piece_5\r\n2021.11.10 20:54:49.397499 [ 279995 ] {} <Error> Application: Code: 473. DB::Exception: Received from sas-r782nt7qad8udmma.db.yandex.net:9440. DB::Exception: WRITE locking attempt on \"xxx\" has timed out! (120000ms) Possible deadlock avoided. Client should retry.. Stack trace:\r\n`\r\n\r\n<details><summary>Stack</summary>\r\n<p>\r\n\r\n```\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x944bdda in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n1. DB::IStorage::tryLockTimed(std::__1::shared_ptr<DB::RWLockImpl> const&, DB::RWLockImpl::Type, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> > const&) const @ 0x11248467 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n2. DB::IStorage::lockForAlter(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000l> > const&) @ 0x11248a02 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n3. DB::InterpreterAlterQuery::execute() @ 0x10a3b568 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n4. DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) @ 0x110732e1 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n5. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, bool) @ 0x110711a3 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n6. DB::TCPHandler::runImpl() @ 0x1195ea24 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n7. DB::TCPHandler::run() @ 0x11971b19 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n8. Poco::Net::TCPServerConnection::start() @ 0x1453bf2f in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n9. Poco::Net::TCPServerDispatcher::run() @ 0x1453d9ba in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n10. Poco::PooledThread::run() @ 0x1466fdf9 in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n11. Poco::ThreadImpl::runnableEntry(void*) @ 0x1466c08a in /usr/lib/debug/.build-id/66/99b86599a2121e78e0d42dd67791abd9ae5265.debug\r\n12. start_thread @ 0x76db in /lib/x86_64-linux-gnu/libpthread-2.27.so\r\n13. __clone @ 0x12171f in /lib/x86_64-linux-gnu/libc-2.27.so\r\n. (DEADLOCK_AVOIDED), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x91d6e7a in /clickhouse-copier/clickhouse-with-new-params\r\n1. DB::readException(DB::ReadBuffer&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, bool) @ 0x92372b4 in /clickhouse-copier/clickhouse-with-new-params\r\n2. DB::Connection::receiveException() const @ 0x112bd9e2 in /clickhouse-copier/clickhouse-with-new-params\r\n3. DB::Connection::receivePacket() @ 0x112c7729 in /clickhouse-copier/clickhouse-with-new-params\r\n4. DB::MultiplexedConnections::receivePacketUnlocked(std::__1::function<void (int, Poco::Timespan, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>, bool) @ 0x112ed4c2 in /clickhouse-copier/clickhouse-with-new-params\r\n5. DB::MultiplexedConnections::receivePacket() @ 0x112ed120 in /clickhouse-copier/clickhouse-with-new-params\r\n6. DB::RemoteQueryExecutor::read() @ 0x100f1829 in /clickhouse-copier/clickhouse-with-new-params\r\n7. DB::ClusterCopier::executeQueryOnCluster(std::__1::shared_ptr<DB::Cluster> const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Settings const&, DB::ClusterCopier::ClusterExecutionMode) const @ 0x9341de2 in /clickhouse-copier/clickhouse-with-new-params\r\n8. DB::ClusterCopier::tryMoveAllPiecesToDestinationTable(DB::TaskTable const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x933e212 in /clickhouse-copier/clickhouse-with-new-params\r\n9. DB::ClusterCopier::tryProcessTable(DB::ConnectionTimeouts const&, DB::TaskTable&) @ 0x9334a20 in /clickhouse-copier/clickhouse-with-new-params\r\n10. DB::ClusterCopier::process(DB::ConnectionTimeouts const&) @ 0x9331983 in /clickhouse-copier/clickhouse-with-new-params\r\n11. DB::ClusterCopierApp::mainImpl() @ 0x9322e5d in /clickhouse-copier/clickhouse-with-new-params\r\n12. DB::ClusterCopierApp::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x9325d5d in /clickhouse-copier/clickhouse-with-new-params\r\n13. Poco::Util::Application::run() @ 0x13f687a3 in /clickhouse-copier/clickhouse-with-new-params\r\n14. mainEntryClickHouseClusterCopier(int, char**) @ 0x9326006 in /clickhouse-copier/clickhouse-with-new-params\r\n15. main @ 0x91d1ade in /clickhouse-copier/clickhouse-with-new-params\r\n16. __libc_start_main @ 0x21bf7 in /lib/x86_64-linux-gnu/libc-2.27.so\r\n17. _start @ 0x919a1ae in /clickhouse-copier/clickhouse-with-new-params\r\n (version 21.9.1.7735)\r\n\r\n\r\n```\r\n\r\n</p>\r\n</details>\r\n\r\n<img width=\"1190\" alt=\"Screenshot 2021-11-11 at 16 38 26\" src=\"https://user-images.githubusercontent.com/504986/141307837-a74babfd-89f2-4bd7-85bb-aac99e80f622.png\">\r\n\r\nClickHouse version is 21.10.2.15","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31298/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31293","id":1050828346,"node_id":"I_kwDOA5dJV84-ol46","number":31293,"title":"Data race in CompletedPipelineExecutor","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-11T10:30:05Z","updated_at":"2021-12-08T14:47:06Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/17ffd0e17b6e85696ba06f6143b1fc02237c2316/stress_test_(thread).html#fail1\r\n\r\n```\r\nWARNING: ThreadSanitizer: data race (pid=495)\r\n  Read of size 8 at 0x7f623b1711d0 by thread T566:\r\n    #0 std::__1::unique_ptr<DB::CompletedPipelineExecutor::Data, std::__1::default_delete<DB::CompletedPipelineExecutor::Data> >::operator*() const obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1623:20 (clickhouse+0x16f1818d)\r\n    #1 DB::CompletedPipelineExecutor::execute()::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/CompletedPipelineExecutor.cpp:81:28 (clickhouse+0x16f1818d)\r\n    #2 decltype(std::__1::forward<DB::CompletedPipelineExecutor::execute()::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::CompletedPipelineExecutor::execute()::$_0&>(DB::CompletedPipelineExecutor::execute()::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x16f1818d)\r\n    #3 decltype(auto) std::__1::__apply_tuple_impl<DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&>(DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x16f1818d)\r\n    #4 decltype(auto) std::__1::apply<DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&>(DB::CompletedPipelineExecutor::execute()::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x16f1818d)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:188:13 (clickhouse+0x16f1818d)\r\n    #6 decltype(std::__1::forward<DB::CompletedPipelineExecutor::execute()::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'()&>(DB::CompletedPipelineExecutor::execute()::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16f1818d)\r\n    #7 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'()&>(ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'()&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16f1818d)\r\n    #8 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16f1818d)\r\n    #9 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::CompletedPipelineExecutor::execute()::$_0>(DB::CompletedPipelineExecutor::execute()::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16f1818d)\r\n    #10 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x9c2cc0d)\r\n    #11 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x9c2cc0d)\r\n    #12 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:274:17 (clickhouse+0x9c2cc0d)\r\n    #13 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:73 (clickhouse+0x9c303d0)\r\n    #14 decltype(std::__1::forward<void>(fp)()) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x9c303d0)\r\n    #15 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x9c303d0)\r\n    #16 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x9c303d0)\r\n\r\n  Previous write of size 8 at 0x7f623b1711d0 by thread T531:\r\n    #0 std::__1::unique_ptr<DB::CompletedPipelineExecutor::Data, std::__1::default_delete<DB::CompletedPipelineExecutor::Data> >::reset(DB::CompletedPipelineExecutor::Data*) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1656:20 (clickhouse+0x16f17e18)\r\n    #1 std::__1::unique_ptr<DB::CompletedPipelineExecutor::Data, std::__1::default_delete<DB::CompletedPipelineExecutor::Data> >::~unique_ptr() obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1612:19 (clickhouse+0x16f17e18)\r\n    #2 DB::CompletedPipelineExecutor::~CompletedPipelineExecutor() obj-x86_64-linux-gnu/../src/Processors/Executors/CompletedPipelineExecutor.cpp:113:1 (clickhouse+0x16f17e18)\r\n    #3 DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:355:13 (clickhouse+0x16eaa1f8)\r\n    #4 DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1844:9 (clickhouse+0x16ebdaa7)\r\n    #5 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a2f5b02)\r\n    #6 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a2f6312)\r\n    #7 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a47a255)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a47842f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a476b07)\r\n\r\n  Location is stack of thread T531.\r\n\r\n  Thread T566 'Formatter' (tid=1795, running) created by thread T333 at:\r\n    #0 pthread_create <null> (clickhouse+0x9b44f5d)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x9c2fe46)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'(), void>(void&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x9c2fe46)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9c2b274)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:171:5 (clickhouse+0x9c315c5)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda0'()>(void&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:169:38 (clickhouse+0x9c315c5)\r\n    #6 void ThreadPoolImpl<ThreadFromGlobalPool>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:139:35 (clickhouse+0x9c2d806)\r\n    #7 ThreadPoolImpl<ThreadFromGlobalPool>::scheduleOrThrowOnError(std::__1::function<void ()>, int) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:159:5 (clickhouse+0x9c2d3df)\r\n    #8 DB::ParallelFormattingOutputFormat::scheduleFormatterThreadForUnitWithNumber(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h:207:14 (clickhouse+0x16ff91cd)\r\n    #9 DB::ParallelFormattingOutputFormat::addChunk(DB::Chunk, DB::ParallelFormattingOutputFormat::ProcessingUnitType, bool) obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ParallelFormattingOutputFormat.cpp:57:9 (clickhouse+0x16ff7fbb)\r\n    #10 DB::ParallelFormattingOutputFormat::consume(DB::Chunk) obj-x86_64-linux-gnu/../src/Processors/Formats/Impl/ParallelFormattingOutputFormat.h:120:9 (clickhouse+0x16ff96e7)\r\n    #11 DB::IOutputFormat::work() obj-x86_64-linux-gnu/../src/Processors/Formats/IOutputFormat.cpp:89:13 (clickhouse+0x16f377d8)\r\n    #12 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:88:20 (clickhouse+0x16f1ffa1)\r\n    #13 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:105:13 (clickhouse+0x16f1ffa1)\r\n    #14 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x16f1ffa1)\r\n    #15 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x16f1ffa1)\r\n    #16 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x16f1ffa1)\r\n    #17 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x16f1ffa1)\r\n    #18 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x16f1df09)\r\n    #19 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x16f1df09)\r\n    #20 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:599:17 (clickhouse+0x16f1df09)\r\n    #21 DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:487:5 (clickhouse+0x16f1bec3)\r\n    #22 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:826:9 (clickhouse+0x16f1bec3)\r\n    #23 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:407:9 (clickhouse+0x16f1b736)\r\n    #24 DB::CompletedPipelineExecutor::execute() obj-x86_64-linux-gnu/../src/Processors/Executors/CompletedPipelineExecutor.cpp:99:18 (clickhouse+0x16f17b62)\r\n    #25 DB::executeQuery(DB::ReadBuffer&, DB::WriteBuffer&, bool, std::__1::shared_ptr<DB::Context>, std::__1::function<void (std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)>, std::__1::optional<DB::FormatSettings> const&) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:1090:22 (clickhouse+0x163610a7)\r\n    #26 DB::HTTPHandler::processQuery(DB::HTTPServerRequest&, DB::HTMLForm&, DB::HTTPServerResponse&, DB::HTTPHandler::Output&, std::__1::optional<DB::CurrentThread::QueryScope>&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:785:5 (clickhouse+0x16e56868)\r\n    #27 DB::HTTPHandler::handleRequest(DB::HTTPServerRequest&, DB::HTTPServerResponse&) obj-x86_64-linux-gnu/../src/Server/HTTPHandler.cpp:923:9 (clickhouse+0x16e5aa98)\r\n    #28 DB::HTTPServerConnection::run() obj-x86_64-linux-gnu/../src/Server/HTTP/HTTPServerConnection.cpp:58:34 (clickhouse+0x16ec7531)\r\n    #29 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x1a2f5b02)\r\n    #30 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20 (clickhouse+0x1a2f6312)\r\n    #31 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x1a47a255)\r\n    #32 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a47842f)\r\n    #33 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a476b07)\r\n\r\n  Thread T531 'TCPHandler' (tid=1638, running) created by thread T248 at:\r\n    #0 pthread_create <null> (clickhouse+0x9b44f5d)\r\n    #1 Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6 (clickhouse+0x1a476598)\r\n    #2 Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2 (clickhouse+0x1a477d6c)\r\n    #3 Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10 (clickhouse+0x1a47c046)\r\n    #4 Poco::ThreadPool::getThread() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:461:14 (clickhouse+0x1a47c046)\r\n    #5 Poco::ThreadPool::startWithPriority(Poco::Thread::Priority, Poco::Runnable&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:365:2 (clickhouse+0x1a47c427)\r\n    #6 Poco::Net::TCPServerDispatcher::enqueue(Poco::Net::StreamSocket const&) obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:152:17 (clickhouse+0x1a2f67ee)\r\n    #7 Poco::Net::TCPServer::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServer.cpp:148:21 (clickhouse+0x1a2f5387)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x1a47842f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x1a476b07)\r\n\r\nSUMMARY: ThreadSanitizer: data race obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1623:20 in std::__1::unique_ptr<DB::CompletedPipelineExecutor::Data, std::__1::default_delete<DB::CompletedPipelineExecutor::Data> >::operator*() const\r\n==================\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31293/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31284","id":1050600280,"node_id":"I_kwDOA5dJV84-nuNY","number":31284,"title":"Why CH still report 'Table columns structure in ZooKeeper is different from local table structure' even if the columns are the same?","user":{"login":"fsgdgsdfgsgd","id":29115270,"node_id":"MDQ6VXNlcjI5MTE1Mjcw","avatar_url":"https://avatars.githubusercontent.com/u/29115270?v=4","gravatar_id":"","url":"https://api.github.com/users/fsgdgsdfgsgd","html_url":"https://github.com/fsgdgsdfgsgd","followers_url":"https://api.github.com/users/fsgdgsdfgsgd/followers","following_url":"https://api.github.com/users/fsgdgsdfgsgd/following{/other_user}","gists_url":"https://api.github.com/users/fsgdgsdfgsgd/gists{/gist_id}","starred_url":"https://api.github.com/users/fsgdgsdfgsgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/fsgdgsdfgsgd/subscriptions","organizations_url":"https://api.github.com/users/fsgdgsdfgsgd/orgs","repos_url":"https://api.github.com/users/fsgdgsdfgsgd/repos","events_url":"https://api.github.com/users/fsgdgsdfgsgd/events{/privacy}","received_events_url":"https://api.github.com/users/fsgdgsdfgsgd/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2021-11-11T05:52:24Z","updated_at":"2021-11-12T02:40:03Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi All,\r\nI tried to re-create a table with replicated merge tree engine. But I found that CH always report 'Table columns structure in ZooKeeper is different from local table structure'. I compared the data in this ZK node: '/{ReplicatedMergeTree engine path}/columns' with the columns of my table. It looks like the columns are the same. But CH still reported this exception. Is there any way to let me know which column is different?\r\nI also have a replica node. I tried to use the create table query that select from system.tables table of this node. But it also report this exception. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31284/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31281","id":1050520865,"node_id":"I_kwDOA5dJV84-na0h","number":31281,"title":" with clause use  constant can not filter","user":{"login":"zuoym","id":10591266,"node_id":"MDQ6VXNlcjEwNTkxMjY2","avatar_url":"https://avatars.githubusercontent.com/u/10591266?v=4","gravatar_id":"","url":"https://api.github.com/users/zuoym","html_url":"https://github.com/zuoym","followers_url":"https://api.github.com/users/zuoym/followers","following_url":"https://api.github.com/users/zuoym/following{/other_user}","gists_url":"https://api.github.com/users/zuoym/gists{/gist_id}","starred_url":"https://api.github.com/users/zuoym/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zuoym/subscriptions","organizations_url":"https://api.github.com/users/zuoym/orgs","repos_url":"https://api.github.com/users/zuoym/repos","events_url":"https://api.github.com/users/zuoym/events{/privacy}","received_events_url":"https://api.github.com/users/zuoym/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"assignees":[{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-11-11T02:59:50Z","updated_at":"2021-11-11T13:43:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n\r\nClickHouse server version : version 21.8.10.19 (official build)\r\n\r\nEMPLOYEES table desc\r\n\r\nEMPLOYEE_ID\tInt64\r\nFIRST_NAME\tString\r\nLAST_NAME\tNullable(String)\r\nEMAIL\tNullable(String)\r\nPHONE_NUMBER\tString\r\nHIRE_DATE\tDateTime\r\nJOB_ID\tNullable(String)\r\nSALARY\tDecimal(8, 2)\r\nCOMMISSION_PCT\tDecimal(2, 2)\r\nMANAGER_ID\tInt64\r\nDEPARTMENT_ID\tInt64\r\n\r\ndata ：\r\n128\tSteven\tMarkle\tSMARKLE\t650.124.1434\t2008-03-08 00:00:00\tST_CLERK\t2200.00\t0.00\t120\t50\r\n136\tHazel\tPhiltanker\tHPHILTAN\t650.127.1634\t2008-02-06 00:00:00\tST_CLERK\t2200.00\t0.00\t122\t50\r\n149\tEleni\tZlotkey\tEZLOTKEY\t011.44.1344.429018\t2008-01-29 00:00:00\tSA_MAN\t10500.00\t0.20\t100\t80\r\n164\tMattea\tMarvins\tMMARVINS\t011.44.1346.329268\t2008-01-24 00:00:00\tSA_REP\t7200.00\t0.10\t147\t80\r\n165\tDavid\tLee\tDLEE\t011.44.1346.529268\t2008-02-23 00:00:00\tSA_REP\t6800.00\t0.10\t147\t80\r\n166\tSundar\tAnde\tSANDE\t011.44.1346.629268\t2008-03-24 00:00:00\tSA_REP\t6400.00\t0.10\t147\t80\r\n167\tAmit\tBanda\tABANDA\t011.44.1346.729268\t2008-04-21 00:00:00\tSA_REP\t6200.00\t0.10\t147\t80\r\n173\tSundita\tKumar\tSKUMAR\t011.44.1343.329268\t2008-04-21 00:00:00\tSA_REP\t6100.00\t0.10\t148\t80\r\n179\tCharles\tJohnson\tCJOHNSON\t011.44.1644.429262\t2008-01-04 00:00:00\tSA_REP\t6200.00\t0.10\t149\t80\r\n183\tGirard\tGeoni\tGGEONI\t650.507.9879\t2008-02-03 00:00:00\tSH_CLERK\t2800.00\t0.00\t120\t50\r\n199\tDouglas\tGrant\tDGRANT\t650.507.9844\t2008-01-13 00:00:00\tSH_CLERK\t2600.00\t0.00\t124\t50\r\n100\tSteven\tKing\tSKING\t515.123.4567\t2003-06-17 00:00:00\tAD_PRES\t24000.00\t0.00\t0\t90\r\n115\tAlexander\tKhoo\tAKHOO\t515.127.4562\t2003-05-18 00:00:00\tPU_CLERK\t3100.00\t0.00\t114\t30\r\n\r\nquery 1 using sql：\r\nwith catg0 as (\r\nselect EMAIL from EMPLOYEES where SALARY >= 3000\r\n\r\n) ,\r\n catg1 as (\r\nselect EMAIL from EMPLOYEES where SALARY < 3000\r\n) \r\n ,\r\n catg2 as (\r\nselect EMAIL,1 RICH  from catg0\r\nUNION ALL\r\nselect EMAIL,0 RICH  from catg1\r\n) \r\nselect EMAIL, RICH from catg2\r\n**this is OK**\r\n\r\n\r\n\r\nquery 2 using sql ：\r\n\r\nwith catg0 as (\r\nselect EMAIL from EMPLOYEES where SALARY >= 3000\r\n\r\n) ,\r\n catg1 as (\r\nselect EMAIL from EMPLOYEES where SALARY < 3000\r\n) \r\n ,\r\n catg2 as (\r\nselect EMAIL,1 RICH  from catg0\r\nUNION ALL\r\nselect EMAIL,0 RICH  from catg1\r\n) \r\nselect * from catg2 where rich =1\r\n\r\n**produces an error**\r\nerror message：\r\n[Code: 47, SQL State: ]  ClickHouse exception, code: 47, host: 172.16.75.6, port: 8123; Code: 47, e.displayText() = DB::Exception: Missing columns: 'rich' while processing query: 'WITH catg0 AS (SELECT EMAIL FROM EMPLOYEES WHERE SALARY >= 3000), catg1 AS (SELECT EMAIL FROM EMPLOYEES WHERE SALARY < 3000), catg2 AS (SELECT EMAIL, 1 AS RICH FROM catg0 UNION ALL SELECT EMAIL, 0 AS RICH FROM catg1) SELECT EMAIL, RICH FROM catg2 WHERE rich = 1', required columns: 'EMAIL' 'RICH' 'rich' 'EMAIL' 'RICH' 'rich' (version 21.8.10.19 (official build))\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31281/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31264","id":1050380360,"node_id":"I_kwDOA5dJV84-m4hI","number":31264,"title":"Defining volume_priority expliclty in storage_configuration","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-10T22:44:13Z","updated_at":"2021-11-10T22:46:21Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Now a volume_priority is defined by an order in XML. \r\nThis is unorthodox way because an order in XML should not have sense.\r\n\r\nSo please implement `volume_priority`\r\n\r\n```\r\n<storage_configuration>\r\n    ...\r\n    <policies>\r\n        <moving_from_ssd_to_hdd>\r\n            <volumes>\r\n                <hot>\r\n\r\n                    <volume_priority>1</volume_priority>\r\n\r\n                    <disk>fast_ssd1</disk>\r\n                    <disk>fast_ssd2</disk>\r\n                    <max_data_part_size_bytes>1073741824</max_data_part_size_bytes>\r\n                </hot>\r\n                <cold>\r\n\r\n                    <volume_priority>2</volume_priority>\r\n\r\n                    <disk>disk1</disk>\r\n                </cold>\r\n            </volumes>\r\n            <move_factor>0.2</move_factor>\r\n        </moving_from_ssd_to_hdd>\r\n...\r\n</storage_configuration>\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31264/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31257","id":1050080257,"node_id":"PR_kwDOA5dJV84uXGuy","number":31257,"title":"Merge #15765","user":{"login":"FArthur-cmd","id":58165623,"node_id":"MDQ6VXNlcjU4MTY1NjIz","avatar_url":"https://avatars.githubusercontent.com/u/58165623?v=4","gravatar_id":"","url":"https://api.github.com/users/FArthur-cmd","html_url":"https://github.com/FArthur-cmd","followers_url":"https://api.github.com/users/FArthur-cmd/followers","following_url":"https://api.github.com/users/FArthur-cmd/following{/other_user}","gists_url":"https://api.github.com/users/FArthur-cmd/gists{/gist_id}","starred_url":"https://api.github.com/users/FArthur-cmd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/FArthur-cmd/subscriptions","organizations_url":"https://api.github.com/users/FArthur-cmd/orgs","repos_url":"https://api.github.com/users/FArthur-cmd/repos","events_url":"https://api.github.com/users/FArthur-cmd/events{/privacy}","received_events_url":"https://api.github.com/users/FArthur-cmd/received_events","type":"User","site_admin":false},"labels":[{"id":1304141686,"node_id":"MDU6TGFiZWwxMzA0MTQxNjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-improvement","name":"pr-improvement","color":"007700","default":false,"description":"Pull request with some product improvements"},{"id":2107435505,"node_id":"MDU6TGFiZWwyMTA3NDM1NTA1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/submodule%20changed","name":"submodule changed","color":"b7130b","default":false,"description":"At least one submodule changed in this PR."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-10T16:58:57Z","updated_at":"2022-01-28T17:46:00Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/31257","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31257","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/31257.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/31257.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- Improvement\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nMerge #15765 (Dynamic reload of server TLS certificates on config reload)\r\ncc @johnskopis\r\n\r\nDetailed description / Documentation draft:\r\nMerge #15765","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31257/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31245","id":1049791268,"node_id":"I_kwDOA5dJV84-kosk","number":31245,"title":"multi thread write same batch data to clickhouse ReplicatedMergeTree, will data duplicate?","user":{"login":"lifulong","id":450539,"node_id":"MDQ6VXNlcjQ1MDUzOQ==","avatar_url":"https://avatars.githubusercontent.com/u/450539?v=4","gravatar_id":"","url":"https://api.github.com/users/lifulong","html_url":"https://github.com/lifulong","followers_url":"https://api.github.com/users/lifulong/followers","following_url":"https://api.github.com/users/lifulong/following{/other_user}","gists_url":"https://api.github.com/users/lifulong/gists{/gist_id}","starred_url":"https://api.github.com/users/lifulong/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lifulong/subscriptions","organizations_url":"https://api.github.com/users/lifulong/orgs","repos_url":"https://api.github.com/users/lifulong/repos","events_url":"https://api.github.com/users/lifulong/events{/privacy}","received_events_url":"https://api.github.com/users/lifulong/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-10T12:59:21Z","updated_at":"2021-11-11T02:11:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> we need to write data to clickhouse exectly once, want to use  ReplicatedMergeTree to achieve this goal, but worry about data duplicate while do retry in some case\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31245/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31230","id":1049596484,"node_id":"I_kwDOA5dJV84-j5JE","number":31230,"title":"修改mysql库引擎连接地址","user":{"login":"JiangTaoShi","id":38731433,"node_id":"MDQ6VXNlcjM4NzMxNDMz","avatar_url":"https://avatars.githubusercontent.com/u/38731433?v=4","gravatar_id":"","url":"https://api.github.com/users/JiangTaoShi","html_url":"https://github.com/JiangTaoShi","followers_url":"https://api.github.com/users/JiangTaoShi/followers","following_url":"https://api.github.com/users/JiangTaoShi/following{/other_user}","gists_url":"https://api.github.com/users/JiangTaoShi/gists{/gist_id}","starred_url":"https://api.github.com/users/JiangTaoShi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/JiangTaoShi/subscriptions","organizations_url":"https://api.github.com/users/JiangTaoShi/orgs","repos_url":"https://api.github.com/users/JiangTaoShi/repos","events_url":"https://api.github.com/users/JiangTaoShi/events{/privacy}","received_events_url":"https://api.github.com/users/JiangTaoShi/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-10T09:34:14Z","updated_at":"2021-11-11T08:05:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"用mysql库引擎 影射了几张表，现在突然发现走外网连接地址一直超时，修改引擎连接为内网也修改不了，删除此数据表一直超时。","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31230/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31222","id":1049444265,"node_id":"I_kwDOA5dJV84-jT-p","number":31222,"title":"How to monitor the MaterializedMySQL synchronization status???","user":{"login":"hcymysql","id":19261879,"node_id":"MDQ6VXNlcjE5MjYxODc5","avatar_url":"https://avatars.githubusercontent.com/u/19261879?v=4","gravatar_id":"","url":"https://api.github.com/users/hcymysql","html_url":"https://github.com/hcymysql","followers_url":"https://api.github.com/users/hcymysql/followers","following_url":"https://api.github.com/users/hcymysql/following{/other_user}","gists_url":"https://api.github.com/users/hcymysql/gists{/gist_id}","starred_url":"https://api.github.com/users/hcymysql/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hcymysql/subscriptions","organizations_url":"https://api.github.com/users/hcymysql/orgs","repos_url":"https://api.github.com/users/hcymysql/repos","events_url":"https://api.github.com/users/hcymysql/events{/privacy}","received_events_url":"https://api.github.com/users/hcymysql/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-10T06:19:26Z","updated_at":"2021-11-11T09:08:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In the  /var/lib/clickhouse/metadata/test/directory, could not be found the .metadata file\r\n\r\nClickHouse server version 21.7.4","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31222/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31221","id":1049318643,"node_id":"I_kwDOA5dJV84-i1Tz","number":31221,"title":"S3 Table Function Ignoring VersionId","user":{"login":"nb-themes","id":86988769,"node_id":"MDQ6VXNlcjg2OTg4NzY5","avatar_url":"https://avatars.githubusercontent.com/u/86988769?v=4","gravatar_id":"","url":"https://api.github.com/users/nb-themes","html_url":"https://github.com/nb-themes","followers_url":"https://api.github.com/users/nb-themes/followers","following_url":"https://api.github.com/users/nb-themes/following{/other_user}","gists_url":"https://api.github.com/users/nb-themes/gists{/gist_id}","starred_url":"https://api.github.com/users/nb-themes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nb-themes/subscriptions","organizations_url":"https://api.github.com/users/nb-themes/orgs","repos_url":"https://api.github.com/users/nb-themes/repos","events_url":"https://api.github.com/users/nb-themes/events{/privacy}","received_events_url":"https://api.github.com/users/nb-themes/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":2020016497,"node_id":"MDU6TGFiZWwyMDIwMDE2NDk3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-s3","name":"comp-s3","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-10T02:01:22Z","updated_at":"2021-12-13T14:47:38Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Summary: S3 versionId is being ignored by the S3 table function\r\n\r\nS3 has object versioning and for the object url to specify a version other than the latest the parameter \"versionId=x\" is added.\r\n\r\nWhen querying on clickhouse with the S3 Table Function, the following is expected to select from the s3 object with versionId=testVersionId\r\n`SELECT * FROM s3('https://s3.us-east-2.amazonaws.com/my-test-bucket-768/data.csv?versionId=testVersionId', 'CSV', 'column1 UInt32, column2 UInt32, column3 UInt32') LIMIT 2;`\r\n\r\nUnfortunately it instead returns the latest. \r\n\r\nThis has been reproduced on the following versions:\r\nv21.8.4\r\nv21.9.4.35 (latest)\r\n\r\nThis can be worked around by instead creating a signed url and using the url table function instead, however it would be preferable to not have to do that.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31221/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31184","id":1048629989,"node_id":"I_kwDOA5dJV84-gNLl","number":31184,"title":"Materialized column not visible in the right table of a LEFT JOIN","user":{"login":"marcioapm","id":212230,"node_id":"MDQ6VXNlcjIxMjIzMA==","avatar_url":"https://avatars.githubusercontent.com/u/212230?v=4","gravatar_id":"","url":"https://api.github.com/users/marcioapm","html_url":"https://github.com/marcioapm","followers_url":"https://api.github.com/users/marcioapm/followers","following_url":"https://api.github.com/users/marcioapm/following{/other_user}","gists_url":"https://api.github.com/users/marcioapm/gists{/gist_id}","starred_url":"https://api.github.com/users/marcioapm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/marcioapm/subscriptions","organizations_url":"https://api.github.com/users/marcioapm/orgs","repos_url":"https://api.github.com/users/marcioapm/repos","events_url":"https://api.github.com/users/marcioapm/events{/privacy}","received_events_url":"https://api.github.com/users/marcioapm/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-11-09T13:49:44Z","updated_at":"2021-12-29T14:35:39Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Materialized column not visible in the right table of a LEFT JOIN.\r\n\r\nTested on 21.10.2.15\r\n\r\nCreate the following example data:\r\n```\r\ncreate table test1(col UInt64, col_sq UInt64 MATERIALIZED col*col) Engine=MergeTree partition by tuple() order by tuple();\r\ninsert into test1 values (1),(2);\r\n\r\ncreate table test2(col UInt64) Engine=MergeTree partition by tuple() order by tuple();\r\ninsert into test2 values (1),(2);\r\n```\r\n\r\nExecute the following:\r\n```\r\nSELECT t1.col, t1.col_sq\r\nFROM test2 t2\r\nLEFT JOIN test1 t1 ON t1.col = t2.col;\r\n```\r\n\r\nObserve the exception:\r\n```\r\nReceived exception from server (version 21.10.2):\r\nCode: 8. DB::Exception: Received from 127.0.0.1:9000. DB::Exception: Cannot find column `col_sq` in source stream, there are only columns: [t1.col]. (THERE_IS_NO_COLUMN)\r\n```\r\n\r\nExpected:\r\nThe query succeeds.\r\n\r\nWorkaround (thanks Denny Crane):\r\n```SELECT t1.col, t1.col_sq\r\nFROM test2 t2\r\nLEFT JOIN (SELECT col, col_sq FROM test1) t1 ON t1.col = t2.col;\r\n```\r\n\r\nWorkaround result:\r\n```\r\n┌─t1.col─┬─col_sq─┐\r\n│      1 │      1 │\r\n│      2 │      4 │\r\n└────────┴────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31184/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31182","id":1048587119,"node_id":"PR_kwDOA5dJV84uSNex","number":31182,"title":"Memory overcommit","user":{"login":"novikd","id":10158699,"node_id":"MDQ6VXNlcjEwMTU4Njk5","avatar_url":"https://avatars.githubusercontent.com/u/10158699?v=4","gravatar_id":"","url":"https://api.github.com/users/novikd","html_url":"https://github.com/novikd","followers_url":"https://api.github.com/users/novikd/followers","following_url":"https://api.github.com/users/novikd/following{/other_user}","gists_url":"https://api.github.com/users/novikd/gists{/gist_id}","starred_url":"https://api.github.com/users/novikd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/novikd/subscriptions","organizations_url":"https://api.github.com/users/novikd/orgs","repos_url":"https://api.github.com/users/novikd/repos","events_url":"https://api.github.com/users/novikd/events{/privacy}","received_events_url":"https://api.github.com/users/novikd/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"},{"id":2250495937,"node_id":"MDU6TGFiZWwyMjUwNDk1OTM3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/force%20tests","name":"force tests","color":"59d16d","default":false,"description":"Force test ignoring fast test output. Also forces full perf test run."},{"id":2600973660,"node_id":"MDU6TGFiZWwyNjAwOTczNjYw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/ded%20moroz","name":"ded moroz","color":"0E8A16","default":false,"description":"To make people wonder"}],"state":"open","locked":false,"assignee":{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false},"assignees":[{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false}],"milestone":null,"comments":12,"created_at":"2021-11-09T13:08:38Z","updated_at":"2022-01-23T18:01:28Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/31182","html_url":"https://github.com/ClickHouse/ClickHouse/pull/31182","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/31182.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/31182.patch","merged_at":null},"body":"Changelog category (leave one):\r\n\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n\r\nAdd memory overcommit to `MemoryTracker`. Added `guaranteed` settings for memory limits which represent soft memory limits. In case when hard memory limit is reached, `MemoryTracker` tries to cancel the most overcommited query.\r\nNew setting `memory_usage_overcommit_max_wait_microseconds` specifies how long queries may wait another query to stop.\r\n\r\nCloses #28375\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31182/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31167","id":1048125897,"node_id":"I_kwDOA5dJV84-eSHJ","number":31167,"title":"Failed to build cluster with clickhouse-keeper","user":{"login":"TheXs","id":33443441,"node_id":"MDQ6VXNlcjMzNDQzNDQx","avatar_url":"https://avatars.githubusercontent.com/u/33443441?v=4","gravatar_id":"","url":"https://api.github.com/users/TheXs","html_url":"https://github.com/TheXs","followers_url":"https://api.github.com/users/TheXs/followers","following_url":"https://api.github.com/users/TheXs/following{/other_user}","gists_url":"https://api.github.com/users/TheXs/gists{/gist_id}","starred_url":"https://api.github.com/users/TheXs/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/TheXs/subscriptions","organizations_url":"https://api.github.com/users/TheXs/orgs","repos_url":"https://api.github.com/users/TheXs/repos","events_url":"https://api.github.com/users/TheXs/events{/privacy}","received_events_url":"https://api.github.com/users/TheXs/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false},"assignees":[{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-11-09T02:45:33Z","updated_at":"2021-11-10T09:36:32Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, there.\r\nJust experiencing some problems when I try to build a clickhouse cluster by using clickhouse-keeper as coordinator on kubernetes environment. I put the config file and logs downbelow, thanks for any reply!\r\n\r\nconfig:\r\n\r\n```\r\n01-clickhouse-listen.xml: |\r\n    <yandex>\r\n        <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->\r\n        <listen_host>::</listen_host>\r\n        <listen_host>0.0.0.0</listen_host>\r\n        <listen_try>1</listen_try>\r\n        <path>/data/clickhouse</path>\r\n        <timezone>Asia/Shanghai</timezone>\r\n    </yandex>\r\n\r\nkeeper.xml: |\r\n    <yandex>\r\n        <keeper_server>\r\n            <tcp_port>9181</tcp_port>\r\n            <server_id>1</server_id>\r\n            <log_storage_path>/var/lib/clickhouse/coordination/log</log_storage_path>\r\n            <snapshot_storage_path>/var/lib/clickhouse/coordination/snapshots</snapshot_storage_path>\r\n            <coordination_settings>\r\n                <operation_timeout_ms>10000</operation_timeout_ms>\r\n                <session_timeout_ms>30000</session_timeout_ms>\r\n                <raft_logs_level>trace</raft_logs_level>\r\n            </coordination_settings>\r\n           <raft_configuration>\r\n               <server>\r\n                   <id>1</id>\r\n                   <hostname>chi-clickhouse-keeper-raw-replicated-0-0</hostname>\r\n                   <port>44444</port>\r\n                   <can_become_leader>true</can_become_leader>\r\n                   <priority>3</priority>\r\n               </server>\r\n               <server>\r\n                   <id>2</id>\r\n                   <hostname>chi-clickhouse-keeper-raw-replicated-1-0</hostname>\r\n                   <port>44444</port>\r\n                   <can_become_leader>true</can_become_leader>\r\n                   <start_as_follower>true</start_as_follower>\r\n                   <priority>3</priority>\r\n               </server>\r\n          </raft_configuration>\r\n        </keeper_server>\r\n    </yandex>\r\n\r\n  init-keeper.xml: |\r\n    <yandex>\r\n        <zookeeper>\r\n            <node index=\"1\">\r\n                <host>chi-clickhouse-keeper-raw-replicated-0-0</host>\r\n                <port>9181</port>\r\n            </node>\r\n             <node index=\"2\">\r\n                <host>chi-clickhouse-keeper-raw-replicated-1-0</host>\r\n                <port>9181</port>\r\n            </node>\r\n        </zookeeper>\r\n    </yandex>\r\n```\r\n\r\nlogs:\r\n```\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/chop-generated-macros.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/init-keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/01-clickhouse-listen.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/02-clickhouse-logger.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/03-clickhouse-querylog.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/04-clickhouse-partlog.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/chop-generated-settings.xml'.\r\nLogging debug to /var/log/clickhouse-server/clickhouse-server.log\r\nLogging errors to /var/log/clickhouse-server/clickhouse-server.err.log\r\nLogging debug to console\r\n2021.11.09 10:27:17.407090 [ 1 ] {} <Information> Application: Will watch for the process with pid 53\r\n2021.11.09 10:27:17.407239 [ 53 ] {} <Information> Application: Forked a child process to watch\r\n2021.11.09 10:27:17.407628 [ 53 ] {} <Information> SentryWriter: Sending crash reports is disabled\r\n2021.11.09 10:27:17.477388 [ 53 ] {} <Information> : Starting ClickHouse 21.8.10.19 with revision 54453, build id: DAA0935EF70BB2E3D67A30F487FC02820BCC1820, PID 53\r\n2021.11.09 10:27:17.477600 [ 53 ] {} <Information> Application: starting up\r\n2021.11.09 10:27:17.477646 [ 53 ] {} <Information> Application: OS name: Linux, version: 3.10.0-1160.36.2.el7.x86_64, architecture: x86_64\r\n2021.11.09 10:27:17.808790 [ 53 ] {} <Information> Application: Calculated checksum of the binary: FC9D7137B8289DF0EDB5C20D505A6E83, integrity check passed.\r\n2021.11.09 10:27:17.808860 [ 53 ] {} <Information> Application: It looks like the process has no CAP_IPC_LOCK capability, binary mlock will be disabled. It could happen due to incorrect ClickHouse package installation. You could resolve the problem manually with 'sudo setcap cap_ipc_lock=+ep /usr/bin/clickhouse'. Note that it will not work on 'nosuid' mounted filesystems.\r\n2021.11.09 10:27:17.809236 [ 53 ] {} <Debug> Application: rlimit on number of file descriptors is 1048576\r\n2021.11.09 10:27:17.809257 [ 53 ] {} <Debug> Application: Initializing DateLUT.\r\n2021.11.09 10:27:17.809286 [ 53 ] {} <Debug> Application: Setting up /var/lib/clickhouse/tmp/ to store temporary data in it\r\n2021.11.09 10:27:17.809730 [ 53 ] {} <Debug> Application: Configuration parameter 'interserver_http_host' doesn't exist or exists and empty. Will use 'chi-clickhouse-keeper-raw-replicated-0-0-0.chi-clickhouse-keeper-raw-replicated-0-0.ck-keeper-raw.svc.cluster.local' as replica host.\r\n2021.11.09 10:27:17.809758 [ 53 ] {} <Debug> Application: Initiailizing interserver credentials.\r\n2021.11.09 10:27:17.809965 [ 53 ] {} <Information> SensitiveDataMaskerConfigRead: 1 query masking rules loaded.\r\n2021.11.09 10:27:17.811031 [ 53 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/config.xml'\r\nProcessing configuration file '/etc/clickhouse-server/config.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/chop-generated-macros.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/init-keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/01-clickhouse-listen.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/02-clickhouse-logger.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/03-clickhouse-querylog.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/04-clickhouse-partlog.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/chop-generated-remote_servers.xml'.\r\nMerging configuration file '/etc/clickhouse-server/config.d/chop-generated-settings.xml'.\r\nSaved preprocessed configuration to '/data/clickhouse/preprocessed_configs/config.xml'.\r\n2021.11.09 10:27:17.815737 [ 53 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/config.xml', performing update on configuration\r\n2021.11.09 10:27:17.817155 [ 53 ] {} <Information> Application: Setting max_server_memory_usage was set to 226.25 GiB (251.39 GiB available * 0.90 max_server_memory_usage_to_ram_ratio)\r\n2021.11.09 10:27:17.820251 [ 53 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/config.xml', performed update on configuration\r\n2021.11.09 10:27:17.824371 [ 53 ] {} <Debug> ConfigReloader: Loading config '/etc/clickhouse-server/users.xml'\r\nProcessing configuration file '/etc/clickhouse-server/users.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/chop-generated-macros.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/init-keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/conf.d/keeper.xml'.\r\nMerging configuration file '/etc/clickhouse-server/users.d/01-clickhouse-user.xml'.\r\nMerging configuration file '/etc/clickhouse-server/users.d/02-clickhouse-default-profile.xml'.\r\nMerging configuration file '/etc/clickhouse-server/users.d/03-database-ordinary.xml'.\r\nMerging configuration file '/etc/clickhouse-server/users.d/chop-generated-profiles.xml'.\r\nMerging configuration file '/etc/clickhouse-server/users.d/chop-generated-users.xml'.\r\nSaved preprocessed configuration to '/data/clickhouse/preprocessed_configs/users.xml'.\r\n2021.11.09 10:27:17.825860 [ 53 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performing update on configuration\r\n2021.11.09 10:27:17.826616 [ 53 ] {} <Debug> ConfigReloader: Loaded config '/etc/clickhouse-server/users.xml', performed update on configuration\r\n2021.11.09 10:27:17.826979 [ 53 ] {} <Debug> Access(user directories): Added users.xml access storage 'users.xml', path: /etc/clickhouse-server/users.xml\r\n2021.11.09 10:27:17.827164 [ 53 ] {} <Warning> Access(local directory): File /var/lib/clickhouse/access/users.list doesn't exist\r\n2021.11.09 10:27:17.827202 [ 53 ] {} <Warning> Access(local directory): Recovering lists in directory /var/lib/clickhouse/access/\r\n2021.11.09 10:27:17.827420 [ 53 ] {} <Debug> Access(user directories): Added local directory access storage 'local directory', path: /var/lib/clickhouse/access/\r\n2021.11.09 10:27:17.828020 [ 53 ] {} <Debug> KeeperDispatcher: Initializing storage dispatcher\r\n2021.11.09 10:27:17.828761 [ 53 ] {} <Information> KeeperLogStore: force_sync enabled\r\n2021.11.09 10:27:17.828859 [ 53 ] {} <Debug> KeeperDispatcher: Waiting server to initialize\r\n2021.11.09 10:27:17.828876 [ 53 ] {} <Debug> KeeperStateMachine: Totally have 0 snapshots\r\n2021.11.09 10:27:17.828889 [ 53 ] {} <Debug> KeeperStateMachine: No existing snapshots, last committed log index 0\r\n2021.11.09 10:27:17.828912 [ 53 ] {} <Warning> KeeperLogStore: Removing all changelogs\r\n2021.11.09 10:27:17.831389 [ 53 ] {} <Information> RaftInstance: Raft ASIO listener initiated, UNSECURED\r\n2021.11.09 10:27:17.831455 [ 53 ] {} <Information> RaftInstance: parameters: timeout 1000 - 2000, heartbeat 500, leadership expiry 10000, max batch 100, backoff 50, snapshot distance 100000, log sync stop gap 99999, reserved logs 100000, client timeout 10000, auto forwarding ON, API call type ASYNC, custom commit quorum size 0, custom election quorum size 0, snapshot receiver INCLUDED, leadership transfer wait time 0, grace period of lagging state machine 0\r\n2021.11.09 10:27:17.831478 [ 53 ] {} <Information> RaftInstance: new timeout range: 1000 -- 2000\r\n2021.11.09 10:27:17.831511 [ 53 ] {} <Information> RaftInstance:    === INIT RAFT SERVER ===\r\ncommit index 0\r\nterm 0\r\nelection timer allowed\r\nlog store start 1, end 0\r\nconfig log idx 0, prev log idx 0\r\n2021.11.09 10:27:17.831598 [ 53 ] {} <Information> RaftInstance: peer 1: DC ID 0, chi-clickhouse-keeper-raw-replicated-0-0:44444, voting member, 3\r\npeer 2: DC ID 0, chi-clickhouse-keeper-raw-replicated-1-0:44444, voting member, 3\r\nmy id: 1, voting_member\r\nnum peers: 1\r\n2021.11.09 10:27:17.831621 [ 53 ] {} <Information> RaftInstance: global manager does not exist. will use local thread for commit and append\r\n2021.11.09 10:27:17.831737 [ 53 ] {} <Information> RaftInstance: wait for HB, for 50 + [1000, 2000] ms\r\n2021.11.09 10:27:17.831870 [ 111 ] {} <Information> RaftInstance: bg append_entries thread initiated\r\n2021.11.09 10:27:17.881954 [ 53 ] {} <Debug> RaftInstance: server 1 started\r\n2021.11.09 10:27:17.882189 [ 53 ] {} <Debug> KeeperDispatcher: Server initialized, waiting for quorum\r\n2021.11.09 10:27:19.831175 [ 62 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:19.831398 [ 62 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:19.831465 [ 62 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:19.831525 [ 62 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:19.831617 [ 62 ] {} <Debug> RaftInstance: socket 0x7f2d100d8d98 to chi-clickhouse-keeper-raw-replicated-1-0:44444 is not opened yet\r\n2021.11.09 10:27:20.875995 [ 65 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:20.876117 [ 65 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:20.876159 [ 65 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:20.876244 [ 65 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 1\r\n2021.11.09 10:27:20.876292 [ 65 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:20.876318 [ 65 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:22.462723 [ 67 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:22.462891 [ 67 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:22.462951 [ 67 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:22.463028 [ 67 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 2\r\n2021.11.09 10:27:22.463102 [ 67 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:22.463152 [ 67 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:24.277610 [ 68 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:24.277739 [ 68 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:24.277776 [ 68 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:24.277820 [ 68 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 3\r\n2021.11.09 10:27:24.277868 [ 68 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:24.277893 [ 68 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:25.841108 [ 69 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:25.841216 [ 69 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:25.841256 [ 69 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:25.841312 [ 69 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 4\r\n2021.11.09 10:27:25.841372 [ 69 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:25.841400 [ 69 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:27.129808 [ 70 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:27.129970 [ 70 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:27.130029 [ 70 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:27.130111 [ 70 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 5\r\n2021.11.09 10:27:27.130186 [ 70 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:27.130233 [ 70 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:28.447504 [ 71 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:28.447631 [ 71 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:28.447673 [ 71 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:28.447719 [ 71 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 6\r\n2021.11.09 10:27:28.447768 [ 71 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:28.447798 [ 71 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:29.823149 [ 72 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:29.823296 [ 72 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:29.823364 [ 72 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:29.823460 [ 72 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 7\r\n2021.11.09 10:27:29.823521 [ 72 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:29.823593 [ 72 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:31.533910 [ 73 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:31.534096 [ 73 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:31.534146 [ 73 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:31.534202 [ 73 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 8\r\n2021.11.09 10:27:31.534265 [ 73 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:31.534299 [ 73 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:33.424502 [ 74 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:33.424589 [ 74 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:33.424613 [ 74 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:33.424643 [ 74 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 9\r\n2021.11.09 10:27:33.424671 [ 74 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:33.424687 [ 74 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:34.592857 [ 75 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:34.592943 [ 75 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:34.592983 [ 75 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:34.593018 [ 75 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 10\r\n2021.11.09 10:27:34.593050 [ 75 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:34.593069 [ 75 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:36.337412 [ 76 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:36.337622 [ 76 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:36.337691 [ 76 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:36.337766 [ 76 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 11\r\n2021.11.09 10:27:36.337843 [ 76 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:36.337908 [ 76 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:37.877269 [ 77 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:37.877425 [ 77 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:37.877495 [ 77 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:37.877613 [ 77 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 12\r\n2021.11.09 10:27:37.877680 [ 77 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:37.877717 [ 77 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:39.035072 [ 78 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:39.035239 [ 78 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:39.035305 [ 78 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:39.035334 [ 78 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 13\r\n2021.11.09 10:27:39.035367 [ 78 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:39.035385 [ 78 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:40.364751 [ 79 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:40.364917 [ 79 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:40.364978 [ 79 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:40.365056 [ 79 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 14\r\n2021.11.09 10:27:40.365133 [ 79 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:40.365177 [ 79 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:41.946468 [ 80 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:41.946663 [ 80 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:41.946713 [ 80 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:41.946779 [ 80 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 15\r\n2021.11.09 10:27:41.946843 [ 80 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:41.946880 [ 80 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:43.944168 [ 81 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:43.944314 [ 81 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:43.944367 [ 81 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:43.944467 [ 81 ] {} <Warning> RaftInstance: connection to peer 2 is not active long time: 26112 ms, need reconnection for prevote\r\n2021.11.09 10:27:43.944532 [ 81 ] {} <Information> RaftInstance: reset RPC client for peer 2\r\n2021.11.09 10:27:43.944827 [ 81 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 16\r\n2021.11.09 10:27:43.944896 [ 81 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:43.944945 [ 81 ] {} <Debug> RaftInstance: socket 0x7f2cd654e018 to chi-clickhouse-keeper-raw-replicated-1-0:44444 is not opened yet\r\n2021.11.09 10:27:45.624281 [ 82 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:45.624441 [ 82 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:45.624508 [ 82 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:45.624633 [ 82 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 17\r\n2021.11.09 10:27:45.624714 [ 82 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:45.624755 [ 82 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:47.013055 [ 84 ] {} <Warning> RaftInstance: Election timeout, initiate leader election\r\n2021.11.09 10:27:47.013158 [ 84 ] {} <Information> RaftInstance: [PRIORITY] decay, target 1 -> 1, mine 3\r\n2021.11.09 10:27:47.013203 [ 84 ] {} <Information> RaftInstance: [ELECTION TIMEOUT] current role: follower, log last term 0, state term 0, target p 1, my p 3, hb dead, pre-vote NOT done\r\n2021.11.09 10:27:47.013249 [ 84 ] {} <Warning> RaftInstance: total 1 nodes (including this node) responded for pre-vote (term 0, live 0, dead 1), at least 2 nodes should respond. failure count 18\r\n2021.11.09 10:27:47.013296 [ 84 ] {} <Information> RaftInstance: [PRE-VOTE INIT] my id 1, my role follower, term 0, log idx 0, log term 0, priority (target 1 / mine 3)\r\n2021.11.09 10:27:47.013320 [ 84 ] {} <Warning> RaftInstance: failed to send prevote request: peer 2 (chi-clickhouse-keeper-raw-replicated-1-0:44444) is busy\r\n2021.11.09 10:27:47.883594 [ 53 ] {} <Error> void DB::KeeperStorageDispatcher::initialize(const Poco::Util::AbstractConfiguration &, bool): Code: 568, e.displayText() = DB::Exception: Failed to wait RAFT initialization, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8fdbc9a in /usr/bin/clickhouse\r\n1. DB::KeeperServer::waitInit() @ 0x112f0dd8 in /usr/bin/clickhouse\r\n2. DB::KeeperStorageDispatcher::initialize(Poco::Util::AbstractConfiguration const&, bool) @ 0x1133ad17 in /usr/bin/clickhouse\r\n3. DB::Context::initializeKeeperStorageDispatcher() const @ 0x1000c0ee in /usr/bin/clickhouse\r\n4. DB::Server::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x905ff83 in /usr/bin/clickhouse\r\n5. Poco::Util::Application::run() @ 0x13bcf043 in /usr/bin/clickhouse\r\n6. DB::Server::run() @ 0x9052d0f in /usr/bin/clickhouse\r\n7. mainEntryClickHouseServer(int, char**) @ 0x90510b3 in /usr/bin/clickhouse\r\n8. main @ 0x8fd69fe in /usr/bin/clickhouse\r\n9. __libc_start_main @ 0x270b3 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n10. _start @ 0x8fa1cae in /usr/bin/clickhouse\r\n (version 21.8.10.19 (official build))\r\n2021.11.09 10:27:47.889258 [ 53 ] {} <Debug> KeeperDispatcher: Shutting down storage dispatcher\r\n2021.11.09 10:27:47.889550 [ 53 ] {} <Information> RaftInstance: shutting down raft core\r\n2021.11.09 10:27:47.889620 [ 53 ] {} <Information> RaftInstance: sent stop signal to the commit thread.\r\n2021.11.09 10:27:47.889664 [ 53 ] {} <Information> RaftInstance: cancelled all schedulers.\r\n2021.11.09 10:27:47.889693 [ 53 ] {} <Information> RaftInstance: commit thread stopped.\r\n2021.11.09 10:27:47.889730 [ 53 ] {} <Information> RaftInstance: all pending commit elements dropped.\r\n2021.11.09 10:27:47.889760 [ 53 ] {} <Information> RaftInstance: reset all pointers.\r\n2021.11.09 10:27:47.889804 [ 53 ] {} <Information> RaftInstance: joined terminated commit thread.\r\n2021.11.09 10:27:47.889870 [ 53 ] {} <Information> RaftInstance: sent stop signal to background append thread.\r\n2021.11.09 10:27:47.889881 [ 111 ] {} <Information> RaftInstance: bg append_entries thread terminated\r\n2021.11.09 10:27:47.890133 [ 53 ] {} <Information> RaftInstance: clean up auto-forwarding queue: 0 elems\r\n2021.11.09 10:27:47.890167 [ 53 ] {} <Information> RaftInstance: clean up auto-forwarding clients\r\n2021.11.09 10:27:47.890193 [ 53 ] {} <Information> RaftInstance: raft_server shutdown completed.\r\n2021.11.09 10:27:47.890487 [ 87 ] {} <Error> RaftInstance: failed to accept a rpc connection due to error 125\r\n2021.11.09 10:27:47.890968 [ 62 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 22\r\n2021.11.09 10:27:47.890831 [ 99 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 39\r\n2021.11.09 10:27:47.891028 [ 103 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 33\r\n2021.11.09 10:27:47.890981 [ 104 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 32\r\n2021.11.09 10:27:47.891072 [ 107 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 27\r\n2021.11.09 10:27:47.890824 [ 92 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 44\r\n2021.11.09 10:27:47.891012 [ 91 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 46\r\n2021.11.09 10:27:47.891263 [ 74 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 15\r\n2021.11.09 10:27:47.891025 [ 93 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 42\r\n2021.11.09 10:27:47.891214 [ 65 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 23\r\n2021.11.09 10:27:47.891297 [ 68 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 19\r\n2021.11.09 10:27:47.891089 [ 106 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 29\r\n2021.11.09 10:27:47.891456 [ 79 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 8\r\n2021.11.09 10:27:47.891437 [ 77 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 9\r\n2021.11.09 10:27:47.891520 [ 84 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 3\r\n2021.11.09 10:27:47.891419 [ 78 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 10\r\n2021.11.09 10:27:47.891483 [ 82 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 5\r\n2021.11.09 10:27:47.891487 [ 80 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 6\r\n2021.11.09 10:27:47.891449 [ 81 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 7\r\n2021.11.09 10:27:47.891764 [ 105 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 31\r\n2021.11.09 10:27:47.891776 [ 83 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 4\r\n2021.11.09 10:27:47.891837 [ 64 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 24\r\n2021.11.09 10:27:47.892104 [ 73 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 14\r\n2021.11.09 10:27:47.892125 [ 71 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 16\r\n2021.11.09 10:27:47.892230 [ 88 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 45\r\n2021.11.09 10:27:47.892390 [ 95 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 40\r\n2021.11.09 10:27:47.892463 [ 89 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 42\r\n2021.11.09 10:27:47.892475 [ 97 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 36\r\n2021.11.09 10:27:47.892490 [ 100 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 35\r\n2021.11.09 10:27:47.892467 [ 98 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 38\r\n2021.11.09 10:27:47.892662 [ 85 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 47\r\n2021.11.09 10:27:47.892777 [ 102 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 30\r\n2021.11.09 10:27:47.890820 [ 96 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 40\r\n2021.11.09 10:27:47.892784 [ 101 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 37\r\n2021.11.09 10:27:47.892862 [ 67 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 20\r\n2021.11.09 10:27:47.892819 [ 90 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 34\r\n2021.11.09 10:27:47.892870 [ 66 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 21\r\n2021.11.09 10:27:47.892964 [ 108 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 28\r\n2021.11.09 10:27:47.893121 [ 70 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 18\r\n2021.11.09 10:27:47.893168 [ 69 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 17\r\n2021.11.09 10:27:47.893186 [ 72 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 13\r\n2021.11.09 10:27:47.893225 [ 75 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 12\r\n2021.11.09 10:27:47.893180 [ 63 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 25\r\n2021.11.09 10:27:47.893173 [ 86 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 1\r\n2021.11.09 10:27:47.893207 [ 76 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 11\r\n2021.11.09 10:27:47.893267 [ 94 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 2\r\n2021.11.09 10:27:47.893228 [ 109 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 26\r\n2021.11.09 10:27:47.900919 [ 87 ] {} <Information> RaftInstance: end of asio worker thread, remaining threads: 0\r\n2021.11.09 10:27:47.901805 [ 53 ] {} <Debug> KeeperDispatcher: Dispatcher shut down\r\n2021.11.09 10:27:47.902710 [ 53 ] {} <Error> Application: DB::Exception: Failed to wait RAFT initialization\r\n2021.11.09 10:27:47.902775 [ 53 ] {} <Information> Application: shutting down\r\n2021.11.09 10:27:47.902815 [ 53 ] {} <Debug> Application: Uninitializing subsystem: Logging Subsystem\r\n2021.11.09 10:27:47.902994 [ 54 ] {} <Information> BaseDaemon: Stop SignalListener thread\r\n2021.11.09 10:27:47.922281 [ 1 ] {} <Information> Application: Child process exited normally with code 70.\r\n```\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31167/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31141","id":1047286475,"node_id":"I_kwDOA5dJV84-bFLL","number":31141,"title":"Engine URL - should not discard inserts in case unreachable http server","user":{"login":"adubovikov","id":4513061,"node_id":"MDQ6VXNlcjQ1MTMwNjE=","avatar_url":"https://avatars.githubusercontent.com/u/4513061?v=4","gravatar_id":"","url":"https://api.github.com/users/adubovikov","html_url":"https://github.com/adubovikov","followers_url":"https://api.github.com/users/adubovikov/followers","following_url":"https://api.github.com/users/adubovikov/following{/other_user}","gists_url":"https://api.github.com/users/adubovikov/gists{/gist_id}","starred_url":"https://api.github.com/users/adubovikov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adubovikov/subscriptions","organizations_url":"https://api.github.com/users/adubovikov/orgs","repos_url":"https://api.github.com/users/adubovikov/repos","events_url":"https://api.github.com/users/adubovikov/events{/privacy}","received_events_url":"https://api.github.com/users/adubovikov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-08T10:40:16Z","updated_at":"2021-11-08T14:48:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> (you don't have to strictly follow this form)\r\n\r\nIn case you create a material view that writes data to an external table with ENGINE URL, you can have loss if your http server is unreachable\r\n\r\n> A clear and concise description of what is the intended usage scenario is.\r\n\r\nCREATE TABLE main (x String) ENGINE = Memory\r\n\r\ninsert into main (x) VALUES ('test string 01')\r\n\r\nselect * from main;\r\n\r\nSELECT *\r\nFROM main\r\n\r\n┌─x──────────────┐\r\n│ test string 01 │\r\n└────────────────┘\r\n\r\nCREATE TABLE main_url\r\n(\r\n    x String\r\n)\r\nENGINE = URL('http://127.0.0.1:9998/', 'JSON')\r\n\r\n# port 9998 is not open!\r\n\r\nCREATE MATERIALIZED VIEW main_mv TO main_url\r\n(\r\n    x String\r\n) AS SELECT x FROM main\r\n\r\n\r\ninsert into main (x) VALUES ('test string 02')\r\n\r\nCode: 1000. DB::Exception: Received from localhost:9000. DB::Exception: Connection refused. \r\n\r\n\r\nselect * from main;\r\n\r\nSELECT *\r\nFROM main\r\n\r\n┌─x──────────────┐\r\n│ test string 01 │\r\n└────────────────┘\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\n\r\n> A clear and concise description of what you want to happen.\r\n\r\nin case you have URL Engine type and the destination IP/port is unreachable  - the error class should be in warning level and should not discard inserts to the main table.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31141/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31126","id":1046746077,"node_id":"I_kwDOA5dJV84-ZBPd","number":31126,"title":"How to process map with duplicated keys","user":{"login":"taiyang-li","id":8181003,"node_id":"MDQ6VXNlcjgxODEwMDM=","avatar_url":"https://avatars.githubusercontent.com/u/8181003?v=4","gravatar_id":"","url":"https://api.github.com/users/taiyang-li","html_url":"https://github.com/taiyang-li","followers_url":"https://api.github.com/users/taiyang-li/followers","following_url":"https://api.github.com/users/taiyang-li/following{/other_user}","gists_url":"https://api.github.com/users/taiyang-li/gists{/gist_id}","starred_url":"https://api.github.com/users/taiyang-li/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/taiyang-li/subscriptions","organizations_url":"https://api.github.com/users/taiyang-li/orgs","repos_url":"https://api.github.com/users/taiyang-li/repos","events_url":"https://api.github.com/users/taiyang-li/events{/privacy}","received_events_url":"https://api.github.com/users/taiyang-li/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":3068847854,"node_id":"MDU6TGFiZWwzMDY4ODQ3ODU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-map-datatype","name":"comp-map-datatype","color":"b5bcff","default":false,"description":"Relates to Map datatype"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-07T13:22:19Z","updated_at":"2021-11-27T13:32:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the issue**\r\nCurrently data type map allow duplicated keys, which may be confusing for many users. \r\n\r\n**How to reproduce**\r\n``` sql\r\nSELECT\r\n    map(1, 2, 1, 3, 3, 4) AS a,\r\n    a[1]\r\n\r\nQuery id: 109bf467-4fff-459f-a6a9-8b8692f2bb26\r\n\r\n┌─a─────────────┬─arrayElement(map(1, 2, 1, 3, 3, 4), 1)─┐\r\n│ {1:2,1:3,3:4} │                                      2 │\r\n└───────────────┴────────────────────────────────────────┘\r\n\r\n1 rows in set. Elapsed: 0.002 sec. \r\n```\r\n\r\n**Expected behavior**\r\nMap should not allow duplicated keys. \r\nWhen creating map with `map` function, the later key/value pair should override previous kv pair with the same key. \r\nWhen get keys using `mapKeys` function, CH should return uniq keys.\r\nWhen get values using `mapValues` function, CH should return latest values correspond to uniq keys. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31126/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31121","id":1046638378,"node_id":"I_kwDOA5dJV84-Ym8q","number":31121,"title":"Native multiline editing should be activated also when I paste multiline query.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-07T01:46:34Z","updated_at":"2021-11-07T01:46:34Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the issue**\r\n\r\nNow multiline query is pasted like this:\r\n\r\n```\r\n:) SELECT passenger_count, toYear(pickup_date) AS year, round(trip_distance) AS distance, count(*)\r\n:-] FROM trips_mergetree\r\n:-] GROUP BY passenger_count, year, distance\r\n:-] ORDER BY year, count(*) DESC\r\n```\r\n\r\nAnd I cannot navigate and edit it by pressing up/down arrow keys.\r\n\r\nCC @amosbird ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31121/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31120","id":1046624172,"node_id":"I_kwDOA5dJV84-Yjes","number":31120,"title":"FixedString GROUP BY aggregation method slower than similar keys_* method","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-07T00:07:47Z","updated_at":"2021-11-07T00:23:22Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the situation**\r\n\r\nGROUP BY fixed_string key works slower than it's direct analog but with UInt key.\r\n\r\n**How to reproduce**\r\nClickHouse 21.10\r\n\r\n\r\n```\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',7)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey_fixed_string\r\n\r\n1 rows in set. Elapsed: 11.025 sec. Processed 1.00 billion rows, 8.00 GB (90.71 million rows/s., 725.68 MB/s.)\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',8)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key  ;\r\n\r\nkey_fixed_string\r\n\r\nElapsed: 9.072 sec. Processed 1.00 billion rows, 8.00 GB (110.23 million rows/s., 881.85 MB/s.) -- because of alignment?\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',7)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key, key_2  ;\r\n\r\nkeys64\r\n\r\nElapsed: 7.163 sec. Processed 1.00 billion rows, 8.00 GB (139.61 million rows/s., 1.12 GB/s.)\r\n\r\n\r\n###############\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('1',1)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey_fixed_string\r\n\r\nElapsed: 7.002 sec. Processed 1.00 billion rows, 8.00 GB (142.82 million rows/s., 1.14 GB/s.)\r\n\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',7)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey_fixed_string\r\n\r\n1 rows in set. Elapsed: 11.025 sec. Processed 1.00 billion rows, 8.00 GB (90.71 million rows/s., 725.68 MB/s.)\r\n\r\nSELECT count() FROM (SELECT  materialize(reinterpretAsUInt64(toFixedString('12345',7))) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey64\r\n\r\nElapsed: 4.015 sec. Processed 1.00 billion rows, 8.00 GB (249.11 million rows/s., 1.99 GB/s.)\r\n\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',9)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey_fixed_string\r\n\r\nElapsed: 9.532 sec. Processed 1.00 billion rows, 8.00 GB (104.91 million rows/s., 839.29 MB/s.)\r\n\r\nSELECT count() FROM (SELECT  materialize(reinterpretAsUInt128(toFixedString('12345',7))) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkeys128\r\n\r\nElapsed: 6.313 sec. Processed 1.00 billion rows, 8.00 GB (158.41 million rows/s., 1.27 GB/s.)\r\n\r\n\r\nSELECT count() FROM (SELECT  materialize(toFixedString('12345',17)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkey_fixed_string\r\n\r\nElapsed: 11.535 sec. Processed 1.00 billion rows, 8.00 GB (86.70 million rows/s., 693.60 MB/s.)\r\n\r\nSELECT count() FROM (SELECT  materialize(reinterpretAsUInt256(toFixedString('12345',7))) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nkeys256\r\n\r\nElapsed: 9.626 sec. Processed 1.00 billion rows, 8.00 GB (103.89 million rows/s., 831.10 MB/s.)\r\n```\r\n\r\nIt also affect IPv6 (because it uses FixedString(16) internally)\r\n\r\n```\r\nSELECT count() FROM (SELECT  materialize(CAST(toFixedString('1',16) AS IPv6)) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nElapsed: 8.899 sec. Processed 1.00 billion rows, 8.00 GB (112.38 million rows/s., 899.07 MB/s.)\r\n\r\nSELECT count() FROM (SELECT  materialize(reinterpretAsUInt128(CAST(toFixedString('1',16) AS IPv6))) as key, 1 as key_2 FROM numbers(1000000000)) GROUP BY key;\r\n\r\nElapsed: 6.551 sec. Processed 1.00 billion rows, 8.00 GB (152.65 million rows/s., 1.22 GB/s.)\r\n```\r\n\r\n**Expected performance**\r\n\r\nThe same performance for FixedString and UInt* keys.\r\n\r\n**Additional context**\r\n\r\nProbably it's possible just to use key* hash tables for short FixedString rows?\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31120/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31116","id":1046317684,"node_id":"I_kwDOA5dJV84-XYp0","number":31116,"title":"BloomFilter: use multiplication instead of modulo","user":{"login":"Bulat-Ziganshin","id":1798016,"node_id":"MDQ6VXNlcjE3OTgwMTY=","avatar_url":"https://avatars.githubusercontent.com/u/1798016?v=4","gravatar_id":"","url":"https://api.github.com/users/Bulat-Ziganshin","html_url":"https://github.com/Bulat-Ziganshin","followers_url":"https://api.github.com/users/Bulat-Ziganshin/followers","following_url":"https://api.github.com/users/Bulat-Ziganshin/following{/other_user}","gists_url":"https://api.github.com/users/Bulat-Ziganshin/gists{/gist_id}","starred_url":"https://api.github.com/users/Bulat-Ziganshin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bulat-Ziganshin/subscriptions","organizations_url":"https://api.github.com/users/Bulat-Ziganshin/orgs","repos_url":"https://api.github.com/users/Bulat-Ziganshin/repos","events_url":"https://api.github.com/users/Bulat-Ziganshin/events{/privacy}","received_events_url":"https://api.github.com/users/Bulat-Ziganshin/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-06T00:07:41Z","updated_at":"2021-11-23T00:11:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> Сильные программисты прибавляют и умножают, слабые - отнимают и делят (c) Евангелие от Уоррена\r\n\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/blob/1bf375e2b761db5b99b0f403b90c412a530f4d5c/src/Interpreters/BloomFilter.cpp#L67-L71\r\n\r\nSlow modulo operation can be replaced with fast multiplication:\r\n```\r\nuint64 index(uint64 hash, uint64 size) {\r\n    return (uint128(hash)*size) >> 64;}\r\n```\r\nThis function just returns the high word (RDX register) of extended multiplication result.\r\n\r\nThis change will require modification of the pos calculation. For the starter, we can try `pos = index(hash1 + i * hash2, size)`, but the problem is that generated indexes almost don't depend on lower bit of hash1 and hash2.\r\n\r\nSo, we need to mix lower bits of hash1/hash2 into higher bits as we loop through i. We can do that with multiplication, or we can use crc(hash1/2) and manually add resulting 32-bit value to higher bits of hash1/2. F.e.\r\n\r\n```\r\nfor i = 0 .. hashes/2:\r\n    pos = index(hash1, size)\r\n    filter[pos] = 1\r\n    hash1 += hash2*K1  // or hash1 += crc32(hash2) << 32\r\n\r\n    pos = index(hash2, size)\r\n    filter[pos] = 1\r\n    hash2 += hash1*K2\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31116/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31111","id":1046044130,"node_id":"I_kwDOA5dJV84-WV3i","number":31111,"title":"StringHashTable::dispatch masking optimization","user":{"login":"Bulat-Ziganshin","id":1798016,"node_id":"MDQ6VXNlcjE3OTgwMTY=","avatar_url":"https://avatars.githubusercontent.com/u/1798016?v=4","gravatar_id":"","url":"https://api.github.com/users/Bulat-Ziganshin","html_url":"https://github.com/Bulat-Ziganshin","followers_url":"https://api.github.com/users/Bulat-Ziganshin/followers","following_url":"https://api.github.com/users/Bulat-Ziganshin/following{/other_user}","gists_url":"https://api.github.com/users/Bulat-Ziganshin/gists{/gist_id}","starred_url":"https://api.github.com/users/Bulat-Ziganshin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bulat-Ziganshin/subscriptions","organizations_url":"https://api.github.com/users/Bulat-Ziganshin/orgs","repos_url":"https://api.github.com/users/Bulat-Ziganshin/repos","events_url":"https://api.github.com/users/Bulat-Ziganshin/events{/privacy}","received_events_url":"https://api.github.com/users/Bulat-Ziganshin/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-05T16:45:31Z","updated_at":"2021-11-21T11:47:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"https://github.com/ClickHouse/ClickHouse/blob/f95cd5b1e4caae09299fa8bf0955d11f70d71cc4/src/Common/HashTable/StringHashTable.h#L280-L290\r\n\r\nYour trick is absolutely amazing, but it can be made even better by replacing condition with `(p&4095) <= (4096-8)`. It's one micro-op more, but half the unpredicted branch less (plus deeper speculation for further executed code).\r\n\r\nMoreover, we can try to squeeze a few more ops by replacing `n[0] &= -1ul >> s` with `n[0] &= mask_table[sz]` and avoiding to compute `s` at all for this branch of computations.\r\n\r\nFinally, I wonder why you are using `switch ((sz - 1) >> 3)` instead of\r\n\r\n```\r\nswitch (sz) {\r\ncase 1: case 2: ... case 8:\r\n```\r\n(and maybe special cases for 8/16/24) ? Is it because compilers are too stupid to use jump tables for my code?\r\n\r\nAlso, I don't like the idea of spilling untrackable non-portable code. In this particular case, you can define PAGESIZE = 4096 somewhere in global headers and use it instead of literal constants.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111/reactions","total_count":5,"+1":5,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31111/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31107","id":1045918393,"node_id":"I_kwDOA5dJV84-V3K5","number":31107,"title":"Problem with hdfs integration","user":{"login":"rkozlo","id":44550015,"node_id":"MDQ6VXNlcjQ0NTUwMDE1","avatar_url":"https://avatars.githubusercontent.com/u/44550015?v=4","gravatar_id":"","url":"https://api.github.com/users/rkozlo","html_url":"https://github.com/rkozlo","followers_url":"https://api.github.com/users/rkozlo/followers","following_url":"https://api.github.com/users/rkozlo/following{/other_user}","gists_url":"https://api.github.com/users/rkozlo/gists{/gist_id}","starred_url":"https://api.github.com/users/rkozlo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rkozlo/subscriptions","organizations_url":"https://api.github.com/users/rkozlo/orgs","repos_url":"https://api.github.com/users/rkozlo/repos","events_url":"https://api.github.com/users/rkozlo/events{/privacy}","received_events_url":"https://api.github.com/users/rkozlo/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2192299304,"node_id":"MDU6TGFiZWwyMTkyMjk5MzA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-hdfs","name":"comp-hdfs","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-05T14:37:01Z","updated_at":"2021-11-08T12:22:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi,\r\nI am trying to integrate ch with hdfs and have some problems. First problem is with connecting to kerberos. Here is my config for hdfs:\r\n`<yandex>\r\n<!-- Global configuration options for HDFS engine type -->\r\n    <hdfs>\r\n        <hadoop_kerberos_keytab>/etc/clickhouse-server/xxx@xx.xx.keytab</hadoop_kerberos_keytab>\r\n        <hadoop_kerberos_principal>xxx@xx.xx</hadoop_kerberos_principal>\r\n        <hadoop_security_authentication>kerberos</hadoop_security_authentication>\r\n        <hadoop_security_kerberos_ticket_cache_path>/tmp/chtic</hadoop_security_kerberos_ticket_cache_path>\r\n        <hadoop_security_kerberos_ticket_cache_path>/tmp</hadoop_security_kerberos_ticket_cache_path>\r\n    </hdfs>\r\n\r\n    <hdfs_dedicatedcachepath>\r\n        <hadoop_security_kerberos_ticket_cache_path>/tmp</hadoop_security_kerberos_ticket_cache_path>\r\n    </hdfs_dedicatedcachepath> \r\n</yandex>`\r\n\r\nWith this config i get error\r\n`Code: 210. DB::Exception: Received from localhost:9000. DB::Exception: Unable to connect to HDFS: AccessControlException: Failed to evaluate challenge: GSSAPI error in client while negotiating security context in gss_init_sec_context() in SASL library.  This is most likely due insufficient credentials or malicious interactions.. (NETWORK_ERROR)`\r\nI handled the error manually by adding this line to the config:\r\n`<hadoop_kerberos_kinit_command>true</hadoop_kerberos_kinit_command>`\r\nAnd now queries seems to be working but with empty result. On clickhouse 21.3 version there was no useful infos in logs, installed 21.9 on separate host and now I can see this in logs:\r\n`2021.11.05 15:21:17.327058 [ 4622 ] {0f9419ca-ec0e-4911-b970-8bd03589d312} <Warning> StorageHDFS: No file in HDFS matches the path: hdfs://namenode-1.hadoop.xxxx.xxx.xx:8020/user/xxx/test_chtable1`\r\nI have checked on hdfs, file exists, user has privileges to this file. Tried creating new one with different format and still its the same. \r\nWhat is weird that insert seems to be working.\r\n\r\nConnecting by giving hostname, through nameservice, to hdfs engine table, directly to host without hdfs engine table still the same result. I have done everything based on [documentation](https://clickhouse.com/docs/en/engines/table-engines/integrations/hdfs/)\r\nI would appreciate any ideas what is going on here. Didn't find anyone reporting such problems.\r\n\r\nThe same problem with 21.11\r\nHdfs 2.6","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31107/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31106","id":1045878489,"node_id":"I_kwDOA5dJV84-VtbZ","number":31106,"title":"Request update to behaviour of `domain` and `toplevelDomain` functions for single labels.","user":{"login":"saradickinson","id":6229980,"node_id":"MDQ6VXNlcjYyMjk5ODA=","avatar_url":"https://avatars.githubusercontent.com/u/6229980?v=4","gravatar_id":"","url":"https://api.github.com/users/saradickinson","html_url":"https://github.com/saradickinson","followers_url":"https://api.github.com/users/saradickinson/followers","following_url":"https://api.github.com/users/saradickinson/following{/other_user}","gists_url":"https://api.github.com/users/saradickinson/gists{/gist_id}","starred_url":"https://api.github.com/users/saradickinson/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/saradickinson/subscriptions","organizations_url":"https://api.github.com/users/saradickinson/orgs","repos_url":"https://api.github.com/users/saradickinson/repos","events_url":"https://api.github.com/users/saradickinson/events{/privacy}","received_events_url":"https://api.github.com/users/saradickinson/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-05T13:55:04Z","updated_at":"2022-01-06T18:32:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\n[Documentation for the functions mentioned here ](https://github.com/ClickHouse/ClickHouse/blob/0b0b481825ba2e71074823d2d0bbce043e6e9b4f/docs/en/sql-reference/functions/url-functions.md)\r\nBoth the `domain` and `toplevelDomain` functions return an empty string if a single label is supplied e.g. `com`.\r\n\r\n**How to reproduce**\r\n* 21.3.17.2\r\n* tested with clickhouse-client\r\n```\r\n:) select domain('com')\r\n\r\nSELECT domain('com')\r\n\r\nQuery id: f235b335-c139-4ebb-856a-f2c797262c93\r\n\r\n┌─domain('com')─┐\r\n│               │\r\n└───────────────┘\r\n```\r\n\r\n**Expected behavior**\r\nA single label is technically a valid domain name. It would be nice if `.com`, `com` and `com.` all returned `com` as the TLD from this function. I realise the documentation makes clear the functions are not RFC compliant, but it would make them more useful for parsing domain names if the behaviour was updated. [QNAME Minimisation](https://datatracker.ietf.org/doc/html/rfc7816) use is increasing so single label queries from recursives to authoritatives are becoming more common.\r\n\r\n**Documentation**\r\nIf the behaviour isn't updated, then perhaps this can just be documented more clearly - thanks!","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31106/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31094","id":1045437496,"node_id":"I_kwDOA5dJV84-UBw4","number":31094,"title":"GDELT dataset","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":3360106261,"node_id":"MDU6TGFiZWwzMzYwMTA2MjYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/dataset","name":"dataset","color":"00CCCC","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-05T04:13:43Z","updated_at":"2021-11-05T04:13:43Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://www.gdeltproject.org/data.html","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31094/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31093","id":1045426562,"node_id":"I_kwDOA5dJV84-T_GC","number":31093,"title":"s3-style URL does not work.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-05T03:45:58Z","updated_at":"2021-11-08T07:42:43Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the issue**\r\n\r\nThis works successfully:\r\n```\r\naws s3 cp 'hits.csv' 's3://milovidov-clickhouse-test/hits.csv'\r\n```\r\n\r\nThis does not:\r\n```\r\nSELECT count() FROM s3('s3://milovidov-clickhouse-test/hits.csv', '...', '...', 'CSV', 'WatchID UInt64, JavaEnable UInt8, Title String, GoodEvent Int16, EventTime DateTime, EventDate Date, CounterID UInt32, ClientIP UInt32, RegionID UInt32, UserID UInt64, CounterClass Int8, OS UInt8, UserAgent UInt8, URL String, Referer String, Refresh UInt8, RefererCategoryID UInt16, RefererRegionID UInt32, URLCategoryID UInt16, URLRegionID UInt32, ResolutionWidth UInt16, ResolutionHeight UInt16, ResolutionDepth UInt8, FlashMajor UInt8, FlashMinor UInt8, FlashMinor2 String, NetMajor UInt8, NetMinor UInt8, UserAgentMajor UInt16, UserAgentMinor FixedString(2), CookieEnable UInt8, JavascriptEnable UInt8, IsMobile UInt8, MobilePhone UInt8, MobilePhoneModel String, Params String, IPNetworkID UInt32, TraficSourceID Int8, SearchEngineID UInt16, SearchPhrase String, AdvEngineID UInt8, IsArtifical UInt8, WindowClientWidth UInt16, WindowClientHeight UInt16, ClientTimeZone Int16, ClientEventTime DateTime, SilverlightVersion1 UInt8, SilverlightVersion2 UInt8, SilverlightVersion3 UInt32, SilverlightVersion4 UInt16, PageCharset String, CodeVersion UInt32, IsLink UInt8, IsDownload UInt8, IsNotBounce UInt8, FUniqID UInt64, OriginalURL String, HID UInt32, IsOldCounter UInt8, IsEvent UInt8, IsParameter UInt8, DontCountHits UInt8, WithHash UInt8, HitColor FixedString(1), LocalEventTime DateTime, Age UInt8, Sex UInt8, Income UInt8, Interests UInt16, Robotness UInt8, RemoteIP UInt32, WindowName Int32, OpenerName Int32, HistoryLength Int16, BrowserLanguage FixedString(2), BrowserCountry FixedString(2), SocialNetwork String, SocialAction String, HTTPError UInt16, SendTiming UInt32, DNSTiming UInt32, ConnectTiming UInt32, ResponseStartTiming UInt32, ResponseEndTiming UInt32, FetchTiming UInt32, SocialSourceNetworkID UInt8, SocialSourcePage String, ParamPrice Int64, ParamOrderID String, ParamCurrency FixedString(3), ParamCurrencyID UInt16, OpenstatServiceName String, OpenstatCampaignID String, OpenstatAdID String, OpenstatSourceID String, UTMSource String, UTMMedium String, UTMCampaign String, UTMContent String, UTMTerm String, FromTag String, HasGCLID UInt8, RefererHash UInt64, URLHash UInt64, CLID UInt32')\r\n```\r\n\r\nIt argues about the URL:\r\n\r\n```\r\nCode: 36. DB::Exception: Bucket or key name are invalid in S3 URI. (BAD_ARGUMENTS)\r\n```\r\n\r\nBut I'm using the same URL as with `s3 aws` tool :(","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31093/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31092","id":1045363005,"node_id":"I_kwDOA5dJV84-Tvk9","number":31092,"title":" Is there  such function：encodeURLComponent?","user":{"login":"lichong2005","id":3353561,"node_id":"MDQ6VXNlcjMzNTM1NjE=","avatar_url":"https://avatars.githubusercontent.com/u/3353561?v=4","gravatar_id":"","url":"https://api.github.com/users/lichong2005","html_url":"https://github.com/lichong2005","followers_url":"https://api.github.com/users/lichong2005/followers","following_url":"https://api.github.com/users/lichong2005/following{/other_user}","gists_url":"https://api.github.com/users/lichong2005/gists{/gist_id}","starred_url":"https://api.github.com/users/lichong2005/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/lichong2005/subscriptions","organizations_url":"https://api.github.com/users/lichong2005/orgs","repos_url":"https://api.github.com/users/lichong2005/repos","events_url":"https://api.github.com/users/lichong2005/events{/privacy}","received_events_url":"https://api.github.com/users/lichong2005/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-05T01:10:01Z","updated_at":"2022-01-24T15:39:15Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"there is the function : decodeURLComponent ,why not encodeURLComponent?\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31092/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31091","id":1045357574,"node_id":"I_kwDOA5dJV84-TuQG","number":31091,"title":"Replica recognizes that part is corrupted, deletes it, but does not fetch from another replica","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":3167230064,"node_id":"MDU6TGFiZWwzMTY3MjMwMDY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.8-affected","name":"v21.8-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-05T00:57:14Z","updated_at":"2021-11-10T17:03:56Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"2 replicas - 1 shard\r\n\r\n```sql\r\ncreate table test_corr on cluster all ( A Int64, S String) \r\nEngine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/test_corr4','{replica}') \r\nOrder by tuple() settings min_bytes_for_wide_part=0;\r\n\r\ninsert into test_corr select number, '' from numbers(100000000);\r\n\r\noptimize table test_corr final;\r\n\r\n--replica 1\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\n┌───count()─┬─_part──────┐\r\n│ 100000000 │ all_0_95_4 │\r\n└───────────┴────────────┘\r\n\r\n--replica 2\r\n┌───count()─┬─_part──────┐\r\n│ 100000000 │ all_0_95_4 │\r\n└───────────┴────────────┘\r\n\r\n--replica 1\r\n>/var/lib/clickhouse/data/default/test_corr/all_0_95_4/S.bin\r\n-rw-r----- 1 clickhouse clickhouse         0 Nov  5 00:44 S.bin\r\n-rw-r----- 1 clickhouse clickhouse    293016 Nov  5 00:43 S.mrk2\r\n\r\nselect distinct S from test_corr;\r\n0 rows in set. Elapsed: 0.004 sec.\r\n\r\nReceived exception from server (version 21.8.7):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: \r\nCan't adjust last granule because it has 8161 rows, but try to subtract 65505 rows.: While executing MergeTreeThread.\r\n\r\n\r\nSELECT DISTINCT S FROM test_corr;\r\nOk.\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\nOk.\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\n\r\nSELECT now()\r\n┌───────────────now()─┐\r\n│ 2021-11-05 00:45:38 │\r\n└─────────────────────┘\r\n\r\n\r\nSELECT now()\r\n┌───────────────now()─┐\r\n│ 2021-11-05 00:55:58 │\r\n└─────────────────────┘\r\n\r\n\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\nOk.\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\n\r\n--replica 2 is OK\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\n┌───count()─┬─_part──────┐\r\n│ 100000000 │ all_0_95_4 │\r\n└───────────┴────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31091/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31090","id":1045351201,"node_id":"I_kwDOA5dJV84-Tssh","number":31090,"title":"CH does not see that part is corrupted.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-05T00:39:24Z","updated_at":"2021-11-10T00:13:25Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"2 replica - 1 shard.\r\n\r\n```sql\r\ncreate table test_corr on cluster all ( A Int64, S String) \r\nEngine = ReplicatedMergeTree('/clickhouse/{cluster}/tables/test_corr3','{replica}') Order by tuple() settings min_bytes_for_wide_part=0;\r\n\r\ninsert into test_corr select number, '' from numbers(100000);\r\n\r\noptimize table test_corr final;\r\n\r\n-- replica 1\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\n┌─count()─┬─_part─────┐\r\n│  100000 │ all_0_0_1 │\r\n└─────────┴───────────┘\r\n\r\n-- replica 2\r\nSELECT count(), _part　FROM test_corr　GROUP BY _part;\r\n┌─count()─┬─_part─────┐\r\n│  100000 │ all_0_0_1 │\r\n└─────────┴───────────┘\r\n\r\n-- replica 1\r\n-- corrupt part\r\n>/var/lib/clickhouse/data/default/test_corr/all_0_0_1/S.bin\r\n\r\nls -l /var/lib/clickhouse/data/default/test_corr/all_0_0_1/S.*\r\n-rw-r----- 1 clickhouse clickhouse   0 Nov  4 23:52 /var/lib/clickhouse/data/default/test_corr/all_0_0_1/S.bin\r\n-rw-r----- 1 clickhouse clickhouse 336 Nov  4 23:51 /var/lib/clickhouse/data/default/test_corr/all_0_0_1/S.mrk2\r\n\r\n\r\nselect now();\r\n┌───────────────now()─┐\r\n│ 2021-11-04 23:54:33 │\r\n└─────────────────────┘\r\n\r\nSELECT DISTINCT S FROM test_corr;\r\nReceived exception from server (version 21.8.7):\r\nCode: 49. DB::Exception: Received from localhost:9000. DB::Exception: \r\nCan't adjust last granule because it has 8161 rows, but try to subtract 65505 rows.: \r\nWhile executing MergeTree.\r\n\r\n--- No error ????????????????????????????\r\nselect  S from test_corr limit 2;\r\nOk.\r\n0 rows in set. Elapsed: 0.065 sec.\r\n\r\n\r\n-- 40 minutes later\r\nSELECT now()\r\n┌───────────────now()─┐\r\n│ 2021-11-05 00:38:08 │\r\n└─────────────────────┘\r\n\r\n\r\nSELECT DISTINCT S FROM test_corr;\r\n\r\nReceived exception from server (version 21.8.7):\r\nCode: 49. DB::Exception: Received from localhost:9000. \r\nDB::Exception: Can't adjust last granule because it has 8161 rows, but try to subtract 65505 rows.: \r\nWhile executing MergeTree.\r\n\r\n\r\nselect  S from test_corr limit 2;\r\nOk.\r\n0 rows in set. Elapsed: 0.086 sec.\r\n\r\n\r\n-- replica 2 all is OK.\r\nSELECT DISTINCT S FROM test_corr;\r\n┌─S─┐\r\n│   │\r\n└───┘\r\n1 rows in set. Elapsed: 0.001 sec. Processed 100.00 thousand rows, 900.00 KB (67.25 million rows/s., 605.22 MB/s.)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31090/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31089","id":1045350959,"node_id":"I_kwDOA5dJV84-Tsov","number":31089,"title":"Support more secure password storage","user":{"login":"paulwouters","id":83961974,"node_id":"MDQ6VXNlcjgzOTYxOTc0","avatar_url":"https://avatars.githubusercontent.com/u/83961974?v=4","gravatar_id":"","url":"https://api.github.com/users/paulwouters","html_url":"https://github.com/paulwouters","followers_url":"https://api.github.com/users/paulwouters/followers","following_url":"https://api.github.com/users/paulwouters/following{/other_user}","gists_url":"https://api.github.com/users/paulwouters/gists{/gist_id}","starred_url":"https://api.github.com/users/paulwouters/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/paulwouters/subscriptions","organizations_url":"https://api.github.com/users/paulwouters/orgs","repos_url":"https://api.github.com/users/paulwouters/repos","events_url":"https://api.github.com/users/paulwouters/events{/privacy}","received_events_url":"https://api.github.com/users/paulwouters/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-11-05T00:38:44Z","updated_at":"2021-11-06T01:08:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Currently, the option to store a password is basically cleartext or sha256(passwprd). These are very weak.\r\n\r\nIt would be good to add a method like argon2 (RFC 9106) or PBKDF2 (older and weaker, but NIST compliant). Or to get something modern secure and still NIST compliant by using pbkdf2(argon2(password, ...)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31089/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31083","id":1045083685,"node_id":"I_kwDOA5dJV84-SrYl","number":31083,"title":"Use PGM index for ASOF JOIN","user":{"login":"4ertus2","id":8061274,"node_id":"MDQ6VXNlcjgwNjEyNzQ=","avatar_url":"https://avatars.githubusercontent.com/u/8061274?v=4","gravatar_id":"","url":"https://api.github.com/users/4ertus2","html_url":"https://github.com/4ertus2","followers_url":"https://api.github.com/users/4ertus2/followers","following_url":"https://api.github.com/users/4ertus2/following{/other_user}","gists_url":"https://api.github.com/users/4ertus2/gists{/gist_id}","starred_url":"https://api.github.com/users/4ertus2/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/4ertus2/subscriptions","organizations_url":"https://api.github.com/users/4ertus2/orgs","repos_url":"https://api.github.com/users/4ertus2/repos","events_url":"https://api.github.com/users/4ertus2/events{/privacy}","received_events_url":"https://api.github.com/users/4ertus2/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-04T18:36:47Z","updated_at":"2021-12-15T16:52:49Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"It's possible to improve ASOF JOIN performance using PGM index.\r\nLecture (in Russian): https://disk.yandex.ru/i/YfTgLe-ViuEXIw\r\n\r\nCurrent SortedLookupVector class uses binary search to find nearest value. While it's impossible to tune worst case it's possible to speedup real data cases using Piecewise Geometric Model index (https://pgm.di.unipi.it/) here.\r\n\r\nSomeone could implement PGMLookupVector and use it instead of SortedLookupVector in AsofRowRefs.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31083/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31074","id":1044522402,"node_id":"I_kwDOA5dJV84-QiWi","number":31074,"title":"AWS S3 interface endpoint support in ClickHouse ","user":{"login":"davidshtian","id":14228056,"node_id":"MDQ6VXNlcjE0MjI4MDU2","avatar_url":"https://avatars.githubusercontent.com/u/14228056?v=4","gravatar_id":"","url":"https://api.github.com/users/davidshtian","html_url":"https://github.com/davidshtian","followers_url":"https://api.github.com/users/davidshtian/followers","following_url":"https://api.github.com/users/davidshtian/following{/other_user}","gists_url":"https://api.github.com/users/davidshtian/gists{/gist_id}","starred_url":"https://api.github.com/users/davidshtian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/davidshtian/subscriptions","organizations_url":"https://api.github.com/users/davidshtian/orgs","repos_url":"https://api.github.com/users/davidshtian/repos","events_url":"https://api.github.com/users/davidshtian/events{/privacy}","received_events_url":"https://api.github.com/users/davidshtian/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-04T09:35:49Z","updated_at":"2021-11-04T09:35:49Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nAs [AWS PrivateLink for Amazon S3](https://aws.amazon.com/blogs/aws/aws-privatelink-for-amazon-s3-now-available/) has been generally available in Feb 2021, it would be great to support accessing AWS S3 through interface endpoint in ClickHouse. Amazon S3 interface endpoint does allow access from on premises, so it would be necessary for on premises or hybrid environment ClickHouse users to have this feature enabled to access the S3 data securely.\r\n\r\nHowever, the S3 interface endpoint DNS names are like this _**vpce-1a2b3c4d-5e6f.s3.us-east-1.vpce.amazonaws.com**_, which is not supported in current ClickHouse versions (might be some legacy DNS name parsing code issues). Hope ClickHouse community could support AWS S3 interface endpoint. Thanks~\r\n\r\n**Describe the solution you'd like**\r\n\r\nAdd the alternative DNS name parsing like **_s3.us-east-1.vpce.amazonaws.com_** besides of **_s3.us-east-1.amazonaws.com_**.\r\n\r\n**Describe alternatives you've considered**\r\n\r\nFor workaround, users have to hard code DNS resolution like editing _/etc/hosts_ file, but this is not a long-term graceful solution.\r\n\r\n**Additional context**\r\n\r\nPlease check #31065 for the same issue. Thanks~\r\n\r\nNote: More information about AWS PrivateLink for Amazon S3, please refer to https://docs.aws.amazon.com/AmazonS3/latest/userguide/privatelink-interface-endpoints.html.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31074/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31073","id":1044502356,"node_id":"I_kwDOA5dJV84-QddU","number":31073,"title":"Query parameters doesn't support TAB in string. (or other control characters)","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-04T09:11:30Z","updated_at":"2021-11-04T09:11:30Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nIt's not possible to have TAB in string passed via query parameters.\r\n\r\n**How to reproduce**\r\nClickHouse 21.8\r\n```\r\n$ clickhouse-client -mn --query \"SELECT 'lalala something'\"\r\nlalala\\tsomething\r\n\r\n$ clickhouse-client -mn --query \"SELECT {query_3:String}\" --param_query_3='lalala\tsomething'\r\n\r\n\r\nError on processing query: Code: 457. DB::Exception: Value lalala       something cannot be parsed as String for query parameter 'query_3' because it isn't parsed completely: only 6 of 16 bytes was parsed: lalala. (BAD_QUERY_PARAMETER), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x9b3eb14 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long, unsigned long, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, unsigned long&&, unsigned long&&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) @ 0x126b5941 in /usr/bin/clickhouse\r\n2. DB::ReplaceQueryParameterVisitor::visitQueryParameter(std::__1::shared_ptr<DB::IAST>&) @ 0x126b5204 in /usr/bin/clickhouse\r\n3. DB::ReplaceQueryParameterVisitor::visit(std::__1::shared_ptr<DB::IAST>&) @ 0x126b4adb in /usr/bin/clickhouse\r\n4. DB::ReplaceQueryParameterVisitor::visit(std::__1::shared_ptr<DB::IAST>&) @ 0x126b4adb in /usr/bin/clickhouse\r\n5. DB::ReplaceQueryParameterVisitor::visit(std::__1::shared_ptr<DB::IAST>&) @ 0x126b4adb in /usr/bin/clickhouse\r\n6. DB::ReplaceQueryParameterVisitor::visit(std::__1::shared_ptr<DB::IAST>&) @ 0x126b4adb in /usr/bin/clickhouse\r\n7. DB::ClientBase::processOrdinaryQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::IAST>) @ 0x12f1caf3 in /usr/bin/clickhouse\r\n8. DB::ClientBase::processParsedSingleQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::IAST>, std::__1::optional<bool>, bool) @ 0x12f1bc8f in /usr/bin/clickhouse\r\n9. DB::Client::executeMultiQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x9bfb1c2 in /usr/bin/clickhouse\r\n10. DB::ClientBase::processQueryText(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) @ 0x12f225c6 in /usr/bin/clickhouse\r\n11. DB::ClientBase::runNonInteractive() @ 0x12f24b05 in /usr/bin/clickhouse\r\n12. DB::Client::main(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x9bfd487 in /usr/bin/clickhouse\r\n13. Poco::Util::Application::run() @ 0x15c64043 in /usr/bin/clickhouse\r\n14. mainEntryClickHouseClient(int, char**) @ 0x9c09bb5 in /usr/bin/clickhouse\r\n15. main @ 0x9b3906a in /usr/bin/clickhouse\r\n16. __libc_start_main @ 0x270b3 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n17. _start @ 0x9afe7ae in /usr/bin/clickhouse\r\n (version 21.11.1.8526)\r\n(query: SELECT {query_3:String})\r\n```\r\n\r\n**Expected behavior**\r\nBoth queries should work.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31073/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31070","id":1044431716,"node_id":"I_kwDOA5dJV84-QMNk","number":31070,"title":"Bitmap can only support Int. When can they be extended to UUID?","user":{"login":"ch0shim","id":43160338,"node_id":"MDQ6VXNlcjQzMTYwMzM4","avatar_url":"https://avatars.githubusercontent.com/u/43160338?v=4","gravatar_id":"","url":"https://api.github.com/users/ch0shim","html_url":"https://github.com/ch0shim","followers_url":"https://api.github.com/users/ch0shim/followers","following_url":"https://api.github.com/users/ch0shim/following{/other_user}","gists_url":"https://api.github.com/users/ch0shim/gists{/gist_id}","starred_url":"https://api.github.com/users/ch0shim/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ch0shim/subscriptions","organizations_url":"https://api.github.com/users/ch0shim/orgs","repos_url":"https://api.github.com/users/ch0shim/repos","events_url":"https://api.github.com/users/ch0shim/events{/privacy}","received_events_url":"https://api.github.com/users/ch0shim/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-04T07:26:30Z","updated_at":"2021-11-04T07:26:51Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> (you don't have to strictly follow this form)\r\n\r\n**Use case**\r\n\r\n> A clear and concise description of what is the intended usage scenario is.\r\n\r\n**Describe the solution you'd like**\r\n\r\n> A clear and concise description of what you want to happen.\r\n\r\n**Describe alternatives you've considered**\r\n\r\n> A clear and concise description of any alternative solutions or features you've considered.\r\n\r\n**Additional context**\r\n\r\n> Add any other context or screenshots about the feature request here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31070/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31068","id":1044392897,"node_id":"I_kwDOA5dJV84-QCvB","number":31068,"title":"Zookeeper node loss using ReplicatedAggregatingMergeTree engine","user":{"login":"jztian","id":31120351,"node_id":"MDQ6VXNlcjMxMTIwMzUx","avatar_url":"https://avatars.githubusercontent.com/u/31120351?v=4","gravatar_id":"","url":"https://api.github.com/users/jztian","html_url":"https://github.com/jztian","followers_url":"https://api.github.com/users/jztian/followers","following_url":"https://api.github.com/users/jztian/following{/other_user}","gists_url":"https://api.github.com/users/jztian/gists{/gist_id}","starred_url":"https://api.github.com/users/jztian/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jztian/subscriptions","organizations_url":"https://api.github.com/users/jztian/orgs","repos_url":"https://api.github.com/users/jztian/repos","events_url":"https://api.github.com/users/jztian/events{/privacy}","received_events_url":"https://api.github.com/users/jztian/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-11-04T06:05:25Z","updated_at":"2021-11-17T07:11:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"> You have to provide the following information whenever possible.\r\n\r\nWhen using the replicatedaggregatingmergetree engine, when we write data to the distributed table or perform (optimize table) merging parts, we will report an error that the node on the zookeeper is lost. The specific error report is shown in the figure below：\r\n![image](https://user-images.githubusercontent.com/31120351/140265542-5b4214dc-0a13-42b6-bed7-40b957ba18ab.png)\r\n\r\n\r\nCREATE TABLE  order_db.t_trade_order_payment_thirdparty_coupon on cluster ch_2shards_3replicas (\r\n  `id`                       Int64 NOT NULL,\r\n  `order_id`                 SimpleAggregateFunction(anyLast, Int64)  NOT NULL COMMENT 'order id',\r\n  `created_at`               SimpleAggregateFunction(anyLast, Nullable(DateTime)),\r\n  `updated_at`               SimpleAggregateFunction(anyLast, Nullable(DateTime)),\r\n  `created_by`               SimpleAggregateFunction(anyLast, Nullable(String)),\r\n  `updated_by`               SimpleAggregateFunction(anyLast, Nullable(String)),\r\n  `last_updated_at`          SimpleAggregateFunction(anyLast, DateTime) NOT NULL  COMMENT '最后更新时间'\r\n) ENGINE = ReplicatedAggregatingMergeTree( '/clickhouse1/table/{shard}/order_db/t_trade_order_payment_thirdparty_coupon','{replica}') \r\nPARTITION BY (intHash64(id) % 6)\r\nORDER BY id\r\nSETTINGS index_granularity = 8192,use_minimalistic_part_header_in_zookeeper = 1,storage_policy = 'policy_all';\r\n\r\n\r\nCREATE TABLE   order_db.t_trade_order_payment_thirdparty_coupon_all ON CLUSTER ch_2shards_3replicas AS order_db.t_trade_order_payment_thirdparty_coupon\r\nENGINE = Distributed(ch_2shards_3replicas, order_db, t_trade_order_payment_thirdparty_coupon, intHash64(id));\r\n\r\n> A clear and concise description of what works not as it is supposed to.\r\n\r\n    ClickHouse server version 21.8.9.13 (official build)\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31068/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31065","id":1044297383,"node_id":"I_kwDOA5dJV84-Pran","number":31065,"title":"AWS S3 interface endpoint support ?","user":{"login":"KageYang","id":10446524,"node_id":"MDQ6VXNlcjEwNDQ2NTI0","avatar_url":"https://avatars.githubusercontent.com/u/10446524?v=4","gravatar_id":"","url":"https://api.github.com/users/KageYang","html_url":"https://github.com/KageYang","followers_url":"https://api.github.com/users/KageYang/followers","following_url":"https://api.github.com/users/KageYang/following{/other_user}","gists_url":"https://api.github.com/users/KageYang/gists{/gist_id}","starred_url":"https://api.github.com/users/KageYang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KageYang/subscriptions","organizations_url":"https://api.github.com/users/KageYang/orgs","repos_url":"https://api.github.com/users/KageYang/repos","events_url":"https://api.github.com/users/KageYang/events{/privacy}","received_events_url":"https://api.github.com/users/KageYang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"Ryado","id":5509570,"node_id":"MDQ6VXNlcjU1MDk1NzA=","avatar_url":"https://avatars.githubusercontent.com/u/5509570?v=4","gravatar_id":"","url":"https://api.github.com/users/Ryado","html_url":"https://github.com/Ryado","followers_url":"https://api.github.com/users/Ryado/followers","following_url":"https://api.github.com/users/Ryado/following{/other_user}","gists_url":"https://api.github.com/users/Ryado/gists{/gist_id}","starred_url":"https://api.github.com/users/Ryado/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ryado/subscriptions","organizations_url":"https://api.github.com/users/Ryado/orgs","repos_url":"https://api.github.com/users/Ryado/repos","events_url":"https://api.github.com/users/Ryado/events{/privacy}","received_events_url":"https://api.github.com/users/Ryado/received_events","type":"User","site_admin":false},"assignees":[{"login":"Ryado","id":5509570,"node_id":"MDQ6VXNlcjU1MDk1NzA=","avatar_url":"https://avatars.githubusercontent.com/u/5509570?v=4","gravatar_id":"","url":"https://api.github.com/users/Ryado","html_url":"https://github.com/Ryado","followers_url":"https://api.github.com/users/Ryado/followers","following_url":"https://api.github.com/users/Ryado/following{/other_user}","gists_url":"https://api.github.com/users/Ryado/gists{/gist_id}","starred_url":"https://api.github.com/users/Ryado/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Ryado/subscriptions","organizations_url":"https://api.github.com/users/Ryado/orgs","repos_url":"https://api.github.com/users/Ryado/repos","events_url":"https://api.github.com/users/Ryado/events{/privacy}","received_events_url":"https://api.github.com/users/Ryado/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-11-04T02:25:42Z","updated_at":"2022-01-24T15:41:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"When setting up AWS S3 interface endpoint in merge-s3.xml configuration file , it showed the error as follows (DNS name resolving error) . Thus , does Clickhouse support AWS s3 interface endpoint at the moment ? Please kindly adivse. Tks\r\n\r\nError Message:\r\n2021.11.02 04:00:02.232923 [ 3234 ] {} <Error> AWSClient: Failed to make request to: **https://s3.ap-northeast-1.vpce.amazonaws.com/clickhouse-********.vpce-xxxxxxxxxxxxx-5zavnc2x/mergetree/**phakvwaqrjfzzjxxmyqlyxyeaduhsyht: Poco::Exception. Code: 1000, e.code() = 0, e.displayText() = Host not found: s3.ap-northeast-1.vpce.amazonaws.com, Stack trace (when copying this message, always include the lines below):\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31065/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31064","id":1044267414,"node_id":"I_kwDOA5dJV84-PkGW","number":31064,"title":"executable engine - respect limit","user":{"login":"mikeTWC1984","id":31977106,"node_id":"MDQ6VXNlcjMxOTc3MTA2","avatar_url":"https://avatars.githubusercontent.com/u/31977106?v=4","gravatar_id":"","url":"https://api.github.com/users/mikeTWC1984","html_url":"https://github.com/mikeTWC1984","followers_url":"https://api.github.com/users/mikeTWC1984/followers","following_url":"https://api.github.com/users/mikeTWC1984/following{/other_user}","gists_url":"https://api.github.com/users/mikeTWC1984/gists{/gist_id}","starred_url":"https://api.github.com/users/mikeTWC1984/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mikeTWC1984/subscriptions","organizations_url":"https://api.github.com/users/mikeTWC1984/orgs","repos_url":"https://api.github.com/users/mikeTWC1984/repos","events_url":"https://api.github.com/users/mikeTWC1984/events{/privacy}","received_events_url":"https://api.github.com/users/mikeTWC1984/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-04T01:15:47Z","updated_at":"2021-11-04T01:38:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n**Describe the unexpected behaviour**\r\n\r\nLIMIT clause has no effect on executable \r\n\r\n```sql\r\nselect * from executable('myscript', 'TabSeparated', 'value String')\r\nlimit 100 \r\n```\r\nexecution time is the same with or without LIMIT clause\r\n\r\n**Expected behavior**\r\n\r\nChild process should be killed once LIMIT is reached (limit 100 should act like **head -n 100**)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31064/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31061","id":1044062561,"node_id":"I_kwDOA5dJV84-OyFh","number":31061,"title":"Corrupted data part doesn't get restored automatically from replica","user":{"login":"malikas05","id":23461587,"node_id":"MDQ6VXNlcjIzNDYxNTg3","avatar_url":"https://avatars.githubusercontent.com/u/23461587?v=4","gravatar_id":"","url":"https://api.github.com/users/malikas05","html_url":"https://github.com/malikas05","followers_url":"https://api.github.com/users/malikas05/followers","following_url":"https://api.github.com/users/malikas05/following{/other_user}","gists_url":"https://api.github.com/users/malikas05/gists{/gist_id}","starred_url":"https://api.github.com/users/malikas05/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/malikas05/subscriptions","organizations_url":"https://api.github.com/users/malikas05/orgs","repos_url":"https://api.github.com/users/malikas05/repos","events_url":"https://api.github.com/users/malikas05/events{/privacy}","received_events_url":"https://api.github.com/users/malikas05/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-03T20:03:06Z","updated_at":"2021-11-05T03:23:43Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi all!\r\n\r\nMy team and I have been working on setting up the Clickhouse cluster and POCing it as the solution to our analytical data store. As part of this effort, in preparation for rolling out the cluster, we were conducting different experiments in order to know more about the behavior as well as be ready for any unexpected outcomes. One of such experiments was to corrupt some data parts and observe whether those parts would be recovered automatically from the second replica. (By the way, just to give you some context on how our cluster looks like -- it contains 3 shards, 2 replicas each where most of the tables use the **ReplicatedSummingMergeTree** engine.) However, it appears that data corruption in Clickhouse is discovered/repaired from replicas only when data is accessed (e.g., via a query). This is also mentioned in the official Clickhouse docs found here: \r\n\r\n> When the server starts (or establishes a new session with ZooKeeper), it only checks the quantity and sizes of all files. If the file sizes match but bytes have been changed somewhere in the middle, this is not detected immediately, but only when attempting to read the data for a SELECT query. The query throws an exception about a non-matching checksum or size of a compressed block. In this case, data parts are added to the verification queue and copied from the replicas if necessary.\r\nhttps://clickhouse.com/docs/en/engines/table-engines/mergetree-family/replication/#recovery-after-failures\r\n\r\nThis has been also verified by us. We executed the query to pull some data from that specific broken data part, and initially, it returned the exception:\r\n> Code: 33. DB::Exception: Received from ******.com:9000. DB::Exception: Cannot read all data. Bytes read: 22694. Bytes expected: 29294.: (while reading column ******): (while reading from part /opt/ch-data/store/719/7191f35e-d754-45f4-b191-f35ed75485f4/1606626000_1_1_3/ from mark 72 with max_rows_to_read = 8192): While executing MergeTreeThread.\r\n\r\nHowever, the second attempt was successful, and the returned dataset was exactly the same as the one returned by replica 2. It was also confirmed that the broken data part appeared in the /detached folder and it was restored in the filesystem. \r\n\r\nThe point is that we assumed that the recovery would be kicked off automatically in the background rather than after some action triggered by the client. This leads us to start being anxious about the possible complete data loss when the repair process is not identified automatically. For example, the following set of circumstances could lead to complete data loss:\r\n1. Data gets corrupted on replica1 at time t0.\r\n2. Same data part on replica 2 gets corrupted sometime later, say t0 + 10 days.\r\n2. Corrupted data on replica1 is only accessed and attempted to get restored from replica2 at time t0 + 20 days.\r\n\r\nObviously, if the corrupted data was identified/fixed quickly, then there is a low risk of replica2 getting corrupted before replica1 has a chance to repair its data. However, in reality, the data may not be recognized as corrupted for a long time (e.g., 20 days, as in the example above), then it means that during that whole time that we're running on a single, accurate replica, there's a risk of us completely losing the data.\r\n\r\n**As such, we wanted to know whether Clickhouse provides any other mechanism/option for recognizing corrupted data more quickly. If there is no such, does anyone have any experience tackling this problem? Any suggestions/insights will be much appreciated. Thank you in advance!**","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31061/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31058","id":1043735177,"node_id":"I_kwDOA5dJV84-NiKJ","number":31058,"title":"SET DEFAULT ROLE  support for ON CLUSTER ","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-03T15:20:05Z","updated_at":"2021-11-03T15:20:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nAbility to set default role on cluster for user.\r\n\r\n**Describe the solution you'd like**\r\n\r\n```\r\nSET DEFAULT ROLE ON CLUSTER 'xxx'  role1 TO user\r\nOR\r\nSET ON CLUSTER 'xxx' DEFAULT ROLE   role1 TO user\r\n```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31058/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31054","id":1043603964,"node_id":"I_kwDOA5dJV84-NCH8","number":31054,"title":"Reestablishing MySQL connection takes very long time","user":{"login":"ibradwan","id":67283130,"node_id":"MDQ6VXNlcjY3MjgzMTMw","avatar_url":"https://avatars.githubusercontent.com/u/67283130?v=4","gravatar_id":"","url":"https://api.github.com/users/ibradwan","html_url":"https://github.com/ibradwan","followers_url":"https://api.github.com/users/ibradwan/followers","following_url":"https://api.github.com/users/ibradwan/following{/other_user}","gists_url":"https://api.github.com/users/ibradwan/gists{/gist_id}","starred_url":"https://api.github.com/users/ibradwan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ibradwan/subscriptions","organizations_url":"https://api.github.com/users/ibradwan/orgs","repos_url":"https://api.github.com/users/ibradwan/repos","events_url":"https://api.github.com/users/ibradwan/events{/privacy}","received_events_url":"https://api.github.com/users/ibradwan/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-03T13:37:05Z","updated_at":"2021-11-03T14:45:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello there,\r\n\r\nWe have been noticing some unexpected behavior with the queries that join between a ClickHouse table and an external MySQL table. It seems like that the connection to the MySQL server gets dropped and takes a lot of time (sometimes up to 20 mins) to be reestablished.\r\n\r\nAfter some investigations, this is what seems to be happening:\r\nOne query starts processing and it takes much more time than expected (e.g., 4000x). When checking the logs, almost all of the execution time was between these two steps:\r\n```\r\n2021.11.03 06:07:52.779615 [ 1913 ] {fcd65360-5bd0-4d65-ab21-91b535d804f0} <Debug> executeQuery: (from [::ffff:10.10.40.253]:49255, user: reader)\r\n2021.11.03 06:23:30.749391 [ 1913 ] {fcd65360-5bd0-4d65-ab21-91b535d804f0} <Information> Application: Connection to mysql server has been reestablished. Connection id changed: [ERRFMT] -> [ERRFMT]\r\n```\r\nAs you can see there are around 16 mins between the two logs. Any queries that arrive during that period keep waiting. When the clock hits `2021.11.03 06:23:30`, all these blocked queries continue execution and finish in the expected time (less than a second in this case).\r\n\r\nSamples:\r\n```\r\n2021.11.03 06:09:53.317024 [ 25019 ] {d4655c34-4370-4526-bb6c-65cd5cdb2187} <Debug> executeQuery: (from [::ffff:10.10.40.253]:50430, user: reader)\r\n2021.11.03 06:23:30.787029 [ 25019 ] {d4655c34-4370-4526-bb6c-65cd5cdb2187} <Trace> ContextAccess (reader): Access granted: SELECT(ProjectId, UserId, SessionId, IsFavorite) ON mysql.tbl\r\n```\r\n```\r\n2021.11.03 06:11:48.988851 [ 30436 ] {7d994654-fc5c-4ce6-95e1-951d166d7224} <Debug> executeQuery: (from [::ffff:10.10.40.254]:50036, user: reader)\r\n2021.11.03 06:23:30.807949 [ 30436 ] {7d994654-fc5c-4ce6-95e1-951d166d7224} <Trace> ContextAccess (reader): Access granted: SELECT(ProjectId, UserId, SessionId, IsFavorite) ON mysql.tbl\r\n```\r\n\r\nIt's as if they are all using the same MySQL connection from the pool and are all waiting for this process to complete. So I have two questions here:\r\n1. Why the connection could take this long to be reestablished?\r\n2. Why all queries are waiting on the same connection? Should not we have 15 connections by default and even if we just randomize, some of these queries should be using different connections and proceed? (the number of blocked queries in this occurrence was around 20, but this is a recurring pattern that happens almost on a daily basis).\r\n\r\n**Notes:**\r\n- This is not a consistent behavior. Sometimes, the reestablishment process takes no time at all.\r\n- I checked our MySQL server, the number of active connections is not remotely close to the maximum allowed number.\r\n- Our server version is **20.10.7.4**.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31054/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31005","id":1042476685,"node_id":"I_kwDOA5dJV84-Iu6N","number":31005,"title":"Repeated ATTACH PARTITION after modifying target table does nothing","user":{"login":"amralaa-MSFT","id":44748533,"node_id":"MDQ6VXNlcjQ0NzQ4NTMz","avatar_url":"https://avatars.githubusercontent.com/u/44748533?v=4","gravatar_id":"","url":"https://api.github.com/users/amralaa-MSFT","html_url":"https://github.com/amralaa-MSFT","followers_url":"https://api.github.com/users/amralaa-MSFT/followers","following_url":"https://api.github.com/users/amralaa-MSFT/following{/other_user}","gists_url":"https://api.github.com/users/amralaa-MSFT/gists{/gist_id}","starred_url":"https://api.github.com/users/amralaa-MSFT/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amralaa-MSFT/subscriptions","organizations_url":"https://api.github.com/users/amralaa-MSFT/orgs","repos_url":"https://api.github.com/users/amralaa-MSFT/repos","events_url":"https://api.github.com/users/amralaa-MSFT/events{/privacy}","received_events_url":"https://api.github.com/users/amralaa-MSFT/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-02T15:01:30Z","updated_at":"2021-11-02T15:01:30Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"# Description\r\nRepeated ATTACH PARTITION after modifying target table does nothing. On the other hand, attaching after modifying source tables inserts new rows version (in addition to old existing rows version attached before).\r\n\r\n# Expected behavior\r\nOriginal rows from source table should appear after repeated attach following an update in target table.\r\n\r\n# How to reproduce\r\n\r\nUsed ClickHouse server version is 21.7.11.3 (official build).\r\n\r\n1. Create two `ReplicatedMergeTree` tables with sample data using:\r\n```sql\r\nDROP TABLE IF EXISTS test.temp_source_sample_replicated ON CLUSTER clickhouse_cluster;\r\nDROP TABLE IF EXISTS test.temp_target_replicated ON CLUSTER clickhouse_cluster;\r\n\r\nCREATE TABLE test.temp_source_sample_replicated on cluster clickhouse_cluster\r\n(\r\n    `Timestamp` DateTime DEFAULT now(),\r\n    `Id` UInt64 default rand64(),\r\n    `Url` String,\r\n    `Note` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/test/temp_source_sample_replicated/{layer}-{shard}', '{replica}')\r\nPARTITION BY toYYYYMM(Timestamp)\r\nORDER BY (Timestamp, Url, intHash32(Id), Id)\r\nSAMPLE BY intHash32(Id)\r\nSETTINGS storage_policy = 'move_from_ssd_to_hdd', index_granularity = 8192\r\n;\r\n\r\nCREATE TABLE test.temp_target_replicated on cluster clickhouse_cluster\r\n(\r\n    `Timestamp` DateTime DEFAULT now(),\r\n    `Id` UInt64 default rand64(),\r\n    `Url` String,\r\n    `Note` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/test/temp_target_replicated/{layer}-{shard}', '{replica}')\r\nPARTITION BY toYYYYMM(Timestamp)\r\nORDER BY (Timestamp, Url, intHash32(Id), Id)\r\nSAMPLE BY intHash32(Id)\r\nSETTINGS storage_policy = 'move_from_ssd_to_hdd', index_granularity = 8192\r\n;\r\n\r\nINSERT INTO test.temp_source_sample_replicated (Url, Note) VALUES ('inserted_into_source', 'Inserted into source table')  ;\r\nINSERT INTO test.temp_target_replicated (Url, Note) VALUES ('inserted_into_target', 'Inserted into target table')  ;\r\n```\r\n\r\nNow, there should be 1 part in each table:\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Source Table │ 2021-11-02 14:36:32 │ 3439297718530077921 │ inserted_into_source │ Inserted into source table │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Target Table │ 2021-11-02 14:36:32 │ 15576159249491229584 │ inserted_into_target │ Inserted into target table │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n2. Attach source partition to target table:\r\n```sql\r\nALTER TABLE test.temp_target_replicated\r\n    ATTACH PARTITION '202111' FROM test.temp_source_sample_replicated;\r\n```\r\n\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Source Table │ 2021-11-02 14:36:32 │ 3439297718530077921 │ inserted_into_source │ Inserted into source table │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Target Table │ 2021-11-02 14:36:32 │  3439297718530077921 │ inserted_into_source │ Inserted into source table │\r\n│ Target Table │ 2021-11-02 14:36:32 │ 15576159249491229584 │ inserted_into_target │ Inserted into target table │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n3. Update source table:\r\n```sql\r\nALTER TABLE test.temp_source_sample_replicated\r\n    UPDATE Note = concat(Note, ' (mod 1)')\r\n    WHERE 1 = 1\r\n;\r\n```\r\n\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0_1\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────────────┐\r\n│ Source Table │ 2021-11-02 14:36:32 │ 3439297718530077921 │ inserted_into_source │ Inserted into source table (mod 1) │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Target Table │ 2021-11-02 14:36:32 │  3439297718530077921 │ inserted_into_source │ Inserted into source table │\r\n│ Target Table │ 2021-11-02 14:36:32 │ 15576159249491229584 │ inserted_into_target │ Inserted into target table │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n5. Attach source partition to target table again:\r\n```sql\r\nALTER TABLE test.temp_target_replicated\r\n    ATTACH PARTITION '202111' FROM test.temp_source_sample_replicated;\r\n```\r\n\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0_1\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_4_4_0\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────────────┐\r\n│ Source Table │ 2021-11-02 14:36:32 │ 3439297718530077921 │ inserted_into_source │ Inserted into source table (mod 1) │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────────────┐\r\n│ Target Table │ 2021-11-02 14:36:32 │  3439297718530077921 │ inserted_into_source │ Inserted into source table (mod 1) │\r\n│ Target Table │ 2021-11-02 14:36:32 │  3439297718530077921 │ inserted_into_source │ Inserted into source table         │\r\n│ Target Table │ 2021-11-02 14:36:32 │ 15576159249491229584 │ inserted_into_target │ Inserted into target table         │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n**As you can see, attaching again after modifying source results in both versions in target table.**\r\n\r\n6. Alternatively, let's try attaching after modifying target table. Repeat steps 1. and 2. to start from a similar state.\r\n\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Source Table │ 2021-11-02 14:41:05 │ 5001499213188034149 │ inserted_into_source │ Inserted into source table │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Target Table │ 2021-11-02 14:41:05 │  5001499213188034149 │ inserted_into_source │ Inserted into source table │\r\n│ Target Table │ 2021-11-02 14:41:05 │ 16925418104615267922 │ inserted_into_target │ Inserted into target table │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n7.  Update target table:\r\n```sql\r\nALTER TABLE test.temp_target_replicated\r\n    UPDATE Note = concat(Note, ' (mod 1)')\r\n    WHERE 1 = 1\r\n;\r\n```\r\n\r\n<details> <summary>Current state</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0_3\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0_3\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Source Table │ 2021-11-02 14:41:05 │ 5001499213188034149 │ inserted_into_source │ Inserted into source table │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────────────┐\r\n│ Target Table │ 2021-11-02 14:41:05 │  5001499213188034149 │ inserted_into_source │ Inserted into source table (mod 1) │\r\n│ Target Table │ 2021-11-02 14:41:05 │ 16925418104615267922 │ inserted_into_target │ Inserted into target table (mod 1) │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\n8. Attach source partition to target table again:\r\n```sql\r\nALTER TABLE test.temp_target_replicated\r\n    ATTACH PARTITION '202111' FROM test.temp_source_sample_replicated;\r\n```\r\n\r\n<details> <summary>Current state (no change)</summary>\r\n\r\n```\r\nls -1d /drives/ssd1/clickhouse/data/test/temp_{source_sample,target}_replicated/2021*\r\n/drives/ssd1/clickhouse/data/test/temp_source_sample_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_0_0_0_3\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0\r\n/drives/ssd1/clickhouse/data/test/temp_target_replicated/202111_2_2_0_3\r\n\r\nclickhouse-client -sm --query \"SELECT 'Source Table' AS table, * FROM test.temp_source_sample_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬──────────────────Id─┬─Url──────────────────┬─Note───────────────────────┐\r\n│ Source Table │ 2021-11-02 14:41:05 │ 5001499213188034149 │ inserted_into_source │ Inserted into source table │\r\n└──────────────┴─────────────────────┴─────────────────────┴──────────────────────┴────────────────────────────┘\r\n\r\nclickhouse-client -sm --query \"SELECT 'Target Table' AS table, * FROM test.temp_target_replicated ORDER BY Url FORMAT PrettyCompactMonoBlock\"\r\n┌─table────────┬───────────Timestamp─┬───────────────────Id─┬─Url──────────────────┬─Note───────────────────────────────┐\r\n│ Target Table │ 2021-11-02 14:41:05 │  5001499213188034149 │ inserted_into_source │ Inserted into source table (mod 1) │\r\n│ Target Table │ 2021-11-02 14:41:05 │ 16925418104615267922 │ inserted_into_target │ Inserted into target table (mod 1) │\r\n└──────────────┴─────────────────────┴──────────────────────┴──────────────────────┴────────────────────────────────────┘\r\n```\r\n\r\n</details>\r\n\r\nAs you can see, attaching partition again after modifying target table does nothing. If I search query log using last attach query id, I see that it skipped attaching the source part because its hash exists.\r\n<details> <summary>Part 202111_0_0_0 (hash 30D83BB4B9B6F8A9720816E6AE0B2E9C) has been already attached</summary>\r\n\r\n```\r\n2021.11.02 14:44:06.884563 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Debug> executeQuery: (from [::ffff:127.0.0.1]:58490, using production parser) ALTER TABLE test.temp_target_replicated ATTACH PARTITION '202111' FROM test.temp_source_sample_replicated;\r\n2021.11.02 14:44:06.884652 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Trace> ContextAccess (default): Access granted: SELECT ON test.temp_source_sample_replicated\r\n2021.11.02 14:44:06.884764 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Trace> ContextAccess (default): Access granted: INSERT, ALTER DELETE ON test.temp_target_replicated\r\n2021.11.02 14:44:06.885203 [ 11333 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 2.26 TiB.\r\n2021.11.02 14:44:06.886033 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Debug> test.temp_target_replicated: Cloning 1 parts\r\n2021.11.02 14:44:06.886057 [ 11333 ] {} <Trace> system.trace_log (532ac6ab-360e-4f75-9838-a071724b88c2): Renaming temporary part tmp_insert_202111_141418_141418_0 to 202111_3601643_3601643_0.\r\n2021.11.02 14:44:06.886241 [ 11333 ] {} <Trace> SystemLog (system.trace_log): Flushed system log up to offset 25319178\r\n2021.11.02 14:44:06.895068 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Information> test.temp_target_replicated: Trying to attach 202111_0_0_0 with hash_hex 30D83BB4B9B6F8A9720816E6AE0B2E9C\r\n2021.11.02 14:44:06.898827 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Information> test.temp_target_replicated: Part 202111_0_0_0 (hash 30D83BB4B9B6F8A9720816E6AE0B2E9C) has been already attached\r\n2021.11.02 14:44:06.903560 [ 15523 ] {8173b3a6-e04d-42d7-b6f0-7f16ed085ee6} <Debug> MemoryTracker: Peak memory usage (for query): 0.00 B.\r\n2021.11.02 14:44:06.903814 [ 15523 ] {} <Debug> TCPHandler: Processed in 0.019593894 sec.\r\n2021.11.02 14:44:06.906373 [ 11365 ] {} <Debug> test.temp_target_replicated (ReplicatedMergeTreeQueue): Pulling 1 entries to queue: log-0000000004 - log-0000000004\r\n2021.11.02 14:44:06.915270 [ 11365 ] {} <Debug> test.temp_target_replicated (ReplicatedMergeTreeQueue): Pulled 1 entries to queue.\r\n2021.11.02 14:44:06.915452 [ 5843 ] {} <Debug> test.temp_target_replicated: Executing log entry queue-0000000004 to replace parts range 202111_0_0_0 with 0 parts from test.temp_source_sample_replicated\r\n2021.11.02 14:44:06.915492 [ 5843 ] {} <Information> test.temp_target_replicated: All parts from REPLACE PARTITION command have been already attached\r\n2021.11.02 14:44:06.918400 [ 15531 ] {} <Trace> TCPSHandlerFactory: TCP Request. Address: [::ffff:127.0.0.1]:37618\r\n2021.11.02 14:44:06.921016 [ 15531 ] {} <Debug> TCPHandler: Connected ClickHouse client version 21.7.0, revision: 54449, user: default.\r\n```\r\n\r\n</details>","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31005/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/31000","id":1042331023,"node_id":"I_kwDOA5dJV84-ILWP","number":31000,"title":"kafka materialized view utilizing getDict not caching","user":{"login":"tmada","id":8443811,"node_id":"MDQ6VXNlcjg0NDM4MTE=","avatar_url":"https://avatars.githubusercontent.com/u/8443811?v=4","gravatar_id":"","url":"https://api.github.com/users/tmada","html_url":"https://github.com/tmada","followers_url":"https://api.github.com/users/tmada/followers","following_url":"https://api.github.com/users/tmada/following{/other_user}","gists_url":"https://api.github.com/users/tmada/gists{/gist_id}","starred_url":"https://api.github.com/users/tmada/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tmada/subscriptions","organizations_url":"https://api.github.com/users/tmada/orgs","repos_url":"https://api.github.com/users/tmada/repos","events_url":"https://api.github.com/users/tmada/events{/privacy}","received_events_url":"https://api.github.com/users/tmada/received_events","type":"User","site_admin":false},"labels":[{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-02T13:16:29Z","updated_at":"2021-11-17T12:22:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"After upgrading from 21.1.7.1 to 21.8.8 we have some unexpected behavior surround a pipeline where we move data from \r\nkafka -> kafka table -> materialized view -> mergetree table. We are no longer able to utilize getDict inside the materialized view as dictionary entries are no longer caching (in a timely manner?) and data no longer inserts to our destination table as a result.\r\n\r\nAt first we were hitting the timeout of 60s and found: query_wait_timeout_milliseconds and a page detailing the cache parameters. I increased that to 10m + as we're handling large bulk where my dictionary is a custom sql script that calls a remote mysql server using complex_key_cache \r\n\r\nThat didn't work and profiled the mysql engine and noticed that we were constantly requesting the same lookups even within the same batch. \r\n\r\nThat is to say, the same DDL was being repeated in my audit: \r\nSELECT id, val FROM tab where id = 123;\r\nSELECT id, val FROM tab where id = 123;\r\nSELECT id, val FROM tab where id = 123;\r\n..\r\n.\r\nThis occurred in the same batch. Seconds apart, minutes apart... The reason for multiple calls to the same id was likely because the dataset being read utilized the same id lookup. One would have expected this to be a cache hit though. Ultimately the script would timeout again so I could never get this working.\r\n\r\n**How to reproduce**\r\nSetup a pipeline as per above and using the create syntax below\r\n\r\n* Which ClickHouse server version to use\r\n21.8.8\r\n* Which interface to use, if matters\r\n* Non-default settings, if any\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n```\r\nCREATE TABLE chkafka.kafka_queue\r\n(\r\n\r\n    json String\r\n)\r\nENGINE = Kafka\r\nSETTINGS kafka_broker_list = 'host1,host2',\r\n kafka_topic_list = '...',\r\n kafka_group_name = 'group1',\r\n kafka_format = 'JSONAsString',\r\n kafka_row_delimiter = '\\n',\r\n kafka_skip_broken_messages = 1,\r\n kafka_num_consumers = 2\r\n```\r\n--\r\n```\r\nCREATE MATERIALIZED VIEW chkafka.mv TO chkafka.dest_table\r\n(\r\nSELECT \r\n    _topic AS kafka_topic,\r\n\r\n    _offset AS kafka_offset,\r\n\r\n    visitParamExtractString(json,\r\n 'channelIdentifier') AS channelIdentifier,\r\n\r\n    visitParamExtractInt(json,\r\n 'messageTimestampMicros') AS messageTimestampMicros,\r\n...\r\ndictGet('FlowidProperty', 'value', (Listing, 'x')) = 'true' AS field,\r\n...\r\nFROM chkafka.kafka_queue\r\n)\r\n```\r\n--\r\n```\r\nCREATE TABLE dest_table (\r\n...\r\n)\r\nENGINE = ReplacingMergeTree\r\nPARTITION BY toDate(toDateTime(intDiv(messageTimestampMicros,\r\n 1000000)))\r\nORDER BY (channelIdentifier,\r\n messageTimestampMicros,\r\n kafka_topic,\r\n kafka_offset)\r\nTTL toDate(toDateTime(intDiv(messageTimestampMicros,\r\n 1000000))) + toIntervalMonth(1)\r\nSETTINGS index_granularity = 8192\r\n```\r\n--\r\n```\r\n<dictionaries>\r\n  <dictionary>\r\n    <name>FlowidProperty</name>\r\n    <source>\r\n      <executable>\r\n        <command>/flowid_property.sh</command>\r\n        <format>TSV</format>\r\n      </executable>\r\n    </source>\r\n    <lifetime>604800</lifetime>\r\n    <layout>\r\n      <complex_key_cache>\r\n        <size_in_cells>1000000</size_in_cells>\r\n        <query_wait_timeout_milliseconds>1200000</query_wait_timeout_milliseconds>\r\n        <!-- Tried this and didn't work, expected a commit to cache after 100 records? this would make sense?\r\n<max_update_queue_size>100</max_update_queue_size>-->\r\n        <allow_read_expired_keys>1</allow_read_expired_keys>\r\n      </complex_key_cache>\r\n    </layout>\r\n    <structure>\r\n      <key>\r\n        <attribute>\r\n          <name>flowid</name>\r\n          <type>Int64</type>\r\n        </attribute>\r\n        <attribute>\r\n          <name>type</name>\r\n          <type>String</type>\r\n        </attribute>\r\n      </key>\r\n      <attribute>\r\n        <name>value</name>\r\n        <type>String</type>\r\n        <null_value>false</null_value>\r\n      </attribute>\r\n    </structure>\r\n  </dictionary>\r\n</dictionaries>\r\n```\r\n--\r\n```\r\nflowid_property.sh example\r\n\r\n#!/bin/bash\r\nwhile read flowid type; do\r\nmysql -N -h ... -u ... --password='...' host -e \"\\\r\nselect '$flowid', '$type', substring(P.value, 2) from Property P \\\r\njoin U on U.id=P.entity \\\r\nwhere U.flowId_id=$flowid and P.type = '$type'\"\r\ndone\r\n```\r\n--\r\n\r\n\r\n* Sample data for all these tables, use [clickhouse-obfuscator](https://github.com/ClickHouse/ClickHouse/blob/master/programs/obfuscator/Obfuscator.cpp#L42-L80) if necessary\r\n* Queries to run that lead to unexpected result\r\n\r\n**Expected behavior**\r\nProper caching and not redundant calls to our SQL servers. Even if we have a timeout we should be caching what we collect for efficiency. \r\n\r\n**Error message and/or stacktrace**\r\n2021.11.01 09:32:07.462425 [ 46879 ] {} <Error> void DB::StorageKafka::threadFunc(size_t): Code: 159, e.displayText() = DB::Exception: Dictionary FlowidProperty source seems unavailable, because 60000 ms timeout exceeded.: while executing 'FUNCTION dictGet('FlowidProperty' : 36, 'value' : 37, tuple(visitParamExtractInt(json, 'Listing'), 'x') :: 10) -> dictGet('FlowidProperty', 'value', tuple(visitParamExtractInt(json, 'Listing'), 'x')) String : 5': while pushing to view chkafka.mv, Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x8fa40fa in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n1. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) @ 0xe3df29e in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n2. DB::CacheDictionaryUpdateQueue<(DB::DictionaryKeyType)1>::waitForCurrentUpdateFinish(std::__1::shared_ptr<DB::CacheDictionaryUpdateUnit<(DB::DictionaryKeyType)1> >&) const @ 0xe6f86b1 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n3. DB::CacheDictionary<(DB::DictionaryKeyType)1>::getColumns(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&) const @ 0xe6ed8f9 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n4. DB::CacheDictionary<(DB::DictionaryKeyType)1>::getColumn(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::IDataType const> const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&) const @ 0xe6eca1b in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n5. DB::FunctionDictGetNoType<(DB::DictionaryGetFunctionType)0>::executeDictionaryRequest(std::__1::shared_ptr<DB::IDictionary const>&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&, std::__1::vector<std::__1::shared_ptr<DB::IDataType const>, std::__1::allocator<std::__1::shared_ptr<DB::IDataType const> > > const&, std::__1::shared_ptr<DB::IDataType const> const&, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > const&) const @ 0xb5ecc0b in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n6. DB::FunctionDictGetNoType<(DB::DictionaryGetFunctionType)0>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0xb5ea2c6 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n7. DB::FunctionToExecutableFunctionAdaptor::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0xb134f2e in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n8. DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xfa7e49e in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n9. DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const @ 0xfa7eab2 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n10. DB::ExpressionActions::execute(DB::Block&, unsigned long&, bool) const @ 0x100e53d5 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n11. DB::ExpressionTransform::transform(DB::Chunk&) @ 0x111ae73c in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n12. DB::ISimpleTransform::transform(DB::Chunk&, DB::Chunk&) @ 0x111aead0 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n13. DB::ISimpleTransform::work() @ 0x111b19a7 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n14. void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) @ 0x11064f3d in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n15. DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) @ 0x11061ad1 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n16. DB::PipelineExecutor::executeStep(std::__1::atomic<bool>*) @ 0x110604fc in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n17. DB::PullingPipelineExecutor::pull(DB::Chunk&) @ 0x1106daea in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n18. DB::PullingPipelineExecutor::pull(DB::Block&) @ 0x1106dcf0 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n19. DB::PipelineExecutingBlockInputStream::readImpl() @ 0x1105c974 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n20. DB::IBlockInputStream::read() @ 0xfd98a64 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n21. DB::MaterializingBlockInputStream::readImpl() @ 0x1039eddd in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n22. DB::IBlockInputStream::read() @ 0xfd98a64 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n23. DB::SquashingBlockInputStream::readImpl() @ 0xfdbbaff in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n24. DB::IBlockInputStream::read() @ 0xfd98a64 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n25. DB::ConvertingBlockInputStream::readImpl() @ 0xfd979e9 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n26. DB::IBlockInputStream::read() @ 0xfd98a64 in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n27. DB::PushingToViewsBlockOutputStream::process(DB::Block const&, DB::PushingToViewsBlockOutputStream::ViewInfo&) @ 0x1039998c in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n28. DB::PushingToViewsBlockOutputStream::write(DB::Block const&) @ 0x10398f0e in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n29. DB::AddingDefaultBlockOutputStream::write(DB::Block const&) @ 0x103a268b in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n30. DB::CountingBlockOutputStream::write(DB::Block const&) @ 0x100a9e7e in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n31. DB::copyData(DB::IBlockInputStream&, DB::IBlockOutputStream&, std::__1::function<void (DB::Block const&)> const&, std::__1::atomic<bool>*) @ 0xfdbceca in /usr/lib/debug/.build-id/cf/9372422c6e1c1883b50dfe211010eab113dbcf.debug\r\n\r\n**Additional context**\r\nI can make calls to all of our external dictionaries just fine (bulk queries 100's of records) within a database editor or clickhouse client console. This appears to impact large lookups? \r\n\r\nIs it recommended to use getDict in this pipeline?\r\n\r\nI tried reducing the kafka number of messages from 1mil to 250k and was met with the same behavior. \r\nI also tried to use max_update_queue_size with the hopes it would commit to cache more frequent (before a timeout). That too didn't work.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/31000/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30999","id":1042330590,"node_id":"I_kwDOA5dJV84-ILPe","number":30999,"title":"grant drop database on db1 to user1, drop database failed, but drop database on cluster ok","user":{"login":"qmdt","id":8369775,"node_id":"MDQ6VXNlcjgzNjk3NzU=","avatar_url":"https://avatars.githubusercontent.com/u/8369775?v=4","gravatar_id":"","url":"https://api.github.com/users/qmdt","html_url":"https://github.com/qmdt","followers_url":"https://api.github.com/users/qmdt/followers","following_url":"https://api.github.com/users/qmdt/following{/other_user}","gists_url":"https://api.github.com/users/qmdt/gists{/gist_id}","starred_url":"https://api.github.com/users/qmdt/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qmdt/subscriptions","organizations_url":"https://api.github.com/users/qmdt/orgs","repos_url":"https://api.github.com/users/qmdt/repos","events_url":"https://api.github.com/users/qmdt/events{/privacy}","received_events_url":"https://api.github.com/users/qmdt/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-11-02T13:16:03Z","updated_at":"2021-11-04T08:42:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi，\r\nWhen I try to figure out the drop privilege of clickhouse cluster ,I find that a user could not drop database with tables in it with \"drop database\" privilege,the server returns error said \"DB::Exception: user1: Not enough privileges. To execute this query it's necessary to have grant DROP TABLE ON db1.testtb1\".\r\nBut **when excute the drop database SQL with \"on cluster\", it did work, which means the tables in the database are dropped too.** I think thats strange. my steps are below:\r\n\r\n1.# create role\r\ncreate role1 on cluster default_cluster;\r\n2. # create user\r\ncreate user1 on cluster default_cluster;\r\n3. # create database\r\ncreate database db1 on cluster default_cluster;\r\n4. # create table\r\nCREATE TABLE db1.testtb1\r\n(\r\n    `id` String,\r\n    `price` Float64,\r\n    `create_time` DateTime\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/db1/tables/{shard}/testtb1', '{replica}')\r\nPARTITION BY toYYYYMM(create_time)\r\nORDER BY id\r\nSETTINGS index_granularity = 8192\r\n5. # grant drop database privilege to role\r\ngrant drop database to role1 on cluster default_cluster;\r\n6. # grant role to user\r\ngrant role1 to user1 on cluster default_cluster;\r\n7. login one of the servers in the cluster with user1,and excute the drop command:\r\ndrop database db1;\r\n\r\nand the results were :\r\nReceived exception from server (version 21.3.4):\r\nCode: 497. DB::Exception: Received from xxxxx DB::Exception: user1: Not enough privileges. To execute this query it's necessary to have grant DROP TABLE ON db1.testtb1.\r\n\r\n8.excute the command with \"on cluster\" :\r\ndrop database db1 on cluster default_cluster;\r\n\r\nit works,the database db1 and the table testtb1 are all dropped on any server of the cluster. \r\nI think this is strange， if it can‘t be excuted on one server ,how could it be excuted on cluster ?   As I did in my steps, the \"drop database\" privilege should be the same on all server，I think step8  should have the same result as step7. \r\n\r\n\r\nmy  clickhouse version is 21.3.4,I use the tcp interface,all default settings,\r\nTHANKS FOR ANY HELP. ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30999/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30989","id":1042142501,"node_id":"I_kwDOA5dJV84-HdUl","number":30989,"title":"Add support for SASL_SSL method for Kafka engine (it was somehow worked many years ago, but it was never tested or documented)","user":{"login":"Nikoslav","id":93379419,"node_id":"U_kgDOBZDbWw","avatar_url":"https://avatars.githubusercontent.com/u/93379419?v=4","gravatar_id":"","url":"https://api.github.com/users/Nikoslav","html_url":"https://github.com/Nikoslav","followers_url":"https://api.github.com/users/Nikoslav/followers","following_url":"https://api.github.com/users/Nikoslav/following{/other_user}","gists_url":"https://api.github.com/users/Nikoslav/gists{/gist_id}","starred_url":"https://api.github.com/users/Nikoslav/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Nikoslav/subscriptions","organizations_url":"https://api.github.com/users/Nikoslav/orgs","repos_url":"https://api.github.com/users/Nikoslav/repos","events_url":"https://api.github.com/users/Nikoslav/events{/privacy}","received_events_url":"https://api.github.com/users/Nikoslav/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1334071168,"node_id":"MDU6TGFiZWwxMzM0MDcxMTY4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-kafka","name":"comp-kafka","color":"b5bcff","default":false,"description":"Kafka Engine"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-02T10:33:35Z","updated_at":"2022-01-15T23:44:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Clickhouse kafka engine connecting to kafka broker fails using sasl_ssl. This started after the change from openSSL to boringSSL. \r\n\r\nKafka settings in clickhouse config:\r\n```\r\n<debug>all</debug>\r\n<ssl_key_location>/etc/clickhouse-server/ssl/server.key.pem</ssl_key_location>\r\n<ssl_key_password>***</ssl_key_password>\r\n<ssl_certificate_location>/etc/clickhouse-server/ssl/server.cert.pem</ssl_certificate_location>\r\n<ssl_ca_location>/etc/clickhouse-server/ssl/ca.cert.pem</ssl_ca_location>\r\n<ssl_endpoint_identification_algorithm>https</ssl_endpoint_identification_algorithm> \r\n<sasl_mechanism>PLAIN</sasl_mechanism>\r\n<security_protocol>sasl_ssl</security_protocol>\r\n<sasl_username>***</sasl_username>\r\n<sasl_password>***</sasl_password>\r\n```\r\nSame CA signed certs used in kafka server and clickhouse kafka config.\r\nIf ssl_endpoint_identification_algorithm is set to none, kafka engine successfully connects to kafka brokers.\r\n\r\nI used kafkacat(kafkacat/stable,now 1.6.0-1 amd64) for tests and using the same settings as above, it connects successfully even with ssl_endpoint_identification_algorithm set to \"https\".\r\n\r\n*** OS**\r\nDebian GNU/Linux 11 (bullseye)\r\n\r\n*** Clickhouse server version**\r\n21.10.2.15\r\n\r\n*** Kafka version**\r\nconfluent-kafka-2.12/stable,now 5.5.5-1\r\n\r\n\r\n*** Example kafka table**\r\n```\r\nCREATE TABLE kafka.test\r\n(\r\n    `date` Date,\r\n    `string` String,\r\n    `int1` Nullable(Int64),\r\n    `int2` Array(Nullable(Int64))\r\n)\r\nENGINE = Kafka()\r\nSETTINGS kafka_broker_list = 'host1:9092,host2:9092,host3:9092', kafka_topic_list = 'test', kafka_group_name = 'test', kafka_format = 'AvroConfluent'\r\n```\r\n\r\n\r\nPart of log:\r\n```\r\n2021.10.29 12:03:47.878420 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:SASL] [thrd:app]: Selected provider PLAIN (builtin) for SASL mechanism PLAIN\r\n2021.10.29 12:03:47.878443 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:OPENSSL] [thrd:app]: Using OpenSSL version BoringSSL (0x1010107f, librdkafka built with 0x1010107f)\r\n2021.10.29 12:03:47.878470 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:SSL] [thrd:app]: Loading CA certificate(s) from file /etc/clickhouse-server/ssl/ca.cert.pem\r\n2021.10.29 12:03:47.911346 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:SSL] [thrd:app]: Loading public key from file /etc/clickhouse-server/ssl/server.cert.pem\r\n2021.10.29 12:03:47.911530 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:SSL] [thrd:app]: Loading private key file from /etc/clickhouse-server/ssl/server.key.pem\r\n2021.10.29 12:03:47.912021 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:MEMBERID] [thrd:app]: Group \"test\": updating member id \"(not-set)\" -> \"\"\r\n2021.10.29 12:03:47.912101 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:WAKEUPFD] [thrd:app]: GroupCoordinator: Enabled low-latency ops queue wake-ups\r\n2021.10.29 12:03:47.912224 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:BROKER] [thrd:app]: GroupCoordinator: Added new broker with NodeId -1\r\n2021.10.29 12:03:47.912358 [ 1996179 ] {} <Debug> StorageKafka (test): [rdk:BRKMAIN] [thrd:GroupCoordinator]: GroupCoordinator: Enter main broker thread\r\n2021.10.29 12:03:47.912509 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:WAKEUPFD] [thrd:app]: sasl_ssl://host1:9092/bootstrap: Enabled low-latency ops queue wake-ups\r\n2021.10.29 12:03:47.912518 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPSTATE] [thrd:main]: Group \"test\" changed state init -> query-coord (join-state init)\r\n2021.10.29 12:03:47.912608 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:BROADCAST] [thrd:main]: Broadcasting state change\r\n2021.10.29 12:03:47.912664 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:BROKER] [thrd:app]: sasl_ssl://host1:9092/bootstrap: Added new broker with NodeId -1\r\n2021.10.29 12:03:47.912677 [ 1996181 ] {} <Debug> StorageKafka (test): [rdk:BRKMAIN] [thrd::0/internal]: :0/internal: Enter main broker thread\r\n2021.10.29 12:03:47.912704 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: sasl_ssl://host1:9092/bootstrap: Selected for cluster connection: coordinator query (broker has 0 connection attempt(s))\r\n2021.10.29 12:03:47.912794 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n2021.10.29 12:03:47.912818 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:WAKEUPFD] [thrd:app]: sasl_ssl://host2:9092/bootstrap: Enabled low-latency ops queue wake-ups\r\n2021.10.29 12:03:47.912843 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BRKMAIN] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Enter main broker thread\r\n2021.10.29 12:03:47.912926 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:BROKER] [thrd:app]: sasl_ssl://host2:9092/bootstrap: Added new broker with NodeId -1\r\n2021.10.29 12:03:47.912931 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Received CONNECT op\r\n2021.10.29 12:03:47.912976 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:WAKEUPFD] [thrd:app]: sasl_ssl://host3:9092/bootstrap: Enabled low-latency ops queue wake-ups\r\n2021.10.29 12:03:47.912980 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:STATE] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Broker changed state INIT -> TRY_CONNECT\r\n2021.10.29 12:03:47.913023 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BROADCAST] [thrd:sasl_ssl://host1:9092/bootstrap]: Broadcasting state change\r\n2021.10.29 12:03:47.913053 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: broker in state TRY_CONNECT connecting\r\n2021.10.29 12:03:47.913070 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:STATE] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Broker changed state TRY_CONNECT -> CONNECT\r\n2021.10.29 12:03:47.913084 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BROADCAST] [thrd:sasl_ssl://host1:9092/bootstrap]: Broadcasting state change\r\n2021.10.29 12:03:47.913088 [ 1996183 ] {} <Debug> StorageKafka (test): [rdk:BRKMAIN] [thrd:sasl_ssl://host2:9092/bootstrap]: sasl_ssl://host2:9092/bootstrap: Enter main broker thread\r\n2021.10.29 12:03:47.913122 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:BROKER] [thrd:app]: sasl_ssl://host3:9092/bootstrap: Added new broker with NodeId -1\r\n2021.10.29 12:03:47.913186 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:INIT] [thrd:app]: librdkafka v1.6.1 (0x10601ff) ClickHouse-host-kafka-test#consumer-1 initialized (builtin.features gzip,snappy,ssl,sasl,regex,lz4,sasl_gssapi,sasl_plain,sasl_scram,plugins,zstd,sasl_oauthbearer, GCC GXX PKGCONFIG OSXLD LIBDL PLUGINS ZLIB SSL SASL_CYRUS ZSTD HDRHISTOGRAM LZ4_EXT SNAPPY SOCKEM SASL_SCRAM CRC32C_HW, debug 0xfffff)\r\n2021.10.29 12:03:47.913243 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]: Client configuration:\r\n2021.10.29 12:03:47.913261 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   client.id = ClickHouse-host-kafka-test\r\n2021.10.29 12:03:47.913275 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   client.software.name = ClickHouse\r\n2021.10.29 12:03:47.913293 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   client.software.version = v21.10.2.15-stable\r\n2021.10.29 12:03:47.913280 [ 1996184 ] {} <Debug> StorageKafka (test): [rdk:BRKMAIN] [thrd:sasl_ssl://host3:9092/bootstrap]: sasl_ssl://host3:9092/bootstrap: Enter main broker thread\r\n2021.10.29 12:03:47.913328 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   metadata.broker.list = host1:9092,host2:9092,host3:9092\r\n2021.10.29 12:03:47.913400 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   debug = generic,broker,topic,metadata,feature,queue,msg,protocol,cgrp,security,fetch,interceptor,plugin,consumer,admin,eos,mock,assignor,conf,all\r\n2021.10.29 12:03:47.913416 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   log_cb = 0x14252a20\r\n2021.10.29 12:03:47.913430 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   opaque = 0x7f599014c018\r\n2021.10.29 12:03:47.913444 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   default_topic_conf = 0x7f5990143180\r\n2021.10.29 12:03:47.913457 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   security.protocol = sasl_ssl\r\n2021.10.29 12:03:47.913471 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl.key.location = [redacted]\r\n2021.10.29 12:03:47.913486 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl.key.password = [redacted]\r\n2021.10.29 12:03:47.913498 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl_key = [redacted]\r\n2021.10.29 12:03:47.913511 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl.certificate.location = /etc/clickhouse-server/ssl/server.cert.pem\r\n2021.10.29 12:03:47.913524 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl.ca.location = /etc/clickhouse-server/ssl/ca.cert.pem\r\n2021.10.29 12:03:47.913541 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   ssl.endpoint.identification.algorithm = https\r\n2021.10.29 12:03:47.913557 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   sasl.mechanisms = PLAIN\r\n2021.10.29 12:03:47.913570 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   sasl.username = [redacted]\r\n2021.10.29 12:03:47.913582 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   sasl.password = [redacted]\r\n2021.10.29 12:03:47.913595 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   group.id = test\r\n2021.10.29 12:03:47.913610 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   enable.auto.commit = false\r\n2021.10.29 12:03:47.913623 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   enable.auto.offset.store = false\r\n2021.10.29 12:03:47.913638 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   queued.min.messages = 1048545\r\n2021.10.29 12:03:47.913653 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   fetch.min.bytes = 10000\r\n2021.10.29 12:03:47.913676 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   rebalance_cb = 0x14255f80\r\n2021.10.29 12:03:47.913691 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   enable.partition.eof = false\r\n2021.10.29 12:03:47.913706 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]: Default topic configuration:\r\n2021.10.29 12:03:47.913719 [ 1996178 ] {} <Debug> StorageKafka (test): [rdk:CONF] [thrd:app]:   auto.offset.reset = smallest\r\n2021.10.29 12:03:47.913793 [ 1996168 ] {} <Debug> StorageKafka (test): Started streaming to 1 attached views\r\n2021.10.29 12:03:47.914413 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Connecting to ipv4#host1IP:9092 (sasl_ssl) with socket 78\r\n2021.10.29 12:03:47.915322 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPOP] [thrd:main]: Group \"test\" received op GET_SUBSCRIPTION in state query-coord (join-state init)\r\n2021.10.29 12:03:47.915357 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Connected to ipv4#host1IP:9092\r\n2021.10.29 12:03:47.915390 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 47ms: coordinator query\r\n2021.10.29 12:03:47.915429 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n2021.10.29 12:03:47.915511 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPOP] [thrd:main]: Group \"test\" received op GET_SUBSCRIPTION in state query-coord (join-state init)\r\n2021.10.29 12:03:47.915503 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:FAIL] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: No further error information available (after 2ms in state CONNECT) (_TRANSPORT)\r\n2021.10.29 12:03:47.915553 [ 1996168 ] {} <Trace> StorageKafka (test): Already subscribed to topics: []\r\n2021.10.29 12:03:47.915547 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 47ms: coordinator query\r\n2021.10.29 12:03:47.915567 [ 1996182 ] {} <Error> StorageKafka (test): [rdk:FAIL] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: No further error information available (after 2ms in state CONNECT)\r\n2021.10.29 12:03:47.915600 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n2021.10.29 12:03:47.915581 [ 1996168 ] {} <Trace> StorageKafka (test): No assignment\r\n2021.10.29 12:03:47.915677 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPOP] [thrd:main]: Group \"test\" received op GET_SUBSCRIPTION in state query-coord (join-state init)\r\n2021.10.29 12:03:47.915679 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:STATE] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Broker changed state CONNECT -> DOWN\r\n2021.10.29 12:03:47.915759 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BROADCAST] [thrd:sasl_ssl://host1:9092/bootstrap]: Broadcasting state change\r\n2021.10.29 12:03:47.915788 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BUFQ] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Purging bufq with 0 buffers\r\n2021.10.29 12:03:47.915800 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 46ms: coordinator query\r\n2021.10.29 12:03:47.915818 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BUFQ] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Purging bufq with 0 buffers\r\n2021.10.29 12:03:47.915845 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n2021.10.29 12:03:47.915861 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BUFQ] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Updating 0 buffers on connection reset\r\n2021.10.29 12:03:47.915906 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPOP] [thrd:main]: Group \"test\" received op SUBSCRIBE in state query-coord (join-state init)\r\n2021.10.29 12:03:47.915918 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:STATE] [thrd:sasl_ssl://host1:9092/bootstrap]: sasl_ssl://host1:9092/bootstrap: Broker changed state DOWN -> INIT\r\n2021.10.29 12:03:47.915971 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:SUBSCRIBE] [thrd:main]: Group \"test\": subscribe to new subscription of 1 topics (join-state init)\r\n2021.10.29 12:03:47.915988 [ 1996182 ] {} <Debug> StorageKafka (test): [rdk:BROADCAST] [thrd:sasl_ssl://host1:9092/bootstrap]: Broadcasting state change\r\n2021.10.29 12:03:47.916035 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 46ms: coordinator query\r\n2021.10.29 12:03:47.916055 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n2021.10.29 12:03:47.916081 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPOP] [thrd:main]: Group \"test\" received op GET_SUBSCRIPTION in state query-coord (join-state init)\r\n2021.10.29 12:03:47.916127 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CONNECT] [thrd:main]: Not selecting any broker for cluster connection: still suppressed for 46ms: coordinator query\r\n2021.10.29 12:03:47.916163 [ 1996180 ] {} <Debug> StorageKafka (test): [rdk:CGRPQUERY] [thrd:main]: Group \"test\": no broker available for coordinator query: intervaled in state query-coord\r\n\r\n```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30989/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30988","id":1042063246,"node_id":"I_kwDOA5dJV84-HJ-O","number":30988,"title":"Missing columns in system.data_skipping_indices","user":{"login":"Webbmekanikern","id":8144421,"node_id":"MDQ6VXNlcjgxNDQ0MjE=","avatar_url":"https://avatars.githubusercontent.com/u/8144421?v=4","gravatar_id":"","url":"https://api.github.com/users/Webbmekanikern","html_url":"https://github.com/Webbmekanikern","followers_url":"https://api.github.com/users/Webbmekanikern/followers","following_url":"https://api.github.com/users/Webbmekanikern/following{/other_user}","gists_url":"https://api.github.com/users/Webbmekanikern/gists{/gist_id}","starred_url":"https://api.github.com/users/Webbmekanikern/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Webbmekanikern/subscriptions","organizations_url":"https://api.github.com/users/Webbmekanikern/orgs","repos_url":"https://api.github.com/users/Webbmekanikern/repos","events_url":"https://api.github.com/users/Webbmekanikern/events{/privacy}","received_events_url":"https://api.github.com/users/Webbmekanikern/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-02T09:20:10Z","updated_at":"2021-11-02T09:20:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"ClickHouse version: 21.10.2.15\r\n\r\nAccording to [the documentation](https://clickhouse.com/docs/en/operations/system-tables/data_skipping_indices/), these columns should be available in the `system.data_skipping_indices` table:\r\n- data_compressed_bytes\r\n- data_uncompressed_bytes\r\n- marks_bytes\r\n\r\nThese are however missing when I run `SELECT * FROM system.data_skipping_indices LIMIT 1 FORMAT Vertical`:\r\n\r\n```\r\nRow 1:\r\n──────\r\ndatabase:    myDatabase\r\ntable:       myTable\r\nname:        myIndexName\r\ntype:        bloom_filter\r\nexpr:        myColumn\r\ngranularity: 1\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30988/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30983","id":1041921845,"node_id":"I_kwDOA5dJV84-Gnc1","number":30983,"title":"Failing to run a query with dimensions with spaces once adding row policies","user":{"login":"michalsinger","id":6329904,"node_id":"MDQ6VXNlcjYzMjk5MDQ=","avatar_url":"https://avatars.githubusercontent.com/u/6329904?v=4","gravatar_id":"","url":"https://api.github.com/users/michalsinger","html_url":"https://github.com/michalsinger","followers_url":"https://api.github.com/users/michalsinger/followers","following_url":"https://api.github.com/users/michalsinger/following{/other_user}","gists_url":"https://api.github.com/users/michalsinger/gists{/gist_id}","starred_url":"https://api.github.com/users/michalsinger/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/michalsinger/subscriptions","organizations_url":"https://api.github.com/users/michalsinger/orgs","repos_url":"https://api.github.com/users/michalsinger/repos","events_url":"https://api.github.com/users/michalsinger/events{/privacy}","received_events_url":"https://api.github.com/users/michalsinger/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-02T06:17:22Z","updated_at":"2021-11-02T06:17:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":" have a weird issue. I use `` for fields with spaces.  For example \"select distinct(`with space`) from table\". Once i added row policies to the system, this fail started to fail on:\r\n\"Received exception from server (version 21.1.7):\r\nCode: 62. DB::Exception: Received from 10.10.1.56:9000. DB::Exception: Syntax error (lambda expression): failed at position 6 ('space'): space. Expected one of: LIKE, GLOBAL NOT IN, DIV, IS, OR, QuestionMark, BETWEEN, NOT LIKE, MOD, AND, Comma, IN, ILIKE, Dot, NOT ILIKE, NOT, Arrow, token, NOT IN, GLOBAL IN.\"\r\nIf i remove the policies this does not fail. When i remove the row policies this succeeds again. \r\n\r\nProduce:\r\nadd field with space, try querying it using tilda: ` because of the space\r\nthis will fail\r\n\r\n* Which ClickHouse server version to use\r\n*  21.1.7\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n* Sample data for all these tables, use [clickhouse-obfuscator](https://github.com/ClickHouse/ClickHouse/blob/master/programs/obfuscator/Obfuscator.cpp#L42-L80) if necessary\r\n* Queries to run that lead to unexpected result\r\n\r\n**Expected behavior**\r\nQuery is succesful once i remove row policies\r\n\r\n**Error message and/or stacktrace**\r\n\"Received exception from server (version 21.1.7):\r\nCode: 62. DB::Exception: Received from 10.10.1.56:9000. DB::Exception: Syntax error (lambda expression): failed at position 6 ('space'): space. Expected one of: LIKE, GLOBAL NOT IN, DIV, IS, OR, QuestionMark, BETWEEN, NOT LIKE, MOD, AND, Comma, IN, ILIKE, Dot, NOT ILIKE, NOT, Arrow, token, NOT IN, GLOBAL IN.\"\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30983/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30978","id":1041676784,"node_id":"PR_kwDOA5dJV84t8zHA","number":30978,"title":"Release pull request for branch 21.11","user":{"login":"robot-clickhouse","id":41385210,"node_id":"MDQ6VXNlcjQxMzg1MjEw","avatar_url":"https://avatars.githubusercontent.com/u/41385210?v=4","gravatar_id":"","url":"https://api.github.com/users/robot-clickhouse","html_url":"https://github.com/robot-clickhouse","followers_url":"https://api.github.com/users/robot-clickhouse/followers","following_url":"https://api.github.com/users/robot-clickhouse/following{/other_user}","gists_url":"https://api.github.com/users/robot-clickhouse/gists{/gist_id}","starred_url":"https://api.github.com/users/robot-clickhouse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/robot-clickhouse/subscriptions","organizations_url":"https://api.github.com/users/robot-clickhouse/orgs","repos_url":"https://api.github.com/users/robot-clickhouse/repos","events_url":"https://api.github.com/users/robot-clickhouse/events{/privacy}","received_events_url":"https://api.github.com/users/robot-clickhouse/received_events","type":"User","site_admin":false},"labels":[{"id":1261360622,"node_id":"MDU6TGFiZWwxMjYxMzYwNjIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/release","name":"release","color":"34d3a1","default":false,"description":"Label for release pull request"},{"id":2107435505,"node_id":"MDU6TGFiZWwyMTA3NDM1NTA1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/submodule%20changed","name":"submodule changed","color":"b7130b","default":false,"description":"At least one submodule changed in this PR."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-11-01T21:56:54Z","updated_at":"2022-01-23T16:08:37Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30978","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30978","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30978.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30978.patch","merged_at":null},"body":"\nThis PullRequest is part of ClickHouse release cycle. It's used by CI system only. Don't perform any changes with it.\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30978/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30975","id":1041520185,"node_id":"I_kwDOA5dJV84-FFY5","number":30975,"title":"Exception thrown when using sumMapFiltered on a distributed table","user":{"login":"ivantopo","id":1302854,"node_id":"MDQ6VXNlcjEzMDI4NTQ=","avatar_url":"https://avatars.githubusercontent.com/u/1302854?v=4","gravatar_id":"","url":"https://api.github.com/users/ivantopo","html_url":"https://github.com/ivantopo","followers_url":"https://api.github.com/users/ivantopo/followers","following_url":"https://api.github.com/users/ivantopo/following{/other_user}","gists_url":"https://api.github.com/users/ivantopo/gists{/gist_id}","starred_url":"https://api.github.com/users/ivantopo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ivantopo/subscriptions","organizations_url":"https://api.github.com/users/ivantopo/orgs","repos_url":"https://api.github.com/users/ivantopo/repos","events_url":"https://api.github.com/users/ivantopo/events{/privacy}","received_events_url":"https://api.github.com/users/ivantopo/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-11-01T19:34:50Z","updated_at":"2021-12-20T09:10:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe what's wrong**\r\n\r\nAn exception is thrown when trying to use the `sumMapFiltered` function on a distributed table. \r\n\r\n**Does it reproduce on recent release?**\r\n\r\nYes. The tests below ran on ClickHouse 21.9.3\r\n\r\n\r\n**How to reproduce**\r\n\r\nCreate the test tables and insert a couple rows of data:\r\n```sql\r\nCREATE TABLE local ON CLUSTER 'cluster_name' (\r\n  time     DateTime,\r\n  data     Nested(\r\n    key      Int16,\r\n    value    Int64\r\n  )\r\n) ENGINE = MergeTree()\r\n  PARTITION BY toYYYYMM(time)\r\n  ORDER BY time\r\n\r\nCREATE TABLE distributed ON CLUSTER 'cluster_name' (\r\n  time     DateTime,\r\n  data     Nested(\r\n    key      Int16,\r\n    value    Int64\r\n  )\r\n) ENGINE = Distributed('cluster_name', 'cluster_name', 'local') \r\n\r\nINSERT INTO local VALUES\r\n  (1635794296, [1, 2, 3], [10, 10, 10]),\r\n  (1635794296, [1, 2, 3], [10, 10, 10]),\r\n  (1635794296, [1, 2, 3], [10, 10, 10])\r\n```\r\n\r\nThen run this query against the distributed table:\r\n\r\n```sql\r\nSELECT sumMapFiltered([toInt16(1),toInt16(2)])(data.key, data.value) FROM distributed\r\n```\r\n\r\n**Expected behavior**\r\n\r\nI would expect the query to return the correct aggregation value, but an error is thrown instead\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\nReceived exception from server (version 21.9.3):\r\nCode: 62. DB::Exception: Received from localhost:9000. DB::Exception: Syntax error (data type): failed at position 34 ('['): [1, 2]), Array(Int16), Array(Int64)). Expected one of: number, nested table, literal, NULL, identifier, data type argument, string literal, name and type pair list, list of elements, data type, list, delimited by binary operators, name and type pair: while receiving packet from [redacted]:9000: While executing Remote. (SYNTAX_ERROR)\r\n```\r\n\r\n**Additional context**\r\n\r\nThe exact same query works as expected when executed against the local table.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30975/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30969","id":1041240762,"node_id":"I_kwDOA5dJV84-EBK6","number":30969,"title":"Add size/endianness traits to Hash Function descriptors","user":{"login":"Bulat-Ziganshin","id":1798016,"node_id":"MDQ6VXNlcjE3OTgwMTY=","avatar_url":"https://avatars.githubusercontent.com/u/1798016?v=4","gravatar_id":"","url":"https://api.github.com/users/Bulat-Ziganshin","html_url":"https://github.com/Bulat-Ziganshin","followers_url":"https://api.github.com/users/Bulat-Ziganshin/followers","following_url":"https://api.github.com/users/Bulat-Ziganshin/following{/other_user}","gists_url":"https://api.github.com/users/Bulat-Ziganshin/gists{/gist_id}","starred_url":"https://api.github.com/users/Bulat-Ziganshin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Bulat-Ziganshin/subscriptions","organizations_url":"https://api.github.com/users/Bulat-Ziganshin/orgs","repos_url":"https://api.github.com/users/Bulat-Ziganshin/repos","events_url":"https://api.github.com/users/Bulat-Ziganshin/events{/privacy}","received_events_url":"https://api.github.com/users/Bulat-Ziganshin/received_events","type":"User","site_admin":false},"labels":[{"id":1546440057,"node_id":"MDU6TGFiZWwxNTQ2NDQwMDU3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/development","name":"development","color":"fc80df","default":false,"description":"Developement process & source code & implementation details"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-11-01T14:30:55Z","updated_at":"2022-01-15T23:53:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"CH hashtable implementation has a lot of hash functions with different properties, but there is no way to account for individual hash function properties, so it's done on ad-hoc basis. One particular example is [computing bucket number for two-level hashtable](https://github.com/ClickHouse/ClickHouse/blob/1ea637d996715d2a047f8cd209b478e946bdbfb0/src/Common/HashTable/TwoLevelHashTable.h#L56), which doesn't work for subtables larger than 16M cells.\r\n\r\nBased on my experience with handling hash functions, I propose to add the following traits to each hash function:\r\n\r\n1. bitness, i.e. real number of bits in the result. well, alternative is to compute it from the return type of hash function using C++ magic\r\n2. endianness, i.e. whether hash contains better distributed bits on the high end (such as x*CONST hash function), low end, or everywhere (such as CRC hash function)\r\n\r\nGoing deeper, you can even make hash transformers, f.e. two-row hash tables may employ a transfomer that consumes the best 8 bits of the hash function and returns the remainder (with bitness-=8 and hash result either shifted or masked).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969/reactions","total_count":2,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30969/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30935","id":1040573547,"node_id":"I_kwDOA5dJV84-BeRr","number":30935,"title":"Time (32)/Time64 datatype","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1481221764,"node_id":"MDU6TGFiZWwxNDgxMjIxNzY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-datetime","name":"comp-datetime","color":"b5bcff","default":false,"description":"date & time & timezone related"},{"id":1507888214,"node_id":"MDU6TGFiZWwxNTA3ODg4MjE0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-accepted","name":"st-accepted","color":"e5b890","default":false,"description":"The issue is in our backlog, ready to take"},{"id":3086255531,"node_id":"MDU6TGFiZWwzMDg2MjU1NTMx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/warmup%20task","name":"warmup task","color":"FBCA04","default":false,"description":"The task for new ClickHouse team members. Low risk, moderate complexity, no urgency."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-10-31T20:24:06Z","updated_at":"2021-11-20T13:40:56Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Something like https://dev.mysql.com/doc/refman/8.0/en/time.html (maybe domain using Decimal for example).\r\n\r\nSome Mysql drivers unable to work using Mysql wire protocol because of lack of Time datatype (they use this type on connect for some internal needs).\r\n\r\nAlso Tableu treats Clickhouse compatible on 98%\r\n<img width=\"1153\" alt=\"Screen Shot 2021-10-31 at 2 26 09 PM\" src=\"https://user-images.githubusercontent.com/19737682/139600022-4338ef8b-296e-41ed-b7a1-c5e687d0f379.png\">\r\n \r\nOther databases also have Time or Interval type and use this type operations with DateTime\r\n\r\nLike\r\n\r\n```\r\nSELECT (now() + 30000) - now()\r\n┌─minus(plus(now(), 30000), now())─┐\r\n│                            30000 │\r\n└──────────────────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30935/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30934","id":1040570437,"node_id":"I_kwDOA5dJV84-BdhF","number":30934,"title":"toString does not respect output_format_decimal_trailing_zero","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-31T20:08:45Z","updated_at":"2021-11-01T07:34:31Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```sql\r\nset output_format_decimal_trailing_zeros=1;\r\nSELECT toDecimal64(1, 4)\r\n┌─toDecimal64(1, 4)─┐\r\n│            1.0000 │\r\n└───────────────────┘\r\n\r\nselect toString(toDecimal64(1,4));\r\n┌─toString(toDecimal64(1, 4))─┐\r\n│ 1                           │\r\n└─────────────────────────────┘\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30934/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30895","id":1040429081,"node_id":"I_kwDOA5dJV84-A7AZ","number":30895,"title":"Functions `makeDate`, `makeDateTime`, `YYYYMMDDToDate`, `changeYear` and similar.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1481221764,"node_id":"MDU6TGFiZWwxNDgxMjIxNzY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-datetime","name":"comp-datetime","color":"b5bcff","default":false,"description":"date & time & timezone related"},{"id":3086255531,"node_id":"MDU6TGFiZWwzMDg2MjU1NTMx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/warmup%20task","name":"warmup task","color":"FBCA04","default":false,"description":"The task for new ClickHouse team members. Low risk, moderate complexity, no urgency."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-31T10:13:36Z","updated_at":"2021-10-31T10:13:36Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe solution you'd like:**\r\n\r\n```\r\nmakeDate(year, month, day)\r\nmakeDate32(year, month, day)\r\nmakeDateTime(year, month, day, hour, minute, second, [timezone])\r\nmakeDateTime64(year, month, day, hour, minute, second, [resolution], [fraction], [timezone])\r\n\r\nYYYYMMDDToDate(num)\r\nYYYYMMDDToDate32(num)\r\nYYYYMMDDhhmmssToDateTime(num, [timezone])\r\nYYYYMMDDhhmmssToDateTime64(num, [resolution], [timezone]) -- no fraction arg, it's implicitly extracted from floating point or decimal num.\r\n\r\n-- returns the same data type as dt:\r\nchangeYear(dt, new_value)\r\nchangeMonth(dt, new_value)\r\nchangeDay(dt, new_value)\r\n\r\n-- if argument is Date - returns DateTime, if Date32, returns DateTime64:\r\nchangeHour(dt, new_value)\r\nchangeMinute(dt, new_value)\r\nchangeSecond(dt, new_value)\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30895/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30890","id":1040312355,"node_id":"I_kwDOA5dJV84-Aegj","number":30890,"title":"Allow to specify settings with dashed-style in addition to underscore_style in command line parameters of clickhouse-client/clickhouse-local.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":3086255531,"node_id":"MDU6TGFiZWwzMDg2MjU1NTMx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/warmup%20task","name":"warmup task","color":"FBCA04","default":false,"description":"The task for new ClickHouse team members. Low risk, moderate complexity, no urgency."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-30T21:55:15Z","updated_at":"2021-10-30T21:55:15Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\n`clickhouse-client --max-memory-usage 1G`","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30890/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30885","id":1040238698,"node_id":"I_kwDOA5dJV84-AMhq","number":30885,"title":"`REGEXP_MATCHES` and `REGEXP_REPLACE` function aliases for compatibility with PostgreSQL.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":644208617,"node_id":"MDU6TGFiZWw2NDQyMDg2MTc=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/easy%20task","name":"easy task","color":"0e8a16","default":false,"description":"Good for first contributors"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-30T16:17:15Z","updated_at":"2021-10-30T16:17:24Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nSome people get used to PostgreSQL. Some tools also.\r\n\r\n**Describe the solution you'd like**\r\n\r\nSimply add the corresponding aliases. These aliases should be case insensitive.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30885/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30884","id":1040233126,"node_id":"I_kwDOA5dJV84-ALKm","number":30884,"title":"`clickhouse-client`: garbage is left in terminal when pressing \"up\" after entering really long query.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-30T16:00:13Z","updated_at":"2021-10-30T16:00:13Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the issue**\r\nSee https://github.com/AmokHuginnsson/replxx/issues/127\r\n\r\n**Additional context**\r\nThis issue exists not only in `replxx`, it also exists in `readline`.\r\nIt's strange that this bug does not bother people and exists for tens of years.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884/reactions","total_count":1,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30884/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30883","id":1040229309,"node_id":"I_kwDOA5dJV84-AKO9","number":30883,"title":"Install: choose the best volume automatically.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-30T15:41:40Z","updated_at":"2021-10-30T15:42:42Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nIf ClickHouse is installed for first time (the data directory does not exist), check if the volume for /var/lib/ has less than 100GB of free space. If that's true check if we have other mount points with larger amount of space and if they are mounted somewhere in /mnt or /opt. Then select the most suitable mount point and create `clickhouse` directory inside it. Make `/var/lib/clickhouse` as a symlink to it.\r\n\r\n**Motivation**\r\n\r\nOften users have small root volume and mount a separate volume somewhere else.\r\n\r\n**Notes**\r\n\r\nThere can be the following caveats:\r\n1. Some \"fake\" volumes may exist with near infinite size. We should not choose them. The check for /opt or /mnt should suffice.\r\n2. Users may have NFS or EFS or Ceph mounted somewhere but not have any intention to put ClickHouse on it.\r\n3. We prefer using symlink instead of changing config file, so ops. engineer can easily find it without looking into config file.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30883/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30875","id":1040039573,"node_id":"I_kwDOA5dJV849_b6V","number":30875,"title":"Is there any possibility the number of  consumers in kafka engine can exceed 16? And why the limit is setting to 16?","user":{"login":"mo-avatar","id":59680192,"node_id":"MDQ6VXNlcjU5NjgwMTky","avatar_url":"https://avatars.githubusercontent.com/u/59680192?v=4","gravatar_id":"","url":"https://api.github.com/users/mo-avatar","html_url":"https://github.com/mo-avatar","followers_url":"https://api.github.com/users/mo-avatar/followers","following_url":"https://api.github.com/users/mo-avatar/following{/other_user}","gists_url":"https://api.github.com/users/mo-avatar/gists{/gist_id}","starred_url":"https://api.github.com/users/mo-avatar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mo-avatar/subscriptions","organizations_url":"https://api.github.com/users/mo-avatar/orgs","repos_url":"https://api.github.com/users/mo-avatar/repos","events_url":"https://api.github.com/users/mo-avatar/events{/privacy}","received_events_url":"https://api.github.com/users/mo-avatar/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"genzgd","id":625882,"node_id":"MDQ6VXNlcjYyNTg4Mg==","avatar_url":"https://avatars.githubusercontent.com/u/625882?v=4","gravatar_id":"","url":"https://api.github.com/users/genzgd","html_url":"https://github.com/genzgd","followers_url":"https://api.github.com/users/genzgd/followers","following_url":"https://api.github.com/users/genzgd/following{/other_user}","gists_url":"https://api.github.com/users/genzgd/gists{/gist_id}","starred_url":"https://api.github.com/users/genzgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/genzgd/subscriptions","organizations_url":"https://api.github.com/users/genzgd/orgs","repos_url":"https://api.github.com/users/genzgd/repos","events_url":"https://api.github.com/users/genzgd/events{/privacy}","received_events_url":"https://api.github.com/users/genzgd/received_events","type":"User","site_admin":false},"assignees":[{"login":"genzgd","id":625882,"node_id":"MDQ6VXNlcjYyNTg4Mg==","avatar_url":"https://avatars.githubusercontent.com/u/625882?v=4","gravatar_id":"","url":"https://api.github.com/users/genzgd","html_url":"https://github.com/genzgd","followers_url":"https://api.github.com/users/genzgd/followers","following_url":"https://api.github.com/users/genzgd/following{/other_user}","gists_url":"https://api.github.com/users/genzgd/gists{/gist_id}","starred_url":"https://api.github.com/users/genzgd/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/genzgd/subscriptions","organizations_url":"https://api.github.com/users/genzgd/orgs","repos_url":"https://api.github.com/users/genzgd/repos","events_url":"https://api.github.com/users/genzgd/events{/privacy}","received_events_url":"https://api.github.com/users/genzgd/received_events","type":"User","site_admin":false},{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-30T03:45:03Z","updated_at":"2022-01-24T16:43:11Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When We try to test Kafka Engine, we find such kind of exception\r\n```\r\n        if (num_consumers > 16)\r\n        {\r\n            throw Exception(\"Number of consumers can not be bigger than 16\", ErrorCodes::BAD_ARGUMENTS);\r\n        }\r\n```\r\n\r\nAs we enlarge the consumers we find the consume rate increase , so why this limit is setting to 16? On what kind of machine  we find this experience value?  If we had better machines can we enlarge it some how ?\r\nThanks\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30875/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30862","id":1039713295,"node_id":"PR_kwDOA5dJV84t25cq","number":30862,"title":"simple fix for #30485 rollup cannot work with injective function","user":{"login":"compasses","id":10161171,"node_id":"MDQ6VXNlcjEwMTYxMTcx","avatar_url":"https://avatars.githubusercontent.com/u/10161171?v=4","gravatar_id":"","url":"https://api.github.com/users/compasses","html_url":"https://github.com/compasses","followers_url":"https://api.github.com/users/compasses/followers","following_url":"https://api.github.com/users/compasses/following{/other_user}","gists_url":"https://api.github.com/users/compasses/gists{/gist_id}","starred_url":"https://api.github.com/users/compasses/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/compasses/subscriptions","organizations_url":"https://api.github.com/users/compasses/orgs","repos_url":"https://api.github.com/users/compasses/repos","events_url":"https://api.github.com/users/compasses/events{/privacy}","received_events_url":"https://api.github.com/users/compasses/received_events","type":"User","site_admin":false},"labels":[{"id":1302792342,"node_id":"MDU6TGFiZWwxMzAyNzkyMzQy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-bugfix","name":"pr-bugfix","color":"ff4080","default":false,"description":"Pull request with bugfix, not backported by default"}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-10-29T15:54:52Z","updated_at":"2021-12-24T04:05:12Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30862","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30862","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30862.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30862.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- Bug Fix (user-visible misbehaviour in official stable or prestable release)\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\nfix rollup cannot work with injective function key\r\n\r\nIt's a simple fix, maybe need the original author to check it if they are proper\r\nFixes #30485","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30862/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30842","id":1039442666,"node_id":"PR_kwDOA5dJV84t2Apl","number":30842,"title":"Pass filter form WHERE to HashJoin for right table ","user":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"labels":[{"id":1304141686,"node_id":"MDU6TGFiZWwxMzA0MTQxNjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-improvement","name":"pr-improvement","color":"007700","default":false,"description":"Pull request with some product improvements"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-29T10:57:18Z","updated_at":"2022-01-24T11:50:35Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":true,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/30842","html_url":"https://github.com/ClickHouse/ClickHouse/pull/30842","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/30842.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/30842.patch","merged_at":null},"body":"Changelog category (leave one):\r\n- Improvement\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n* Pass filter form WHERE to HashJoin for right table, ref #26268","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30842/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30838","id":1039337754,"node_id":"I_kwDOA5dJV8498wka","number":30838,"title":"Clickhouse 'OpenTelemetry Support' can not write to jaeger-collector","user":{"login":"godliness","id":10972761,"node_id":"MDQ6VXNlcjEwOTcyNzYx","avatar_url":"https://avatars.githubusercontent.com/u/10972761?v=4","gravatar_id":"","url":"https://api.github.com/users/godliness","html_url":"https://github.com/godliness","followers_url":"https://api.github.com/users/godliness/followers","following_url":"https://api.github.com/users/godliness/following{/other_user}","gists_url":"https://api.github.com/users/godliness/gists{/gist_id}","starred_url":"https://api.github.com/users/godliness/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/godliness/subscriptions","organizations_url":"https://api.github.com/users/godliness/orgs","repos_url":"https://api.github.com/users/godliness/repos","events_url":"https://api.github.com/users/godliness/events{/privacy}","received_events_url":"https://api.github.com/users/godliness/received_events","type":"User","site_admin":false},"labels":[{"id":386401506,"node_id":"MDU6TGFiZWwzODY0MDE1MDY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/duplicate","name":"duplicate","color":"cccccc","default":true,"description":null},{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-29T08:57:02Z","updated_at":"2021-11-08T07:26:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"clickhouse-server(v21.9.5.16-stable) shows error:\r\n```\r\n2021.10.29 08:36:31.416694 [ 59 ] {} <Error> void DB::SystemLog<DB::OpenTelemetrySpanLogElement>::flushImpl(const std::vector<LogElement> &, uint64_t) [LogElement = DB::OpenTelemetrySpanLogElement]: Code: 86. DB::Exception: Received error from remote server /api/v2/spans. HTTP status code: 400 Bad Request, body: Cannot parse Content-Type: mime: no media type\r\n: while writing suffix to view default.zipkin_spans. (RECEIVED_ERROR_FROM_REMOTE_IO_SERVER), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int, bool) @ 0x93a6a9a in /usr/bin/clickhouse\r\n1. DB::assertResponseIsOk(Poco::Net::HTTPRequest const&, Poco::Net::HTTPResponse&, std::__1::basic_istream<char, std::__1::char_traits<char> >&, bool) @ 0xea933e3 in /usr/bin/clickhouse\r\n2. DB::PushingToSinkBlockOutputStream::writeSuffix() @ 0x10b3207f in /usr/bin/clickhouse\r\n3. DB::PushingToViewsBlockOutputStream::writeSuffix() @ 0x10b3b343 in /usr/bin/clickhouse\r\n4. ? @ 0x10b3ea2f in /usr/bin/clickhouse\r\n5. DB::runViewStage(DB::ViewRuntimeData&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::function<void ()>) @ 0x10b37122 in /usr/bin/clickhouse\r\n6. DB::PushingToViewsBlockOutputStream::writeSuffix() @ 0x10b3b64d in /usr/bin/clickhouse\r\n7. DB::SystemLog<DB::OpenTelemetrySpanLogElement>::flushImpl(std::__1::vector<DB::OpenTelemetrySpanLogElement, std::__1::allocator<DB::OpenTelemetrySpanLogElement> > const&, unsigned long) @ 0x10e60117 in /usr/bin/clickhouse\r\n8. DB::SystemLog<DB::OpenTelemetrySpanLogElement>::savingThreadFunction() @ 0x10e5e060 in /usr/bin/clickhouse\r\n9. ThreadFromGlobalPool::ThreadFromGlobalPool<DB::SystemLog<DB::OpenTelemetrySpanLogElement>::startup()::'lambda'()>(DB::OpenTelemetrySpanLogElement&&, DB::SystemLog<DB::OpenTelemetrySpanLogElement>::startup()::'lambda'()&&...)::'lambda'()::operator()() @ 0x10e5df16 in /usr/bin/clickhouse\r\n10. ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) @ 0x93e7abf in /usr/bin/clickhouse\r\n11. ? @ 0x93eb3a3 in /usr/bin/clickhouse\r\n12. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n13. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n (version 21.9.5.16 (official build))\r\n```\r\nDepends on link: https://clickhouse.com/docs/zh/operations/opentelemetry/#integration-with-monitoring-systems\r\n```\r\nCREATE MATERIALIZED VIEW default.zipkin_spans\r\nENGINE = URL('http://simplest-collector.user01.svc.cluster.local:9411/api/v2/spans', 'JSONEachRow')\r\nSETTINGS output_format_json_named_tuples_as_objects = 1,\r\n    output_format_json_array_of_rows = 1 AS\r\nSELECT\r\n    lower(hex(reinterpretAsFixedString(trace_id))) AS traceId,\r\n    lower(hex(parent_span_id)) AS parentId,\r\n    lower(hex(span_id)) AS id,\r\n    operation_name AS name,\r\n    start_time_us AS timestamp,\r\n    finish_time_us - start_time_us AS duration,\r\n    cast(tuple('clickhouse'), 'Tuple(serviceName text)') AS localEndpoint,\r\n    cast(tuple(\r\n        attribute.values[indexOf(attribute.names, 'db.statement')]),\r\n        'Tuple(\"db.statement\" text)') AS tags\r\nFROM system.opentelemetry_span_log\r\n```\r\nSet up jaeger as below:\r\n```\r\njaeger-collector with options:\r\n      collector.zipkin.host-port: \":9411\"\r\n      collector.zipkin.allowed-origins: \"*\"\r\n```\r\n\r\nSeems like clickhouse write to jaeger without header \"content-type\"?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30838/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30836","id":1039308589,"node_id":"I_kwDOA5dJV8498pct","number":30836,"title":"Mongo connect timeout","user":{"login":"huyhandes","id":43632522,"node_id":"MDQ6VXNlcjQzNjMyNTIy","avatar_url":"https://avatars.githubusercontent.com/u/43632522?v=4","gravatar_id":"","url":"https://api.github.com/users/huyhandes","html_url":"https://github.com/huyhandes","followers_url":"https://api.github.com/users/huyhandes/followers","following_url":"https://api.github.com/users/huyhandes/following{/other_user}","gists_url":"https://api.github.com/users/huyhandes/gists{/gist_id}","starred_url":"https://api.github.com/users/huyhandes/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/huyhandes/subscriptions","organizations_url":"https://api.github.com/users/huyhandes/orgs","repos_url":"https://api.github.com/users/huyhandes/repos","events_url":"https://api.github.com/users/huyhandes/events{/privacy}","received_events_url":"https://api.github.com/users/huyhandes/received_events","type":"User","site_admin":false},"labels":[{"id":1365689344,"node_id":"MDU6TGFiZWwxMzY1Njg5MzQ0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-foreign-db","name":"comp-foreign-db","color":"b5bcff","default":false,"description":"Integrations with other databases"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-10-29T08:26:08Z","updated_at":"2021-12-07T23:16:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"After I created successfully, the result return an timeout error (I used MongoDB engine)\r\n[2021-10-29 15:17:28] Poco::Exception. Code: 1000, e.code() = 0, Timeout: connect timed out: 10.10.20.39:27017 (version 21.9.5.16 (official build))\r\nTested with 21.10.2.15 also.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30836/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30821","id":1038706035,"node_id":"I_kwDOA5dJV8496WVz","number":30821,"title":"alter to convert table from the OLD syntax to the NEW syntax (using mutation)","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-28T16:22:05Z","updated_at":"2021-10-28T17:27:46Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Options:\r\n1. `alter table X convert (modify/update/upgrade) format`\r\n2. `alter table Y attach ( attach_data_and_convert_format ) from X`\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30821/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30783","id":1038036439,"node_id":"I_kwDOA5dJV8493y3X","number":30783,"title":"Not able to query system.tables when MySQL database is unavailable","user":{"login":"zhicwu","id":4270380,"node_id":"MDQ6VXNlcjQyNzAzODA=","avatar_url":"https://avatars.githubusercontent.com/u/4270380?v=4","gravatar_id":"","url":"https://api.github.com/users/zhicwu","html_url":"https://github.com/zhicwu","followers_url":"https://api.github.com/users/zhicwu/followers","following_url":"https://api.github.com/users/zhicwu/following{/other_user}","gists_url":"https://api.github.com/users/zhicwu/gists{/gist_id}","starred_url":"https://api.github.com/users/zhicwu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/zhicwu/subscriptions","organizations_url":"https://api.github.com/users/zhicwu/orgs","repos_url":"https://api.github.com/users/zhicwu/repos","events_url":"https://api.github.com/users/zhicwu/events{/privacy}","received_events_url":"https://api.github.com/users/zhicwu/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-28T03:11:37Z","updated_at":"2021-10-28T03:11:37Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"When MySQL database is offline, query against `system.tables` will end up with an error like below:\r\n```\r\nReceived exception from server (version 21.10.2):\r\nCode: 1000. DB::Exception: Received from localhost:9000. DB::Exception: Exception: Connections to all replicas failed: <db>@<host>:<port> as user <user>. (POCO_EXCEPTION)\r\n```\r\nDue to this, clients like JDBC driver can no longer get list of tables, resulting a weird situation where all tables \"disappeared\":\r\n![image](https://user-images.githubusercontent.com/4270380/139179188-40de18ca-ca23-4dbb-963a-586065073533.png)\r\nI didn't test but perhaps similar issue exists for other database engines.\r\n\r\nReproduce steps:\r\n1. create mysql database\r\n   ```sql\r\n   create database m engine=MySQL('<host>:<port>', '<db>', '<user>', '<password>')\r\n   ```\r\n2. shutdown mysql server\r\n3. query system.tables\r\n    ```sql\r\n    select * from system.tables\r\n    ```\r\n\r\nWorkaround:\r\n```sql\r\n-- tedious to query external database one by one as we don't know which one has issue\r\nselect * from system.tables where database != 'm'\r\n```\r\n\r\nExpected behavior:\r\n* warnings in log showing the MySQL database was not available\r\n* return all tables except those not able to access","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30783/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30779","id":1037912821,"node_id":"I_kwDOA5dJV8493Ur1","number":30779,"title":"Insert Exception fails to specify which column caused the exception","user":{"login":"reyjrar","id":99365,"node_id":"MDQ6VXNlcjk5MzY1","avatar_url":"https://avatars.githubusercontent.com/u/99365?v=4","gravatar_id":"","url":"https://api.github.com/users/reyjrar","html_url":"https://github.com/reyjrar","followers_url":"https://api.github.com/users/reyjrar/followers","following_url":"https://api.github.com/users/reyjrar/following{/other_user}","gists_url":"https://api.github.com/users/reyjrar/gists{/gist_id}","starred_url":"https://api.github.com/users/reyjrar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/reyjrar/subscriptions","organizations_url":"https://api.github.com/users/reyjrar/orgs","repos_url":"https://api.github.com/users/reyjrar/repos","events_url":"https://api.github.com/users/reyjrar/events{/privacy}","received_events_url":"https://api.github.com/users/reyjrar/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2021-10-27T22:26:52Z","updated_at":"2021-10-28T23:54:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\n\r\nError message does not specify the column name generating the failed insert. I do, however, get an essay of messages, none of which I can action as a user of the clickhouse.\r\n\r\nI'm working with data where most fields can be null. Some of those fields, I apply `LowCardinality()` to and it helps immensely with query performance. I attempted `Nullable(LowCardinality(String))` and was told the DDL was invalid, so I went with `LowCardinality(Nullable(String))` and everything worked. For a while.  I just upgraded to 21.10. And now I'm getting failures.\r\n\r\nMy issue is not about the failure, it's about the Error message.\r\n\r\n```\r\n# See below for full error message\r\nClickHouse error: Internal Server Error (Code: 44. DB::Exception: ColumnLowCardinality cannot be inside Nullable column: ...\r\n```\r\n\r\n**The error does NOT contain the name of the column that caused the exception.** Because of this, I'm completely at a loss for how to better contribute anything meaningful to this bug report. \r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n\r\n21.10\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n\r\n```\r\n    CREATE OR REPLACE TABLE default.logs_local ON CLUSTER '{cluster}'\r\n    (\r\n        aiv Nullable(String),\r\n        ar LowCardinality(Nullable(String)),\r\n        as Nullable(UInt32),\r\n        at Nullable(String),\r\n        at_pc Nullable(String),\r\n        av Nullable(String),\r\n        b UInt32 DEFAULT 0,\r\n        bui Nullable(String),\r\n        c Nullable(String),\r\n        ca LowCardinality(Nullable(String)),\r\n        cc LowCardinality(Nullable(String)),\r\n        ci UInt8 DEFAULT 0,\r\n        cv LowCardinality(Nullable(String)),\r\n        dms UInt32,\r\n        h LowCardinality(Nullable(String)),\r\n        i String,\r\n        id String,\r\n        it LowCardinality(Nullable(String)),\r\n        j LowCardinality(Nullable(String)),\r\n        k UInt16,\r\n        li Nullable(String),\r\n        m LowCardinality(Nullable(String)),\r\n        n LowCardinality(Nullable(String)),\r\n        org LowCardinality(Nullable(String)),\r\n        p Nullable(String),\r\n        r Nullable(String),\r\n        rd Nullable(String),\r\n        rh Nullable(String),\r\n        s UInt16,\r\n        t DateTime,\r\n        tags Array(Nullable(String)),\r\n        ttl Nullable(UInt16),\r\n        u Nullable(String),\r\n        uc LowCardinality(Nullable(String)),\r\n        xr LowCardinality(Nullable(String))\r\n    )\r\n    Engine=Distributed('{cluster}', 'default', 'logs', rand())\r\n```\r\n* Sample data causing error below\r\n\r\n```\r\n[\r\n    [0] {\r\n        aiv     undef,\r\n        ar      undef,\r\n        as      397907,\r\n        at      undef,\r\n        at_pc   undef,\r\n        av      undef,\r\n        b       11617,\r\n        bui     undef,\r\n        c       \"ANONYMIZED\",\r\n        ca      undef,\r\n        cc      \"US\",\r\n        ci      1,\r\n        cv      \"VALID\",\r\n        dms     1,\r\n        h       \"ANONYMIZED\",\r\n        i       \"104.192.231.9\",\r\n        id      \"I4BOw3wB6beiMM6rOwCN\",\r\n        it      \"business\",\r\n        j       \"6fa3244afc6bb6f9fad207b6b52af26b\",\r\n        k       17372,\r\n        li      undef,\r\n        m       \"GET\",\r\n        n       \"ANONYMIZED\",\r\n        org     865,\r\n        p       \"ANONYMIZED\",\r\n        r       \"ANONYMIZED\",\r\n        rd      \"ANONYMIZED\",\r\n        rh      \"ANONYMIZED\",\r\n        s       200,\r\n        t       \"2021-10-27 19:50:56\",\r\n        tags    \"[]\",\r\n        ttl     52,\r\n        u       \"ANONYMIZED\",\r\n        uc      \"ANONYMIZED\",\r\n        xr      \"OC6Z2SK6IvBpNC98R7\"\r\n    }\r\n]\r\n```\r\n\r\n**Expected behavior**\r\n\r\nColumn name to be present saying \"invalid data for column 'x': <words>\".\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\nClickHouse error: Internal Server Error (Code: 44. DB::Exception: ColumnLowCardinality cannot be inside Nullable column: while executing 'FUNCTION if(isNull(_dummy_0) : 3, defaultValueOfTypeName('LowCardinality(Nullable(String))') :: 2, _CAST(_dummy_0, 'LowCardinality(Nullable(String))') :: 4) -> if(isNull(_dummy_0), defaultValueOfTypeName('LowCardinality(Nullable(String))'), _CAST(_dummy_0, 'LowCardinality(Nullable(String))')) Nullable(String) : 1': While executing ValuesBlockInputFormat. (ILLEGAL_COLUMN) (version 21.10.2.15 (official build)))\r\n        INSERT INTO logs_local ( aiv, ar, as, at, at_pc, av, b, bui, c, ca, cc, ci, cv, dms, h, i, id, it, j, k, li, m, n, org, p, r, rd, rh, s, t, tags, ttl, u, uc, xr ) VALUES at /usr/local/share/perl5/ClickHouse.pm line 229.\r\n```\r\n\r\n\r\n**Additional context**\r\n\r\nA snail can sleep for three years.  (That probably helps as much as this error message helps me)\r\n\r\nThe bulletproof vest was actually invented by a pizza delivery guy from Detroit USA, after he was shot twice on the job. (Also as helpful, but also more interesting than the above error message from Clickhouse)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30779/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30756","id":1037551510,"node_id":"I_kwDOA5dJV84918eW","number":30756,"title":"use-of-uninitialized-value in `FunctionDecrypt`/`FunctionArray`/`FunctionCast`","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-27T15:18:44Z","updated_at":"2021-10-27T15:18:44Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/30717/5d964665be8f4a209f74c3f10fd7a472fc3fd0e7/fuzzer_msan/report.html#fail1\r\n\r\n```\r\n2021.10.27 13:31:14.682820 [ 113 ] {4e8d14aa-2867-4d3b-b663-428d1fd7e0e7} <Debug> executeQuery: (from [::1]:49372) SELECT aes_decrypt_mysql('aes-128-ecb', [[NULL, NULL, NULL], [], []], 'text', 'key')\r\n\r\n==37==WARNING: MemorySanitizer: use-of-uninitialized-value\r\n    #0 0x34862b29 in DB::ColumnNullable::getDataAt(unsigned long) const obj-x86_64-linux-gnu/../src/Columns/ColumnNullable.h:64:13\r\n    #1 0x3167a1de in DB::IColumn::getDataAtWithTerminatingZero(unsigned long) const obj-x86_64-linux-gnu/../src/Columns/IColumn.h:99:16\r\n    #2 0x345c6cc9 in DB::ColumnArray::getDataAt(unsigned long) const obj-x86_64-linux-gnu/../src/Columns/ColumnArray.cpp:172:33\r\n    #3 0x3167a1de in DB::IColumn::getDataAtWithTerminatingZero(unsigned long) const obj-x86_64-linux-gnu/../src/Columns/IColumn.h:99:16\r\n    #4 0x345c6dec in DB::ColumnArray::getDataAt(unsigned long) const obj-x86_64-linux-gnu/../src/Columns/ColumnArray.cpp:179:32\r\n    #5 0x17fa1f0f in DB::FunctionDecrypt<(anonymous namespace)::DecryptMySQLModeImpl>::doDecrypt(evp_cipher_st const*, unsigned long, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&) aes_decrypt_mysql.cpp\r\n    #6 0x17f9ed03 in DB::FunctionDecrypt<(anonymous namespace)::DecryptMySQLModeImpl>::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const aes_decrypt_mysql.cpp\r\n    #7 0x1423e073 in DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423e073)\r\n    #8 0x1423d657 in DB::FunctionToExecutableFunctionAdaptor::executeDryRunImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423d657)\r\n    #9 0x307b47dc in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:215:15\r\n    #10 0x307b3064 in DB::IExecutableFunction::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:161:31\r\n    #11 0x307b45ad in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:207:20\r\n    #12 0x307b6077 in DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:260:22\r\n    #13 0x31b52c7d in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node const*, std::__1::allocator<DB::ActionsDAG::Node const*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:198:37\r\n    #14 0x32479e7f in DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:577:51\r\n    #15 0x3249278e in DB::ActionsMatcher::Data::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.h:169:27\r\n    #16 0x3249278e in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:1067:14\r\n    #17 0x3247bbd7 in DB::ActionsMatcher::visit(std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:634:9\r\n    #18 0x3249a7ac in DB::ActionsMatcher::visit(DB::ASTExpressionList&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:736:17\r\n    #19 0x3247bf7c in DB::ActionsMatcher::visit(std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:638:9\r\n    #20 0x324112b1 in DB::InDepthNodeVisitor<DB::ActionsMatcher, true, false, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) obj-x86_64-linux-gnu/../src/Interpreters/InDepthNodeVisitor.h:34:13\r\n    #21 0x323cd5b1 in DB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ActionsDAG>&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:524:48\r\n    #22 0x323ebad6 in DB::SelectQueryExpressionAnalyzer::appendSelect(DB::ExpressionActionsChain&, bool) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1261:5\r\n    #23 0x323fdd1d in DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterDAGInfo> const&, DB::Block const&) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:1671:24\r\n    #24 0x330bd497 in DB::InterpreterSelectQuery::getSampleBlockImpl() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:626:23\r\n    #25 0x330a12da in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context const>, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&)::$_1::operator()(bool) const obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:491:25\r\n    #26 0x330902e5 in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context const>, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:494:5\r\n    #27 0x3308ac9f in DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context const>, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:155:7\r\n    #28 0x33bd8715 in std::__1::__unique_if<DB::InterpreterSelectQuery>::__unique_single std::__1::make_unique<DB::InterpreterSelectQuery, std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context>&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&>(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context>&, DB::SelectQueryOptions&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068:32\r\n    #29 0x33bd8715 in DB::InterpreterSelectWithUnionQuery::buildCurrentChildInterpreter(std::__1::shared_ptr<DB::IAST> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:216:16\r\n    #30 0x33bd3f64 in DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::Context const>, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:138:13\r\n    #31 0x32f29340 in std::__1::__unique_if<DB::InterpreterSelectWithUnionQuery>::__unique_single std::__1::make_unique<DB::InterpreterSelectWithUnionQuery, std::__1::shared_ptr<DB::IAST>&, std::__1::shared_ptr<DB::Context>&, DB::SelectQueryOptions const&>(std::__1::shared_ptr<DB::IAST>&, std::__1::shared_ptr<DB::Context>&, DB::SelectQueryOptions const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2068:32\r\n    #32 0x32f24716 in DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, std::__1::shared_ptr<DB::Context>, DB::SelectQueryOptions const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterFactory.cpp:118:16\r\n    #33 0x3442e14b in DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:602:28\r\n    #34 0x344266b5 in DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:947:30\r\n    #35 0x3683e305 in DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:316:24\r\n    #36 0x3688010b in DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1842:9\r\n    #37 0x419a4c1b in Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3\r\n    #38 0x419a5f51 in Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20\r\n    #39 0x41dd9bb1 in Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14\r\n    #40 0x41dd5208 in Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11\r\n    #41 0x41dd15b5 in Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27\r\n    #42 0x7fb6d710a608 in start_thread /build/glibc-eX1tMB/glibc-2.31/nptl/pthread_create.c:477:8\r\n    #43 0x7fb6d7031292 in __clone /build/glibc-eX1tMB/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\n  Uninitialized value was stored to memory at\r\n    #0 0x9f4b699 in __msan_memcpy (/workspace/clickhouse+0x9f4b699)\r\n    #1 0x34923142 in DB::ColumnVector<char8_t>::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Columns/ColumnVector.cpp:294:5\r\n    #2 0x34853a50 in DB::ColumnNullable::insertRangeFrom(DB::IColumn const&, unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Columns/ColumnNullable.cpp:168:24\r\n    #3 0x345ca136 in DB::ColumnArray::insertFrom(DB::IColumn const&, unsigned long) obj-x86_64-linux-gnu/../src/Columns/ColumnArray.cpp:317:15\r\n    #4 0x2b361e6d in DB::FunctionArray::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x2b361e6d)\r\n    #5 0x1423e073 in DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423e073)\r\n    #6 0x1423d657 in DB::FunctionToExecutableFunctionAdaptor::executeDryRunImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423d657)\r\n    #7 0x307b47dc in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:215:15\r\n    #8 0x307b3064 in DB::IExecutableFunction::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:161:31\r\n    #9 0x307b45ad in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:207:20\r\n    #10 0x307b6077 in DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:260:22\r\n    #11 0x31b52c7d in DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<DB::ActionsDAG::Node const*, std::__1::allocator<DB::ActionsDAG::Node const*> >, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsDAG.cpp:198:37\r\n    #12 0x32479e7f in DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:577:51\r\n    #13 0x3249278e in DB::ActionsMatcher::Data::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.h:169:27\r\n    #14 0x3249278e in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:1067:14\r\n    #15 0x3247bbd7 in DB::ActionsMatcher::visit(std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:634:9\r\n    #16 0x3248203e in DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:977:17\r\n    #17 0x3247bbd7 in DB::ActionsMatcher::visit(std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:634:9\r\n    #18 0x3249a7ac in DB::ActionsMatcher::visit(DB::ASTExpressionList&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:736:17\r\n    #19 0x3247bf7c in DB::ActionsMatcher::visit(std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) obj-x86_64-linux-gnu/../src/Interpreters/ActionsVisitor.cpp:638:9\r\n    #20 0x324112b1 in DB::InDepthNodeVisitor<DB::ActionsMatcher, true, false, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) obj-x86_64-linux-gnu/../src/Interpreters/InDepthNodeVisitor.h:34:13\r\n\r\n  Uninitialized value was created by a heap allocation\r\n    #0 0x9f5214d in malloc (/workspace/clickhouse+0x9f5214d)\r\n    #1 0xa122b6b in Allocator<false, false>::allocNoTrack(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Common/Allocator.h:227:27\r\n    #2 0x349189fa in Allocator<false, false>::alloc(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Common/Allocator.h:96:16\r\n    #3 0x349189fa in void DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 15ul, 16ul>::alloc<>(unsigned long) obj-x86_64-linux-gnu/../src/Common/PODArray.h:120:65\r\n    #4 0x349189fa in DB::PODArrayBase<1ul, 4096ul, Allocator<false, false>, 15ul, 16ul>::alloc_for_num_elements(unsigned long) obj-x86_64-linux-gnu/../src/Common/PODArray.h:114:9\r\n    #5 0x349189fa in DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul>::PODArray(unsigned long, char8_t const&) obj-x86_64-linux-gnu/../src/Common/PODArray.h:338:15\r\n    #6 0x349189fa in DB::ColumnVector<char8_t>::ColumnVector(unsigned long, char8_t) obj-x86_64-linux-gnu/../src/Columns/ColumnVector.h:130:55\r\n    #7 0x3485de16 in COW<DB::IColumn>::mutable_ptr<DB::ColumnVector<char8_t> > COWHelper<DB::ColumnVectorHelper, DB::ColumnVector<char8_t> >::create<unsigned long, int>(unsigned long&&, int&&) obj-x86_64-linux-gnu/../src/Common/COW.h:285:71\r\n    #8 0x3485de16 in DB::makeNullable(COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&) obj-x86_64-linux-gnu/../src/Columns/ColumnNullable.cpp:659:43\r\n    #9 0x307e5b3d in DB::wrapInNullable(COW<DB::IColumn>::immutable_ptr<DB::IColumn> const&, std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) obj-x86_64-linux-gnu/../src/Functions/FunctionHelpers.cpp:293:16\r\n    #10 0x1578f67e in DB::FunctionCast<DB::CastInternalName>::prepareRemoveNullable(std::__1::shared_ptr<DB::IDataType const> const&, std::__1::shared_ptr<DB::IDataType const> const&, bool) const::'lambda'(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)::operator()(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long) const (/workspace/clickhouse+0x1578f67e)\r\n    #11 0x1578ee4a in COW<DB::IColumn>::immutable_ptr<DB::IColumn> std::__1::__function::__policy_invoker<COW<DB::IColumn>::immutable_ptr<DB::IColumn> (std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)>::__call_impl<std::__1::__function::__default_alloc_func<DB::FunctionCast<DB::CastInternalName>::prepareRemoveNullable(std::__1::shared_ptr<DB::IDataType const> const&, std::__1::shared_ptr<DB::IDataType const> const&, bool) const::'lambda'(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long), COW<DB::IColumn>::immutable_ptr<DB::IColumn> (std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)> >(std::__1::__function::__policy_storage const*, std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long) (/workspace/clickhouse+0x1578ee4a)\r\n    #12 0x1577ab47 in DB::FunctionCast<DB::CastInternalName>::createArrayWrapper(std::__1::shared_ptr<DB::IDataType const> const&, DB::DataTypeArray const&) const::'lambda0'(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)::operator()(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long) const (/workspace/clickhouse+0x1577ab47)\r\n    #13 0x15779e8a in COW<DB::IColumn>::immutable_ptr<DB::IColumn> std::__1::__function::__policy_invoker<COW<DB::IColumn>::immutable_ptr<DB::IColumn> (std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)>::__call_impl<std::__1::__function::__default_alloc_func<DB::FunctionCast<DB::CastInternalName>::createArrayWrapper(std::__1::shared_ptr<DB::IDataType const> const&, DB::DataTypeArray const&) const::'lambda0'(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long), COW<DB::IColumn>::immutable_ptr<DB::IColumn> (std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long)> >(std::__1::__function::__policy_storage const*, std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::shared_ptr<DB::IDataType const> const&, DB::ColumnNullable const*, unsigned long) (/workspace/clickhouse+0x15779e8a)\r\n    #14 0x152a343a in DB::ExecutableFunctionCast::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x152a343a)\r\n    #15 0x307b499d in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:217:15\r\n    #16 0x307b6262 in DB::IExecutableFunction::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:264:18\r\n    #17 0x1423bc6c in DB::IFunctionBase::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const (/workspace/clickhouse+0x1423bc6c)\r\n    #18 0x343ac023 in COW<DB::IColumn>::immutable_ptr<DB::IColumn> DB::castColumn<(DB::CastType)0>(DB::ColumnWithTypeAndName const&, std::__1::shared_ptr<DB::IDataType const> const&) obj-x86_64-linux-gnu/../src/Interpreters/castColumn.cpp:35:27\r\n    #19 0x343ac023 in DB::castColumn(DB::ColumnWithTypeAndName const&, std::__1::shared_ptr<DB::IDataType const> const&) obj-x86_64-linux-gnu/../src/Interpreters/castColumn.cpp:41:12\r\n    #20 0x2b361138 in DB::FunctionArray::executeImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x2b361138)\r\n    #21 0x1423e073 in DB::IFunction::executeImplDryRun(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423e073)\r\n    #22 0x1423d657 in DB::FunctionToExecutableFunctionAdaptor::executeDryRunImpl(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const (/workspace/clickhouse+0x1423d657)\r\n    #23 0x307b47dc in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:215:15\r\n    #24 0x307b3064 in DB::IExecutableFunction::defaultImplementationForConstantArguments(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:161:31\r\n    #25 0x307b45ad in DB::IExecutableFunction::executeWithoutLowCardinalityColumns(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> > const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long, bool) const obj-x86_64-linux-gnu/../src/Functions/IFunction.cpp:207:20\r\n\r\nSUMMARY: MemorySanitizer: use-of-uninitialized-value obj-x86_64-linux-gnu/../src/Columns/ColumnNullable.h:64:13 in DB::ColumnNullable::getDataAt(unsigned long) const\r\n\r\n2021.10.27 13:31:17.644855 [ 54 ] {} <Trace> BaseDaemon: Received signal -3\r\n2021.10.27 13:31:17.645312 [ 337 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.10.27 13:31:17.645740 [ 337 ] {} <Fatal> BaseDaemon: (version 21.11.1.8571, build id: CD3CCA8DB95C8C1988720DDFC033716C991DEE34) (from thread 113) (query_id: 4e8d14aa-2867-4d3b-b663-428d1fd7e0e7) Received signal Unknown signal (-3)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30756/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30754","id":1037436753,"node_id":"I_kwDOA5dJV8491gdR","number":30754,"title":"groupBitmapMerge slow. how to pre-aggregate bitmap? ","user":{"login":"gj-zhang","id":21170229,"node_id":"MDQ6VXNlcjIxMTcwMjI5","avatar_url":"https://avatars.githubusercontent.com/u/21170229?v=4","gravatar_id":"","url":"https://api.github.com/users/gj-zhang","html_url":"https://github.com/gj-zhang","followers_url":"https://api.github.com/users/gj-zhang/followers","following_url":"https://api.github.com/users/gj-zhang/following{/other_user}","gists_url":"https://api.github.com/users/gj-zhang/gists{/gist_id}","starred_url":"https://api.github.com/users/gj-zhang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/gj-zhang/subscriptions","organizations_url":"https://api.github.com/users/gj-zhang/orgs","repos_url":"https://api.github.com/users/gj-zhang/repos","events_url":"https://api.github.com/users/gj-zhang/events{/privacy}","received_events_url":"https://api.github.com/users/gj-zhang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-27T13:38:28Z","updated_at":"2021-11-22T01:07:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I use materialized views to convert the detailed data to a bitmap structure, writing 1 million at a time to dc.detail_table. However, the query is slow, and most of the time is found when multiple rows are merged into one (see the log below). Is there any way to aggregate multiple rows of data into one row ahead of time? Here is my table structure and materialized view. There are 900 million elements in a Bitmap\r\n\r\n```sql\r\nCREATE TABLE dc.detail_table on cluster datacenter\r\n(\r\n    `name` String,\r\n    `u` UInt32,\r\n    `value` String\r\n)\r\nENGINE = ReplicatedMergeTree('path', '{replica}')\r\nPARTITION BY name\r\nORDER BY (value)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE dc.agg_table on cluster datacenter\r\n(\r\n    `name` String,\r\n    `value` String,\r\n    `u`  AggregateFunction(groupBitmap, UInt32)\r\n)\r\nENGINE = ReplicatedMergeTree('path', '{replica}')\r\nPARTITION BY name\r\nORDER BY (name, value)\r\nSETTINGS index_granularity = 128;\r\n\r\nCREATE MATERIALIZED VIEW IF NOT EXISTS dc.view  to dc.agg_table\r\nAS \r\nSELECT \r\nname,\r\nvalue,\r\ngroupBitmapState(user_id) AS u\r\nFROM dc.detail_table\r\nGROUP BY name, value;\r\n```\r\n\r\n```shell\r\nroot@ck:/# clickhouse-client --port 9000 --user default --password default -h 10.100.10.10 --send_logs_level=trace <<< \"SELECT multiIf(tag.value = 0, 'unkown', tag.value = 1, 'men', tag.value = 2, 'women', '') AS key, groupBitmapMerge(u) AS group_by_users FROM dc.agg_table AS tag WHERE tag.name = 'sex' GROUP BY key;\" > /dev/null\r\n[10.100.10.10] 2021.10.27 21:17:38.178827 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> executeQuery: (from 10.100.10.6:61848) SELECT multiIf(tag.value = 0, 'unkown', tag.value = 1, 'men', tag.value = 2, 'women', '') AS key, groupBitmapMerge(u) AS group_by_users FROM dc.agg_table AS tag WHERE tag.name = 'sex' GROUP BY key; \r\n[10.100.10.10] 2021.10.27 21:17:38.179558 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> ContextAccess (default): Access granted: SELECT(tag_id, tag_value, users) ON dc.table\r\n[10.100.10.10] 2021.10.27 21:17:38.179629 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[10.100.10.10] 2021.10.27 21:17:38.179980 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Key condition: (column 0 in ['sex', 'sex'])\r\n[10.100.10.10] 2021.10.27 21:17:38.180257 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): MinMax index condition: (column 0 in ['sex', 'sex'])\r\n[10.100.10.10] 2021.10.27 21:17:38.180362 [ 248 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Running binary search on index range for part d42b1c1615f4b5de00c0e600198b8b9c_5234_5344_3 (2 marks)\r\n[10.100.10.10] 2021.10.27 21:17:38.180364 [ 747 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Running binary search on index range for part d42b1c1615f4b5de00c0e600198b8b9c_5345_5457_4 (2 marks)\r\n[10.100.10.10] 2021.10.27 21:17:38.180357 [ 321 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Running binary search on index range for part d42b1c1615f4b5de00c0e600198b8b9c_5117_5233_3 (2 marks)\r\n[10.100.10.10] 2021.10.27 21:17:38.180402 [ 248 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[10.100.10.10] 2021.10.27 21:17:38.180355 [ 506 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Running binary search on index range for part d42b1c1615f4b5de00c0e600198b8b9c_4484_4999_4 (6 marks)\r\n[10.100.10.10] 2021.10.27 21:17:38.180402 [ 747 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[10.100.10.10] 2021.10.27 21:17:38.180420 [ 248 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[10.100.10.10] 2021.10.27 21:17:38.180362 [ 1014 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Running binary search on index range for part d42b1c1615f4b5de00c0e600198b8b9c_5000_5116_3 (2 marks)\r\n[10.100.10.10] 2021.10.27 21:17:38.180432 [ 747 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[10.100.10.10] 2021.10.27 21:17:38.180448 [ 747 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found continuous range in 2 steps\r\n[10.100.10.10] 2021.10.27 21:17:38.180448 [ 248 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found continuous range in 2 steps\r\n[10.100.10.10] 2021.10.27 21:17:38.180427 [ 506 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[10.100.10.10] 2021.10.27 21:17:38.180418 [ 321 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[10.100.10.10] 2021.10.27 21:17:38.180441 [ 1014 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[10.100.10.10] 2021.10.27 21:17:38.180462 [ 506 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (RIGHT) boundary mark: 6\r\n[10.100.10.10] 2021.10.27 21:17:38.180470 [ 321 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[10.100.10.10] 2021.10.27 21:17:38.180480 [ 1014 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found (RIGHT) boundary mark: 2\r\n[10.100.10.10] 2021.10.27 21:17:38.180512 [ 506 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found continuous range in 5 steps\r\n[10.100.10.10] 2021.10.27 21:17:38.180516 [ 321 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found continuous range in 2 steps\r\n[10.100.10.10] 2021.10.27 21:17:38.180524 [ 1014 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Found continuous range in 2 steps\r\n[10.100.10.10] 2021.10.27 21:17:38.180647 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Selected 5/11 parts by partition key, 5 parts by primary key, 9/9 marks by primary key, 9 marks to read from 5 ranges\r\n[10.100.10.10] 2021.10.27 21:17:38.180813 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> dc.table (41c2977b-4baa-4b8c-81c2-977b4baa8b8c) (SelectExecutor): Reading approx. 974 rows with 5 streams\r\n[10.100.10.10] 2021.10.27 21:17:38.190991 [ 131 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> AggregatingTransform: Aggregating\r\n[10.100.10.10] 2021.10.27 21:17:38.191008 [ 494 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> AggregatingTransform: Aggregating\r\n[10.100.10.10] 2021.10.27 21:17:38.191069 [ 131 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Aggregation method: key_string\r\n[10.100.10.10] 2021.10.27 21:17:38.191087 [ 494 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Aggregation method: key_string\r\n[10.100.10.10] 2021.10.27 21:17:38.191180 [ 777 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> AggregatingTransform: Aggregating\r\n[10.100.10.10] 2021.10.27 21:17:38.191241 [ 777 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Aggregation method: key_string\r\n[10.100.10.10] 2021.10.27 21:17:38.191253 [ 656 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> AggregatingTransform: Aggregating\r\n[10.100.10.10] 2021.10.27 21:17:38.191315 [ 656 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Aggregation method: key_string\r\n[10.100.10.10] 2021.10.27 21:17:38.202372 [ 517 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> AggregatingTransform: Aggregating\r\n[10.100.10.10] 2021.10.27 21:17:38.202423 [ 517 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Aggregation method: key_string\r\n[10.100.10.10] 2021.10.27 21:17:45.239414 [ 656 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> AggregatingTransform: Aggregated. 114 to 1 rows (from 458.56 KiB) in 7.058267413 sec. (16.151 rows/sec., 64.97 KiB/sec.)\r\n[10.100.10.10] 2021.10.27 21:17:45.241330 [ 494 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> AggregatingTransform: Aggregated. 116 to 1 rows (from 466.61 KiB) in 7.060193156 sec. (16.430 rows/sec., 66.09 KiB/sec.)\r\n[10.100.10.10] 2021.10.27 21:17:45.525507 [ 777 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> AggregatingTransform: Aggregated. 120 to 1 rows (from 482.70 KiB) in 7.344350914 sec. (16.339 rows/sec., 65.72 KiB/sec.)\r\n[10.100.10.10] 2021.10.27 21:17:46.001383 [ 517 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> AggregatingTransform: Aggregated. 131 to 1 rows (from 526.94 KiB) in 7.820244049 sec. (16.751 rows/sec., 67.38 KiB/sec.)\r\n[10.100.10.10] 2021.10.27 21:17:57.747605 [ 131 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> AggregatingTransform: Aggregated. 493 to 1 rows (from 1.94 MiB) in 19.566453573 sec. (25.196 rows/sec., 101.35 KiB/sec.)\r\n[10.100.10.10] 2021.10.27 21:17:57.747623 [ 131 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Trace> Aggregator: Merging aggregated data\r\n[10.100.10.10] 2021.10.27 21:17:58.234655 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Information> executeQuery: Read 974 rows, 3.83 MiB in 20.055730795 sec., 48 rows/sec., 195.59 KiB/sec.\r\n[10.100.10.10] 2021.10.27 21:17:58.234713 [ 1583 ] {04ffaa1b-f37e-4f8a-8923-30ea3142bd33} <Debug> MemoryTracker: Peak memory usage (for query): 20.08 MiB.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30754/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30749","id":1037397899,"node_id":"I_kwDOA5dJV8491W-L","number":30749,"title":"Assertion failed in StorageFileLog","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false},"assignees":[{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-10-27T13:04:27Z","updated_at":"2021-10-29T13:08:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/a29711f1d075f389bbb0306742cd8ffcc5a4f1ba/stress_test_(debug).html#fail1\r\n\r\n```\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:23.972377 [ 12641 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.030821 [ 12641 ] {} <Fatal> BaseDaemon: (version 21.11.1.8572 (official build), build id: F85C80E704BE4D3572BBAB58238989B6BF9C761C) (from thread 12394) (query_id: 3e73fe60-83e1-4b5d-94ed-afa35a8c1631) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.057719 [ 12641 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.092459 [ 12641 ] {} <Fatal> BaseDaemon: Stack trace: 0x7fc722efb18b 0x7fc722eda859 0x7fc722eda729 0x7fc722eebf36 0x224e56b5 0x224e1edc 0x22190800 0x218f52f3 0x218eeecb 0x218ee3c5 0x21c456bd 0x21c4607a 0x21f069db 0x21f045a4 0x22b6f88c 0x22b7ef25 0x26fd99b9 0x26fda1c8 0x27128114 0x27124bfa 0x271239dc 0x7fc7230dd609 0x7fc722fd7293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.288863 [ 12641 ] {} <Fatal> BaseDaemon: 4. gsignal @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.357029 [ 12641 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.367961 [ 12641 ] {} <Fatal> BaseDaemon: 6. ? @ 0x25729 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:24.435352 [ 12641 ] {} <Fatal> BaseDaemon: 7. ? @ 0x36f36 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:26.084892 [ 12641 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../src/Storages/FileLog/StorageFileLog.cpp:0: DB::StorageFileLog::updateFileInfos() @ 0x224e56b5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:27.022278 [ 12641 ] {} <Fatal> BaseDaemon: 9. ./obj-x86_64-linux-gnu/../src/Storages/FileLog/StorageFileLog.cpp:331: DB::StorageFileLog::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x224e1edc in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:27.538749 [ 12641 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Storages/IStorage.cpp:109: DB::IStorage::read(DB::QueryPlan&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo&, std::__1::shared_ptr<DB::Context const>, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x22190800 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:29.835916 [ 12641 ] {} <Fatal> BaseDaemon: 11. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1959: DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&) @ 0x218f52f3 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:36.435718 [ 12641 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1014: DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::optional<DB::Pipe>) @ 0x218eeecb in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:38.201830 [ 12641 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:561: DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x218ee3c5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:42.916395 [ 12641 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:263: DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x21c456bd in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:43.716904 [ 12641 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:329: DB::InterpreterSelectWithUnionQuery::execute() @ 0x21c4607a in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.27 12:56:45.806894 [ 496 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```\r\n\r\n#25969\r\ncc: @kssenii ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30749/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30742","id":1037202350,"node_id":"I_kwDOA5dJV8490nOu","number":30742,"title":"Hope the `Map(String,Object)` data type is expected to be officially compatible","user":{"login":"frank-lam","id":19153458,"node_id":"MDQ6VXNlcjE5MTUzNDU4","avatar_url":"https://avatars.githubusercontent.com/u/19153458?v=4","gravatar_id":"","url":"https://api.github.com/users/frank-lam","html_url":"https://github.com/frank-lam","followers_url":"https://api.github.com/users/frank-lam/followers","following_url":"https://api.github.com/users/frank-lam/following{/other_user}","gists_url":"https://api.github.com/users/frank-lam/gists{/gist_id}","starred_url":"https://api.github.com/users/frank-lam/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/frank-lam/subscriptions","organizations_url":"https://api.github.com/users/frank-lam/orgs","repos_url":"https://api.github.com/users/frank-lam/repos","events_url":"https://api.github.com/users/frank-lam/events{/privacy}","received_events_url":"https://api.github.com/users/frank-lam/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":3761121284,"node_id":"LA_kwDOA5dJV87gLigE","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/support-services","name":"support-services","color":"1A70E6","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false},"assignees":[{"login":"melvynator","id":5473562,"node_id":"MDQ6VXNlcjU0NzM1NjI=","avatar_url":"https://avatars.githubusercontent.com/u/5473562?v=4","gravatar_id":"","url":"https://api.github.com/users/melvynator","html_url":"https://github.com/melvynator","followers_url":"https://api.github.com/users/melvynator/followers","following_url":"https://api.github.com/users/melvynator/following{/other_user}","gists_url":"https://api.github.com/users/melvynator/gists{/gist_id}","starred_url":"https://api.github.com/users/melvynator/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/melvynator/subscriptions","organizations_url":"https://api.github.com/users/melvynator/orgs","repos_url":"https://api.github.com/users/melvynator/repos","events_url":"https://api.github.com/users/melvynator/events{/privacy}","received_events_url":"https://api.github.com/users/melvynator/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-10-27T09:43:08Z","updated_at":"2022-01-24T16:43:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I hope the Map(String,JsonObject) data type can be officially compatible, for example, my data is as follows:\r\n```json\r\n{\r\n    \"apiVersion\": \"6.0\",\r\n    \"version\": \"1.10.2\",\r\n    \"deviceId\": \"16\",\r\n    \"brand\": \"mobile\",\r\n    \"deviceModel\": \"Mozilla/5.0 /4.0 Chrome/83.0.4103.106\",\r\n    \"cpu\": \"\",\r\n    \"resolution\": \"384*792\",\r\n    \"uuid\": \"38bd9e54e1***42b5d3d0b7c\",\r\n    \"eventId\": \"webrtc\",\r\n    \"arg1\": \"usg_http_refreshToken\",\r\n    \"arg2\": \"\",\r\n    \"arg3\": \"944a846c2****6cca1d9c7d82d7\",\r\n    \"args\": {\r\n        \"url\": \"xxx.xxx.com/v1/usg/acs/token\",\r\n        \"responseCode\": \"200\",\r\n        \"isSuccess\": true\r\n    },\r\n    \"timestampTz\": \"2021-10-27 11:06:28.601 +0800\"\r\n}\r\n```\r\nIn this scenario, the args field in my table can only be defined as String, select like: **JSONExtractString(args, \"url\")** / **JSONExtractInt(args,\"responseCode\")** / **JSONExtractBool(args, \"isSuccess\")**, but in SQL is not elegant, compared to Map type can use **args['url']** way more succinctly, with presto we can get the fields directly like **args.url** / **args.responseCode** / **args.isSuccess**.\r\nIt is also unclear whether there is a significant difference between the performance of the **JSONExtract*** function with **String** type storage and the underlying performance of the **Map** type storage.\r\nExpect official compatibility with JsonObject types, such data compatibility is the best.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30742/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30731","id":1037073839,"node_id":"I_kwDOA5dJV8490H2v","number":30731,"title":"OSS zero copy replication","user":{"login":"leosunli","id":3837921,"node_id":"MDQ6VXNlcjM4Mzc5MjE=","avatar_url":"https://avatars.githubusercontent.com/u/3837921?v=4","gravatar_id":"","url":"https://api.github.com/users/leosunli","html_url":"https://github.com/leosunli","followers_url":"https://api.github.com/users/leosunli/followers","following_url":"https://api.github.com/users/leosunli/following{/other_user}","gists_url":"https://api.github.com/users/leosunli/gists{/gist_id}","starred_url":"https://api.github.com/users/leosunli/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/leosunli/subscriptions","organizations_url":"https://api.github.com/users/leosunli/orgs","repos_url":"https://api.github.com/users/leosunli/repos","events_url":"https://api.github.com/users/leosunli/events{/privacy}","received_events_url":"https://api.github.com/users/leosunli/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-27T07:32:22Z","updated_at":"2021-10-29T09:19:41Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\r\n\r\n> A clear and concise description of what is the intended usage scenario is.\r\n\r\n**Zero-copy replication for ReplicatedMergeTree over OSS storage**\r\n\r\n> Detailed description / Documentation draft.\r\n\r\n**Zero-copy replication over OSS storage**\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30731/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30730","id":1037071296,"node_id":"I_kwDOA5dJV8490HPA","number":30730,"title":"NAS zero copy replication","user":{"login":"Alpha-su","id":35217514,"node_id":"MDQ6VXNlcjM1MjE3NTE0","avatar_url":"https://avatars.githubusercontent.com/u/35217514?v=4","gravatar_id":"","url":"https://api.github.com/users/Alpha-su","html_url":"https://github.com/Alpha-su","followers_url":"https://api.github.com/users/Alpha-su/followers","following_url":"https://api.github.com/users/Alpha-su/following{/other_user}","gists_url":"https://api.github.com/users/Alpha-su/gists{/gist_id}","starred_url":"https://api.github.com/users/Alpha-su/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Alpha-su/subscriptions","organizations_url":"https://api.github.com/users/Alpha-su/orgs","repos_url":"https://api.github.com/users/Alpha-su/repos","events_url":"https://api.github.com/users/Alpha-su/events{/privacy}","received_events_url":"https://api.github.com/users/Alpha-su/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-27T07:29:11Z","updated_at":"2021-10-27T07:37:12Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Reasons for the proposal**\r\n> I saw the new feature **'Separation of storage and compute'** in roadmap 2021 but it only support S3 and HDFS filesystem, i want to extend this so that it can support zero copy replication on **NAS**(Network Attached Storage). \r\nA NAS system is a storage device connected to a network that allows storage and retrieval of data from a centralized location for authorized network users and heterogeneous clients, it means that different users on different ECS(Elastic Container Service)  can access NAS just like local disk. But now on clickhouse, we still need to copy data though the data path is the same one using NAS.\r\n\r\n**Use case**\r\n\r\n> Different Clickhouse server have the same data path which is using NAS, so the data path can be accessed by different server. As NAS is infinitely scalable, we can add servers to improve computing performance.\r\n\r\n**Describe the solution you'd like**\r\n\r\n>Clickhouse cluster will synchronize two types of data: metadata in memory and part data on disk, if we have the same data path, we can only  synchronize metadata in cluster.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30730/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30724","id":1037027743,"node_id":"I_kwDOA5dJV849z8mf","number":30724,"title":"prefer_global_in_and_join=1 DB::Exception: Unknown table function tuple","user":{"login":"Funnyang","id":9402399,"node_id":"MDQ6VXNlcjk0MDIzOTk=","avatar_url":"https://avatars.githubusercontent.com/u/9402399?v=4","gravatar_id":"","url":"https://api.github.com/users/Funnyang","html_url":"https://github.com/Funnyang","followers_url":"https://api.github.com/users/Funnyang/followers","following_url":"https://api.github.com/users/Funnyang/following{/other_user}","gists_url":"https://api.github.com/users/Funnyang/gists{/gist_id}","starred_url":"https://api.github.com/users/Funnyang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Funnyang/subscriptions","organizations_url":"https://api.github.com/users/Funnyang/orgs","repos_url":"https://api.github.com/users/Funnyang/repos","events_url":"https://api.github.com/users/Funnyang/events{/privacy}","received_events_url":"https://api.github.com/users/Funnyang/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-10-27T06:30:38Z","updated_at":"2021-10-28T05:55:59Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"when \r\n```\r\nset prefer_global_in_and_join=1;\r\nset distributed_product_mode='global';\r\n```\r\nexecute sql（date_dim is a distributed table）\r\n```\r\nselect count(*) from date_dim where d_year in (1999, 1999 + 1, 1999 + 2, 1999 + 3);\r\n```\r\ngot this:\r\n```\r\nReceived exception from server (version 21.8.5):\r\nCode: 46. DB::Exception: Received from localhost:9000. DB::Exception: Unknown table function tuple: While processing d_year IN (1999, 1999 + 1, 1999 + 2, 1999 + 3).\r\n```\r\nhowever, use `d_year in (1999, 2000, 2001, 2002)` is ok, for example:\r\n```\r\nselect count(*) from date_dim where d_year in (1999, 2000, 2001, 2002);\r\n```\r\n\r\n**How to reproduce**\r\n* ClickHouse server version 21.8.5\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30724/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30721","id":1036899321,"node_id":"I_kwDOA5dJV849zdP5","number":30721,"title":"Multi-table JOIN Code: 47, e.displayText() = DB::Exception: Missing columns","user":{"login":"guoqlin","id":49741140,"node_id":"MDQ6VXNlcjQ5NzQxMTQw","avatar_url":"https://avatars.githubusercontent.com/u/49741140?v=4","gravatar_id":"","url":"https://api.github.com/users/guoqlin","html_url":"https://github.com/guoqlin","followers_url":"https://api.github.com/users/guoqlin/followers","following_url":"https://api.github.com/users/guoqlin/following{/other_user}","gists_url":"https://api.github.com/users/guoqlin/gists{/gist_id}","starred_url":"https://api.github.com/users/guoqlin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/guoqlin/subscriptions","organizations_url":"https://api.github.com/users/guoqlin/orgs","repos_url":"https://api.github.com/users/guoqlin/repos","events_url":"https://api.github.com/users/guoqlin/events{/privacy}","received_events_url":"https://api.github.com/users/guoqlin/received_events","type":"User","site_admin":false},"labels":[{"id":3260776666,"node_id":"MDU6TGFiZWwzMjYwNzc2NjY2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/potential%20bug","name":"potential bug","color":"ffc080","default":false,"description":"To be reviewed by developers and confirmed/rejected."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-10-27T02:12:23Z","updated_at":"2021-10-29T02:21:12Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Here are the steps to redo the problem\r\n\r\nFirst built table\r\ncreate DATABASE if not exists smartbimpp;\r\ncreate table `smartbimpp`.`avhnzf9rr2vuyesdvryc6w`(`B商品名称` Nullable(String),`B订单数` Nullable(Int64)) Engine=Log;\r\ncreate table `smartbimpp`.`m1nckbdef438o9em38zepg`(`A商品名称` Nullable(String),`B商品名称` Nullable(String),`同时购买A和B订单数` Nullable(Int64),`总订单数` Nullable(Int64)) Engine=Log;\r\ncreate table `smartbimpp`.`3qcnfud0fy4lbircffaduw`(`A商品名称` Nullable(String),`A订单数` Nullable(Int64)) Engine=Log;\r\n\r\nThen execute the following statement\r\n\r\nSELECT\r\n\t`m1nckbdef438o9em38zepg`.`B商品名称` AS `B商品名称`,\r\n\t`m1nckbdef438o9em38zepg`.`同时购买A和B订单数` AS `同时购买A和B订单数2`\r\nFROM\r\n\t`smartbimpp`.`3qcnfud0fy4lbircffaduw` `3qcnfud0fy4lbircffaduw` \r\nRIGHT JOIN `smartbimpp`.`m1nckbdef438o9em38zepg` `m1nckbdef438o9em38zepg` ON\r\n\t`3qcnfud0fy4lbircffaduw`.`A商品名称` = `m1nckbdef438o9em38zepg`.`A商品名称`\r\nLEFT JOIN `smartbimpp`.`avhnzf9rr2vuyesdvryc6w` `avhnzf9rr2vuyesdvryc6w` ON\r\n\t`m1nckbdef438o9em38zepg`.`B商品名称` = `avhnzf9rr2vuyesdvryc6w`.`B商品名称`\r\n\r\n\r\nClickHouse exception, code: 47, host: 10.10.111.129, port: 8123; Code: 47, e.displayText() = DB::Exception: Missing columns: '同时购买A和B订单数' while processing query: 'SELECT A商品名称 AS --3qcnfud0fy4lbircffaduw.A商品名称, 同时购买A和B订单数, m1nckbdef438o9em38zepg.A商品名称 AS --m1nckbdef438o9em38zepg.A商品名称, B商品名称 AS --m1nckbdef438o9em38zepg.B商品名称 FROM smartbimpp.3qcnfud0fy4lbircffaduw AS 3qcnfud0fy4lbircffaduw ALL RIGHT JOIN\r\nsmartbimpp.m1nckbdef438o9em38zepg AS m1nckbdef438o9em38zepg ON --3qcnfud0fy4lbircffaduw.A商品名称 = --m1nckbdef438o9em38zepg.A商品名称', required columns: 'A商品名称' '同时购买A和B订单数' 'm1nckbdef438o9em38zepg.A商品名称' 'B商品名称', maybe you meant: '['A商品名称']' '['A商品名称']', joined columns: 'm1nckbdef438o9em38zepg.A商品名称' 'B商品名称' 'm1nckbdef438o9em38zepg.同时购买A和B订单数' 'm1nckbdef438o9em38zepg.总订单数' (version 21.9.5.16 (official build))\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30721/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30715","id":1036600214,"node_id":"I_kwDOA5dJV849yUOW","number":30715,"title":"More functions for `Map` data type","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":{"login":"hexiaoting","id":7898329,"node_id":"MDQ6VXNlcjc4OTgzMjk=","avatar_url":"https://avatars.githubusercontent.com/u/7898329?v=4","gravatar_id":"","url":"https://api.github.com/users/hexiaoting","html_url":"https://github.com/hexiaoting","followers_url":"https://api.github.com/users/hexiaoting/followers","following_url":"https://api.github.com/users/hexiaoting/following{/other_user}","gists_url":"https://api.github.com/users/hexiaoting/gists{/gist_id}","starred_url":"https://api.github.com/users/hexiaoting/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexiaoting/subscriptions","organizations_url":"https://api.github.com/users/hexiaoting/orgs","repos_url":"https://api.github.com/users/hexiaoting/repos","events_url":"https://api.github.com/users/hexiaoting/events{/privacy}","received_events_url":"https://api.github.com/users/hexiaoting/received_events","type":"User","site_admin":false},"assignees":[{"login":"hexiaoting","id":7898329,"node_id":"MDQ6VXNlcjc4OTgzMjk=","avatar_url":"https://avatars.githubusercontent.com/u/7898329?v=4","gravatar_id":"","url":"https://api.github.com/users/hexiaoting","html_url":"https://github.com/hexiaoting","followers_url":"https://api.github.com/users/hexiaoting/followers","following_url":"https://api.github.com/users/hexiaoting/following{/other_user}","gists_url":"https://api.github.com/users/hexiaoting/gists{/gist_id}","starred_url":"https://api.github.com/users/hexiaoting/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/hexiaoting/subscriptions","organizations_url":"https://api.github.com/users/hexiaoting/orgs","repos_url":"https://api.github.com/users/hexiaoting/repos","events_url":"https://api.github.com/users/hexiaoting/events{/privacy}","received_events_url":"https://api.github.com/users/hexiaoting/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-10-26T18:12:57Z","updated_at":"2021-10-27T09:39:12Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Idea after conversation with [cLoki](https://github.com/lmangani/cLoki).\r\n\r\n`mapReplace(map1, map2)` - replaces values for keys in map1 with the values of the corresponding keys in map2; adds keys from map2 that don't exist in map1.\r\n\r\nHigher order functions: `mapFilter`, and even `mapMap`.\r\nThey can take a lambda function with k, v pair as arguments.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30715/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30706","id":1036527176,"node_id":"I_kwDOA5dJV849yCZI","number":30706,"title":"abort() called in capnproto on exception destruction","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":2104602822,"node_id":"MDU6TGFiZWwyMTA0NjAyODIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/fuzz","name":"fuzz","color":"abc4ea","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-10-26T16:46:00Z","updated_at":"2021-12-24T10:45:02Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/0/b549bddc11081641ca5523308406a587c7fb5753/stress_test_(memory).html#fail1\r\n\r\nPossibly the ExceptionImpl was destroyed on a different thread than created it:\r\nhttps://github.com/capnproto/capnproto/blob/fe30e978ae789c704fb7622173acce4c6878125b/c%2B%2B/src/kj/exception.c%2B%2B#L892-L903\r\n\r\n```\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.588711 [ 23555 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.589082 [ 23555 ] {} <Fatal> BaseDaemon: (version 21.11.1.8564 (official build), build id: 2F76347EDBC8AF49AE8D4D8182535717573C3A2D) (from thread 2248) (query_id: 0d2f01e9-1719-4293-ba86-cad365fe6dc1) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.592848 [ 23555 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.607520 [ 23555 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f6c16a5318b 0x7f6c16a32859 0x42481b2f 0x4ad578b2 0x4aad94ab 0x36965e46 0x36965928 0x36945759 0x30d5e96d 0x369892fe 0x34f0c890 0x34f08cf2 0x313b60f5 0x313c7503 0x31359c0a 0x3684ca78 0x3680384d 0x3684418c 0x41969e7c 0x4196b1b2 0x41d9ee12 0x41d9a469 0x41d96816 0x7f6c16c08609 0x7f6c16b2f293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.630359 [ 23555 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:58:59.633964 [ 23555 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.169897 [ 23604 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.170555 [ 23604 ] {} <Fatal> BaseDaemon: (version 21.11.1.8564 (official build), build id: 2F76347EDBC8AF49AE8D4D8182535717573C3A2D) (from thread 6551) (query_id: 11feb6f7-81c1-447c-8c1a-e93899541657) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.170652 [ 23604 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.170788 [ 23604 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f6c16a5318b 0x7f6c16a32859 0x42481b2f 0x4ad578b2 0x4aad94ab 0x36965e46 0x36965928 0x36945759 0x30d5e96d 0x369892fe 0x34f0c890 0x34f08cf2 0x313b60f5 0x313c7503 0x31359c0a 0x3684ca78 0x3680384d 0x3684418c 0x41969e7c 0x4196b1b2 0x41d9ee12 0x41d9a469 0x41d96816 0x7f6c16c08609 0x7f6c16b2f293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.179384 [ 23604 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.179540 [ 23604 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.824788 [ 23642 ] {} <Fatal> BaseDaemon: ########################################\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.824905 [ 23642 ] {} <Fatal> BaseDaemon: (version 21.11.1.8564 (official build), build id: 2F76347EDBC8AF49AE8D4D8182535717573C3A2D) (from thread 10640) (query_id: 745f5bcc-ebda-4adf-9eb6-1110ada31510) Received signal Aborted (6)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.825602 [ 23642 ] {} <Fatal> BaseDaemon: \r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.825747 [ 23642 ] {} <Fatal> BaseDaemon: Stack trace: 0x7f6c16a5318b 0x7f6c16a32859 0x42481b2f 0x4ad578b2 0x4aad94ab 0x36965e46 0x36965928 0x36945759 0x30d5e96d 0x369892fe 0x34f0c890 0x34f08cf2 0x313b60f5 0x313c7503 0x31359c0a 0x3684ca78 0x3680384d 0x3684418c 0x41969e7c 0x4196b1b2 0x41d9ee12 0x41d9a469 0x41d96816 0x7f6c16c08609 0x7f6c16b2f293\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.830180 [ 23642 ] {} <Fatal> BaseDaemon: 4. raise @ 0x4618b in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.830332 [ 23642 ] {} <Fatal> BaseDaemon: 5. abort @ 0x25859 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.934373 [ 23604 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/capnproto/c++/src/kj/exception.c++:894: kj::ExceptionImpl::~ExceptionImpl() @ 0x42481b2f in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:00.946600 [ 23642 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/capnproto/c++/src/kj/exception.c++:894: kj::ExceptionImpl::~ExceptionImpl() @ 0x42481b2f in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.024574 [ 23555 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../contrib/capnproto/c++/src/kj/exception.c++:894: kj::ExceptionImpl::~ExceptionImpl() @ 0x42481b2f in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.027453 [ 23642 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/libcxxabi/src/cxa_exception.cpp:0: __cxa_decrement_exception_refcount @ 0x4ad578b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.034699 [ 23555 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/libcxxabi/src/cxa_exception.cpp:0: __cxa_decrement_exception_refcount @ 0x4ad578b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.045406 [ 23604 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../contrib/libcxxabi/src/cxa_exception.cpp:0: __cxa_decrement_exception_refcount @ 0x4ad578b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.054604 [ 23642 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/libcxx/src/support/runtime/exception_pointer_cxxabi.ipp:18: std::exception_ptr::~exception_ptr() @ 0x4aad94ab in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.054605 [ 23555 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/libcxx/src/support/runtime/exception_pointer_cxxabi.ipp:18: std::exception_ptr::~exception_ptr() @ 0x4aad94ab in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:01.055338 [ 23604 ] {} <Fatal> BaseDaemon: 8. ./obj-x86_64-linux-gnu/../contrib/libcxx/src/support/runtime/exception_pointer_cxxabi.ipp:18: std::exception_ptr::~exception_ptr() @ 0x4aad94ab in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.241839 [ 23642 ] {} <Fatal> BaseDaemon: 9.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2191: ~__policy_func\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.241840 [ 23555 ] {} <Fatal> BaseDaemon: 9.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2191: ~__policy_func\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.248759 [ 23642 ] {} <Fatal> BaseDaemon: 9.2. inlined from ../contrib/libcxx/include/functional:2547: ~function\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.248759 [ 23555 ] {} <Fatal> BaseDaemon: 9.2. inlined from ../contrib/libcxx/include/functional:2547: ~function\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.248865 [ 23642 ] {} <Fatal> BaseDaemon: 9. ../src/Processors/Executors/ExecutingGraph.h:70: DB::ExecutingGraph::Node::~Node() @ 0x36965e46 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.248874 [ 23555 ] {} <Fatal> BaseDaemon: 9. ../src/Processors/Executors/ExecutingGraph.h:70: DB::ExecutingGraph::Node::~Node() @ 0x36965e46 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.333433 [ 23604 ] {} <Fatal> BaseDaemon: 9.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2191: ~__policy_func\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.333542 [ 23604 ] {} <Fatal> BaseDaemon: 9.2. inlined from ../contrib/libcxx/include/functional:2547: ~function\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.333608 [ 23604 ] {} <Fatal> BaseDaemon: 9. ../src/Processors/Executors/ExecutingGraph.h:70: DB::ExecutingGraph::Node::~Node() @ 0x36965e46 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.493883 [ 23642 ] {} <Fatal> BaseDaemon: 10.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph::Node>::operator()(DB::ExecutingGraph::Node*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.494056 [ 23642 ] {} <Fatal> BaseDaemon: 10.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >::reset(DB::ExecutingGraph::Node*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.494440 [ 23555 ] {} <Fatal> BaseDaemon: 10.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph::Node>::operator()(DB::ExecutingGraph::Node*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.494631 [ 23555 ] {} <Fatal> BaseDaemon: 10.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >::reset(DB::ExecutingGraph::Node*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.495844 [ 23642 ] {} <Fatal> BaseDaemon: 10.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.495982 [ 23555 ] {} <Fatal> BaseDaemon: 10.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.496343 [ 23642 ] {} <Fatal> BaseDaemon: 10.4. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >::destroy(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.496428 [ 23555 ] {} <Fatal> BaseDaemon: 10.4. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >::destroy(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.497470 [ 23642 ] {} <Fatal> BaseDaemon: 10.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.497698 [ 23555 ] {} <Fatal> BaseDaemon: 10.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.504202 [ 23642 ] {} <Fatal> BaseDaemon: 10.6. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.505618 [ 23555 ] {} <Fatal> BaseDaemon: 10.6. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.506749 [ 23642 ] {} <Fatal> BaseDaemon: 10.7. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destruct_at_end(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.507113 [ 23642 ] {} <Fatal> BaseDaemon: 10.8. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.510898 [ 23642 ] {} <Fatal> BaseDaemon: 10.9. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.511412 [ 23642 ] {} <Fatal> BaseDaemon: 10.10. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.511522 [ 23642 ] {} <Fatal> BaseDaemon: 10. ../src/Processors/Executors/ExecutingGraph.h:10: DB::ExecutingGraph::~ExecutingGraph() @ 0x36965928 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.511596 [ 23555 ] {} <Fatal> BaseDaemon: 10.7. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destruct_at_end(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.511856 [ 23555 ] {} <Fatal> BaseDaemon: 10.8. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.511972 [ 23555 ] {} <Fatal> BaseDaemon: 10.9. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.512073 [ 23555 ] {} <Fatal> BaseDaemon: 10.10. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.512201 [ 23555 ] {} <Fatal> BaseDaemon: 10. ../src/Processors/Executors/ExecutingGraph.h:10: DB::ExecutingGraph::~ExecutingGraph() @ 0x36965928 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.562183 [ 23604 ] {} <Fatal> BaseDaemon: 10.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph::Node>::operator()(DB::ExecutingGraph::Node*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.562378 [ 23604 ] {} <Fatal> BaseDaemon: 10.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >::reset(DB::ExecutingGraph::Node*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.562511 [ 23604 ] {} <Fatal> BaseDaemon: 10.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.562645 [ 23604 ] {} <Fatal> BaseDaemon: 10.4. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >::destroy(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.562817 [ 23604 ] {} <Fatal> BaseDaemon: 10.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.567325 [ 23604 ] {} <Fatal> BaseDaemon: 10.6. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::destroy<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >(std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > >&, std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.568208 [ 23604 ] {} <Fatal> BaseDaemon: 10.7. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::__destruct_at_end(std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.568401 [ 23604 ] {} <Fatal> BaseDaemon: 10.8. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> >, std::__1::allocator<std::__1::unique_ptr<DB::ExecutingGraph::Node, std::__1::default_delete<DB::ExecutingGraph::Node> > > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.568496 [ 23604 ] {} <Fatal> BaseDaemon: 10.9. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.568563 [ 23604 ] {} <Fatal> BaseDaemon: 10.10. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.568624 [ 23604 ] {} <Fatal> BaseDaemon: 10. ../src/Processors/Executors/ExecutingGraph.h:10: DB::ExecutingGraph::~ExecutingGraph() @ 0x36965928 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.816970 [ 23555 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph>::operator()(DB::ExecutingGraph*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.817811 [ 23555 ] {} <Fatal> BaseDaemon: 11.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph, std::__1::default_delete<DB::ExecutingGraph> >::reset(DB::ExecutingGraph*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.817927 [ 23555 ] {} <Fatal> BaseDaemon: 11.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.818021 [ 23555 ] {} <Fatal> BaseDaemon: 11. ../src/Processors/Executors/PipelineExecutor.cpp:68: DB::PipelineExecutor::~PipelineExecutor() @ 0x36945759 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.842602 [ 23642 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph>::operator()(DB::ExecutingGraph*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.842784 [ 23642 ] {} <Fatal> BaseDaemon: 11.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph, std::__1::default_delete<DB::ExecutingGraph> >::reset(DB::ExecutingGraph*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.842863 [ 23642 ] {} <Fatal> BaseDaemon: 11.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.843273 [ 23642 ] {} <Fatal> BaseDaemon: 11. ../src/Processors/Executors/PipelineExecutor.cpp:68: DB::PipelineExecutor::~PipelineExecutor() @ 0x36945759 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.925960 [ 23604 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::ExecutingGraph>::operator()(DB::ExecutingGraph*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.927251 [ 23604 ] {} <Fatal> BaseDaemon: 11.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::ExecutingGraph, std::__1::default_delete<DB::ExecutingGraph> >::reset(DB::ExecutingGraph*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.927377 [ 23604 ] {} <Fatal> BaseDaemon: 11.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:02.927479 [ 23604 ] {} <Fatal> BaseDaemon: 11. ../src/Processors/Executors/PipelineExecutor.cpp:68: DB::PipelineExecutor::~PipelineExecutor() @ 0x36945759 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.712138 [ 23604 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::PipelineExecutor, std::__1::allocator<DB::PipelineExecutor> >::__on_zero_shared() @ 0x30d5e96d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.713332 [ 23555 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::PipelineExecutor, std::__1::allocator<DB::PipelineExecutor> >::__on_zero_shared() @ 0x30d5e96d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.713432 [ 23642 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::PipelineExecutor, std::__1::allocator<DB::PipelineExecutor> >::__on_zero_shared() @ 0x30d5e96d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.900663 [ 23604 ] {} <Fatal> BaseDaemon: 13.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.900663 [ 23555 ] {} <Fatal> BaseDaemon: 13.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.909029 [ 23604 ] {} <Fatal> BaseDaemon: 13.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.909199 [ 23604 ] {} <Fatal> BaseDaemon: 13. ../src/Processors/Executors/PullingPipelineExecutor.cpp:35: DB::PullingPipelineExecutor::~PullingPipelineExecutor() @ 0x369892fe in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.916524 [ 23555 ] {} <Fatal> BaseDaemon: 13.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.916657 [ 23555 ] {} <Fatal> BaseDaemon: 13. ../src/Processors/Executors/PullingPipelineExecutor.cpp:35: DB::PullingPipelineExecutor::~PullingPipelineExecutor() @ 0x369892fe in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.941266 [ 23642 ] {} <Fatal> BaseDaemon: 13.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.941402 [ 23642 ] {} <Fatal> BaseDaemon: 13.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:03.941472 [ 23642 ] {} <Fatal> BaseDaemon: 13. ../src/Processors/Executors/PullingPipelineExecutor.cpp:35: DB::PullingPipelineExecutor::~PullingPipelineExecutor() @ 0x369892fe in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.088874 [ 23555 ] {} <Fatal> BaseDaemon: 14.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::PullingPipelineExecutor>::operator()(DB::PullingPipelineExecutor*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.089131 [ 23604 ] {} <Fatal> BaseDaemon: 14.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::PullingPipelineExecutor>::operator()(DB::PullingPipelineExecutor*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.089440 [ 23555 ] {} <Fatal> BaseDaemon: 14.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::PullingPipelineExecutor, std::__1::default_delete<DB::PullingPipelineExecutor> >::reset(DB::PullingPipelineExecutor*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090072 [ 23555 ] {} <Fatal> BaseDaemon: 14.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090121 [ 23642 ] {} <Fatal> BaseDaemon: 14.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:0: std::__1::default_delete<DB::PullingPipelineExecutor>::operator()(DB::PullingPipelineExecutor*) const\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090161 [ 23604 ] {} <Fatal> BaseDaemon: 14.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::PullingPipelineExecutor, std::__1::default_delete<DB::PullingPipelineExecutor> >::reset(DB::PullingPipelineExecutor*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090186 [ 23555 ] {} <Fatal> BaseDaemon: 14. ../src/Storages/StorageFile.cpp:255: DB::StorageFileSource::~StorageFileSource() @ 0x34f0c890 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090276 [ 23642 ] {} <Fatal> BaseDaemon: 14.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<DB::PullingPipelineExecutor, std::__1::default_delete<DB::PullingPipelineExecutor> >::reset(DB::PullingPipelineExecutor*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090351 [ 23604 ] {} <Fatal> BaseDaemon: 14.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090402 [ 23642 ] {} <Fatal> BaseDaemon: 14.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090453 [ 23604 ] {} <Fatal> BaseDaemon: 14. ../src/Storages/StorageFile.cpp:255: DB::StorageFileSource::~StorageFileSource() @ 0x34f0c890 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.090502 [ 23642 ] {} <Fatal> BaseDaemon: 14. ../src/Storages/StorageFile.cpp:255: DB::StorageFileSource::~StorageFileSource() @ 0x34f0c890 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:06.984084 [ 23642 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::StorageFileSource, std::__1::allocator<DB::StorageFileSource> >::__on_zero_shared() @ 0x34f08cf2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.065670 [ 23604 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::StorageFileSource, std::__1::allocator<DB::StorageFileSource> >::__on_zero_shared() @ 0x34f08cf2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.154022 [ 23555 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2615: std::__1::__shared_ptr_emplace<DB::StorageFileSource, std::__1::allocator<DB::StorageFileSource> >::__on_zero_shared() @ 0x34f08cf2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398014 [ 23555 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398032 [ 23642 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398032 [ 23604 ] {} <Fatal> BaseDaemon: 16.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2518: std::__1::__shared_weak_count::__release_shared()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398126 [ 23555 ] {} <Fatal> BaseDaemon: 16.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398225 [ 23642 ] {} <Fatal> BaseDaemon: 16.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398287 [ 23604 ] {} <Fatal> BaseDaemon: 16.2. inlined from ../contrib/libcxx/include/memory:3212: ~shared_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398678 [ 23555 ] {} <Fatal> BaseDaemon: 16.3. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >::destroy(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398699 [ 23604 ] {} <Fatal> BaseDaemon: 16.3. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >::destroy(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.398704 [ 23642 ] {} <Fatal> BaseDaemon: 16.3. inlined from ../contrib/libcxx/include/memory:891: std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >::destroy(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.399419 [ 23555 ] {} <Fatal> BaseDaemon: 16.4. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.399496 [ 23642 ] {} <Fatal> BaseDaemon: 16.4. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.399500 [ 23604 ] {} <Fatal> BaseDaemon: 16.4. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:539: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::integral_constant<bool, true>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.400921 [ 23555 ] {} <Fatal> BaseDaemon: 16.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.400963 [ 23642 ] {} <Fatal> BaseDaemon: 16.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.400999 [ 23604 ] {} <Fatal> BaseDaemon: 16.5. inlined from ../contrib/libcxx/include/__memory/allocator_traits.h:487: void std::__1::allocator_traits<std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::destroy<std::__1::shared_ptr<DB::IProcessor> >(std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> >&, std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.401529 [ 23604 ] {} <Fatal> BaseDaemon: 16.6. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destruct_at_end(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.401535 [ 23555 ] {} <Fatal> BaseDaemon: 16.6. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destruct_at_end(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.402753 [ 23642 ] {} <Fatal> BaseDaemon: 16.6. inlined from ../contrib/libcxx/include/vector:428: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::__destruct_at_end(std::__1::shared_ptr<DB::IProcessor>*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.402830 [ 23555 ] {} <Fatal> BaseDaemon: 16.7. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.402832 [ 23604 ] {} <Fatal> BaseDaemon: 16.7. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.402916 [ 23642 ] {} <Fatal> BaseDaemon: 16.7. inlined from ../contrib/libcxx/include/vector:371: std::__1::__vector_base<std::__1::shared_ptr<DB::IProcessor>, std::__1::allocator<std::__1::shared_ptr<DB::IProcessor> > >::clear()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.402981 [ 23555 ] {} <Fatal> BaseDaemon: 16.8. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403023 [ 23604 ] {} <Fatal> BaseDaemon: 16.8. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403048 [ 23642 ] {} <Fatal> BaseDaemon: 16.8. inlined from ../contrib/libcxx/include/vector:465: ~__vector_base\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403099 [ 23555 ] {} <Fatal> BaseDaemon: 16.9. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403115 [ 23604 ] {} <Fatal> BaseDaemon: 16.9. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403135 [ 23642 ] {} <Fatal> BaseDaemon: 16.9. inlined from ../contrib/libcxx/include/vector:557: ~vector\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403202 [ 23604 ] {} <Fatal> BaseDaemon: 16. ../src/QueryPipeline/QueryPipeline.cpp:29: DB::QueryPipeline::~QueryPipeline() @ 0x313b60f5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403217 [ 23642 ] {} <Fatal> BaseDaemon: 16. ../src/QueryPipeline/QueryPipeline.cpp:29: DB::QueryPipeline::~QueryPipeline() @ 0x313b60f5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.403203 [ 23555 ] {} <Fatal> BaseDaemon: 16. ../src/QueryPipeline/QueryPipeline.cpp:29: DB::QueryPipeline::~QueryPipeline() @ 0x313b60f5 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.774163 [ 23555 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/QueryPipeline/QueryPipeline.cpp:535: DB::QueryPipeline::reset() @ 0x313c7503 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.774165 [ 23642 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/QueryPipeline/QueryPipeline.cpp:535: DB::QueryPipeline::reset() @ 0x313c7503 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.830285 [ 23642 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./obj-x86_64-linux-gnu/../src/QueryPipeline/BlockIO.cpp:0: DB::BlockIO::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.830516 [ 23642 ] {} <Fatal> BaseDaemon: 18. ../src/QueryPipeline/BlockIO.cpp:32: DB::BlockIO::operator=(DB::BlockIO&&) @ 0x31359c0a in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.830647 [ 23555 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./obj-x86_64-linux-gnu/../src/QueryPipeline/BlockIO.cpp:0: DB::BlockIO::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.830770 [ 23555 ] {} <Fatal> BaseDaemon: 18. ../src/QueryPipeline/BlockIO.cpp:32: DB::BlockIO::operator=(DB::BlockIO&&) @ 0x31359c0a in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.879351 [ 23604 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/QueryPipeline/QueryPipeline.cpp:535: DB::QueryPipeline::reset() @ 0x313c7503 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.923798 [ 23604 ] {} <Fatal> BaseDaemon: 18.1. inlined from ./obj-x86_64-linux-gnu/../src/QueryPipeline/BlockIO.cpp:0: DB::BlockIO::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:07.923932 [ 23604 ] {} <Fatal> BaseDaemon: 18. ../src/QueryPipeline/BlockIO.cpp:32: DB::BlockIO::operator=(DB::BlockIO&&) @ 0x31359c0a in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.377457 [ 23604 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:36: DB::QueryState::operator=(DB::QueryState&&) @ 0x3684ca78 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.377744 [ 23555 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:36: DB::QueryState::operator=(DB::QueryState&&) @ 0x3684ca78 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.388893 [ 23642 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:36: DB::QueryState::operator=(DB::QueryState&&) @ 0x3684ca78 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.860231 [ 23642 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:100: DB::QueryState::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.860289 [ 23555 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:100: DB::QueryState::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.860421 [ 23642 ] {} <Fatal> BaseDaemon: 20. ../src/Server/TCPHandler.cpp:474: DB::TCPHandler::runImpl() @ 0x3680384d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.860432 [ 23555 ] {} <Fatal> BaseDaemon: 20. ../src/Server/TCPHandler.cpp:474: DB::TCPHandler::runImpl() @ 0x3680384d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.886717 [ 23604 ] {} <Fatal> BaseDaemon: 20.1. inlined from ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.h:100: DB::QueryState::reset()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:09.888054 [ 23604 ] {} <Fatal> BaseDaemon: 20. ../src/Server/TCPHandler.cpp:474: DB::TCPHandler::runImpl() @ 0x3680384d in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.832242 [ 23555 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:0: DB::TCPHandler::run() @ 0x3684418c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.832352 [ 23642 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:0: DB::TCPHandler::run() @ 0x3684418c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.853408 [ 23604 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:0: DB::TCPHandler::run() @ 0x3684418c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.865865 [ 23642 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x41969e7c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.865865 [ 23604 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x41969e7c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:10.865865 [ 23555 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x41969e7c in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018520 [ 23604 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1655: std::__1::unique_ptr<Poco::Net::TCPServerConnection, std::__1::default_delete<Poco::Net::TCPServerConnection> >::reset(Poco::Net::TCPServerConnection*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018622 [ 23555 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1655: std::__1::unique_ptr<Poco::Net::TCPServerConnection, std::__1::default_delete<Poco::Net::TCPServerConnection> >::reset(Poco::Net::TCPServerConnection*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018646 [ 23604 ] {} <Fatal> BaseDaemon: 23.2. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018781 [ 23604 ] {} <Fatal> BaseDaemon: 23. ../contrib/poco/Net/src/TCPServerDispatcher.cpp:116: Poco::Net::TCPServerDispatcher::run() @ 0x4196b1b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018789 [ 23555 ] {} <Fatal> BaseDaemon: 23.2. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.018944 [ 23555 ] {} <Fatal> BaseDaemon: 23. ../contrib/poco/Net/src/TCPServerDispatcher.cpp:116: Poco::Net::TCPServerDispatcher::run() @ 0x4196b1b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.019750 [ 23642 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1655: std::__1::unique_ptr<Poco::Net::TCPServerConnection, std::__1::default_delete<Poco::Net::TCPServerConnection> >::reset(Poco::Net::TCPServerConnection*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.021724 [ 23642 ] {} <Fatal> BaseDaemon: 23.2. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.021899 [ 23642 ] {} <Fatal> BaseDaemon: 23. ../contrib/poco/Net/src/TCPServerDispatcher.cpp:116: Poco::Net::TCPServerDispatcher::run() @ 0x4196b1b2 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.125609 [ 23555 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:0: Poco::PooledThread::run() @ 0x41d9ee12 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.125687 [ 23642 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:0: Poco::PooledThread::run() @ 0x41d9ee12 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.125801 [ 23604 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:0: Poco::PooledThread::run() @ 0x41d9ee12 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.247505 [ 23642 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x41d9a469 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.248367 [ 23555 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x41d9a469 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.248367 [ 23604 ] {} <Fatal> BaseDaemon: 25. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:56: Poco::(anonymous namespace)::RunnableHolder::run() @ 0x41d9a469 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.291922 [ 23642 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/SharedPtr.h:277: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::get()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.292184 [ 23642 ] {} <Fatal> BaseDaemon: 26.2. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::assign(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.295907 [ 23642 ] {} <Fatal> BaseDaemon: 26.3. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:208: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::operator=(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.296005 [ 23642 ] {} <Fatal> BaseDaemon: 26. ../contrib/poco/Foundation/src/Thread_POSIX.cpp:360: Poco::ThreadImpl::runnableEntry(void*) @ 0x41d96816 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.296615 [ 23642 ] {} <Fatal> BaseDaemon: 27. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.296787 [ 23642 ] {} <Fatal> BaseDaemon: 28. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313276 [ 23604 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/SharedPtr.h:277: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::get()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313276 [ 23555 ] {} <Fatal> BaseDaemon: 26.1. inlined from ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/SharedPtr.h:277: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::get()\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313471 [ 23555 ] {} <Fatal> BaseDaemon: 26.2. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::assign(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313471 [ 23604 ] {} <Fatal> BaseDaemon: 26.2. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::assign(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313613 [ 23604 ] {} <Fatal> BaseDaemon: 26.3. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:208: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::operator=(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313613 [ 23555 ] {} <Fatal> BaseDaemon: 26.3. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:208: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::operator=(Poco::Runnable*)\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313733 [ 23555 ] {} <Fatal> BaseDaemon: 26. ../contrib/poco/Foundation/src/Thread_POSIX.cpp:360: Poco::ThreadImpl::runnableEntry(void*) @ 0x41d96816 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313733 [ 23604 ] {} <Fatal> BaseDaemon: 26. ../contrib/poco/Foundation/src/Thread_POSIX.cpp:360: Poco::ThreadImpl::runnableEntry(void*) @ 0x41d96816 in /usr/bin/clickhouse\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.313983 [ 23555 ] {} <Fatal> BaseDaemon: 27. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.314111 [ 23604 ] {} <Fatal> BaseDaemon: 27. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.314131 [ 23555 ] {} <Fatal> BaseDaemon: 28. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:11.314246 [ 23604 ] {} <Fatal> BaseDaemon: 28. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:17.175819 [ 23555 ] {} <Fatal> BaseDaemon: Checksum of the binary: BBB923EB541850D9499393D9125CA113, integrity check passed.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:17.195976 [ 23604 ] {} <Fatal> BaseDaemon: Checksum of the binary: BBB923EB541850D9499393D9125CA113, integrity check passed.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:17.465336 [ 23642 ] {} <Fatal> BaseDaemon: Checksum of the binary: BBB923EB541850D9499393D9125CA113, integrity check passed.\r\n/var/log/clickhouse-server/clickhouse-server.log.2:2021.10.26 18:59:24.007576 [ 526 ] {} <Fatal> Application: Child process was terminated by signal 6.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30706/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30691","id":1036318682,"node_id":"I_kwDOA5dJV849xPfa","number":30691,"title":"RabbitMQ engine truncates the last character","user":{"login":"WRMSRwasTaken","id":4244869,"node_id":"MDQ6VXNlcjQyNDQ4Njk=","avatar_url":"https://avatars.githubusercontent.com/u/4244869?v=4","gravatar_id":"","url":"https://api.github.com/users/WRMSRwasTaken","html_url":"https://github.com/WRMSRwasTaken","followers_url":"https://api.github.com/users/WRMSRwasTaken/followers","following_url":"https://api.github.com/users/WRMSRwasTaken/following{/other_user}","gists_url":"https://api.github.com/users/WRMSRwasTaken/gists{/gist_id}","starred_url":"https://api.github.com/users/WRMSRwasTaken/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/WRMSRwasTaken/subscriptions","organizations_url":"https://api.github.com/users/WRMSRwasTaken/orgs","repos_url":"https://api.github.com/users/WRMSRwasTaken/repos","events_url":"https://api.github.com/users/WRMSRwasTaken/events{/privacy}","received_events_url":"https://api.github.com/users/WRMSRwasTaken/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false},"assignees":[{"login":"kssenii","id":54203879,"node_id":"MDQ6VXNlcjU0MjAzODc5","avatar_url":"https://avatars.githubusercontent.com/u/54203879?v=4","gravatar_id":"","url":"https://api.github.com/users/kssenii","html_url":"https://github.com/kssenii","followers_url":"https://api.github.com/users/kssenii/followers","following_url":"https://api.github.com/users/kssenii/following{/other_user}","gists_url":"https://api.github.com/users/kssenii/gists{/gist_id}","starred_url":"https://api.github.com/users/kssenii/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kssenii/subscriptions","organizations_url":"https://api.github.com/users/kssenii/orgs","repos_url":"https://api.github.com/users/kssenii/repos","events_url":"https://api.github.com/users/kssenii/events{/privacy}","received_events_url":"https://api.github.com/users/kssenii/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-10-26T13:42:43Z","updated_at":"2021-11-02T13:27:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nI am using [pmacct](https://github.com/pmacct/pmacct) to stream network flow data via RabbitMQ to ClickHouse.\r\n\r\nThe data generated from pmacct in RabbitMQ looks like this when I request a message from the RabbitMQ web management:\r\n\r\nExchange: `pmacct`\r\nRouting Key: `acct`\r\nProperties: `delivery_mode: 2` & `content_type: application/json`\r\nPayload:\r\n```json\r\n{\"event_type\": \"purge\", \"as_src\": 1234, \"as_dst\": 0, \"as_path\": \"\", \"local_pref\": 100, \"med\": 0, \"peer_as_dst\": 0, \"ip_src\": \"<redacted ipv6>\", \"ip_dst\": \"<redacted ipv6>\", \"port_src\": 443, \"port_dst\": 41930, \"ip_proto\": \"tcp\", \"tos\": 0, \"stamp_inserted\": \"2021-10-26 15:20:00\", \"stamp_updated\": \"2021-10-26 15:23:14\", \"packets\": 2, \"bytes\": 1216, \"writer_id\": \"default_amqp/449206\"}\r\n```\r\n\r\nI created the RabbitMQ engine table with:\r\n```sql\r\ncreate table pmacct.ingest (json String) engine = RabbitMQ settings rabbitmq_host_port = '10.10.3.8:5672', rabbitmq_exchange_name = 'pmacct', rabbitmq_routing_key_list = 'acct', rabbitmq_exchange_type = 'direct', rabbitmq_persistent = 1, rabbitmq_format = 'LineAsString'\r\n```\r\n\r\nSELECT-ing the data from that table gives me:\r\n```\r\n{\"event_type\": \"purge\", \"as_src\": 1234, \"as_dst\": 0, \"as_path\": \"\", \"local_pref\": 100, \"med\": 0, \"peer_as_dst\": 0, \"ip_src\": \"<redacted ipv6>\", \"ip_dst\": \"<redacted ipv6>\", \"port_src\": 443, \"port_dst\": 41930, \"ip_proto\": \"tcp\", \"tos\": 0, \"stamp_inserted\": \"2021-10-26 15:20:00\", \"stamp_updated\": \"2021-10-26 15:23:14\", \"packets\": 2, \"bytes\": 1216, \"writer_id\": \"default_amqp/449206\"\r\n```\r\nWhere it is missing the `}` at the end, producing invalid JSON. Also, creating the table with `JSON` as `rabbitmq_format` does not work, SELECT-ing data from it yields the error: `DB::Exception: Format JSON is not suitable for input: While executing SourceFromInputStream.`, maybe because of the invalid JSON? Does ClickHouse truncate the last character because it is expecting a newline character?\r\n\r\nMy current workaround is adding the `}` back to each line with `concat()` in a MATERIALIZED VIEW:\r\n```sql\r\nCREATE MATERIALIZED VIEW ingest_mv TO data AS\r\nSELECT\r\n    JSONExtractString(concat(json, '}'), 'event_type') AS event_type,\r\n    JSONExtractInt(concat(json, '}'), 'as_src') AS as_src,\r\n    JSONExtractInt(concat(json, '}'), 'as_dst') AS as_dst,\r\n    JSONExtractString(concat(json, '}'), 'as_path') AS as_path,\r\n    JSONExtractInt(concat(json, '}'), 'local_pref') AS local_pref,\r\n    JSONExtractInt(concat(json, '}'), 'med') AS med,\r\n    JSONExtractInt(concat(json, '}'), 'peer_as_dst') AS peer_as_dst,\r\n    IPv6StringToNum(JSONExtractString(concat(json, '}'), 'ip_src')) AS ip_src,\r\n    IPv6StringToNum(JSONExtractString(concat(json, '}'), 'ip_dst')) AS ip_dst,\r\n    JSONExtractInt(concat(json, '}'), 'port_src') AS port_src,\r\n    JSONExtractInt(concat(json, '}'), 'port_dst') AS port_dst,\r\n    JSONExtractString(concat(json, '}'), 'ip_proto') AS ip_proto,\r\n    JSONExtractInt(concat(json, '}'), 'tos') AS tos,\r\n    parseDateTime32BestEffortOrZero(JSONExtractString(concat(json, '}'), 'stamp_inserted')) AS stamp_inserted,\r\n    parseDateTime32BestEffortOrZero(JSONExtractString(concat(json, '}'), 'stamp_updated')) AS stamp_updated,\r\n    JSONExtractInt(concat(json, '}'), 'packets') AS packets,\r\n    JSONExtractInt(concat(json, '}'), 'bytes') AS bytes,\r\n    JSONExtractString(concat(json, '}'), 'writer_id') AS writer_id\r\nFROM ingest;\r\n```\r\n\r\nThere is another issue for me: After dropping the MATERIALIZED VIEW, the RabbitMQ table does not get filled anymore. SELECT-ing data from the RabbitMQ table says `0 rows in set.` and the messages start filling up in the RabbitMQ queue. I have to drop the RabbitMQ engine table aswell and re-create it, then I am able to SELECT message again. Also, ClickHouse sometimes crashes while dropping the view, as seen in https://github.com/ClickHouse/ClickHouse/issues/29315 with the exact stack trace.\r\n\r\nVersions are:\r\nClickhouse 21.4.6.55-1\r\nRabbitMQ 3.8.22-1\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30691/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/30683","id":1036136980,"node_id":"I_kwDOA5dJV849wjIU","number":30683,"title":"ExpressionAnalyzer.cpp:555: member access within address which does not point to an object of type 'const DB::ASTFunction' ","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-10-26T10:50:18Z","updated_at":"2021-10-26T10:50:18Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"https://clickhouse-test-reports.s3.yandex.net/30651/70097b5c4e26a5422d9fc1a795a493b96bb5521d/fuzzer_ubsan/report.html#fail1\r\n\r\n```\r\n../src/Interpreters/ExpressionAnalyzer.cpp:555:19: runtime error: member access within address 0x7f9ba926f8b8 which does not point to an object of type 'const DB::ASTFunction'\r\n0x7f9ba926f8b8: note: object is of type 'DB::IAST'\r\n 00 00 00 00  08 d3 1b 09 00 00 00 00  b8 f8 26 a9 9b 7f 00 00  a0 f8 26 a9 9b 7f 00 00  f0 a1 79 92\r\n              ^~~~~~~~~~~~~~~~~~~~~~~\r\n              vptr for 'DB::IAST'\r\n    #0 0x1c7433ca in DB::ExpressionAnalyzer::makeAggregateDescriptions(std::__1::shared_ptr<DB::ActionsDAG>&, std::__1::vector<DB::AggregateDescription, std::__1::allocator<DB::AggregateDescription> >&) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:555:19\r\n    #1 0x1c741a24 in DB::ExpressionAnalyzer::analyzeAggregation(std::__1::shared_ptr<DB::ActionsDAG>&) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:325:5\r\n    #2 0x1c740e08 in DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>, unsigned long, bool, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::SubqueryForSet, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, DB::SubqueryForSet> > >, std::__1::unordered_map<DB::PreparedSetKey, std::__1::shared_ptr<DB::Set>, DB::PreparedSetKey::Hash, std::__1::equal_to<DB::PreparedSetKey>, std::__1::allocator<std::__1::pair<DB::PreparedSetKey const, std::__1::shared_ptr<DB::Set> > > >) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:254:5\r\n    #3 0x1c75d91e in DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>) obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.h:101:11\r\n    #4 0x1d7bc0c6 in DB::KeyCondition::getBlockWithConstants(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>) obj-x86_64-linux-gnu/../src/Storages/MergeTree/KeyCondition.cpp:439:44\r\n    #5 0x1d7bc4e9 in DB::KeyCondition::KeyCondition(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::ExpressionActions> const&, bool, bool) obj-x86_64-linux-gnu/../src/Storages/MergeTree/KeyCondition.cpp:478:34\r\n    #6 0x1d89a9d2 in DB::PartitionPruner::PartitionPruner(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, bool) obj-x86_64-linux-gnu/../src/Storages/MergeTree/PartitionPruner.h:29:11\r\n    #7 0x1d831219 in DB::MergeTreeData::totalRowsByPartitionPredicateImpl(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > > const&) const obj-x86_64-linux-gnu/../src/Storages/MergeTree/MergeTreeData.cpp:848:21\r\n    #8 0x1dacd8dc in DB::StorageMergeTree::totalRowsByPartitionPredicate(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>) const obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:222:12\r\n    #9 0x1cb2d65f in DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1764:33\r\n    #10 0x1cb2620e in DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::optional<DB::Pipe>) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1011:9\r\n    #11 0x1cb25438 in DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:560:5\r\n    #12 0x1cdc27bc in DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:262:38\r\n    #13 0x1cdc3939 in DB::InterpreterSelectWithUnionQuery::execute() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:329:5\r\n    #14 0x1d0736d0 in DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:632:32\r\n    #15 0x1d070ce9 in DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:947:30\r\n    #16 0x1dc7c21c in DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:316:24\r\n    #17 0x1dc97cb5 in DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1837:9\r\n    #18 0x1ea679ab in Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3\r\n    #19 0x1ea67e44 in Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:115:20\r\n    #20 0x1eb58366 in Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14\r\n    #21 0x1eb55c2b in Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27\r\n    #22 0x7fa0424a2608 in start_thread /build/glibc-eX1tMB/glibc-2.31/nptl/pthread_create.c:477:8\r\n    #23 0x7fa0423c9292 in __clone /build/glibc-eX1tMB/glibc-2.31/misc/../sysdeps/unix/sysv/linux/x86_64/clone.S:95\r\n\r\nSUMMARY: UndefinedBehaviorSanitizer: undefined-behavior ../src/Interpreters/ExpressionAnalyzer.cpp:555:19 in \r\n2021.10.25 22:13:09.193458 [ 43 ] {} <Trace> BaseDaemon: Received signal -3\r\n2021.10.25 22:13:09.193658 [ 460 ] {} <Fatal> BaseDaemon: ########################################\r\n2021.10.25 22:13:09.193736 [ 460 ] {} <Fatal> BaseDaemon: (version 21.11.1.8558, build id: 024681AF387E7B05A6FD27C407AE8067E84BF856) (from thread 109) (query_id: 13ae7423-1e08-428a-b9be-a1b2f51e5b3c) Received signal Unknown signal (-3)\r\n2021.10.25 22:13:09.193781 [ 460 ] {} <Fatal> BaseDaemon: Sanitizer trap.\r\n2021.10.25 22:13:09.193870 [ 460 ] {} <Fatal> BaseDaemon: Stack trace: 0xd33c8ca 0x1be75c11 0xd2fde26 0xd30fb59 0x1c7433cb 0x1c741a25 0x1c740e09 0x1c75d91f 0x1d7bc0c7 0x1d7bc4ea 0x1d89a9d3 0x1d83121a 0x1dacd8dd 0x1cb2d660 0x1cb2620f 0x1cb25439 0x1cdc27bd 0x1cdc393a 0x1d0736d1 0x1d070cea 0x1dc7c21d 0x1dc97cb6 0x1ea679ac 0x1ea67e45 0x1eb58367 0x1eb55c2c 0x7fa0424a2609 0x7fa0423c9293\r\n2021.10.25 22:13:09.206170 [ 460 ] {} <Fatal> BaseDaemon: 0.1. inlined from ./obj-x86_64-linux-gnu/../src/Common/StackTrace.cpp:305: StackTrace::tryCapture()\r\n2021.10.25 22:13:09.206198 [ 460 ] {} <Fatal> BaseDaemon: 0. ../src/Common/StackTrace.cpp:266: StackTrace::StackTrace() @ 0xd33c8ca in /workspace/clickhouse\r\n2021.10.25 22:13:09.231514 [ 460 ] {} <Fatal> BaseDaemon: 1. ./obj-x86_64-linux-gnu/../base/daemon/BaseDaemon.cpp:400: sanitizerDeathCallback() @ 0x1be75c11 in /workspace/clickhouse\r\n2021.10.25 22:13:09.420267 [ 153 ] {} <Trace> SystemLog (system.query_log): Flushing system log, 1117 entries to flush up to offset 569898\r\n2021.10.25 22:13:09.475134 [ 153 ] {} <Debug> DiskLocal: Reserving 3.63 MiB on disk `default`, having unreserved 1.01 TiB.\r\n2021.10.25 22:13:09.485723 [ 153 ] {} <Trace> system.query_log (00db6041-f907-4566-80db-6041f9077566): Renaming temporary part tmp_insert_202110_329_329_0 to 202110_329_329_0.\r\n2021.10.25 22:13:09.486401 [ 153 ] {} <Trace> SystemLog (system.query_log): Flushed system log up to offset 569898\r\n2021.10.25 22:13:09.758263 [ 158 ] {} <Debug> DiskLocal: Reserving 6.40 MiB on disk `default`, having unreserved 1.01 TiB.\r\n2021.10.25 22:13:09.772834 [ 158 ] {} <Trace> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Renaming temporary part tmp_insert_202110_332_332_0 to 202110_332_332_0.\r\n2021.10.25 22:13:10.000132 [ 295 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 7.12 GiB, peak 7.52 GiB, will set to 7.27 GiB (RSS), difference: 152.09 MiB\r\n2021.10.25 22:13:10.497119 [ 460 ] {} <Fatal> BaseDaemon: 2. __sanitizer::Die() @ 0xd2fde26 in /workspace/clickhouse\r\n2021.10.25 22:13:10.644700 [ 102 ] {} <Trace> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Found 8 old parts to remove.\r\n2021.10.25 22:13:10.644762 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_224_240_3\r\n2021.10.25 22:13:10.647401 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_241_257_3\r\n2021.10.25 22:13:10.649772 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_258_263_1\r\n2021.10.25 22:13:10.651682 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_264_264_0\r\n2021.10.25 22:13:10.651925 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_265_265_0\r\n2021.10.25 22:13:10.652187 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_266_266_0\r\n2021.10.25 22:13:10.652440 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_267_267_0\r\n2021.10.25 22:13:10.652692 [ 102 ] {} <Debug> system.query_thread_log (ceb36874-beca-4863-8eb3-6874beca5863): Removing part from filesystem 202110_268_268_0\r\n2021.10.25 22:13:11.000127 [ 295 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 7.27 GiB, peak 7.52 GiB, will set to 7.46 GiB (RSS), difference: 189.78 MiB\r\n2021.10.25 22:13:11.454040 [ 105 ] {} <Trace> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Found 7 old parts to remove.\r\n2021.10.25 22:13:11.454094 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_249_254_1\r\n2021.10.25 22:13:11.455692 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_255_259_1\r\n2021.10.25 22:13:11.457124 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_260_264_1\r\n2021.10.25 22:13:11.458739 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_265_265_0\r\n2021.10.25 22:13:11.459014 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_266_266_0\r\n2021.10.25 22:13:11.459799 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_267_267_0\r\n2021.10.25 22:13:11.460591 [ 105 ] {} <Debug> system.text_log (77b1fdb0-6fe4-4560-b7b1-fdb06fe43560): Removing part from filesystem 202110_268_268_0\r\n2021.10.25 22:13:11.748278 [ 460 ] {} <Fatal> BaseDaemon: 3. ? @ 0xd30fb59 in /workspace/clickhouse\r\n2021.10.25 22:13:11.819459 [ 460 ] {} <Fatal> BaseDaemon: 4. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:0: DB::ExpressionAnalyzer::makeAggregateDescriptions(std::__1::shared_ptr<DB::ActionsDAG>&, std::__1::vector<DB::AggregateDescription, std::__1::allocator<DB::AggregateDescription> >&) @ 0x1c7433cb in /workspace/clickhouse\r\n2021.10.25 22:13:11.886155 [ 460 ] {} <Fatal> BaseDaemon: 5.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:664: std::__1::vector<DB::AggregateDescription, std::__1::allocator<DB::AggregateDescription> >::empty() const\r\n2021.10.25 22:13:11.886182 [ 460 ] {} <Fatal> BaseDaemon: 5. ../src/Interpreters/ExpressionAnalyzer.cpp:326: DB::ExpressionAnalyzer::analyzeAggregation(std::__1::shared_ptr<DB::ActionsDAG>&) @ 0x1c741a25 in /workspace/clickhouse\r\n2021.10.25 22:13:11.951735 [ 460 ] {} <Fatal> BaseDaemon: 6. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.cpp:0: DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>, unsigned long, bool, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::SubqueryForSet, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, DB::SubqueryForSet> > >, std::__1::unordered_map<DB::PreparedSetKey, std::__1::shared_ptr<DB::Set>, DB::PreparedSetKey::Hash, std::__1::equal_to<DB::PreparedSetKey>, std::__1::allocator<std::__1::pair<DB::PreparedSetKey const, std::__1::shared_ptr<DB::Set> > > >) @ 0x1c740e09 in /workspace/clickhouse\r\n2021.10.25 22:13:12.000125 [ 295 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 7.46 GiB, peak 7.52 GiB, will set to 7.48 GiB (RSS), difference: 23.32 MiB\r\n2021.10.25 22:13:12.037715 [ 460 ] {} <Fatal> BaseDaemon: 7. ./obj-x86_64-linux-gnu/../src/Interpreters/ExpressionAnalyzer.h:101: DB::ExpressionAnalyzer::ExpressionAnalyzer(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x1c75d91f in /workspace/clickhouse\r\n2021.10.25 22:13:12.247617 [ 460 ] {} <Fatal> BaseDaemon: 8.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1229: __compressed_pair_elem<nullptr_t, void>\r\n2021.10.25 22:13:12.247664 [ 460 ] {} <Fatal> BaseDaemon: 8.2. inlined from ../contrib/libcxx/include/memory:1314: __compressed_pair<nullptr_t, std::__1::__default_init_tag>\r\n2021.10.25 22:13:12.247716 [ 460 ] {} <Fatal> BaseDaemon: 8.3. inlined from ../contrib/libcxx/include/vector:438: __vector_base\r\n2021.10.25 22:13:12.247772 [ 460 ] {} <Fatal> BaseDaemon: 8.4. inlined from ../contrib/libcxx/include/vector:497: vector\r\n2021.10.25 22:13:12.247819 [ 460 ] {} <Fatal> BaseDaemon: 8. ../src/Storages/MergeTree/KeyCondition.cpp:439: DB::KeyCondition::getBlockWithConstants(std::__1::shared_ptr<DB::IAST> const&, std::__1::shared_ptr<DB::TreeRewriterResult const> const&, std::__1::shared_ptr<DB::Context const>) @ 0x1d7bc0c7 in /workspace/clickhouse\r\n2021.10.25 22:13:12.458532 [ 460 ] {} <Fatal> BaseDaemon: 9.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:2844: std::__1::shared_ptr<DB::TreeRewriterResult const>::operator->() const\r\n2021.10.25 22:13:12.458616 [ 460 ] {} <Fatal> BaseDaemon: 9. ../src/Storages/MergeTree/KeyCondition.cpp:480: DB::KeyCondition::KeyCondition(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::ExpressionActions> const&, bool, bool) @ 0x1d7bc4ea in /workspace/clickhouse\r\n2021.10.25 22:13:12.637171 [ 460 ] {} <Fatal> BaseDaemon: 10. ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/PartitionPruner.h:29: DB::PartitionPruner::PartitionPruner(std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, bool) @ 0x1d89a9d3 in /workspace/clickhouse\r\n2021.10.25 22:13:12.815038 [ 460 ] {} <Fatal> BaseDaemon: 11.1. inlined from ./obj-x86_64-linux-gnu/../src/Storages/MergeTree/PartitionPruner.h:37: DB::PartitionPruner::isUseless() const\r\n2021.10.25 22:13:12.815090 [ 460 ] {} <Fatal> BaseDaemon: 11. ../src/Storages/MergeTree/MergeTreeData.cpp:849: DB::MergeTreeData::totalRowsByPartitionPredicateImpl(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>, std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > > const&) const @ 0x1d83121a in /workspace/clickhouse\r\n2021.10.25 22:13:12.889823 [ 460 ] {} <Fatal> BaseDaemon: 12. ./obj-x86_64-linux-gnu/../src/Storages/StorageMergeTree.cpp:0: DB::StorageMergeTree::totalRowsByPartitionPredicate(DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context const>) const @ 0x1dacd8dd in /workspace/clickhouse\r\n2021.10.25 22:13:13.000121 [ 295 ] {} <Trace> AsynchronousMetrics: MemoryTracking: was 7.48 GiB, peak 7.52 GiB, will set to 7.52 GiB (RSS), difference: 41.47 MiB\r\n2021.10.25 22:13:13.010849 [ 460 ] {} <Fatal> BaseDaemon: 13. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&) @ 0x1cb2d660 in /workspace/clickhouse\r\n2021.10.25 22:13:13.144687 [ 460 ] {} <Fatal> BaseDaemon: 14. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:1013: DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::optional<DB::Pipe>) @ 0x1cb2620f in /workspace/clickhouse\r\n2021.10.25 22:13:13.277758 [ 460 ] {} <Fatal> BaseDaemon: 15. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectQuery.cpp:0: DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x1cb25439 in /workspace/clickhouse\r\n2021.10.25 22:13:13.319452 [ 460 ] {} <Fatal> BaseDaemon: 16. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:0: DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x1cdc27bd in /workspace/clickhouse\r\n2021.10.25 22:13:13.361209 [ 460 ] {} <Fatal> BaseDaemon: 17. ./obj-x86_64-linux-gnu/../src/Interpreters/InterpreterSelectWithUnionQuery.cpp:0: DB::InterpreterSelectWithUnionQuery::execute() @ 0x1cdc393a in /workspace/clickhouse\r\n2021.10.25 22:13:13.418831 [ 460 ] {} <Fatal> BaseDaemon: 18. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:0: DB::executeQueryImpl(char const*, char const*, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum, DB::ReadBuffer*) @ 0x1d0736d1 in /workspace/clickhouse\r\n2021.10.25 22:13:13.482329 [ 460 ] {} <Fatal> BaseDaemon: 19. ./obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:947: DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::shared_ptr<DB::Context>, bool, DB::QueryProcessingStage::Enum) @ 0x1d070cea in /workspace/clickhouse\r\n2021.10.25 22:13:13.531619 [ 460 ] {} <Fatal> BaseDaemon: 20. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:316: DB::TCPHandler::runImpl() @ 0x1dc7c21d in /workspace/clickhouse\r\n2021.10.25 22:13:13.596949 [ 460 ] {} <Fatal> BaseDaemon: 21. ./obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1839: DB::TCPHandler::run() @ 0x1dc97cb6 in /workspace/clickhouse\r\n2021.10.25 22:13:13.601560 [ 460 ] {} <Fatal> BaseDaemon: 22. ./obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:57: Poco::Net::TCPServerConnection::start() @ 0x1ea679ac in /workspace/clickhouse\r\n2021.10.25 22:13:13.609688 [ 460 ] {} <Fatal> BaseDaemon: 23.1. inlined from ./obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:1397: std::__1::default_delete<Poco::Net::TCPServerConnection>::operator()(Poco::Net::TCPServerConnection*) const\r\n2021.10.25 22:13:13.609729 [ 460 ] {} <Fatal> BaseDaemon: 23.2. inlined from ../contrib/libcxx/include/memory:1658: std::__1::unique_ptr<Poco::Net::TCPServerConnection, std::__1::default_delete<Poco::Net::TCPServerConnection> >::reset(Poco::Net::TCPServerConnection*)\r\n2021.10.25 22:13:13.609779 [ 460 ] {} <Fatal> BaseDaemon: 23.3. inlined from ../contrib/libcxx/include/memory:1612: ~unique_ptr\r\n2021.10.25 22:13:13.609834 [ 460 ] {} <Fatal> BaseDaemon: 23. ../contrib/poco/Net/src/TCPServerDispatcher.cpp:116: Poco::Net::TCPServerDispatcher::run() @ 0x1ea67e45 in /workspace/clickhouse\r\n2021.10.25 22:13:13.618224 [ 460 ] {} <Fatal> BaseDaemon: 24. ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:213: Poco::PooledThread::run() @ 0x1eb58367 in /workspace/clickhouse\r\n2021.10.25 22:13:13.626447 [ 460 ] {} <Fatal> BaseDaemon: 25.1. inlined from ./obj-x86_64-linux-gnu/../contrib/poco/Foundation/include/Poco/SharedPtr.h:156: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::assign(Poco::Runnable*)\r\n2021.10.25 22:13:13.626486 [ 460 ] {} <Fatal> BaseDaemon: 25.2. inlined from ../contrib/poco/Foundation/include/Poco/SharedPtr.h:208: Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >::operator=(Poco::Runnable*)\r\n2021.10.25 22:13:13.626527 [ 460 ] {} <Fatal> BaseDaemon: 25. ../contrib/poco/Foundation/src/Thread_POSIX.cpp:360: Poco::ThreadImpl::runnableEntry(void*) @ 0x1eb55c2c in /workspace/clickhouse\r\n2021.10.25 22:13:13.626641 [ 460 ] {} <Fatal> BaseDaemon: 26. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n2021.10.25 22:13:13.626735 [ 460 ] {} <Fatal> BaseDaemon: 27. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n2021.10.25 22:13:13.996058 [ 460 ] {} <Fatal> BaseDaemon: Calculated checksum of the binary: CE366939D8563F8D8C200C0CCDEB5A58. There is no information about the reference checksum.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/30683/timeline","performed_via_github_app":null}]

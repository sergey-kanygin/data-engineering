[{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22104","id":840489912,"node_id":"MDU6SXNzdWU4NDA0ODk5MTI=","number":22104,"title":"Bitmap querying optimization","user":{"login":"victortony","id":13748044,"node_id":"MDQ6VXNlcjEzNzQ4MDQ0","avatar_url":"https://avatars.githubusercontent.com/u/13748044?v=4","gravatar_id":"","url":"https://api.github.com/users/victortony","html_url":"https://github.com/victortony","followers_url":"https://api.github.com/users/victortony/followers","following_url":"https://api.github.com/users/victortony/following{/other_user}","gists_url":"https://api.github.com/users/victortony/gists{/gist_id}","starred_url":"https://api.github.com/users/victortony/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/victortony/subscriptions","organizations_url":"https://api.github.com/users/victortony/orgs","repos_url":"https://api.github.com/users/victortony/repos","events_url":"https://api.github.com/users/victortony/events{/privacy}","received_events_url":"https://api.github.com/users/victortony/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-25T02:08:30Z","updated_at":"2021-03-25T02:08:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, I am using 19.16.3.6 version of clickhouse. I have two query below:\r\nThe first one is :\r\nselect bitmapAndCardinality ( \\ (SELECT uv FROM table1 WHERE (label_name = 'sex') AND (label_value = 2)) , \\ (SELECT uv FROM table1 WHERE (label_name = 'age') AND (label_value = 1)) \\ )\r\n\r\nThe second one is:\r\nselect id, bitmapAndCardinality(b1, b2) from ( SELECT 1 as id, uv as b1 FROM table1 WHERE (label_name = 'sex') AND (label_value = 2) ) a inner join (SELECT 1 as id, uv as b2 FROM table1 WHERE (label_name = 'age') AND (label_value = 1) ) b on a.id = b.id\r\n\r\nCould you please tell me why the 1th query is slower than the 2th query. When the data become more and more, the gap is bigger. Normally, for the same situation: 1th query cost 5 seconds, 2th query cost 0.012 seconds.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22104/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22098","id":840095294,"node_id":"MDU6SXNzdWU4NDAwOTUyOTQ=","number":22098,"title":"runningDifference() across blocks","user":{"login":"simPod","id":327717,"node_id":"MDQ6VXNlcjMyNzcxNw==","avatar_url":"https://avatars.githubusercontent.com/u/327717?v=4","gravatar_id":"","url":"https://api.github.com/users/simPod","html_url":"https://github.com/simPod","followers_url":"https://api.github.com/users/simPod/followers","following_url":"https://api.github.com/users/simPod/following{/other_user}","gists_url":"https://api.github.com/users/simPod/gists{/gist_id}","starred_url":"https://api.github.com/users/simPod/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/simPod/subscriptions","organizations_url":"https://api.github.com/users/simPod/orgs","repos_url":"https://api.github.com/users/simPod/repos","events_url":"https://api.github.com/users/simPod/events{/privacy}","received_events_url":"https://api.github.com/users/simPod/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1464781898,"node_id":"MDU6TGFiZWwxNDY0NzgxODk4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-in-progress","name":"st-in-progress","color":"e5b890","default":false,"description":"We are working on the issue currenlty"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-24T19:29:33Z","updated_at":"2021-03-24T20:05:40Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"So currently `runningDifference()` computes delta only inside single block.\r\n\r\nThis FR is about making it work across multiple blocks.\r\n\r\nI understand that it is not suited to be run on continuous data. Actually, I can't really imagine the use case in production env where I'd like to compute eg. running difference for 2 days of data which I suppose would be across multiple blocks.\r\n\r\nOr, I might also be wrong and understand the limitation poorly ðŸ¤” \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22098/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22086","id":839715696,"node_id":"MDU6SXNzdWU4Mzk3MTU2OTY=","number":22086,"title":"[RFC] Limited support for transactions in MergeTree tables","user":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":13,"created_at":"2021-03-24T13:19:40Z","updated_at":"2021-12-29T07:16:35Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\nMain use cases:\r\n - Ability to execute multiple `INSERT`s (maybe into multiple `MergeTree` tables, including materialized views, #11909) in a single \"transaction\". All `INSERT` queries in a \"transaction\" must either completely succeed or fail. All inserted data parts became visible only when \"transaction\" is committed.\r\n - Ability to execute multiple `SELECT`s in a single \"transaction\", so all of them will read from one consistent snapshot of all `MergeTree` tables.\r\n\r\nIt also may be useful to support transactional `ALTER ... PARTITION` and `ALTER ... UPDATE/DELETE`, but the first implementation probably will support only `SELECT` and `INSERT` queries. Exception will be thrown on attempt to execute non-transactional query inside transaction.\r\n\r\n**Describe the solution you'd like**\r\nMVCC-based transactions on data parts level. It will easily provide Snapshot Isolation (a bit stronger than Repeatable Read) for reading queries. As for writing queries, Snapshot Isolation requires to detect conflicts and rollback a transaction if it tries to modify an object which was modified by concurrent transaction. It's not a problem for concurrent `INSERT` queries, because they just append new data to table by creating new data parts, so two `INSERT`s cannot modify single object and cannot cause write-write conflict (but there is a nuance with `Replacing`, `Collapsing` and other special `*MergeTree` engines). To support transactional `ALTER`s it should be enough to forbid concurrent merging/mutating operations with overlapping source parts sets and do not assign such operations on future parts (because creation of future part can be rolled back).\r\n\r\n### Implementation details for simple MergeTree\r\n**Data parts versioning**\r\nEach writing transaction has some unique identifier, let's name it `tid`. We use `tid`s to understand which transaction has modified/is going to modify each part. We will obtain `tid` in the beginning of transaction and pass it through query/session `Context` anywhere it needed. If a data part is involved in some transaction, then it must contain the following metadata:\r\n - `mintid` - `tid` of transaction that has created the part\r\n - `maxtid` - `NULL` or `tid` of transaction that has removed the part (it's mostly for `ALTER ... PARTITION`s, but it also may be useful for `ALTER UPDATE/DELETE` and background merges, see below)\r\n - `mincsn` - `NULL` or \"cached\" `CSN(mintid)`, described below\r\n - `maxcsn`.- `NULL` or \"cached\" `CSN(maxtid)`, described below\r\n\r\nWhen some query in a transaction creates or removes data part, it writes the corresponding `mintid` or `maxtid`. It's the only difference between transactional and non-transactional writing query. \r\n\r\n**Commit**\r\nCSN (Commit Sequence Number) indicates a point in time when transaction was committed and its changes in part sets became visible for other transactions. We will maintain `tid` to `csn` mapping to understand when each part was committed.\r\n_Note: If same data part is in `Commited` or `Outdated` state it does not mean that part is actually created or removed in terms of transactions. It means only that the corresponding changes are applied to parts set in some table and ready to become visible after the whole transaction is commited. If some query reads data without creating a transaction it will just see parts in `Commited` state._\r\n\r\nWhen transaction is committing, it allocates new CSN from a global monotonic counter (for not replicated case it can be just static variable, for replicated case we can use sequential nodes in ZooKeeper, see below), writes its `tid` and allocated `csn` into transaction log (WAL for not replicated case, for replicated see below) and updates in-memory `tid` to `csn` mapping. After that it writes the corresponding `mincsn`/`maxcsn` into data part (it's an optional step, because we always can get `csn` by `tid` from the mapping or transaction log).\r\n\r\n**Rollback**\r\nTransaction is rolled back automatically on any uncaught exception. On rollback it should change state of created parts to `Outdated` and reset `maxtid` of removed parts to `NULL`.\r\n\r\n**Snapshots and data parts visibility**\r\nSnapshot is just a CSN. When transaction starts it gets current value of CSN counter and uses it as current version, let's name it `snapshotVersion`. Data part is visible for transaction if:\r\n - Part does not have version metadata at all. It means that part was created before transactions were enabled or part was created by non-transactional query. _Note: isolation level falls to Read Uncommitted if there are concurrent non-transactional writing queries._\r\n - Part was created by current transaction, i.e. `mintid == current_tid`\r\n - Part creation was committed before we took the snapshot and part removal was not and part was not removed by current transaction, i.e. `CSN(mintid) && CSN(mintid) <= snapshotVersion && (!CSN(maxtid) || snapshotVersion < CSN(maxtid)) && (!maxtid || maxtid != current_tid)`\r\n\r\n_Note: It's the first reason why do we need separate transaction ids and CSNs with mapping between them. We can use only `tid`s to define parts visibility, but in this case we have to save a list of concurrent transactions when taking a snapshot and check if each of them is already committed or not to determine part visibility._ \r\n\r\n**Backgound merges**\r\nMultiple parts can be merged if all of them are visible with current snapshot version. We also can merge parts if all of them have the same `mintid` and does not have `maxtid`, but I'm not sure if we need merge such parts.\r\n\r\nSince merge does not actually modify data (at least for ordinary MergeTree), we can just choose maximum of CSNs of the source parts as a CSN of result part and write the corresponding `maxtid` to each source part, so transactions will not know anything about background merges. Of course, in this case merge should modify version metadata only when result part is ready. It means that merge will fail if `maxtid` appeared in some source part due to concurrent `ALTER ... PARTITION` or `ALTER ... UPDATE/DELETE`, so such queries should cancel merges of the corresponding parts (if any). It may look like significant change, but we already have `removePartProducingOpsInRange(...)` which does similar thing.\r\n\r\nOn the other hand, it may be useful to allocate `tid`s and `csn`s for merges as for other transactions (and seems like we have to do it for special *MergeTree engines such as `ReplacingMergeTree`).\r\n\r\n**Background cleanup**\r\nAs before, background thread looks for `Outdated` parts with reference counter equal to 1. But it should not remove part if there is no `maxcsn` or if `mincsn` is greater than the minimum `snapshotVersion` of running transactions.\r\n\r\n**ALTER PARTITION, ALTER UPDATE/DELETE and OPTIMIZE**\r\nUnlike `INSERT`s, such queries may try to concurrently remove the same part and replace it with other part causing write-write conflict. We can use `maxtid` to detect such conflicts. Query writes its `tid` into `maxtid` before making any changes. If `maxtid` is not `NULL`, i.e. other transaction is currently tries to remove the part, query throws `Serialization error`.\r\n\r\n**Durability and server startup**\r\nWe have two options here:\r\n1. Write version metadata into data part on disk (see the note about `fsync` below). Transaction log will contain only `(tid, csn)` pairs of committed transactions. On restart we will know `tid`s of every data part and will know if the corresponding transactions were committed or not. It will be easier to rotate the log in this case.\r\n2. Maintain version metadata in memory and write its modifications into transaction log. We will restore data parts state from the log on server startup. To rotate the log we have either write CSNs of old transactions into data parts or write version metadata of all parts into new log.\r\n\r\nOn server startup we will remove all parts which does not have `CSN(mintid)` or have `CSN(maxtid)`.\r\n\r\n_Note: We do not fsync data parts (at least by default), so any data part may become broken in case of hard restart. Committed transaction can be partially rolled back in this case even if all version metadata were reliably written somewhere._\r\n\r\n\r\n### Making it work with ReplicatedMergeTree\r\nFor `ReplicatedMergeTree` we have to allocate CSNs through sequential nodes in ZooKeeper and write `tid`s into these nodes. But `tid`s still can be obtained locally, we only need to guarantee that `tid` is unique across all replicas. So we can use `(snapshotVersion, local_tid_counter, host_id)` tuple as `tid` (we don't really need `snapshotVersion` in `tid`, but it can be useful for introspection, because it's impossible to relate local `tid`s of different replicas). Replica can commit multiple transactions into one CSN to reduce workload on ZooKeeper (it's the second reason why we need separate tids and CSNs). It's possible to ensure that other replicas did not make some commits concurrently by checking cversion of parent node. \r\n\r\nTo make snapshots and part visibility work with replication, we have to add `tid` into `ReplicatedMergeTreeLogEntryData`. Snapshot version on some replica is the maximal CSN such that all corresponding log entries are executed (so replica have all visible parts locally).\r\n\r\nWe can use separate CSN logs in ZooKeeper for different shards.\r\nDistributed transactions across multiple shards are out of scope of this RFC.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086/reactions","total_count":18,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":17,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22086/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22072","id":839548081,"node_id":"MDU6SXNzdWU4Mzk1NDgwODE=","number":22072,"title":"Finalize aggregations at field boundaries by taking advantages of ordering","user":{"login":"jorisgio","id":1108145,"node_id":"MDQ6VXNlcjExMDgxNDU=","avatar_url":"https://avatars.githubusercontent.com/u/1108145?v=4","gravatar_id":"","url":"https://api.github.com/users/jorisgio","html_url":"https://github.com/jorisgio","followers_url":"https://api.github.com/users/jorisgio/followers","following_url":"https://api.github.com/users/jorisgio/following{/other_user}","gists_url":"https://api.github.com/users/jorisgio/gists{/gist_id}","starred_url":"https://api.github.com/users/jorisgio/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jorisgio/subscriptions","organizations_url":"https://api.github.com/users/jorisgio/orgs","repos_url":"https://api.github.com/users/jorisgio/repos","events_url":"https://api.github.com/users/jorisgio/events{/privacy}","received_events_url":"https://api.github.com/users/jorisgio/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-24T09:57:46Z","updated_at":"2021-03-24T09:57:46Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently, there is optimize_aggregation_in_order setttings. This settings is great, but it requires the following :\r\n- it requires reading the primary keys fields, which can be big and slow to read and compare, and aggregate. I understand it is required to merge sort input block, but as far as i understand, it is not strictly required to the optimization, for instance you could only sort block using primary key index, and finalize aggregations at block boundaries to save on memory.\r\nLocally, we have forked the hashing function to have some injective version to be able to\r\n```\r\nGROUP BY hash(primary_key1), hash(primary_key2)\r\n```\r\nAnd use optimisation, which saves on memory and slow aggregation with large string, but it does not save on the reading/merge sort part.\r\n\r\n- it does not support holes and hidden relation in schema fields. On the following table : \r\n ``` \r\n   CREATE TABLE test\r\n    (\r\n       prefix_hash UInt64,\r\n       key String,\r\n       prefix_hash2 UInt64,\r\n       key2 String,\r\n       metric UInt64\r\n    )\r\n    ORDER BY prefix_hash, key, prefix_hash2, key2\r\n  ``` \r\n  where prefix_hash is the hash of the first logical part of key if you try\r\n  ```\r\n  SELECT sum(metric) FROM test GROUP BY prefix_hash, prefix_hash2\r\n  ```\r\n  in theory optimize_aggregation_in_order can work, because if prefix differ then it means aggregation also must differ, but the optimizer cannot know that. The primary key could be reordered but then ordering for others queries is lost.\r\n - Ideally i want to use\r\n  ``` \r\n   CREATE TABLE test\r\n    (\r\n       prefix_hash UInt64,\r\n       key String,\r\n       prefix_hash2 UInt64,\r\n       key2 String,\r\n       full_hash UInt64,\r\n       metric UInt64\r\n    )\r\n    ORDER BY prefix_hash, key, prefix_hash2, key2\r\n  ```\r\nwith `full_hash = hash(key, key2)` and `GROUP BY  full_hash`, but this requires full aggregation and is very memory hungry (and slow due to finalizing aggregation on tens of gigabytes of keys)\r\n\r\nAs far as i understand, it is not possible to read in \"semi_order\" currently using only index to order blocks instead of full mergeSort ? and there is no way to explicitly ask to flush aggregations when some key change by some clever query rewriting ?\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22072/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22065","id":839237390,"node_id":"MDU6SXNzdWU4MzkyMzczOTA=","number":22065,"title":"Upload Logs that Fail to Parse as a String in a Specified Table","user":{"login":"rlazimi-dev","id":47514208,"node_id":"MDQ6VXNlcjQ3NTE0MjA4","avatar_url":"https://avatars.githubusercontent.com/u/47514208?v=4","gravatar_id":"","url":"https://api.github.com/users/rlazimi-dev","html_url":"https://github.com/rlazimi-dev","followers_url":"https://api.github.com/users/rlazimi-dev/followers","following_url":"https://api.github.com/users/rlazimi-dev/following{/other_user}","gists_url":"https://api.github.com/users/rlazimi-dev/gists{/gist_id}","starred_url":"https://api.github.com/users/rlazimi-dev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/rlazimi-dev/subscriptions","organizations_url":"https://api.github.com/users/rlazimi-dev/orgs","repos_url":"https://api.github.com/users/rlazimi-dev/repos","events_url":"https://api.github.com/users/rlazimi-dev/events{/privacy}","received_events_url":"https://api.github.com/users/rlazimi-dev/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-24T00:42:11Z","updated_at":"2021-03-27T22:27:07Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nAllow users to specify a table to upload logs which fail to parse during uploads using the Template Format. \r\n\r\n**Describe the solution you'd like**\r\nMy first suggested, and naive solution, is to store all characters that have been successfully parsed so far into a string, and then to upload that string, as usual, to the table being uploaded to. If parsing is unsuccessful, then simply store, in a string, all characters until `format_template_rows_between_delimiter`, and upload this string to a BACKUP table specified by the user in a new setting (which could be for example: `invalid_logs_table='logs.unstructured_log_rows'`).\r\n\r\nI assume the failure is caused by the implementation not keeping any data in memory while parsing, which in turn I assume is done for efficiency. If this is the case, then my second suggested solution is that clickhouse proceeds until `format_template_rows_between_delimiter` and at least returns the line numbers that caused the issue, while still uploading the logs that can successfully be parsed. Keeping track of line number is negligible relative to keeping track of a line.\r\n\r\nIdeally I would like to be able to run a command similar to this:\r\n```\r\ncat example.log | clickhouse-client --query=\"INSERT INTO logs.main FORMAT Template SETTINGS format_template_row = 'row.template', format_template_resultset = 'log.template', format_template_rows_between_delimiter = '\\n', invalid_logs_table = 'logs.unstructured_log_rows' \"\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nThe alternative is to preprocess a log to ensure all logs are structured correctly before sending them to clickhouse. The solution I'm suggesting is preferred because it does not increase upload time and it will allow clickhouse users to store logs and logs that need more processing in the same place.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22065/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22048","id":838860734,"node_id":"MDU6SXNzdWU4Mzg4NjA3MzQ=","number":22048,"title":"A huge number of MySQL dictionaries with `share_connection` option: SYSTEM RELOAD DICTIONARIES is freezing (version 21.3.2.5 (official build))","user":{"login":"edonin","id":45206522,"node_id":"MDQ6VXNlcjQ1MjA2NTIy","avatar_url":"https://avatars.githubusercontent.com/u/45206522?v=4","gravatar_id":"","url":"https://api.github.com/users/edonin","html_url":"https://github.com/edonin","followers_url":"https://api.github.com/users/edonin/followers","following_url":"https://api.github.com/users/edonin/following{/other_user}","gists_url":"https://api.github.com/users/edonin/gists{/gist_id}","starred_url":"https://api.github.com/users/edonin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/edonin/subscriptions","organizations_url":"https://api.github.com/users/edonin/orgs","repos_url":"https://api.github.com/users/edonin/repos","events_url":"https://api.github.com/users/edonin/events{/privacy}","received_events_url":"https://api.github.com/users/edonin/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"},{"id":2825253639,"node_id":"MDU6TGFiZWwyODI1MjUzNjM5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.3-affected","name":"v21.3-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-23T15:42:18Z","updated_at":"2021-12-14T01:01:54Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"(Sorry, the test case is quite complex).\r\n\r\nWhen upgrading from ClickHouse 20.3 to 21.3.2.5, we found a regression.\r\n\r\nWe defined in ClickHouse nearly 50 MySQL dictionaries.\r\nAll of these dictionaries are pointy to the same MySQL database. Due to this high number, we are using the close connection and share connection option :\r\n```\r\n   <dictionary>\r\n      <name>hour_types</name>\r\n      <source>\r\n         <mysql incl=\"mysql_source\">\r\n            <db>fvrm</db>\r\n            <table>hour_types</table>\r\n            <close_connection>true</close_connection>\r\n            <share_connection>true</share_connection>\r\n         </mysql>\r\n      </source>\r\n      <layout>\r\n         <hashed/>\r\n      </layout>\r\n      <structure>\r\n         <id>\r\n            <name>id</name>\r\n            <type>UInt32</type>\r\n            <expression>CAST(id AS UNSIGNED)</expression>\r\n         </id>\r\n         <attribute>\r\n            <name>name</name>\r\n            <type>String</type>\r\n            <null_value>(UNDEFINED)</null_value>\r\n         </attribute>\r\n      </structure>\r\n      <lifetime>0</lifetime>\r\n   </dictionary>\r\n```\r\n\r\nWe have a script that ask for the reload of the dictionaries, and then create the corresponding dictionary tables  if they are not existing:\r\n```\r\nSYSTEM RELOAD DICTIONARIES;\r\nCREATE TABLE IF NOT EXISTS fvrm.hour_types (`id` UInt64, `name` String) ENGINE = Dictionary(hour_types);\r\n...\r\n```\r\nThe issue here, is when we launch this script a second or a third time without restarting clickhouse server, the query SYSTEM RELOAD DICTIONARIES is blocked.\r\n\r\nFor example, the result of a SHOW PROCESSLIST:\r\n```\r\nRow 1:\r\nâ”€â”€â”€â”€â”€â”€\r\nis_initial_query:     1\r\nuser:                 admin\r\nquery_id:             4f939d04-aa79-4ce2-a17b-d92cb16fb243\r\naddress:              ::1\r\nport:                 35028\r\ninitial_user:         admin\r\ninitial_query_id:     4f939d04-aa79-4ce2-a17b-d92cb16fb243\r\ninitial_address:      ::1\r\ninitial_port:         35028\r\ninterface:            1\r\nos_user:              root\r\nclient_hostname:      localhost\r\nclient_name:          ClickHouse \r\nclient_revision:      54447\r\nclient_version_major: 21\r\nclient_version_minor: 3\r\nclient_version_patch: 2\r\nhttp_method:          0\r\nhttp_user_agent:      \r\nhttp_referer:         \r\nforwarded_for:        \r\nquota_key:            \r\nelapsed:              89.233185793\r\nis_cancelled:         0\r\nread_rows:            0\r\nread_bytes:           0\r\ntotal_rows_approx:    0\r\nwritten_rows:         0\r\nwritten_bytes:        0\r\nmemory_usage:         0\r\npeak_memory_usage:    0\r\nquery:                SYSTEM RELOAD DICTIONARIES;\r\nthread_ids:           [7100]\r\nProfileEvents.Names:  ['Query','ReadCompressedBytes','CompressedReadBufferBlocks','CompressedReadBufferBytes','IOBufferAllocs','IOBufferAllocBytes','ContextLock']\r\nProfileEvents.Values: [1,36,1,10,1,89,11]\r\nSettings.Names:       ['use_uncompressed_cache','load_balancing','max_bytes_before_external_group_by','max_bytes_before_external_sort','max_memory_usage','max_memory_usage_for_all_queries']\r\nSettings.Values:      ['0','random','173224755','173224755','346449510','692899020']\r\n```\r\n\r\nAnd if I activate trace log, the log file is full of:\r\n\r\n```\r\n2021.03.23 16:37:51.824660 [ 8664 ] {} <Trace> mysqlxx::Pool: (fvrm@127.0.0.1:12306 as user clickhouse): Iterating through existing MySQL connections\r\n2021.03.23 16:37:51.824671 [ 8664 ] {} <Trace> mysqlxx::Pool: (fvrm@127.0.0.1:12306 as user clickhouse): Trying to allocate a new connection.\r\n2021.03.23 16:37:51.824676 [ 8664 ] {} <Trace> mysqlxx::Pool: (fvrm@127.0.0.1:12306 as user clickhouse): Unable to create a new connection: Max number of connections has been reached.\r\n2021.03.23 16:37:51.824683 [ 8664 ] {} <Trace> mysqlxx::Pool: (fvrm@127.0.0.1:12306 as user clickhouse): Sleeping for 1 seconds.\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22048/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22041","id":838786029,"node_id":"MDU6SXNzdWU4Mzg3ODYwMjk=","number":22041,"title":"FlatDictionary decrease memory consumption","user":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-03-23T14:27:52Z","updated_at":"2021-03-30T10:27:04Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently for each dictionary attribute we allocate array, and we also have loaded ids array. Loaded ids and attributes arrays size equals to greatest fetched index from source. \r\nThis is suboptimal because we can insert in attributes values only for keys that are actually fetched from source, in loaded ids array we store index of element in attributes array.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22041/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/22004","id":838084316,"node_id":"MDU6SXNzdWU4MzgwODQzMTY=","number":22004,"title":"optimize_read_in_order does not work with ORDER BY OFFSET, LIMIT when the offset is greater than 1400. ","user":{"login":"vinodkmrm","id":53018195,"node_id":"MDQ6VXNlcjUzMDE4MTk1","avatar_url":"https://avatars.githubusercontent.com/u/53018195?v=4","gravatar_id":"","url":"https://api.github.com/users/vinodkmrm","html_url":"https://github.com/vinodkmrm","followers_url":"https://api.github.com/users/vinodkmrm/followers","following_url":"https://api.github.com/users/vinodkmrm/following{/other_user}","gists_url":"https://api.github.com/users/vinodkmrm/gists{/gist_id}","starred_url":"https://api.github.com/users/vinodkmrm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vinodkmrm/subscriptions","organizations_url":"https://api.github.com/users/vinodkmrm/orgs","repos_url":"https://api.github.com/users/vinodkmrm/repos","events_url":"https://api.github.com/users/vinodkmrm/events{/privacy}","received_events_url":"https://api.github.com/users/vinodkmrm/received_events","type":"User","site_admin":false},"labels":[{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-22T20:30:06Z","updated_at":"2021-03-23T22:56:02Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"optimize_read_in_order does not work with ORDER BY OFFSET, LIMIT when the offset is greater than 1400. When the offset is greater than 1400, it looks like records matching the WHERE clause are read and then sorted. \r\n\r\nConsider the following merge tree table - \r\n\r\n```\r\nCREATE TABLE test_in_order\r\n(\r\n    `lowCardKey` UUID,\r\n    `time` UInt64,\r\n    `id` UInt64\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY toYYYYMMDD(toDate(time / 1000))\r\nORDER BY (lowCardKey, time)\r\n\r\n```\r\nThe following query is fast (0.026 sec) when the offset in the limit clause is 1300. The same query with an offset of 1400 is much slower (1.599 sec). Reviewing the logs it looks like optimize_read_in_order is not used. The first query merge sorts 2 blocks, ~50K rows, where as the second query merge sorts 16K blocks, 40 million rows. \r\n\r\n```\r\nQuery 1\r\nSELECT\r\n    lowCardKey,\r\n    time,\r\n    id\r\nFROM test_in_order\r\nWHERE (lowCardKey = '9093d9c4-8707-4d7e-a2ff-439f370dd0ff') AND ((time >= 1506237739) AND (time <= 1716437739))\r\nORDER BY\r\n    lowCardKey ASC,\r\n    time ASC,\r\n    id ASC\r\nLIMIT 1300, 100\r\n```\r\n\r\n[centos07-dev01] 2021.03.22 14:54:35.047768 [ 3495 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> MergingSortedTransform: **Merge sorted 2 blocks, 49152 rows in 0.010000106 sec.**, 4915147.899432266 rows/sec., 200.00 MiB/sec\r\n\r\nQuery 2\r\n\r\n```\r\nSELECT\r\n    lowCardKey,\r\n    time,\r\n    id\r\nFROM test_in_order\r\nWHERE (lowCardKey = '9093d9c4-8707-4d7e-a2ff-439f370dd0ff') AND ((time >= 1506237739) AND (time <= 1716437739))\r\nORDER BY\r\n    lowCardKey ASC,\r\n    time ASC,\r\n    id ASC\r\nLIMIT 1400, 100\r\n\r\n```\r\n[centos07-dev01] 2021.03.22 14:55:22.181391 [ 3497 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> MergingSortedTransform: **Merge sorted 1630 blocks, 40000000 rows in 1.571016641 sec**., 25461219.79620711 rows/sec., 1.01 GiB/sec\r\n\r\n\r\n**ClickHouse server**:  20.8.7, 21.2.4 (logs included here are from 21.2.4)\r\n\r\nComplete logs \r\n\r\nQuery 1\r\n\r\n```\r\nQuery id: 6b02ea63-208d-479d-9f18-6c01a6dd4c19\r\n\r\n[centos07-dev01] 2021.03.22 14:54:35.034501 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> executeQuery: (from [::1]:36188, using production parser) select lowCardKey,time,id from test_in_order where lowCardKey = '9093d9c4-8707-4d7e-a2ff-439f370dd0ff' and time between 1506237739 AND 1716437739 order by lowCardKey, time, id limit 1300,100;\r\n[centos07-dev01] 2021.03.22 14:54:35.034943 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"(time >= 1506237739) AND (time <= 1716437739)\" moved to PREWHERE\r\n[centos07-dev01] 2021.03.22 14:54:35.035415 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> ContextAccess (default): Access granted: SELECT(lowCardKey, time, id) ON default.test_in_order\r\n[centos07-dev01] 2021.03.22 14:54:35.035878 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> default.test_in_order (SelectExecutor): Key condition: (column 1 in [1506237739, +inf)), (column 1 in (-inf, 1716437739]), and, (column 0 in ['9093d9c4-8707-4d7e-a2ff-439f370dd0ff', '9093d9c4-8707-4d7e-a2ff-439f370dd0ff']), and, (column 1 in [1506237739, +inf)), (column 1 in (-inf, 1716437739]), and, and\r\n[centos07-dev01] 2021.03.22 14:54:35.035895 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> default.test_in_order (SelectExecutor): MinMax index condition: (column 0 in [1506237739, +inf)), (column 0 in (-inf, 1716437739]), and, unknown, and, (column 0 in [1506237739, +inf)), (column 0 in (-inf, 1716437739]), and, and\r\n[centos07-dev01] 2021.03.22 14:54:35.036035 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_1_31_2 (3792 marks)\r\n[centos07-dev01] 2021.03.22 14:54:35.036041 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_32_37_1 (769 marks)\r\n[centos07-dev01] 2021.03.22 14:54:35.036055 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036072 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036118 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 769\r\n[centos07-dev01] 2021.03.22 14:54:35.036132 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 19 steps\r\n[centos07-dev01] 2021.03.22 14:54:35.036072 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 3792\r\n[centos07-dev01] 2021.03.22 14:54:35.036173 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 23 steps\r\n[centos07-dev01] 2021.03.22 14:54:35.036314 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_38_38_0 (129 marks)\r\n[centos07-dev01] 2021.03.22 14:54:35.036336 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_39_39_0 (129 marks)\r\n[centos07-dev01] 2021.03.22 14:54:35.036384 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036399 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 129\r\n[centos07-dev01] 2021.03.22 14:54:35.036340 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036409 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 15 steps\r\n[centos07-dev01] 2021.03.22 14:54:35.036431 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 129\r\n[centos07-dev01] 2021.03.22 14:54:35.036452 [ 15386 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 15 steps\r\n[centos07-dev01] 2021.03.22 14:54:35.036596 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_40_40_0 (70 marks)\r\n[centos07-dev01] 2021.03.22 14:54:35.036616 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036628 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 70\r\n[centos07-dev01] 2021.03.22 14:54:35.036638 [ 28261 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 13 steps\r\n[centos07-dev01] 2021.03.22 14:54:35.036896 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> default.test_in_order (SelectExecutor): Selected 5 parts by partition key, 5 parts by primary key, 4884 marks by primary key, 4884 marks to read from 5 ranges\r\n[centos07-dev01] 2021.03.22 14:54:35.036960 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_40_40_0, approx. 563095 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:54:35.036981 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_39_39_0, approx. 1048545 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:54:35.037000 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_38_38_0, approx. 1048545 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:54:35.037062 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_32_37_1, approx. 6291270 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:54:35.037090 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_1_31_2, approx. 11051008 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:54:35.037640 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_1_31_2, approx. 19997537 rows starting from 11051008\r\n[centos07-dev01] 2021.03.22 14:54:35.037666 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[centos07-dev01] 2021.03.22 14:54:35.047768 [ 3495 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> MergingSortedTransform: Merge sorted 2 blocks, 49152 rows in 0.010000106 sec., 4915147.899432266 rows/sec., 200.00 MiB/sec\r\n\r\n[centos07-dev01] 2021.03.22 14:54:35.048875 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Information> executeQuery: Read 319488 rows, 9.75 MiB in 0.014325952 sec., 22301345 rows/sec., 680.58 MiB/sec.\r\n[centos07-dev01] 2021.03.22 14:54:35.048903 [ 3081 ] {6b02ea63-208d-479d-9f18-6c01a6dd4c19} <Debug> MemoryTracker: Peak memory usage (for query): 24.87 MiB.\r\n\r\n100 rows in set. Elapsed: 0.026 sec. Processed 319.49 thousand rows, 10.22 MB (12.11 million rows/s., 387.38 MB/s.) \r\n\r\n```\r\nQuery 2\r\n\r\n```\r\nQuery id: 778e822b-e070-4b07-b4d8-683b6d0573f5\r\n\r\n[centos07-dev01] 2021.03.22 14:55:20.607368 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> executeQuery: (from [::1]:36188, using production parser) select lowCardKey,time,id from test_in_order where lowCardKey = '9093d9c4-8707-4d7e-a2ff-439f370dd0ff' and time between 1506237739 AND 1716437739 order by lowCardKey, time, id limit 1400,100;\r\n[centos07-dev01] 2021.03.22 14:55:20.607845 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> InterpreterSelectQuery: MergeTreeWhereOptimizer: condition \"(time >= 1506237739) AND (time <= 1716437739)\" moved to PREWHERE\r\n[centos07-dev01] 2021.03.22 14:55:20.608337 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> ContextAccess (default): Access granted: SELECT(lowCardKey, time, id) ON default.test_in_order\r\n[centos07-dev01] 2021.03.22 14:55:20.608645 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> default.test_in_order (SelectExecutor): Key condition: (column 1 in [1506237739, +inf)), (column 1 in (-inf, 1716437739]), and, (column 0 in ['9093d9c4-8707-4d7e-a2ff-439f370dd0ff', '9093d9c4-8707-4d7e-a2ff-439f370dd0ff']), and, (column 1 in [1506237739, +inf)), (column 1 in (-inf, 1716437739]), and, and\r\n[centos07-dev01] 2021.03.22 14:55:20.608659 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> default.test_in_order (SelectExecutor): MinMax index condition: (column 0 in [1506237739, +inf)), (column 0 in (-inf, 1716437739]), and, unknown, and, (column 0 in [1506237739, +inf)), (column 0 in (-inf, 1716437739]), and, and\r\n[centos07-dev01] 2021.03.22 14:55:20.608793 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_32_37_1 (769 marks)\r\n[centos07-dev01] 2021.03.22 14:55:20.608783 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_1_31_2 (3792 marks)\r\n[centos07-dev01] 2021.03.22 14:55:20.608826 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:55:20.608841 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 769\r\n[centos07-dev01] 2021.03.22 14:55:20.608832 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:55:20.608853 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 19 steps\r\n[centos07-dev01] 2021.03.22 14:55:20.608869 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 3792\r\n[centos07-dev01] 2021.03.22 14:55:20.608900 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 23 steps\r\n[centos07-dev01] 2021.03.22 14:55:20.608985 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_38_38_0 (129 marks)\r\n[centos07-dev01] 2021.03.22 14:55:20.609018 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609032 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_39_39_0 (129 marks)\r\n[centos07-dev01] 2021.03.22 14:55:20.609057 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609047 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 129\r\n[centos07-dev01] 2021.03.22 14:55:20.609071 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 129\r\n[centos07-dev01] 2021.03.22 14:55:20.609080 [ 3319 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 15 steps\r\n[centos07-dev01] 2021.03.22 14:55:20.609074 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 15 steps\r\n[centos07-dev01] 2021.03.22 14:55:20.609210 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Running binary search on index range for part 19700119_40_40_0 (70 marks)\r\n[centos07-dev01] 2021.03.22 14:55:20.609249 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (LEFT) boundary mark: 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609276 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found (RIGHT) boundary mark: 70\r\n[centos07-dev01] 2021.03.22 14:55:20.609300 [ 3495 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> default.test_in_order (SelectExecutor): Found continuous range in 13 steps\r\n[centos07-dev01] 2021.03.22 14:55:20.609576 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> default.test_in_order (SelectExecutor): Selected 5 parts by partition key, 5 parts by primary key, 4884 marks by primary key, 4884 marks to read from 5 ranges\r\n[centos07-dev01] 2021.03.22 14:55:20.609669 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_40_40_0, approx. 563095 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609729 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_39_39_0, approx. 1048545 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609771 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_38_38_0, approx. 1048545 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609812 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_32_37_1, approx. 6291270 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609849 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_1_31_2, approx. 11051008 rows starting from 0\r\n[centos07-dev01] 2021.03.22 14:55:20.609890 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> MergeTreeSelectProcessor: Reading 5 ranges from part 19700119_1_31_2, approx. 19997537 rows starting from 11051008\r\n[centos07-dev01] 2021.03.22 14:55:20.609936 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Trace> InterpreterSelectQuery: FetchColumns -> Complete\r\n[centos07-dev01] 2021.03.22 14:55:22.181391 [ 3497 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> MergingSortedTransform: Merge sorted 1630 blocks, 40000000 rows in 1.571016641 sec., 25461219.79620711 rows/sec., 1.01 GiB/sec\r\n[centos07-dev01] 2021.03.22 14:55:22.185205 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Information> executeQuery: Read 40000000 rows, 1.19 GiB in 1.577776812 sec., 25352128 rows/sec., 773.69 MiB/sec.\r\n[centos07-dev01] 2021.03.22 14:55:22.185241 [ 3081 ] {778e822b-e070-4b07-b4d8-683b6d0573f5} <Debug> MemoryTracker: Peak memory usage (for query): 24.87 MiB.\r\n\r\n100 rows in set. Elapsed: 1.599 sec. Processed 40.00 million rows, 1.28 GB (25.02 million rows/s., 800.61 MB/s.) \r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/22004/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21999","id":838045126,"node_id":"MDU6SXNzdWU4MzgwNDUxMjY=","number":21999,"title":"Insert when Inf is present stops working after version 21.x","user":{"login":"calebeaires","id":794195,"node_id":"MDQ6VXNlcjc5NDE5NQ==","avatar_url":"https://avatars.githubusercontent.com/u/794195?v=4","gravatar_id":"","url":"https://api.github.com/users/calebeaires","html_url":"https://github.com/calebeaires","followers_url":"https://api.github.com/users/calebeaires/followers","following_url":"https://api.github.com/users/calebeaires/following{/other_user}","gists_url":"https://api.github.com/users/calebeaires/gists{/gist_id}","starred_url":"https://api.github.com/users/calebeaires/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/calebeaires/subscriptions","organizations_url":"https://api.github.com/users/calebeaires/orgs","repos_url":"https://api.github.com/users/calebeaires/repos","events_url":"https://api.github.com/users/calebeaires/events{/privacy}","received_events_url":"https://api.github.com/users/calebeaires/received_events","type":"User","site_admin":false},"labels":[{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":1634867650,"node_id":"MDU6TGFiZWwxNjM0ODY3NjUw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-nullable","name":"comp-nullable","color":"b5bcff","default":false,"description":"Nulls related"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-22T19:33:32Z","updated_at":"2021-12-14T01:03:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I came out with the same problem. Before version 20.x when you divide any number per 0 the result in a Nullable(Int64) field was NULL, but after 21.x version, clickhouse emmits an error. So, with this query the create table with select will fail \r\n\r\n\r\n| col_1       | col_2 | \r\n| -------- |:------:|\r\n| 4            | 0       |\r\n\r\n\r\n```sql\r\n\r\nCREATE TABLE teste3\r\n(\r\n    `col_1` Nullable(Int64),\r\n    `col_2` Nullable(Int64),\r\n    `DIV` Nullable(Int64)\r\n)\r\nENGINE = MergeTree\r\nORDER BY tuple() AS\r\nSELECT\r\n    teste.col_1,\r\n    teste.col_2,\r\n    col_1 / col_2 AS DIV\r\nFROM teste\r\n\r\n```\r\n\r\nError \r\nCode: 70. DB::Exception: Received from localhost:9000. DB::Exception: Unexpected inf or nan to integer conversion: while converting source column DIV to destination column DIV: while executing 'FUNCTION CAST(DIV :: 2, Nullable(Int64) :: 5) -> CAST(DIV, Nullable(Int64)) Nullable(Int64) : 4'. \r\n\r\n\r\nBefore, this division by zero results into null, and the create of table did not fail.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21999/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21996","id":838021177,"node_id":"MDU6SXNzdWU4MzgwMjExNzc=","number":21996,"title":"SAMPLE expression improvements","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-22T19:01:08Z","updated_at":"2021-03-22T20:42:39Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\r\n* multiple sampling expressions: SAMPLE 0.6, 0.25, 0.15\r\n* _sample_id column for multiple sampling expressions\r\n\r\n```\r\nSELECT customer_id, age, income, _sample_id FROM customers SAMPLE 0.6, 0.25, 0.15;\r\nâ”Œâ”€customer_idâ”€â”¬â”€ageâ”€â”¬â”€incomeâ”€â”¬â”€_sample_idâ”€â”\r\nâ”‚  4133899099 â”‚  39 â”‚   1599 â”‚          1 â”‚\r\nâ”‚  1824346326 â”‚  26 â”‚   1826 â”‚          0 â”‚\r\nâ”‚  4242601905 â”‚  35 â”‚   2405 â”‚          0 â”‚\r\nâ”‚  1946142164 â”‚  34 â”‚   1664 â”‚          0 â”‚\r\nâ”‚  2808379605 â”‚  35 â”‚   2105 â”‚          0 â”‚\r\nâ”‚    27074177 â”‚  37 â”‚   1677 â”‚          2 â”‚\r\nâ”‚  2068656331 â”‚  21 â”‚   1831 â”‚          1 â”‚\r\nâ”‚  3181371348 â”‚  38 â”‚   1848 â”‚          0 â”‚\r\nâ”‚   705514490 â”‚  40 â”‚   1990 â”‚          1 â”‚\r\nâ”‚  2987692305 â”‚  35 â”‚   1805 â”‚          0 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\n* (almost) random distribution for SAMPLE expression (RANDOMIZED ALLOCATION syntax)\r\n\r\nInstead of taking single slice of data `[++++--------]`, it would split that slice in multiple sub slices and distribute them evenly across full data set `[+--+--+--+--]` with reasonable sub slice size, so it wouldn't affect performance in dramatic way.\r\n\r\n* SAMPLE WHEN city='Moscow' THEN 0.5 WHEN city='Saint Petersburg' THEN 0.25 ELSE 0.05 END \r\n\r\nWith support for PREWHERE and ORDER BY index scan?\r\n\r\n**Use case**\r\nML\r\n\r\n\r\n**Additional context**\r\nHighly inspired by Teradata [sampling](https://docs.teradata.com/r/2_MC9vCtAJRlKle2Rpb0mA/XTSw8n_~xbTDRIHwHyUiWA) expression\r\nSnowflake sample [docs](https://docs.snowflake.com/en/sql-reference/constructs/sample.html)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21996/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21992","id":837947217,"node_id":"MDU6SXNzdWU4Mzc5NDcyMTc=","number":21992,"title":"Indentical subqueries (or CTE) execute only once. (condition pushdown)","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":2788693937,"node_id":"MDU6TGFiZWwyNzg4NjkzOTM3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-cte","name":"comp-cte","color":"b5bcff","default":false,"description":"common table expression (WITH ... SELECT)"}],"state":"open","locked":false,"assignee":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"assignees":[{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2021-03-22T17:22:55Z","updated_at":"2021-05-11T18:42:58Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"After https://github.com/ClickHouse/ClickHouse/issues/2301 fix, clickhouse execute sub queries from single level of query only once. But it doesn't work in case we are using that sub query in WHERE condition and that conditions is being pushed to the inner query. \r\n\r\n\r\n**Use case**\r\n```\r\nWITH x AS\r\n    (\r\n        SELECT *\r\n        FROM numbers(10000)\r\n        WHERE NOT sleep(1)\r\n    )\r\nSELECT count()\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM numbers(100)\r\n)\r\nWHERE number IN (x)\r\n\r\nQuery id: 6b8ed07a-1513-49f1-9f5e-ef3c32375fda\r\n\r\nâ”Œâ”€count()â”€â”\r\nâ”‚     100 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 2.009 sec. Processed 20.10 thousand rows, 160.80 KB (10.00 thousand rows/s., 80.04 KB/s.)\r\n\r\n\r\nSELECT count()\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM numbers(100)\r\n)\r\nWHERE number IN\r\n(\r\n    SELECT *\r\n    FROM numbers(10000)\r\n    WHERE NOT sleep(1)\r\n)\r\n\r\nQuery id: 6d1f8c64-118f-4a44-a0c3-30c9bf31bca3\r\n\r\nâ”Œâ”€count()â”€â”\r\nâ”‚     100 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 2.005 sec. Processed 20.10 thousand rows, 160.80 KB (10.02 thousand rows/s., 80.19 KB/s.)\r\n\r\n\r\nWITH\r\n    (\r\n        SELECT groupArray(number)\r\n        FROM numbers(10000)\r\n        WHERE NOT sleep(1)\r\n    ) AS x\r\nSELECT count()\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM numbers(100)\r\n)\r\nWHERE number IN\r\n(\r\n    SELECT arrayJoin(x)\r\n)\r\n\r\nQuery id: 4bc49312-9e82-4dca-883d-316734c76ba2\r\n\r\nâ”Œâ”€count()â”€â”\r\nâ”‚     100 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 1.004 sec.\r\n```\r\n\r\n**Describe the solution you'd like**\r\nClickhouse wouldn't push that kind of conditions or would execute them only once.\r\n\r\n**Describe alternatives you've considered**\r\nDisable predicate optimization by hand:\r\n```\r\nset  set enable_optimize_predicate_expression =0;\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21992/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21952","id":837279069,"node_id":"MDU6SXNzdWU4MzcyNzkwNjk=","number":21952,"title":"Is there any news about the plan to support shared (or remote) file system?","user":{"login":"xiaogaozi","id":149645,"node_id":"MDQ6VXNlcjE0OTY0NQ==","avatar_url":"https://avatars.githubusercontent.com/u/149645?v=4","gravatar_id":"","url":"https://api.github.com/users/xiaogaozi","html_url":"https://github.com/xiaogaozi","followers_url":"https://api.github.com/users/xiaogaozi/followers","following_url":"https://api.github.com/users/xiaogaozi/following{/other_user}","gists_url":"https://api.github.com/users/xiaogaozi/gists{/gist_id}","starred_url":"https://api.github.com/users/xiaogaozi/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/xiaogaozi/subscriptions","organizations_url":"https://api.github.com/users/xiaogaozi/orgs","repos_url":"https://api.github.com/users/xiaogaozi/repos","events_url":"https://api.github.com/users/xiaogaozi/events{/privacy}","received_events_url":"https://api.github.com/users/xiaogaozi/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-22T03:23:36Z","updated_at":"2021-06-17T07:43:21Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi folks, according to #17623 the community has a plan to support shared (or remote) file system in this year. Is there any news (like RFC, design document)? We ([JuiceFS](https://github.com/juicedata/juicefs)) are very happy to participate in relevant discussions. Thanks.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952/reactions","total_count":2,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21952/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21885","id":835020771,"node_id":"MDU6SXNzdWU4MzUwMjA3NzE=","number":21885,"title":"ALTER TABLE foo MATERIALIZE COLUMN bar","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351290474,"node_id":"MDU6TGFiZWwxMzUxMjkwNDc0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mutations","name":"comp-mutations","color":"b5bcff","default":false,"description":"ALTER UPDATE/DELETE"},{"id":3086255531,"node_id":"MDU6TGFiZWwzMDg2MjU1NTMx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/warmup%20task","name":"warmup task","color":"FBCA04","default":false,"description":"The task for new ClickHouse team members. Low risk, moderate complexity, no urgency."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-18T16:26:53Z","updated_at":"2021-08-16T09:31:26Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"More natural way to init column values for the past data (instead of ugly `ALTER TABLE foo UPDATE bar=bar WHERE 1`) \r\n\r\nShould work for parts where column does not exists, also should work with MATERIALIZED columns.\r\n\r\nSupersedes https://github.com/ClickHouse/ClickHouse/issues/19785 ","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21885/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21865","id":834732889,"node_id":"MDU6SXNzdWU4MzQ3MzI4ODk=","number":21865,"title":"Support LowCardinality strings in ngrambf_v1","user":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1357507723,"node_id":"MDU6TGFiZWwxMzU3NTA3NzIz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-skipidx","name":"comp-skipidx","color":"b5bcff","default":false,"description":"Data skipping indices"},{"id":1446368065,"node_id":"MDU6TGFiZWwxNDQ2MzY4MDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-lowcardinality","name":"comp-lowcardinality","color":"b5bcff","default":false,"description":"LowCardinality data type"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-18T12:10:58Z","updated_at":"2021-03-18T16:29:48Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Now it support only `String` and `FixedString`\r\n\r\nhttps://github.com/ClickHouse/ClickHouse/blob/33aefa832b3c544934a757cc9d0e284252a1dd3e/src/Storages/MergeTree/MergeTreeIndexFullText.cpp#L809\r\n\r\nSeems that `LowCardinality(String)` can be supported too, because we only use `column->getDataAt` to retrieve data and it will work on `LowCardinality(String)` in same way as for `String`\r\nhttps://github.com/ClickHouse/ClickHouse/blob/33aefa832b3c544934a757cc9d0e284252a1dd3e/src/Storages/MergeTree/MergeTreeIndexFullText.cpp#L543\r\nhttps://github.com/ClickHouse/ClickHouse/blob/33aefa832b3c544934a757cc9d0e284252a1dd3e/src/Storages/MergeTree/MergeTreeIndexFullText.cpp#L145","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21865/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21849","id":834324200,"node_id":"MDU6SXNzdWU4MzQzMjQyMDA=","number":21849,"title":"windowFunnel: misleading description of window","user":{"login":"jinghancc","id":39145303,"node_id":"MDQ6VXNlcjM5MTQ1MzAz","avatar_url":"https://avatars.githubusercontent.com/u/39145303?v=4","gravatar_id":"","url":"https://api.github.com/users/jinghancc","html_url":"https://github.com/jinghancc","followers_url":"https://api.github.com/users/jinghancc/followers","following_url":"https://api.github.com/users/jinghancc/following{/other_user}","gists_url":"https://api.github.com/users/jinghancc/gists{/gist_id}","starred_url":"https://api.github.com/users/jinghancc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jinghancc/subscriptions","organizations_url":"https://api.github.com/users/jinghancc/orgs","repos_url":"https://api.github.com/users/jinghancc/repos","events_url":"https://api.github.com/users/jinghancc/events{/privacy}","received_events_url":"https://api.github.com/users/jinghancc/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-18T02:13:51Z","updated_at":"2021-03-19T02:11:48Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the issue**\r\nDocumentation of `windowFunnel` describes parameter `window` as time interval between two consecutive conditions:\r\n![image](https://user-images.githubusercontent.com/39145303/111563091-3eb71580-87d2-11eb-95e0-7fdfeaafed13.png)\r\n\r\nbut actually, it is the time interval between the first condition and last condition, so it should be:\r\n```\r\ntimestamp of cond1 <= timestamp of cond2 <= ... <= timestamp of condN <= timestamp of cond1 + window.\r\n```\r\nIt would be nice if we could correct this description since it's misleading to users.\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21849/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21845","id":834162332,"node_id":"MDU6SXNzdWU4MzQxNjIzMzI=","number":21845,"title":"TTL GROUP BY should accept ( ) around expression ","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-17T20:42:10Z","updated_at":"2021-03-24T00:49:35Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```\r\nCREATE TABLE agg (a String, b UInt64, c String, d Date)\r\nENGINE=SummingMergeTree()\r\nPARTITION BY toYYYYMM(d)\r\n\r\nORDER BY                        (a, c, toStartOfMonth(d), d)\r\nTTL d + INTERVAL 8 day GROUP BY (a, c, toStartOfMonth(d))\r\n\r\nSET b=sum(b), d=min(toStartOfMonth(d));\r\n\r\nDB::Exception: TTL Expression GROUP BY key should be a prefix of primary key.\r\n```\r\n\r\nto make it work need to replace \r\n`GROUP BY (a, c, toStartOfMonth(d))`\r\nwith\r\n`GROUP BY a, c, toStartOfMonth(d)`\r\n\r\n\r\n```\r\nCREATE TABLE agg (a String, b UInt64, c String, d Date)\r\nENGINE=SummingMergeTree()\r\nPARTITION BY toYYYYMM(d)\r\n\r\nORDER BY                        (a, c, toStartOfMonth(d), d)\r\nTTL d + INTERVAL 8 day GROUP BY a, c, toStartOfMonth(d)\r\n\r\nSET b=sum(b), d=min(toStartOfMonth(d));\r\n\r\nOk.\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21845/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21838","id":833842514,"node_id":"MDU6SXNzdWU4MzM4NDI1MTQ=","number":21838,"title":"JOIN: `Unknown identifier error message` instead of `Column is not under aggregate function`","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":0,"created_at":"2021-03-17T14:36:32Z","updated_at":"2021-03-24T00:43:30Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**How to reproduce**\r\n```\r\nSELECT\r\n    number,\r\n    count(),\r\n    d\r\nFROM\r\n(\r\n    SELECT\r\n        number,\r\n        number AS d\r\n    FROM numbers(10)\r\n) AS x\r\nGROUP BY number\r\n\r\nQuery id: 8076200e-8882-4407-8b9d-253d01b41ec6\r\n\r\n\r\nReceived exception from server (version 21.1.7):\r\nCode: 215. DB::Exception: Received from localhost:9000. DB::Exception: Column `d` is not under aggregate function and not in GROUP BY: While processing number, count(), d.\r\n\r\n\r\nSELECT\r\n    number,\r\n    count(),\r\n    d\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM system.one\r\n) AS a\r\n,\r\n(\r\n    SELECT\r\n        number,\r\n        number AS d\r\n    FROM numbers(10)\r\n) AS x\r\nGROUP BY number\r\n\r\nQuery id: efb99cc6-9367-4e09-bbd9-ea36ffee9c25\r\n\r\n\r\nReceived exception from server (version 21.1.7):\r\nCode: 47. DB::Exception: Received from localhost:9000. DB::Exception: Unknown identifier: 'd'.\r\n\r\nSELECT\r\n    number,\r\n    count(),\r\n    x.d\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM system.one\r\n) AS a\r\n,\r\n(\r\n    SELECT\r\n        number,\r\n        number AS d\r\n    FROM numbers(10)\r\n) AS x\r\nGROUP BY number\r\n\r\nReceived exception from server (version 21.1.7):\r\nCode: 47. DB::Exception: Received from localhost:9000. DB::Exception: Unknown identifier: 'd'.\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21838/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21834","id":833773859,"node_id":"MDU6SXNzdWU4MzM3NzM4NTk=","number":21834,"title":"Generate random data with boundaries","user":{"login":"dchimeno","id":36852967,"node_id":"MDQ6VXNlcjM2ODUyOTY3","avatar_url":"https://avatars.githubusercontent.com/u/36852967?v=4","gravatar_id":"","url":"https://api.github.com/users/dchimeno","html_url":"https://github.com/dchimeno","followers_url":"https://api.github.com/users/dchimeno/followers","following_url":"https://api.github.com/users/dchimeno/following{/other_user}","gists_url":"https://api.github.com/users/dchimeno/gists{/gist_id}","starred_url":"https://api.github.com/users/dchimeno/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dchimeno/subscriptions","organizations_url":"https://api.github.com/users/dchimeno/orgs","repos_url":"https://api.github.com/users/dchimeno/repos","events_url":"https://api.github.com/users/dchimeno/events{/privacy}","received_events_url":"https://api.github.com/users/dchimeno/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-17T13:25:16Z","updated_at":"2021-03-24T03:34:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"\r\n**Use case**\r\nIt could be useful to generate random data with boundaries.\r\nCurrently, both generateRandom functions and engine, allow to define a table like:\r\n\r\n```sql\r\nCREATE TABLE randomt\r\n(\r\n    `value` Decimal(5,2),\r\n    `date` Date,\r\n    `zone_id` UInt64,\r\n    `type` Enum8('blue' = 0, 'green' = 1, 'red' = 2),\r\n    `interests` Array(String)\r\n)\r\nENGINE = GenerateRandom(123, 5, 3)\r\n```\r\n```sql\r\nSELECT *\r\nFROM randomt\r\nLIMIT 5\r\n```\r\n\r\n```shell\r\nâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€valueâ”€â”¬â”€â”€â”€â”€â”€â”€â”€dateâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€zone_idâ”€â”¬â”€typeâ”€â”€â”¬â”€interestsâ”€â”€â”€â”€â”€â”€â”\r\nâ”‚  17418655.70 â”‚ 2018-08-16 â”‚  9921422126199728620 â”‚ green â”‚ ['(nX']        â”‚\r\nâ”‚  -2712197.83 â”‚ 1992-12-19 â”‚  1576572515264284063 â”‚ blue  â”‚ ['','8NM','I'] â”‚\r\nâ”‚  15070040.36 â”‚ 2102-11-15 â”‚   688226268395004343 â”‚ blue  â”‚ ['b7<ZU','m*'] â”‚\r\nâ”‚ -16219929.50 â”‚ 2033-01-18 â”‚ 12076865034075318107 â”‚ red   â”‚ ['3']          â”‚\r\nâ”‚   2771016.64 â”‚ 2051-03-21 â”‚  8931155881545802352 â”‚ blue  â”‚ ['oU',';!>V']  â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\nIt works great for fuzzing test and some other things, but I'm thinking in the use case of generated 'mock' data with limits for each column.\r\n\r\nFor some fields (date and enum), the behaviour is correct (in the sense of mock data).\r\n\r\n```sql\r\nSELECT *\r\nFROM randomt\r\nWHERE (date = toDate('2020-10-20')) AND (type = 'blue')\r\nLIMIT 5\r\n```\r\n\r\n```shell\r\nâ”Œâ”€â”€â”€â”€â”€â”€â”€valueâ”€â”¬â”€â”€â”€â”€â”€â”€â”€dateâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€zone_idâ”€â”¬â”€typeâ”€â”¬â”€interestsâ”€â”\r\nâ”‚  -459221.48 â”‚ 2020-10-20 â”‚ 16696499279086187682 â”‚ blue â”‚ ['R']     â”‚\r\nâ”‚ -5533672.64 â”‚ 2020-10-20 â”‚  5903887098779430673 â”‚ blue â”‚ []        â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\nâ”Œâ”€â”€â”€â”€â”€â”€â”€valueâ”€â”¬â”€â”€â”€â”€â”€â”€â”€dateâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€zone_idâ”€â”¬â”€typeâ”€â”¬â”€interestsâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚  7419336.56 â”‚ 2020-10-20 â”‚  4113108383613272096 â”‚ blue â”‚ ['A','~5^H+','\"X@']   â”‚\r\nâ”‚ -2742208.19 â”‚ 2020-10-20 â”‚ 17036311104628448725 â”‚ blue â”‚ [' 4 5:','Nd?0','f9'] â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\nâ”Œâ”€â”€â”€â”€â”€â”€valueâ”€â”¬â”€â”€â”€â”€â”€â”€â”€dateâ”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€zone_idâ”€â”¬â”€typeâ”€â”¬â”€interestsâ”€â”\r\nâ”‚ 3702796.40 â”‚ 2020-10-20 â”‚ 18297471488954938545 â”‚ blue â”‚ []        â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\nBut when selecting others like:\r\n\r\n```sql\r\nselect * from randomt where date=toDate('2020-10-20') and type='blue' and zone_id in (array(1,2,3)) limit 5\r\n```\r\n\r\nThe query hangs out.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nWhat if is possible something like:\r\n```sql\r\nCREATE TABLE randomt\r\n(\r\n    `value` Decimal(5,2) [min=0, max=100]\r\n    `date` Date, [min=toDate('2020-10-01', max=toDate('2021,03-03')\r\n    `zone_id` UInt64, [min=0, max=2000]\r\n    `type` Enum8('blue' = 0, 'green' = 1, 'red' = 2),\r\n    `interests` Array(String) (select interest in another_table)\r\n)\r\nENGINE = GenerateRandom(123, 5, 3)\r\n```\r\nThe syntax could be other, of course, it just as an example to illustrate.\r\nFor strings, min and max probably hasn't make sense, so something like a list of values could be useful.\r\n\r\n\r\n**Describe alternatives you've considered**\r\nclickhouse-obfuscated, but it would take too much disk space for the use case.\r\n\r\n\r\n**Additional context**\r\n\r\nIf it doesn't make sense, or there are currently better alternatives, just say it :)\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21834/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21833","id":833757456,"node_id":"MDU6SXNzdWU4MzM3NTc0NTY=","number":21833,"title":"sequenceMatch: add time window for event chain","user":{"login":"jinghancc","id":39145303,"node_id":"MDQ6VXNlcjM5MTQ1MzAz","avatar_url":"https://avatars.githubusercontent.com/u/39145303?v=4","gravatar_id":"","url":"https://api.github.com/users/jinghancc","html_url":"https://github.com/jinghancc","followers_url":"https://api.github.com/users/jinghancc/followers","following_url":"https://api.github.com/users/jinghancc/following{/other_user}","gists_url":"https://api.github.com/users/jinghancc/gists{/gist_id}","starred_url":"https://api.github.com/users/jinghancc/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jinghancc/subscriptions","organizations_url":"https://api.github.com/users/jinghancc/orgs","repos_url":"https://api.github.com/users/jinghancc/repos","events_url":"https://api.github.com/users/jinghancc/events{/privacy}","received_events_url":"https://api.github.com/users/jinghancc/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-17T13:12:39Z","updated_at":"2021-03-18T07:24:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Use case**\r\nI want to do funnel analysis based on event data that are stored in ClickHouse. Let's define a few elements for funnel analysis:\r\n\r\n- event chain: A (event_id = 1) -> B (event_id = 2) -> C (event_id = 3)\r\n\r\n- time period: 0 (event_ms) ~ 500 (event_ms)\r\n\r\n- time window: 100 (event_ms)\r\n\r\nI want to know, for each user, if there is an event chain (A->B->C) happened within the time period, and intervals between A and C is within the time window.\r\n\r\n**Describe the solution you'd like**\r\n`sequenceMatch` seems like a promising candidate for my use case, but currently it can only add time window for two consecutive events, like A -> B, and B -> C, but not for A -> C. I would like to define time window for the whole event chain, it would be nice if you could add this feature.\r\n```sql\r\nSELECT \r\n  uid, \r\n  sequenceMatch(t<100)('(?1)(?2)(?3)')(event_ms, event_id = 1, event_id = 2, event_id = 3) funnel\r\nFROM (\r\n  /* emulate test dataset */\r\n  SELECT data.1 event_id, data.2  event_ms, data.3 uid\r\n  FROM (\r\n    SELECT arrayJoin(\r\n      [(1, 100, 123), \r\n      (1, 120, 123), \r\n      (1, 130, 123), \r\n      (1, 150, 345), \r\n      (1, 180, 345), \r\n      (2, 150, 123), \r\n      (2, 200, 234), \r\n      (2, 140, 345),\r\n      (2, 210, 345),\r\n      (2, 300, 345),\r\n      (3, 180, 123),\r\n      (3, 250, 123),\r\n      (3, 290, 234),\r\n      (3, 270, 345)]) data)\r\n  )\r\nWHERE event_ms >= 0 AND event_ms <= 500 \r\nGROUP BY uid;\r\n\r\nResults:\r\n123     1  (100 -> 150 -> 180, 120 -> 150 -> 180, 130 -> 150 -> 180)\r\n234     0\r\n345     1 (180 -> 210 -> 270)\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nCurrently I use `join` to find all qualified patterns, but it's pretty slow, especially when the dataset is large.\r\n```sql\r\nSELECT\r\n    t1.event_ms, t2.event_ms, t3.event_ms, t4.event_ms,\r\n    t1.uid, t2.uid, t3.uid, t4.uid\r\nFROM\r\n(SELECT \r\n    uid, event_ms \r\nFROM funnel_join_test_1\r\nWHERE\r\n    event_id = 1 AND event_ms >= 0 AND event_ms <= 500) as t1\r\nASOF left join\r\n(SELECT \r\n    uid, event_ms \r\nFROM funnel_join_test_1\r\nWHERE\r\n    event_id = 2 AND event_ms >= 0 AND event_ms <= 500) as t2\r\nON t1.uid = t2.uid AND t1.event_ms  < t2.event_ms\r\nASOF left join\r\n(SELECT \r\n    uid, event_ms \r\nFROM funnel_join_test_1\r\nWHERE\r\n    event_id = 3 AND event_ms >= 0 and event_ms <= 500) as t3\r\nON t2.uid = t3.uid and t2.event_ms < t3.event_ms\r\nASOF left join\r\n(SELECT \r\n    uid, event_ms \r\nFROM funnel_join_test_1\r\nWHERE\r\n    event_id = 3 AND event_ms >= 0 and event_ms <= 500) as t4\r\nON t3.uid = t4.uid and t4.event_ms < t1.event_ms + 100\r\nWHERE t4.event_ms > 0;\r\n```\r\n\r\n**Additional context**\r\nAdd any other context or screenshots about the feature request here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21833/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21827","id":833585021,"node_id":"MDU6SXNzdWU4MzM1ODUwMjE=","number":21827,"title":"DEFAULT 'now()' + columnâ€™s_TTL sometimes cause error â€œCode 40:..Checksums of parts don't matchâ€ during merges in ReplicatedMergeTree for version 20.10.7.4.","user":{"login":"krebrikov","id":80764208,"node_id":"MDQ6VXNlcjgwNzY0MjA4","avatar_url":"https://avatars.githubusercontent.com/u/80764208?v=4","gravatar_id":"","url":"https://api.github.com/users/krebrikov","html_url":"https://github.com/krebrikov","followers_url":"https://api.github.com/users/krebrikov/followers","following_url":"https://api.github.com/users/krebrikov/following{/other_user}","gists_url":"https://api.github.com/users/krebrikov/gists{/gist_id}","starred_url":"https://api.github.com/users/krebrikov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/krebrikov/subscriptions","organizations_url":"https://api.github.com/users/krebrikov/orgs","repos_url":"https://api.github.com/users/krebrikov/repos","events_url":"https://api.github.com/users/krebrikov/events{/privacy}","received_events_url":"https://api.github.com/users/krebrikov/received_events","type":"User","site_admin":false},"labels":[{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-17T09:43:18Z","updated_at":"2021-03-24T15:52:01Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"1. Concise description of problem:\r\n\r\nSometimes merges in ReplicatedMergeTree table fail due to error: \r\nâ€œ_Code 40:â€¦ Checksums of parts don't match: uncompressed hash of compressed files doesn't match (version 20.10.7.4 (official build)). Data after merge is not byte-identical to data on another replicas..._â€ (there will be detailed error text below).\r\n\r\nAssumed problemâ€™s root cause is somehow related with 1 day column TTL used in this table, since the same environment+cluster+scheme doesnâ€™t encounter Â«ChecksumsÂ» merge errors when columns TTL is not used. \r\n\r\nAmount of errors depends on merge intensity. Daily load of about 30-35 billions of events (table is partitioned by YYYYMMDD) causes scores/several hundreds (by order of magnitude) of such errors per shard per day/partition.\r\n\r\nIn general it looks like parts that fail to merge due to considered error, are merged successfully later. \r\nThat is in most cases this error is NOT regularly repeated for the data it affected once, thus this error does NOT prevent parts to be eventually merged. (I add this to point out that in a whole this error does not yet look like something critical what prevents merges, but rather some unsettling anomaly. However I'm not 100% certain it never escalates to something else).\r\n\r\n2. Background/environment:\r\n\r\n3 nodes Clickhouse cluster (version 20.10.7.4). \r\nMain fact table consists of 3 shards with 2 replicas (circle replication) based on local ReplicatedMergeTree tables all_events_v02.\r\n(I know there are some arguments against circle replication, but we are already using this approach more than 2 years in production, while \"Checksums\" issue/error occured barely two weeks ago. So whatever drawbacks \"circular replication\" has - it is very unlikely to be related to root cause).\r\nData is loaded via: --> Null Table --> MV --> Distributed table all_events_upload --> local RMT tables all_events_v02. There are plenty other tables and MVs, but only this main fact table (containing >90% of databaseâ€™ size) is affected.\r\nTable is daily partitioned: â€˜PARTITION BY toYYYYMMDD(end_datetime)â€™, where field end_datetime is eventâ€™s DateTime.\r\nField end_datetime is part of primary key.\r\nSeveral columns have specific CODEC settings. \r\n\r\nSeveral fields have column TTL set as Â«TTL toDate(end_datetime) + toIntervalDay(1)Â» like:\r\n`wdt` DateTime DEFAULT now() CODEC(Delta(4), ZSTD(1)) TTL toDate(end_datetime) + toIntervalDay(1),\r\n`edt_ms` UInt32 DEFAULT 0 TTL toDate(end_datetime) + toIntervalDay(1),\r\n`ch_host` LowCardinality(String) DEFAULT toString('') TTL toDate(end_datetime) + toIntervalDay(1),\r\n\r\n(So on the next day after `end_datetime` new merges are supposed to clear contents of these columns - and this is when sometimes error occurs).\r\n\r\n3. Error text:\r\n\r\n```\r\n2021.03.16 18:20:11.406853 [ 29317 ] {} <Debug> {databasename}.all_events_v02: Undoing transaction. Removing parts: 20210314_0_621_6.\r\n2021.03.16 18:20:11.406939 [ 29317 ] {} <Error> {databasename}.all_events_v02: Code: 40, e.displayText() = DB::Exception: Checksums of parts don't match: uncompressed hash of compressed files doesn't match (version 20.10.7.4 (official build)). Data after merge is not byte-identical to data on another replicas. There could be several reasons: 1. Using newer version of compression library after server update. 2. Using another compression method. 3. Non-deterministic compression algorithm (highly unlikely). 4. Non-deterministic merge algorithm due to logical error in code. 5. Data corruption in memory due to bug in code. 6. Data corruption in memory due to hardware issue. 7. Manual modification of source data after server startup. 8. Manual modification of checksums stored in ZooKeeper. 9. Part format related settings like 'enable_mixed_granularity_parts' are different on different replicas. We will download merged part from replica to force byte-identical result.\r\n```\r\n( {database} is sometimes database related to first replica, and sometimes database related to second replica - due to circular/ring replication ).\r\n\r\nI verified all 9 suggested causes mentioned in error text, but neither of them seems to be the case.\r\nCustom settings (both system.settings and system.merge_tree_settings and system.replicated_merge_tree_settings) are identical on all nodes. No hardware issues takes place, there was no library upgrades. Etc.\r\n\r\nHowever when column TTL for ReplicateMergeTree table (local all_events_v02) is not applied, and everything else is the same, - than there are no â€˜Checksumsâ€™ errors.\r\n\r\n4. Custom settings:\r\nsystem.settings\r\nâ”Œâ”€nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€valueâ”€â”€â”€â”€â”€â”€â”¬â”€changedâ”€â”\r\nâ”‚ connect_timeout_with_failover_ms        \tâ”‚ 500         \tâ”‚       1 \tâ”‚\r\nâ”‚ connect_timeout_with_failover_secure_ms \tâ”‚ 550         \tâ”‚       1 â”‚\r\nâ”‚ use_uncompressed_cache                  \t\tâ”‚ 0           \tâ”‚       1 â”‚\r\nâ”‚ load_balancing                          \t\t\tâ”‚ in_order    \tâ”‚       1 â”‚\r\nâ”‚ log_queries                             \t\t\tâ”‚ 1           \tâ”‚       1 â”‚\r\nâ”‚ http_connection_timeout                 \t\tâ”‚ 10          \tâ”‚       1 â”‚\r\nâ”‚ max_memory_usage                        \t\tâ”‚ 75000000000 â”‚       1 â”‚\r\nâ”‚ parallel_view_processing                \t\tâ”‚ 1           \tâ”‚       1 â”‚\r\nâ”‚ max_partitions_per_insert_block         \t        â”‚ 10          \tâ”‚       1 â”‚\r\nâ”‚ materialize_ttl_after_modify            \t\tâ”‚ 0           \tâ”‚       1 â”‚\r\nâ”‚ max_memory_usage_for_all_queries        \tâ”‚ 75000000000 â”‚       1 â”‚\r\n\r\nsystem.replicated_merge_tree_settings\r\nâ”Œâ”€nameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€valueâ”€â”¬â”€changedâ”€â”\r\nâ”‚ parts_to_delay_insert            \t\t\tâ”‚ 150   \t\tâ”‚       1 â”‚\r\nâ”‚ parts_to_throw_insert            \t\t\tâ”‚ 600   \t\tâ”‚       1 â”‚\r\nâ”‚ max_delay_to_insert              \t\t\tâ”‚ 5     \t\tâ”‚       1 â”‚\r\nâ”‚ max_suspicious_broken_parts      \t\tâ”‚ 0     \t\tâ”‚       1 â”‚\r\nâ”‚ min_merge_bytes_to_use_direct_io \t\tâ”‚ 0     \t\tâ”‚       1 â”‚\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21827/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21821","id":833553279,"node_id":"MDU6SXNzdWU4MzM1NTMyNzk=","number":21821,"title":"Why not use `mutations_sync` to control synchronicity in `StorageReplcatedMergeTree::alter`","user":{"login":"spongedu","id":4725158,"node_id":"MDQ6VXNlcjQ3MjUxNTg=","avatar_url":"https://avatars.githubusercontent.com/u/4725158?v=4","gravatar_id":"","url":"https://api.github.com/users/spongedu","html_url":"https://github.com/spongedu","followers_url":"https://api.github.com/users/spongedu/followers","following_url":"https://api.github.com/users/spongedu/following{/other_user}","gists_url":"https://api.github.com/users/spongedu/gists{/gist_id}","starred_url":"https://api.github.com/users/spongedu/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/spongedu/subscriptions","organizations_url":"https://api.github.com/users/spongedu/orgs","repos_url":"https://api.github.com/users/spongedu/repos","events_url":"https://api.github.com/users/spongedu/events{/privacy}","received_events_url":"https://api.github.com/users/spongedu/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1669010018,"node_id":"MDU6TGFiZWwxNjY5MDEwMDE4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question-answered","name":"question-answered","color":"bfdadc","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false},"assignees":[{"login":"alesapin","id":3684697,"node_id":"MDQ6VXNlcjM2ODQ2OTc=","avatar_url":"https://avatars.githubusercontent.com/u/3684697?v=4","gravatar_id":"","url":"https://api.github.com/users/alesapin","html_url":"https://github.com/alesapin","followers_url":"https://api.github.com/users/alesapin/followers","following_url":"https://api.github.com/users/alesapin/following{/other_user}","gists_url":"https://api.github.com/users/alesapin/gists{/gist_id}","starred_url":"https://api.github.com/users/alesapin/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alesapin/subscriptions","organizations_url":"https://api.github.com/users/alesapin/orgs","repos_url":"https://api.github.com/users/alesapin/repos","events_url":"https://api.github.com/users/alesapin/events{/privacy}","received_events_url":"https://api.github.com/users/alesapin/received_events","type":"User","site_admin":false}],"milestone":null,"comments":2,"created_at":"2021-03-17T09:04:26Z","updated_at":"2021-11-18T04:43:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\r\nWhen executing `ALTER TABLE` operation on `ReplicatedMergeTree`,  why we use `replication_alter_partitions_sync` instead of `mutations_sync` to control the synchronicity  when wait mutations to finish ? Saying following line:  \r\n\r\nhttps://github.com/ClickHouse/ClickHouse/blob/d02726bcac071d63fb7811e4e3d454da43f8a589/src/Storages/StorageReplicatedMergeTree.cpp#L4601","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21821/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21813","id":833364289,"node_id":"MDU6SXNzdWU4MzMzNjQyODk=","number":21813,"title":"Z-Ordering Index","user":{"login":"bakey","id":1566848,"node_id":"MDQ6VXNlcjE1NjY4NDg=","avatar_url":"https://avatars.githubusercontent.com/u/1566848?v=4","gravatar_id":"","url":"https://api.github.com/users/bakey","html_url":"https://github.com/bakey","followers_url":"https://api.github.com/users/bakey/followers","following_url":"https://api.github.com/users/bakey/following{/other_user}","gists_url":"https://api.github.com/users/bakey/gists{/gist_id}","starred_url":"https://api.github.com/users/bakey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bakey/subscriptions","organizations_url":"https://api.github.com/users/bakey/orgs","repos_url":"https://api.github.com/users/bakey/repos","events_url":"https://api.github.com/users/bakey/events{/privacy}","received_events_url":"https://api.github.com/users/bakey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1507871495,"node_id":"MDU6TGFiZWwxNTA3ODcxNDk1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-community-taken","name":"st-community-taken","color":"e5b890","default":false,"description":"External developer is working on that"}],"state":"open","locked":false,"assignee":{"login":"bakey","id":1566848,"node_id":"MDQ6VXNlcjE1NjY4NDg=","avatar_url":"https://avatars.githubusercontent.com/u/1566848?v=4","gravatar_id":"","url":"https://api.github.com/users/bakey","html_url":"https://github.com/bakey","followers_url":"https://api.github.com/users/bakey/followers","following_url":"https://api.github.com/users/bakey/following{/other_user}","gists_url":"https://api.github.com/users/bakey/gists{/gist_id}","starred_url":"https://api.github.com/users/bakey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bakey/subscriptions","organizations_url":"https://api.github.com/users/bakey/orgs","repos_url":"https://api.github.com/users/bakey/repos","events_url":"https://api.github.com/users/bakey/events{/privacy}","received_events_url":"https://api.github.com/users/bakey/received_events","type":"User","site_admin":false},"assignees":[{"login":"bakey","id":1566848,"node_id":"MDQ6VXNlcjE1NjY4NDg=","avatar_url":"https://avatars.githubusercontent.com/u/1566848?v=4","gravatar_id":"","url":"https://api.github.com/users/bakey","html_url":"https://github.com/bakey","followers_url":"https://api.github.com/users/bakey/followers","following_url":"https://api.github.com/users/bakey/following{/other_user}","gists_url":"https://api.github.com/users/bakey/gists{/gist_id}","starred_url":"https://api.github.com/users/bakey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bakey/subscriptions","organizations_url":"https://api.github.com/users/bakey/orgs","repos_url":"https://api.github.com/users/bakey/repos","events_url":"https://api.github.com/users/bakey/events{/privacy}","received_events_url":"https://api.github.com/users/bakey/received_events","type":"User","site_admin":false}],"milestone":null,"comments":10,"created_at":"2021-03-17T03:23:36Z","updated_at":"2022-01-26T14:22:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nA Z-ordering Index defines an ordering on multi-dimensional data. Data sorted that way can be efficiently filtered by min/max statistics regarding to the columns participating in the ordering.\r\nClickHouse currently only supports lexicographic ordering or numeric ordering via the order BY clause when create table. This strongly prefers the first column, i.e. given the \"ORDER BY (col1, col2, col3)\" clause => col1 will be totally ordered (hence filtering on col1 will be very efficient), but values belonging to col2 and col3 will be scattered throughout the data set (hence filtering on col2 or col3 will barely do any good).\r\n\r\n**Describe the solution you'd like**\r\nWe could extend the syntax of create statement,  e.g. support \"ZORDER BY\" clause to imply the sorting type.\r\n\"ZORDER BY (col1, col2, col3)\" would cluster the rows in a way that filtering on col1, col2, or col3 would be equally efficient.\r\n\r\nIf this looks good.I will participate in this task and submit a pull request.Thanks for review\r\n\r\nReference:\r\n        https://en.wikipedia.org/wiki/Z-order_curve\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21813/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21805","id":833133857,"node_id":"MDU6SXNzdWU4MzMxMzM4NTc=","number":21805,"title":"lastMap, firstMap/anyMap, mergeMap/mapMerge for aggregating/merging arrays (like maxMap but for strings and etc)","user":{"login":"morozovsk","id":1822063,"node_id":"MDQ6VXNlcjE4MjIwNjM=","avatar_url":"https://avatars.githubusercontent.com/u/1822063?v=4","gravatar_id":"","url":"https://api.github.com/users/morozovsk","html_url":"https://github.com/morozovsk","followers_url":"https://api.github.com/users/morozovsk/followers","following_url":"https://api.github.com/users/morozovsk/following{/other_user}","gists_url":"https://api.github.com/users/morozovsk/gists{/gist_id}","starred_url":"https://api.github.com/users/morozovsk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/morozovsk/subscriptions","organizations_url":"https://api.github.com/users/morozovsk/orgs","repos_url":"https://api.github.com/users/morozovsk/repos","events_url":"https://api.github.com/users/morozovsk/events{/privacy}","received_events_url":"https://api.github.com/users/morozovsk/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-16T19:35:08Z","updated_at":"2021-03-27T22:35:22Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"I'm working on [simple backend](https://github.com/amplihouse/amplihouse) for [amplitude sdk](https://github.com/amplitude).\r\nMy backend can get events from clients, put it in clickhouse and I can view data in [metabase](https://github.com/metabase/metabase).\r\n\r\n**Use case**\r\n\r\nI have:\r\n\r\n- table events (MergeTree, os, device_id, ...other user properties)\r\n- aggregating materialized view raw_users (AggregatingMergeTree, SimpleAggregateFunction, GROUP BY os, device_id)\r\n- and final aggregating view users (view, SELECT ... GROUP BY os, device_id)\r\n\r\nFor calculating last users properties I use anyLast, SimpleAggregateFunction(anyLast, String) in mat view and anyLast in final view.\r\n\r\nEverything works fine. But ... for each new custom user property I need add new column.\r\n\r\n**The solution I'd like**\r\n\r\nI woud like to create one new column in table events:\r\n```\r\ncustomUserPropertiesMap Nested (name String, value String)\r\n```\r\nand one new column in table users:\r\n```\r\nSimpleAggregateFunction(anyLastMap, String, String)\r\n```\r\nand use function:\r\n```\r\nanyLastMap\r\n```\r\n**for example:**\r\n![screenshot_2021-03-17_01:30:36](https://user-images.githubusercontent.com/1822063/111393044-877ea980-86c0-11eb-8ac7-c85f22dc42ea.png)\r\n\r\n**Alternatives I'd considered**\r\nI used maxMap but it works only for increased integers.\r\n\r\n**Additional context**\r\n```sql\r\nCREATE TABLE amplihouse.events (\r\n\tos String,\r\n\tdevice_id String,\r\n\tt DateTime,\r\n\tevent_type String,\r\n\tsession_start DateTime,\r\n\tversion String, \r\n\tos_version String, \r\n\tdevice_manufacturer String, \r\n\tdevice_model String,\r\n\tcarrier String,\r\n\tlanguage String,\r\n\tcountry String,\r\n\tcountry_code String,\r\n\tregion String,\r\n\tcity String,\r\n\tip String\r\n) \r\nEngine MergeTree PARTITION BY (os, toYYYYMMDD(t)) ORDER BY (device_id);\r\n\r\nCREATE MATERIALIZED VIEW amplihouse.raw_users (os String,\r\n\tdevice_id String,\r\n\tstart SimpleAggregateFunction(min, DateTime),\r\n\tend SimpleAggregateFunction(max, DateTime),\r\n\tevents SimpleAggregateFunction(sum, UInt64),\r\n\tsessions SimpleAggregateFunction(groupUniqArrayArray, Array(DateTime)),\r\n\tdays SimpleAggregateFunction(groupUniqArrayArray, Array(Date)), \r\n\tversion SimpleAggregateFunction(anyLast, String),\r\n\tstart_version SimpleAggregateFunction(any, String),\r\n\tos_version SimpleAggregateFunction(anyLast, String),\r\n\tdevice_manufacturer SimpleAggregateFunction(anyLast, String),\r\n\tdevice_model SimpleAggregateFunction(anyLast, String),\r\n\tcarrier SimpleAggregateFunction(anyLast, String),\r\n\tlanguage SimpleAggregateFunction(anyLast, String),\r\n\tcountry SimpleAggregateFunction(anyLast, String),\r\n\tcountry_code SimpleAggregateFunction(anyLast, String),\r\n\tregion SimpleAggregateFunction(anyLast, String),\r\n\tcity SimpleAggregateFunction(anyLast, String),\r\n\tip SimpleAggregateFunction(anyLast, String)\r\n) AS SELECT\r\n       os,\r\n       device_id,\r\n       min(t),\r\n       max(t),\r\n       count(),\r\n       groupUniqArray(session_start),\r\n       groupUniqArray(toDate(t)),\r\n       min(version),\r\n       anyLast(device_manufacturer),\r\n       anyLast(device_model),\r\n       anyLast(version),\r\n       anyLast(os_version),\r\n       anyLast(carrier),\r\n       anyLast(language),\r\n       anyLast(country),\r\n       anyLast(country_code),\r\n       anyLast(region),\r\n       anyLast(city),\r\n       anyLast(ip)\r\nFROM amplihouse.events\r\nGROUP BY os, device_id;\r\nENGINE = AggregatingMergeTree PARTITION BY os ORDER BY device_id;\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21805/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21794","id":832873800,"node_id":"MDU6SXNzdWU4MzI4NzM4MDA=","number":21794,"title":"There is no supertype when CROSS to INNER JOIN rewrite WHERE a.key=b.key-1 ","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":16,"created_at":"2021-03-16T14:41:27Z","updated_at":"2022-01-19T12:26:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\nThere is no supertype for types UInt64, Int64. CROSS to INNER JOIN rewrite WHERE a.key=b.key-1 \r\n\r\n**How to reproduce**\r\n21.4.1.6210\r\n```\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT number\r\n    FROM numbers(10)\r\n) AS a\r\n,\r\n(\r\n    SELECT number\r\n    FROM numbers(10)\r\n) AS b\r\nWHERE a.number = (b.number - 1)\r\n\r\nQuery id: 6ca49d4e-e6a1-4dc0-9552-280d481be176\r\n\r\n\r\n0 rows in set. Elapsed: 0.003 sec.\r\n\r\nReceived exception from server (version 21.4.1):\r\nCode: 53. DB::Exception: Received from localhost:9000. DB::Exception: Type mismatch of columns to JOIN by: number: UInt64 at left, minus(b.number, 1): Int64 at right. Can't get supertype: There is no supertype for types UInt64, Int64 because some of them are signed integers and some are unsigned integers, but there is no signed integer type, that can exactly represent all required unsigned integer values.\r\n\r\n\r\nset cross_to_inner_join_rewrite=0;\r\n\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT number\r\n    FROM numbers(10)\r\n) AS a\r\n,\r\n(\r\n    SELECT number\r\n    FROM numbers(10)\r\n) AS b\r\nWHERE a.number = (b.number - 1)\r\n\r\nQuery id: ac47d703-f528-43ea-a0e7-a030a869b568\r\n\r\nâ”Œâ”€numberâ”€â”¬â”€b.numberâ”€â”\r\nâ”‚      0 â”‚        1 â”‚\r\nâ”‚      1 â”‚        2 â”‚\r\nâ”‚      2 â”‚        3 â”‚\r\nâ”‚      3 â”‚        4 â”‚\r\nâ”‚      4 â”‚        5 â”‚\r\nâ”‚      5 â”‚        6 â”‚\r\nâ”‚      6 â”‚        7 â”‚\r\nâ”‚      7 â”‚        8 â”‚\r\nâ”‚      8 â”‚        9 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21794/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21781","id":832740952,"node_id":"MDU6SXNzdWU4MzI3NDA5NTI=","number":21781,"title":"Table Inserts monitoring using ProfileEvent_InsertedRows","user":{"login":"bun4uk","id":14631690,"node_id":"MDQ6VXNlcjE0NjMxNjkw","avatar_url":"https://avatars.githubusercontent.com/u/14631690?v=4","gravatar_id":"","url":"https://api.github.com/users/bun4uk","html_url":"https://github.com/bun4uk","followers_url":"https://api.github.com/users/bun4uk/followers","following_url":"https://api.github.com/users/bun4uk/following{/other_user}","gists_url":"https://api.github.com/users/bun4uk/gists{/gist_id}","starred_url":"https://api.github.com/users/bun4uk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/bun4uk/subscriptions","organizations_url":"https://api.github.com/users/bun4uk/orgs","repos_url":"https://api.github.com/users/bun4uk/repos","events_url":"https://api.github.com/users/bun4uk/events{/privacy}","received_events_url":"https://api.github.com/users/bun4uk/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2021-03-16T12:20:45Z","updated_at":"2021-04-07T16:07:03Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\nWe would like to monitor insert events into our ClickHouse cluster to be sure that our Data Lake works correctly. \r\nCurrently, we can check total rows inserts events using `system.metric_log` table and `ProfileEvent_InsertedRows` column. But we are looking at the way how to monitor each table separately. \r\n\r\n**Describe the solution you'd like**\r\nIt would be nice to have possibility to get `ProfileEvent_InsertedRows` metric and group it by table_name\r\n\r\n**Describe alternatives you've considered**\r\nCurrently, we created our own solution based on a bunch of materialized views for each table that we insert. These materialized views write data to a special table with the next structure: \r\n```\r\nâ”Œâ”€nameâ”€â”€â”€â”€â”€â”¬â”€typeâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€default_typeâ”€â”¬â”€default_expressionâ”€â”¬â”€commentâ”€â”¬â”€codec_expressionâ”€â”¬â”€ttl_expressionâ”€â”\r\nâ”‚ date     â”‚ Date                                                                           â”‚              â”‚                    â”‚         â”‚                  â”‚                â”‚\r\nâ”‚ ts       â”‚ DateTime                                                                       â”‚              â”‚                    â”‚         â”‚                  â”‚                â”‚\r\nâ”‚ table    â”‚ String                                                                         â”‚              â”‚                    â”‚         â”‚                  â”‚                â”‚\r\nâ”‚ rows     â”‚ Int64                                                                          â”‚              â”‚                    â”‚         â”‚                  â”‚                â”‚\r\nâ”‚ hostname â”‚ String                                                                         â”‚              â”‚                    â”‚         â”‚                  â”‚                â”‚\r\nâ”‚ source   â”‚ Enum8('kafka' = 1, 'mysql' = 2, 'matview' = 3, 'aggregation' = 4, 'other' = 5) â”‚ DEFAULT      â”‚ 'other'            â”‚         â”‚                  â”‚                â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\n**Additional context**\r\nBased on these metrics we created dashboards and are working on alerting system.\r\n![CleanShot 2021-03-16 at 14 07 17@2x](https://user-images.githubusercontent.com/14631690/111307879-aea68e00-8662-11eb-8b3a-a62b76c5ef86.png)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781/reactions","total_count":11,"+1":11,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21781/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21763","id":832307780,"node_id":"MDU6SXNzdWU4MzIzMDc3ODA=","number":21763,"title":"Ephemeral Credentials/GRANTS","user":{"login":"WesleyBatista","id":755254,"node_id":"MDQ6VXNlcjc1NTI1NA==","avatar_url":"https://avatars.githubusercontent.com/u/755254?v=4","gravatar_id":"","url":"https://api.github.com/users/WesleyBatista","html_url":"https://github.com/WesleyBatista","followers_url":"https://api.github.com/users/WesleyBatista/followers","following_url":"https://api.github.com/users/WesleyBatista/following{/other_user}","gists_url":"https://api.github.com/users/WesleyBatista/gists{/gist_id}","starred_url":"https://api.github.com/users/WesleyBatista/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/WesleyBatista/subscriptions","organizations_url":"https://api.github.com/users/WesleyBatista/orgs","repos_url":"https://api.github.com/users/WesleyBatista/repos","events_url":"https://api.github.com/users/WesleyBatista/events{/privacy}","received_events_url":"https://api.github.com/users/WesleyBatista/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-16T00:38:30Z","updated_at":"2021-03-17T04:48:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nConsidering the [Playground example](https://clickhouse.tech/docs/en/getting-started/playground/#examples) presented on the official docs, I wish it was possible to execute queries trough HTTP API/Native Client by providing a temporary user credentials, that would be vanished with its privileges after some specified time.\r\n\r\n**Describe the solution you'd like**\r\n\r\nImplement a **`TTL`** option/setting for the [`CREATE USER`](https://clickhouse.tech/docs/en/sql-reference/statements/create/user/) statement, so it is possible to create users that will be automatically removed on the background after a certain time. For example:\r\n```sql\r\nCREATE USER mira WITH TTL 1800\r\n```\r\n\r\n**Describe alternatives you've considered**\r\n\r\nPerhaps could also be implemented on the [`GRANT`](https://clickhouse.tech/docs/en/sql-reference/statements/grant/) statement:\r\n```sql\r\nGRANT SELECT ON my_test_database.* TO mira WITH TTL 1800\r\n```\r\n\r\n```sql\r\nGRANT role_containing_grants TO mira WITH TTL 1800\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21763/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21762","id":832289975,"node_id":"MDU6SXNzdWU4MzIyODk5NzU=","number":21762,"title":"ORDER BY + LIMIT BY optimization","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-15T23:55:39Z","updated_at":"2021-03-16T05:42:43Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n\r\nIn case of using ORDER BY LIMIT BY clause we don't need to sort all rows, we just need to take top/last N rows for each BY column set.\r\n\r\n`table from db_benchmark` taken from https://github.com/h2oai/db-benchmark generated with `Rscript _data/groupby-datagen.R 1e8 1e2 0 0`\r\n\r\n```\r\nCREATE TABLE db_bench (v3 Float64, id6 Int32) ENGINE=MergeTree ORDER BY tuple();\r\n\r\nINSERT INTO db_bench SELECT (rand() % 1000) / 10, number % 1000000 FROM numbers(100000000);\r\n\r\n\r\n\r\nLIMIT BY \r\n\r\nSELECT     id6,     v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC LIMIT 2 BY id6 FORMAT Null;\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 1.49 GiB.\r\n0 rows in set. Elapsed: 22.552 sec. Processed 100.00 million rows, 1.40 GB (4.43 million rows/s., 62.08 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 33.853 sec. Processed 100.00 million rows, 1.20 GB (2.95 million rows/s., 35.45 MB/s.)\r\n\r\n\r\nSELECT     id6,     v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC, id6  LIMIT 2 BY id6 FORMAT Null\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 1.49 GiB.\r\n0 rows in set. Elapsed: 22.500 sec. Processed 100.00 million rows, 1.40 GB (4.44 million rows/s., 62.22 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 55.473 sec. Processed 100.00 million rows, 1.20 GB (1.80 million rows/s., 21.63 MB/s.)\r\n\r\n\r\nSELECT id6, v3 FROM db_bench WHERE v3 is not Null ORDER BY id6, v3 DESC  LIMIT 2 BY id6 FORMAT Null\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 1.48 GiB.\r\n0 rows in set. Elapsed: 25.362 sec. Processed 100.00 million rows, 1.40 GB (3.94 million rows/s., 55.20 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 35.798 sec. Processed 100.00 million rows, 1.20 GB (2.79 million rows/s., 33.52 MB/s.)\r\n\r\n\r\nLIMIT BY with LIMIT\r\n\r\nSELECT     id6,     v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC LIMIT 2 BY id6 LIMIT 2000000 FORMAT Null;\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 1.48 GiB.\r\n0 rows in set. Elapsed: 4.238 sec. Processed 100.00 million rows, 1.40 GB (23.59 million rows/s., 330.33 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 8.225 sec. Processed 100.00 million rows, 1.20 GB (12.16 million rows/s., 145.89 MB/s.)\r\n\r\n\r\nWITH  (SELECT uniqExact(id6)*2 FROM db_bench WHERE v3 IS NOT Null) as x SELECT id6, v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC  LIMIT 2 BY id6 LIMIT x FORMAT Nullã€€;\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 1.51 GiB.\r\n0 rows in set. Elapsed: 4.724 sec. Processed 100.00 million rows, 1.40 GB (21.17 million rows/s., 296.34 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 9.172 sec. Processed 100.00 million rows, 1.20 GB (10.90 million rows/s., 130.83 MB/s.)\r\n\r\n\r\nWITH  (SELECT count()*2 FROM (SELECT 1 as x FROM db_bench WHERE v3 IS NOT NULL GROUP BY id6)) as x SELECT id6, v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC  LIMIT 2 BY id6 LIMIT x FORMAT Nullã€€;\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 2.98 GiB.\r\n0 rows in set. Elapsed: 4.939 sec. Processed 100.00 million rows, 1.40 GB (20.25 million rows/s., 283.47 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 1.24 GiB.\r\n0 rows in set. Elapsed: 11.777 sec. Processed 100.00 million rows, 1.20 GB (8.49 million rows/s., 101.89 MB/s.)\r\n\r\n\r\nGROUP BY with array manipulation\r\n\r\nSELECT id6, arrayJoin(arraySlice(arrayReverseSort(groupArray(v3)), 1, 2)) AS v3 FROM (SELECT id6, v3 FROM db_bench WHERE v3 IS NOT NULL) AS subq GROUP BY id6 FORMAT Null\r\n\r\ntable from db_benchmark\r\nPeak memory usage (for query): 7.31 GiB.\r\n0 rows in set. Elapsed: 7.064 sec. Processed 100.00 million rows, 1.40 GB (14.16 million rows/s., 198.18 MB/s.)\r\n\r\ngenerated table\r\nPeak memory usage (for query): 2.08 GiB.\r\n0 rows in set. Elapsed: 13.869 sec. Processed 100.00 million rows, 1.20 GB (7.21 million rows/s., 86.52 MB/s.)\r\n```\r\n\r\nMergeTree ORDER BY (v3)\r\n\r\n```\r\nCREATE TABLE db_bench\r\n(\r\n    `v3` Float64,\r\n    `id6` Int32\r\n)\r\nENGINE = MergeTree\r\nORDER BY v3\r\n\r\n\r\nOPTIMIZE TABLE db_bench FINAL;\r\n\r\nSELECT count()  FROM db_bench;\r\n\r\nâ”Œâ”€â”€â”€count()â”€â”\r\nâ”‚ 100000000 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n\r\nSELECT     id6,     v3 FROM db_bench WHERE v3 is not Null ORDER BY v3 DESC LIMIT 2 BY id6 FORMAT Null;\r\n\r\nReading 1529 ranges in reverse order from part all_1_96_3, approx. 100000000, up to 100000000 rows starting from 0\r\n\r\n0 rows in set. Elapsed: 14.174 sec. Processed 199.90 million rows, 2.40 GB (14.10 million rows/s., 169.24 MB/s.);\r\n\r\n```\r\n\r\n0 rows in set. Elapsed: 14.174 sec. Processed ***199.90*** million rows, 2.40 GB (14.10 million rows/s., 169.24 MB/s.);","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21762/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21756","id":832001489,"node_id":"MDU6SXNzdWU4MzIwMDE0ODk=","number":21756,"title":"optimize_aggregation_in_order, optimize_read_in_order respect subquery ordering","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"}],"state":"open","locked":false,"assignee":{"login":"KochetovNicolai","id":4092911,"node_id":"MDQ6VXNlcjQwOTI5MTE=","avatar_url":"https://avatars.githubusercontent.com/u/4092911?v=4","gravatar_id":"","url":"https://api.github.com/users/KochetovNicolai","html_url":"https://github.com/KochetovNicolai","followers_url":"https://api.github.com/users/KochetovNicolai/followers","following_url":"https://api.github.com/users/KochetovNicolai/following{/other_user}","gists_url":"https://api.github.com/users/KochetovNicolai/gists{/gist_id}","starred_url":"https://api.github.com/users/KochetovNicolai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KochetovNicolai/subscriptions","organizations_url":"https://api.github.com/users/KochetovNicolai/orgs","repos_url":"https://api.github.com/users/KochetovNicolai/repos","events_url":"https://api.github.com/users/KochetovNicolai/events{/privacy}","received_events_url":"https://api.github.com/users/KochetovNicolai/received_events","type":"User","site_admin":false},"assignees":[{"login":"KochetovNicolai","id":4092911,"node_id":"MDQ6VXNlcjQwOTI5MTE=","avatar_url":"https://avatars.githubusercontent.com/u/4092911?v=4","gravatar_id":"","url":"https://api.github.com/users/KochetovNicolai","html_url":"https://github.com/KochetovNicolai","followers_url":"https://api.github.com/users/KochetovNicolai/followers","following_url":"https://api.github.com/users/KochetovNicolai/following{/other_user}","gists_url":"https://api.github.com/users/KochetovNicolai/gists{/gist_id}","starred_url":"https://api.github.com/users/KochetovNicolai/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/KochetovNicolai/subscriptions","organizations_url":"https://api.github.com/users/KochetovNicolai/orgs","repos_url":"https://api.github.com/users/KochetovNicolai/repos","events_url":"https://api.github.com/users/KochetovNicolai/events{/privacy}","received_events_url":"https://api.github.com/users/KochetovNicolai/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2021-03-15T16:51:17Z","updated_at":"2021-07-15T12:28:05Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"1. `optimize_aggregation_in_order` (and `optimize_read_in_order`  but it's less important) would respect ORDER BY of subquery result \r\nset or subquery GROUP BY (if it's also optimized with `optimize_aggregation_in_order`).\r\n\r\n\r\n2. ORDER BY would respect ordering of GROUP BY result set also (if it's optimized with `optimize_aggregation_in_order`). \r\n```\r\nSELECT key FROM table (which is MergeTree() ORDER BY key) GROUP BY key ORDER BY key\r\n```\r\n\r\n\r\n\r\n**Use case**\r\n```\r\nCREATE TABLE test_aggregation_order(key UInt32, time UInt32) ENGINE=MergeTree() ORDER BY (key, time);\r\nINSERT INTO test_aggregation_order SELECT intDiv(number,10), number FROM numbers(202369185);\r\n\r\n\r\nSELECT\r\n    key,\r\n    groupArray(time)\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM test_aggregation_order\r\n    ORDER BY\r\n        key ASC,\r\n        time ASC\r\n)\r\nGROUP BY key\r\nFORMAT Null\r\n\r\nPeak memory usage (for query): 4.11 GiB.\r\n0 rows in set. Elapsed: 13.438 sec. Processed 202.37 million rows, 1.62 GB (15.06 million rows/s., 120.48 MB/s.)\r\n\r\nWITH arraySort(groupArray(time)) AS sorted\r\nSELECT\r\n    key,\r\n    sorted\r\nFROM test_aggregation_order\r\nGROUP BY key\r\nFORMAT Null\r\n\r\nPeak memory usage (for query): 4.04 GiB.\r\n0 rows in set. Elapsed: 4.876 sec. Processed 202.37 million rows, 1.62 GB (41.50 million rows/s., 332.03 MB/s.)\r\n\r\nset optimize_aggregation_in_order=1;\r\n\r\nWITH arraySort(groupArray(time)) AS sorted\r\nSELECT\r\n    key,\r\n    sorted\r\nFROM test_aggregation_order\r\nGROUP BY key\r\nFORMAT Null\r\n\r\nPeak memory usage (for query): 339.22 MiB.\r\n0 rows in set. Elapsed: 4.542 sec. Processed 202.37 million rows, 1.62 GB (44.56 million rows/s., 356.46 MB/s.)\r\n\r\nWITH arraySort(groupArray(time)) AS sorted\r\nSELECT\r\n    key,\r\n    sorted\r\nFROM test_aggregation_order\r\nGROUP BY key\r\nORDER BY key ASC\r\nFORMAT Null\r\n \r\nPeak memory usage (for query): 1.22 GiB.\r\n0 rows in set. Elapsed: 6.860 sec. Processed 202.37 million rows, 1.62 GB (29.50 million rows/s., 236.01 MB/s.)\r\n \r\nSELECT *\r\nFROM\r\n(\r\n    SELECT *\r\n    FROM test_aggregation_order\r\n    ORDER BY\r\n        key ASC,\r\n        time ASC\r\n)\r\nORDER BY\r\n    key ASC,\r\n    time ASC\r\nFORMAT Null\r\n\r\n Peak memory usage (for query): 1.50 GiB.\r\n\r\nSELECT *\r\nFROM test_aggregation_order\r\nORDER BY\r\n    key ASC,\r\n    time ASC\r\nFORMAT Null\r\n\r\nPeak memory usage (for query): 33.06 MiB.\r\n```\r\n\r\nActually optimize_duplicate_order_by_and_distinct optimization remove all ordering in sub queries and push them up.\r\n\r\n```\r\nEXPLAIN SYNTAX\r\nSELECT *\r\nFROM\r\n(\r\n    SELECT\r\n        key,\r\n        time\r\n    FROM test_aggregation_order\r\n    ORDER BY\r\n        key ASC,\r\n        time ASC\r\n)\r\nORDER BY\r\n    key ASC,\r\n    time ASC\r\n\r\nQuery id: 0f6544b5-9a75-4a92-98f6-1719a7125aac\r\n\r\nâ”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ SELECT                          â”‚\r\nâ”‚     key,                        â”‚\r\nâ”‚     time                        â”‚\r\nâ”‚ FROM                            â”‚\r\nâ”‚ (                               â”‚\r\nâ”‚     SELECT                      â”‚\r\nâ”‚         key,                    â”‚\r\nâ”‚         time                    â”‚\r\nâ”‚     FROM test_aggregation_order â”‚\r\nâ”‚ )                               â”‚\r\nâ”‚ ORDER BY                        â”‚\r\nâ”‚     key ASC,                    â”‚\r\nâ”‚     time ASC                    â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21756/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21740","id":831781735,"node_id":"MDU6SXNzdWU4MzE3ODE3MzU=","number":21740,"title":"Kafka Issue : Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.","user":{"login":"DipalPrajapati","id":3430373,"node_id":"MDQ6VXNlcjM0MzAzNzM=","avatar_url":"https://avatars.githubusercontent.com/u/3430373?v=4","gravatar_id":"","url":"https://api.github.com/users/DipalPrajapati","html_url":"https://github.com/DipalPrajapati","followers_url":"https://api.github.com/users/DipalPrajapati/followers","following_url":"https://api.github.com/users/DipalPrajapati/following{/other_user}","gists_url":"https://api.github.com/users/DipalPrajapati/gists{/gist_id}","starred_url":"https://api.github.com/users/DipalPrajapati/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DipalPrajapati/subscriptions","organizations_url":"https://api.github.com/users/DipalPrajapati/orgs","repos_url":"https://api.github.com/users/DipalPrajapati/repos","events_url":"https://api.github.com/users/DipalPrajapati/events{/privacy}","received_events_url":"https://api.github.com/users/DipalPrajapati/received_events","type":"User","site_admin":false},"labels":[{"id":1334071168,"node_id":"MDU6TGFiZWwxMzM0MDcxMTY4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-kafka","name":"comp-kafka","color":"b5bcff","default":false,"description":"Kafka Engine"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-15T12:54:05Z","updated_at":"2021-07-16T11:57:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have one topic my_topic with 4 partitions. \r\n\r\n```\r\n$ /opt/kafka/bin/kafka-topics.sh --bootstrap-server host1:9092,host2:9092,host3:9092 --describe --topic my_topic\r\nTopic: my_topic       PartitionCount: 4       ReplicationFactor: 2    Configs: compression.type=producer,segment.bytes=1073741824\r\n        Topic: my_topic       Partition: 0    Leader: 1002    Replicas: 1002,1001     Isr: 1002,1001\r\n        Topic: my_topic       Partition: 1    Leader: 1001    Replicas: 1001,1003     Isr: 1001,1003\r\n        Topic: my_topic       Partition: 2    Leader: 1003    Replicas: 1003,1002     Isr: 1003,1002\r\n        Topic: my_topic       Partition: 3    Leader: 1002    Replicas: 1002,1003     Isr: 1002,1003\r\n```\r\n\r\nIt has 3 consumer groups. \r\n\r\n```\r\nmy_topic_raw_group\r\nmy_topic_1_group\r\nmy_topic_2_group\r\n```\r\n\r\nWhich dumps to 3 different tables. \r\n\r\nHere is the create script. \r\n\r\nSHOW CREATE TABLE testKafkaRaw\r\nQuery id: bd494ea5-6303-4a3e-8ef3-61dd93be662b\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE TABLE dma.testKafkaRaw\r\n(\r\n     ----definition\r\n)\r\nENGINE = Kafka()\r\nSETTINGS kafka_broker_list = 'my brokers', kafka_topic_list = 'my_topic', kafka_group_name = 'my_topic_raw_group', kafka_num_consumers = '1', kafka_format = 'CSV', kafka_row_delimiter = ',' â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n1 rows in set. Elapsed: 0.008 sec. \r\nshow create table testKafkatable1;\r\nSHOW CREATE TABLE testKafkatable1\r\nQuery id: c7485709-8df5-4f67-a3d0-32ce95e9b5b9\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE TABLE dma.testKafkatable1\r\n(\r\n    ----definition\r\n)\r\nENGINE = Kafka()\r\nSETTINGS kafka_broker_list = 'my brokers', kafka_topic_list = 'my_topic', kafka_group_name = 'my_topic_1_group', kafka_num_consumers = '1', kafka_format = 'CSV', kafka_row_delimiter = ',' â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n1 rows in set. Elapsed: 0.009 sec. \r\n show create table testKafkatable2;\r\nSHOW CREATE TABLE testKafkatable2\r\nQuery id: b6f19cc1-9395-4f3e-a68b-8e3d1346b5ab\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE TABLE dma.testKafkatable2\r\n(\r\n      ----definition\r\n\r\n)\r\nENGINE = Kafka()\r\nSETTINGS kafka_broker_list = 'my brokers', kafka_topic_list = 'my_topic', kafka_group_name = 'my_topic_2_group', kafka_num_consumers = '1', kafka_format = 'CSV', kafka_row_delimiter = ',' â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\r\n\r\n\r\nIt is giving below error in clickhouse log. \r\n```\r\n2021.03.15 12:24:20.205372 [ 20718 ] {} <Trace> StorageKafka (testKafkaRaw): Nothing to commit.\r\n2021.03.15 12:24:20.206022 [ 20718 ] {} <Trace> StorageKafka (testKafkaRaw): Stream(s) stalled. Reschedule.\r\n2021.03.15 12:24:20.274580 [ 20711 ] {} <Debug> StorageKafka (testKafkatable2): Started streaming to 1 attached views\r\n2021.03.15 12:24:20.276009 [ 20711 ] {} <Trace> StorageKafka (testKafkatable2): Already subscribed to topics: [my_topic]\r\n2021.03.15 12:24:20.276115 [ 20711 ] {} <Trace> StorageKafka (testKafkatable2): Already assigned to: [  ]\r\n2021.03.15 12:24:20.321613 [ 20700 ] {} <Debug> StorageKafka (testKafkatable1): Started streaming to 1 attached views\r\n2021.03.15 12:24:20.326611 [ 20711 ] {} <Warning> StorageKafka (testKafkatable2): Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.\r\n2021.03.15 12:24:20.330550 [ 20711 ] {} <Trace> StorageKafka (testKafkatable2): Nothing to commit.\r\n2021.03.15 12:24:20.331374 [ 20711 ] {} <Trace> StorageKafka (testKafkatable2): Stream(s) stalled. Reschedule.\r\n2021.03.15 12:24:20.333877 [ 20700 ] {} <Trace> StorageKafka (testKafkatable1): Already subscribed to topics: [my_topic]\r\n2021.03.15 12:24:20.333970 [ 20700 ] {} <Trace> StorageKafka (testKafkatable1): Already assigned to: [  ]\r\n2021.03.15 12:24:20.384443 [ 20700 ] {} <Warning> StorageKafka (testKafkatable1): Can't get assignment. It can be caused by some issue with consumer group (not enough partitions?). Will keep trying.\r\n2021.03.15 12:24:20.384823 [ 20700 ] {} <Trace> StorageKafka (testKafkatable1): Nothing to commit.\r\n2021.03.15 12:24:20.385039 [ 20700 ] {} <Trace> StorageKafka (testKafkatable1): Stream(s) stalled. Reschedule.\r\n```\r\n\r\nData is not coming into the tables. \r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21740/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21721","id":831301803,"node_id":"MDU6SXNzdWU4MzEzMDE4MDM=","number":21721,"title":"A new setting to allow (suppress) errors during an insert into Materialized Views.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1350221495,"node_id":"MDU6TGFiZWwxMzUwMjIxNDk1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-matview","name":"comp-matview","color":"b5bcff","default":false,"description":"Materialized views"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-15T00:16:29Z","updated_at":"2021-03-15T07:52:23Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"A new setting for Materialized Views (e.g. suppress_insert_exceptions=1) to silently skip (suppress) errors.\r\n\r\nFor example we have a materialized view which logs inserts into table Engine=Remote and we want to skip insertion errors into this MV.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21721/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21697","id":830843870,"node_id":"MDU6SXNzdWU4MzA4NDM4NzA=","number":21697,"title":"INSERT ... RETURNING clause","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"},{"id":2121848770,"node_id":"MDU6TGFiZWwyMTIxODQ4Nzcw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-sql","name":"comp-sql","color":"b5bcff","default":false,"description":"General SQL support"}],"state":"open","locked":false,"assignee":{"login":"ucasfl","id":22127746,"node_id":"MDQ6VXNlcjIyMTI3NzQ2","avatar_url":"https://avatars.githubusercontent.com/u/22127746?v=4","gravatar_id":"","url":"https://api.github.com/users/ucasfl","html_url":"https://github.com/ucasfl","followers_url":"https://api.github.com/users/ucasfl/followers","following_url":"https://api.github.com/users/ucasfl/following{/other_user}","gists_url":"https://api.github.com/users/ucasfl/gists{/gist_id}","starred_url":"https://api.github.com/users/ucasfl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ucasfl/subscriptions","organizations_url":"https://api.github.com/users/ucasfl/orgs","repos_url":"https://api.github.com/users/ucasfl/repos","events_url":"https://api.github.com/users/ucasfl/events{/privacy}","received_events_url":"https://api.github.com/users/ucasfl/received_events","type":"User","site_admin":false},"assignees":[{"login":"ucasfl","id":22127746,"node_id":"MDQ6VXNlcjIyMTI3NzQ2","avatar_url":"https://avatars.githubusercontent.com/u/22127746?v=4","gravatar_id":"","url":"https://api.github.com/users/ucasfl","html_url":"https://github.com/ucasfl","followers_url":"https://api.github.com/users/ucasfl/followers","following_url":"https://api.github.com/users/ucasfl/following{/other_user}","gists_url":"https://api.github.com/users/ucasfl/gists{/gist_id}","starred_url":"https://api.github.com/users/ucasfl/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ucasfl/subscriptions","organizations_url":"https://api.github.com/users/ucasfl/orgs","repos_url":"https://api.github.com/users/ucasfl/repos","events_url":"https://api.github.com/users/ucasfl/events{/privacy}","received_events_url":"https://api.github.com/users/ucasfl/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-03-13T08:34:01Z","updated_at":"2021-11-24T14:32:05Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nInsert data and also return some result calculated on inserted dataset.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21697/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21694","id":830831322,"node_id":"MDU6SXNzdWU4MzA4MzEzMjI=","number":21694,"title":"`regexp_tree` dictionary type","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-03-13T07:18:57Z","updated_at":"2021-12-23T09:05:55Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Use case**\r\n\r\nUser-Agent parsing: #157.\r\nBut the implementation should be flexible enough to solve more string parsing/extraction tasks.\r\n\r\n\r\n**Describe the solution you'd like**\r\n\r\nThe dictionary data is represented by a tree of regular expressions.\r\n(actually not a tree but multiple trees - there can be multiple root nodes)\r\n\r\nEvery node of the tree can have:\r\n1. Regular expression. If it is matched, then children (if any) will be processed and setting the result attribute (if any, see 2) will be done. Regexp can have subpatterns that can be referenced later.\r\n2. Setting an attribute(s) to some value. Can contain references to previously matched subpatterns. Attribute is processed as a string and then casted to its type (in dictionary spec).\r\n3. Arbitrary number of children nodes of the same type.\r\n\r\nWhen attributes are requested from the dictionary, the tree will be traversed until every attribute is set. If there are multiple matching regexps that have branches setting the same attribute, the first (last?) wins. If the tree has been traversed but attribute was not set, the default value for attribute is assigned.\r\n\r\nThe data can be loaded from table (using any available dictionary source) with the following structure:\r\n\r\nid UInt64,\r\nparent_id UInt64,\r\nregexp String,\r\n*attr_1* String,\r\n...\r\n*attr_n* String\r\n\r\n`id` and `parent_id` only exists to make a tree structure.\r\n`attr_n` is the column for the corresponding attribute - its value is how to set the attribute if regexp matched, examples: `Internet Explorer` (just a string) or `Windows \\1` (string with referenced subpattern) or `\\2` (just a referenced subpattern, for example, if subpattern contains digits - it can be used to set numeric attribute).\r\n\r\nAlternatively, the tree can be specified in YAML file (config) in local filesystem - we should create `YAMLDictionarySource` for this purpose. In this case, `id` and `parent_id` are not specified (implicit).\r\n\r\nThe tree can have multiple children (alternative branches) with different regular expressions. We should combine these regular expressions to match in a single pass and select what branches are applicable. This is possible with Hyperscan. Maybe we should match all the regexps from all tree nodes at once and then traverse the tree just checking the bool flags.\r\n\r\n\r\n**Details**\r\n\r\nMaybe we can also allow to set a temporary (scratch) variables to reference it later in a tree.\r\n\r\nThe dictionary should support `LowCardinality(String)` types.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21694/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21660","id":829957927,"node_id":"MDU6SXNzdWU4Mjk5NTc5Mjc=","number":21660,"title":"ã€Timing triggerã€‘How to routinely delay statistics 3min before specifying the mean value and quantile value of the aggregated latitude","user":{"login":"shadowDy","id":18388143,"node_id":"MDQ6VXNlcjE4Mzg4MTQz","avatar_url":"https://avatars.githubusercontent.com/u/18388143?v=4","gravatar_id":"","url":"https://api.github.com/users/shadowDy","html_url":"https://github.com/shadowDy","followers_url":"https://api.github.com/users/shadowDy/followers","following_url":"https://api.github.com/users/shadowDy/following{/other_user}","gists_url":"https://api.github.com/users/shadowDy/gists{/gist_id}","starred_url":"https://api.github.com/users/shadowDy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/shadowDy/subscriptions","organizations_url":"https://api.github.com/users/shadowDy/orgs","repos_url":"https://api.github.com/users/shadowDy/repos","events_url":"https://api.github.com/users/shadowDy/events{/privacy}","received_events_url":"https://api.github.com/users/shadowDy/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-12T10:07:09Z","updated_at":"2021-03-15T06:38:36Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"sql demoï¼š\r\nselect aï¼Œavg(b)ï¼Œavg(c),quantile(0.900000)(b)/avg(d) bd,quantile(0.900000)(c)/avg(e) ce from xxxx where time='xxx' group by a\r\nfor example:\r\nwhen 10ï¼š56ï¼Œi want execute a sql with time param = 10:53\r\nwhen 10ï¼š57ï¼Œi want execute a sql with time param = 10:54\r\n\r\nThe results of each minute will be written into the new table y to provide queries, and each time the data of the past 3 hours in the y table will be queried\r\n\r\nDoes clickhouse have a timing triggerï¼Ÿ\r\nor How Materialized Views Support Timing Calculation of Quantile\r\n\r\nthanks","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21660/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21645","id":829464254,"node_id":"MDU6SXNzdWU4Mjk0NjQyNTQ=","number":21645,"title":"arrayMax, arrayMin, arrayDifference doesn't have support for Date, DateTime","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":1397894054,"node_id":"MDU6TGFiZWwxMzk3ODk0MDU0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unfinished%20code","name":"unfinished code","color":"ff8800","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-11T19:39:07Z","updated_at":"2021-11-25T16:09:47Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"\r\n\r\n**How to reproduce**\r\n```\r\nWITH\r\n    [now(), now() + 1] AS dt,\r\n    [today(), today() + 1] AS t\r\nSELECT\r\n    arrayMax(dt),\r\n    arrayMin(dt),\r\n    arrayDifference(dt),\r\n    arrayMax(t),\r\n    arrayMin(t),\r\n    arrayDifference(t)\r\n\r\nQuery id: bca32775-b17f-447e-a500-e9b2723e7dc9\r\n\r\n\r\n0 rows in set. Elapsed: 0.009 sec.\r\n\r\nReceived exception from server (version 21.4.1):\r\nCode: 43. DB::Exception: Received from localhost:9000. DB::Exception: array aggregation function cannot be performed on type DateTime: While processing arrayMax([now(), now() + 1] AS dt), arrayMin(dt), arrayDifference(dt), arrayMax([today(), today() + 1] AS t), arrayMin(t), arrayDifference(t).\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21645/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21628","id":829159149,"node_id":"MDU6SXNzdWU4MjkxNTkxNDk=","number":21628,"title":"uniqMap ( ? )","user":{"login":"asafcombo","id":12124039,"node_id":"MDQ6VXNlcjEyMTI0MDM5","avatar_url":"https://avatars.githubusercontent.com/u/12124039?v=4","gravatar_id":"","url":"https://api.github.com/users/asafcombo","html_url":"https://github.com/asafcombo","followers_url":"https://api.github.com/users/asafcombo/followers","following_url":"https://api.github.com/users/asafcombo/following{/other_user}","gists_url":"https://api.github.com/users/asafcombo/gists{/gist_id}","starred_url":"https://api.github.com/users/asafcombo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/asafcombo/subscriptions","organizations_url":"https://api.github.com/users/asafcombo/orgs","repos_url":"https://api.github.com/users/asafcombo/repos","events_url":"https://api.github.com/users/asafcombo/events{/privacy}","received_events_url":"https://api.github.com/users/asafcombo/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-11T13:36:18Z","updated_at":"2021-08-04T10:56:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I was wondering how to create or if there is already a function to do a uniqMap ( which should work in a similar manner to sumMap ).\r\nFor example given the dataset\r\n\r\n```\r\ndate        | id |  value\r\n2021-01-01  |  1 | 1\r\n2021-01-01  |  1 | 2\r\n2021-01-01  |  1 | 2\r\n2021-01-01  |  2 | 1\r\n2021-01-02  |  1 | 1\r\n```\r\n\r\nI can do \r\n\r\n```sql\r\n\r\nSELECT  id, sumMap( array(timestamp),  array(1)) as sm\r\nFROM table\r\nGROUP BY id\r\n\r\n```\r\n\r\nto get something like\r\n```\r\nid | sm\r\n1 | [ ['2021-01-01 ', '2021-01-02 '], [3, 1] ]\r\n2 | [ ['2021-01-01 '], [ 1] ]\r\n\r\n```\r\n\r\nNow, assume that instead of the sum of times a specific `id` is seen daily, what we want is to count the uniq `value` for this id, per day. So the function will look something like\r\n\r\n`uniqMap( array(timestamp),  value   )`\r\n\r\nand the result should be:\r\n\r\n```\r\nid | sm\r\n1 | [ ['2021-01-01 ', '2021-01-02 '], [2, 1] ]\r\n2 | [ ['2021-01-01 '], [ 1] ]\r\n\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21628/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21623","id":829041685,"node_id":"MDU6SXNzdWU4MjkwNDE2ODU=","number":21623,"title":"Mask MySQL engine credentials in `SHOW CREATE` commands","user":{"login":"OmarBazaraa","id":6713329,"node_id":"MDQ6VXNlcjY3MTMzMjk=","avatar_url":"https://avatars.githubusercontent.com/u/6713329?v=4","gravatar_id":"","url":"https://api.github.com/users/OmarBazaraa","html_url":"https://github.com/OmarBazaraa","followers_url":"https://api.github.com/users/OmarBazaraa/followers","following_url":"https://api.github.com/users/OmarBazaraa/following{/other_user}","gists_url":"https://api.github.com/users/OmarBazaraa/gists{/gist_id}","starred_url":"https://api.github.com/users/OmarBazaraa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/OmarBazaraa/subscriptions","organizations_url":"https://api.github.com/users/OmarBazaraa/orgs","repos_url":"https://api.github.com/users/OmarBazaraa/repos","events_url":"https://api.github.com/users/OmarBazaraa/events{/privacy}","received_events_url":"https://api.github.com/users/OmarBazaraa/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-11T11:02:17Z","updated_at":"2021-03-11T11:06:15Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Whenever you create a MySQL engine table or database, you can still get the credentials in plain text.\r\n\r\n```sql\r\nCREATE DATABASE IF NOT EXISTS db_mysql\r\nENGINE = MySQL('mysql-host:port', 'database', 'user', 'password');\r\n```\r\n\r\n```sql\r\nSHOW CREATE DATABASE db_mysql\r\n\r\nQuery id: efa65efe-fa1b-42cc-b931-f6acc21f9add\r\n\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE DATABASE db_mysql                                           â”‚\r\nâ”‚ ENGINE = MySQL('mysql-host:port', 'database', 'user', 'password')  â”‚\r\nâ”‚ SETTINGS mysql_datatypes_support_level = ''                        â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\nI'd like to get a masked version of the credentials, something like this:\r\n\r\n```sql\r\nENGINE = MySQL('mysql-host:port', 'database', '***', '***')\r\n```\r\n\r\nBecause whenever a user has read access on a database, he/she can issue a `SHOW CREATE` command and easily get the credentials.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21623/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21609","id":828293777,"node_id":"MDU6SXNzdWU4MjgyOTM3Nzc=","number":21609,"title":"ReplicatedMergeTree replica template is hardcoding database and table names after upgrade from 20.7.2.30 to 21.1.2","user":{"login":"alvarotuso","id":7347098,"node_id":"MDQ6VXNlcjczNDcwOTg=","avatar_url":"https://avatars.githubusercontent.com/u/7347098?v=4","gravatar_id":"","url":"https://api.github.com/users/alvarotuso","html_url":"https://github.com/alvarotuso","followers_url":"https://api.github.com/users/alvarotuso/followers","following_url":"https://api.github.com/users/alvarotuso/following{/other_user}","gists_url":"https://api.github.com/users/alvarotuso/gists{/gist_id}","starred_url":"https://api.github.com/users/alvarotuso/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alvarotuso/subscriptions","organizations_url":"https://api.github.com/users/alvarotuso/orgs","repos_url":"https://api.github.com/users/alvarotuso/repos","events_url":"https://api.github.com/users/alvarotuso/events{/privacy}","received_events_url":"https://api.github.com/users/alvarotuso/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-10T20:27:02Z","updated_at":"2021-03-10T22:10:06Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\n\r\nOn previous ClickHouse versions (earlier than 20.7.2.30) we've created tables using `ReplicatedMergeTree` with the following replica template:\r\n```\r\nCREATE TABLE mydb.mytable ON CLUSTER '{cluster}' (\r\n    `my_field` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/{installation}/{cluster}/tables/replicated/{database}/{table}', '{replica}')\r\nORDER BY (my_field)\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nRetrieving this info back produces the expected result:\r\n```\r\nSHOW CREATE TABLE mydb.mytable \r\n\r\nCREATE TABLE mydb.mytable \r\n(\r\n    `my_field` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/{installation}/{cluster}/tables/replicated/{database}/{table}', '{replica}')\r\nORDER BY my_field\r\nSETTINGS index_granularity = 8192\r\n```\r\nThe important portion being that the database and table parameters in the replica path remains in template form: `'/clickhouse/{installation}/{cluster}/tables/replicated/{database}/{table}'`\r\n\r\n\r\nHowever, in 21.1.2, the replica path template has a hardcoded table name after creation:\r\n```\r\nSHOW CREATE TABLE mydb.mytable \r\n\r\nCREATE TABLE mydb.mytable \r\n(\r\n    `my_field` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/{installation}/{cluster}/tables/replicated/mydb/mytable', '{replica}')\r\nORDER BY my_field\r\nSETTINGS index_granularity = 8192\r\n```\r\n\r\nThis causes several problems. For example if we try to create a new table using the original one as a base with an `AS` clause, the new table will try to replicate to the same paths and fail:\r\n\r\n```\r\nCREATE TABLE mydb.myothertable AS mydb.mytable\r\n\r\nReceived exception from server (version 21.1.2):\r\nCode: 253. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [mypod:9000]: Code: 253, e.displayText() = DB::Exception: Replica /clickhouse/myinstallation/cluster/tables/replicated/mydb/mytable/replicas/replica-name already exists. (version 21.1.2.15 (official build)).\r\n```\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 21.1.2\r\n* Step by step queries:\r\n```\r\nCREATE DATABASE mydb ON CLUSTER '{cluster}';\r\n\r\nCREATE TABLE mydb.mytable ON CLUSTER '{cluster}' (\r\n    `my_field` String\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/{installation}/{cluster}/tables/replicated/{database}/{table}', '{replica}')\r\nORDER BY (my_field)\r\nSETTINGS index_granularity = 8192;\r\n\r\nCREATE TABLE mydb.myothertable AS mydb.mytable;\r\n\r\nSQL Error [253]: ClickHouse exception, code: 253, host: localhost, port: 18125; Code: 253, e.displayText() = DB::Exception: There was an error on [mypod:9000]: Code: 253, e.displayText() = DB::Exception: Replica /clickhouse/myinstallation/cluster/tables/replicated/mydb/mytable/replicas/replica-name already exists. (version 21.1.2.15 (official build)) (version 21.1.2.15 (official build))\r\n```\r\n\r\n\r\n**Expected behavior**\r\nThe database and table name remain in template form in the table definition, which allows cloning and renaming tables without issues\r\n\r\n**Error message and/or stacktrace**\r\nThis is how the problem shows up:\r\n```\r\nCREATE TABLE mydb.myothertable AS mydb.mytable\r\n\r\nReceived exception from server (version 21.1.2):\r\nCode: 253. DB::Exception: Received from localhost:9000. DB::Exception: There was an error on [mypod:9000]: Code: 253, e.displayText() = DB::Exception: Replica /clickhouse/myinstallation/cluster/tables/replicated/mydb/mytable/replicas/replica-name already exists. (version 21.1.2.15 (official build)).\r\n```\r\n\r\n**Additional context**\r\n- We believe this problem might have started right after 20.7.2.30\r\n- **Weirdly, tables created before the update still work correctly, so it looks like the code that stores the definition is replacing the database and table name in the string**","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21609/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21607","id":828236628,"node_id":"MDU6SXNzdWU4MjgyMzY2Mjg=","number":21607,"title":"Support for views in odbc tables and dictionaries","user":{"login":"siradjev","id":10959667,"node_id":"MDQ6VXNlcjEwOTU5NjY3","avatar_url":"https://avatars.githubusercontent.com/u/10959667?v=4","gravatar_id":"","url":"https://api.github.com/users/siradjev","html_url":"https://github.com/siradjev","followers_url":"https://api.github.com/users/siradjev/followers","following_url":"https://api.github.com/users/siradjev/following{/other_user}","gists_url":"https://api.github.com/users/siradjev/gists{/gist_id}","starred_url":"https://api.github.com/users/siradjev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/siradjev/subscriptions","organizations_url":"https://api.github.com/users/siradjev/orgs","repos_url":"https://api.github.com/users/siradjev/repos","events_url":"https://api.github.com/users/siradjev/events{/privacy}","received_events_url":"https://api.github.com/users/siradjev/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":2034738737,"node_id":"MDU6TGFiZWwyMDM0NzM4NzM3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-odbc","name":"comp-odbc","color":"b5bcff","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-03-10T19:33:27Z","updated_at":"2021-08-17T18:30:42Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Use case**\r\nODBC is one of the main ways in clickhouse for making external integrations for slow changing data or dimensional data. \r\nQuite frequently data that needs to be brought in is a result of some complex query that may include joins or subqueries. \r\nAnother problem lies in having enormous overhead when only few columns are needed from the wide table, but clickhouse will select all of them. \r\nCurrently the only way to achieve it is to either create some sort of view on remote end, or use ETL to get processed data into Clickhouse. \r\nFrequently, in real life, the remote data source access is read-only, and does not allow creating objects on remote side, so solution is not really feasible. \r\n\r\n**Describe the solution you'd like**\r\nAllow view() like syntax for odbc data sources when using in dictionaries and tables. \r\nE.g.\r\n```\r\nCREATE DICTIONARY xxx....\r\n...\r\nSOURCE(\r\n   ODBC(\r\n       CONNECTION_STRING 'DSN=MYSRC' \r\n       TABLE 'SELECT TBL1.NAME, TBL2.NAME FROM TBL1 JOIN TBL2 ON TBL1.ID=TBLD2.ID' \r\n        INVALIDATE_QUERY 'SELECT MAX(LAST_DATA) FROM TBL2'\r\n    )\r\n)\r\n...\r\n```\r\n\r\nOf course in this case, user creating the query is responsible for matching columns to DDL of dictionary/table properly. \r\n\r\n**Describe alternatives you've considered**\r\nThere is no really other alternative except views (unnecessary objects + permissions issue) or custom data processing (huge overhead in terms of point of failures and data inconsistencies for simple task). \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21607/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21598","id":827947612,"node_id":"MDU6SXNzdWU4Mjc5NDc2MTI=","number":21598,"title":"Push-down predicates to the RIGHT table in JOIN","user":{"login":"OmarBazaraa","id":6713329,"node_id":"MDQ6VXNlcjY3MTMzMjk=","avatar_url":"https://avatars.githubusercontent.com/u/6713329?v=4","gravatar_id":"","url":"https://api.github.com/users/OmarBazaraa","html_url":"https://github.com/OmarBazaraa","followers_url":"https://api.github.com/users/OmarBazaraa/followers","following_url":"https://api.github.com/users/OmarBazaraa/following{/other_user}","gists_url":"https://api.github.com/users/OmarBazaraa/gists{/gist_id}","starred_url":"https://api.github.com/users/OmarBazaraa/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/OmarBazaraa/subscriptions","organizations_url":"https://api.github.com/users/OmarBazaraa/orgs","repos_url":"https://api.github.com/users/OmarBazaraa/repos","events_url":"https://api.github.com/users/OmarBazaraa/events{/privacy}","received_events_url":"https://api.github.com/users/OmarBazaraa/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-10T15:51:25Z","updated_at":"2021-05-18T08:00:51Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Queries with `JOIN` clause is taking much longer time than expected because predicates are not pushed down to the RIGHT table.\r\n\r\nConsider this simple query:\r\n\r\n```sql\r\nSELECT *\r\nFROM db.left_table\r\nGLOBAL LEFT JOIN db.right_table AS R USING (Id1, Id2, Id3)\r\nWHERE Id1 = <id>\r\nLIMIT 1\r\n\r\n1 rows in set. Elapsed: 2.178 sec. Processed 49.95 million rows, 1.35 GB (22.94 million rows/s., 619.26 MB/s.)\r\n```\r\n\r\nBy manually passing the `WHERE` predicate to the RIGHT table, the performance become much better:\r\n\r\n```sql\r\nSELECT *\r\nFROM db.left_table\r\nGLOBAL LEFT JOIN (\r\n    SELECT *\r\n    FROM db.right_table\r\n    WHERE Id1 = <id>\r\n)\r\nAS R\r\nUSING Id1, Id2, Id3\r\nWHERE Id1 = <id>\r\nLIMIT 1\r\n\r\n1 rows in set. Elapsed: 0.262 sec. Processed 1.70 million rows, 171.59 MB (6.46 million rows/s., 653.88 MB/s.)\r\n```\r\n\r\nI want to be able to create the following `VIEW` and access it without that performance degradation:\r\n\r\n```sql\r\nCREATE VIEW IF NOT EXISTS db.joined_view AS\r\nSELECT *\r\nFROM db.left_table\r\nGLOBAL LEFT JOIN db.right_table AS R USING (Id1, Id2, Id3)\r\n```\r\n\r\n```sql\r\nSELECT *\r\nFROM db.joined_view\r\nWHERE Id1 = <id>\r\nLIMIT 1\r\n```\r\n\r\n**Server version:** 21.1","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21598/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21584","id":827321186,"node_id":"MDU6SXNzdWU4MjczMjExODY=","number":21584,"title":"EXPLAIN SYNTAX produces query with ambiguous column","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-10T08:11:59Z","updated_at":"2021-04-13T13:26:40Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**How to reproduce**\r\nClickhouse 21.4\r\n```\r\nEXPLAIN SYNTAX\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nINNER JOIN tbl AS z ON x.key_1 = z.key_1\r\nFORMAT TSVRaw\r\n\r\nQuery id: 5582fbcf-2134-4dc7-89e8-d37e9ae3b060\r\n\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nALL INNER JOIN tbl AS z ON key_1 = z.key_1\r\n\r\n\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nALL INNER JOIN tbl AS z ON key_1 = z.key_1\r\n\r\nQuery id: b09e46b8-bf78-4882-903a-04150bf85821\r\n\r\n\r\n0 rows in set. Elapsed: 0.002 sec.\r\n\r\nReceived exception from server (version 21.4.1):\r\nCode: 352. DB::Exception: Received from localhost:9000. DB::Exception: Column 'key_1' is ambiguous: While processing key_1 = z.key_1.\r\n\r\nEXPLAIN SYNTAX\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nCROSS JOIN tbl AS z\r\nWHERE x.key_1 = z.key_1\r\nFORMAT TSVRaw\r\n\r\nQuery id: 4e196c98-54f3-40d4-ae8c-11e5f0800031\r\n\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nALL INNER JOIN tbl AS z ON key_1 = z.key_1\r\nWHERE key_1 = z.key_1\r\n\r\nWITH tbl AS\r\n    (\r\n        SELECT intDiv(number, 10) AS key_1\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nALL INNER JOIN tbl AS z ON key_1 = z.key_1\r\nWHERE key_1 = z.key_1\r\n\r\nQuery id: 9b50bb1d-9b4e-4508-8401-2498caf2b051\r\n\r\n\r\n0 rows in set. Elapsed: 0.004 sec.\r\n\r\nReceived exception from server (version 21.4.1):\r\nCode: 352. DB::Exception: Received from localhost:9000. DB::Exception: Ambiguous column 'key_1': While processing WITH tbl AS (SELECT intDiv(number, 10) AS key_1 FROM numbers(1000)) SELECT count() FROM tbl AS x ALL INNER JOIN tbl AS z ON key_1 = z.key_1 WHERE key_1 = z.key_1.\r\n```\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21584/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21583","id":827314042,"node_id":"MDU6SXNzdWU4MjczMTQwNDI=","number":21583,"title":"(cond_1 AND  cond_2) OR (cond_1 AND cond_3) optimization","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401777979,"node_id":"MDU6TGFiZWwxNDAxNzc3OTc5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-optimizers","name":"comp-optimizers","color":"b5bcff","default":false,"description":"Query optimizations"}],"state":"open","locked":false,"assignee":{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false},"assignees":[{"login":"CurtizJ","id":20361854,"node_id":"MDQ6VXNlcjIwMzYxODU0","avatar_url":"https://avatars.githubusercontent.com/u/20361854?v=4","gravatar_id":"","url":"https://api.github.com/users/CurtizJ","html_url":"https://github.com/CurtizJ","followers_url":"https://api.github.com/users/CurtizJ/followers","following_url":"https://api.github.com/users/CurtizJ/following{/other_user}","gists_url":"https://api.github.com/users/CurtizJ/gists{/gist_id}","starred_url":"https://api.github.com/users/CurtizJ/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/CurtizJ/subscriptions","organizations_url":"https://api.github.com/users/CurtizJ/orgs","repos_url":"https://api.github.com/users/CurtizJ/repos","events_url":"https://api.github.com/users/CurtizJ/events{/privacy}","received_events_url":"https://api.github.com/users/CurtizJ/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-03-10T08:05:55Z","updated_at":"2021-12-21T13:20:08Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Use case**\r\n(cond_1 AND  cond_2) OR (cond_1 AND cond_3) can be rewritten as (cond_1) AND ((cond_2) OR (cond_3))\r\nSo it would be possible to push cond_1 to the join clause and (cond_2 OR cond_3) to the WHERE clause if they both applied to one table.\r\n```\r\nEXPLAIN SYNTAX\r\nWITH tbl AS\r\n    (\r\n        SELECT\r\n            intDiv(number, 10) AS key_1,\r\n            number AS key_2\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\n, tbl AS z\r\nWHERE ((x.key_1 = z.key_1) AND (z.key_2 = 4)) OR ((x.key_1 = z.key_1) AND (z.key_2 = 1))\r\n\r\nQuery id: 4fc73b13-a62b-488c-ad7f-de01cf79f86b\r\n\r\nâ”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ WITH tbl AS                                                                          â”‚\r\nâ”‚     (                                                                                â”‚\r\nâ”‚         SELECT                                                                       â”‚\r\nâ”‚             intDiv(number, 10) AS key_1,                                             â”‚\r\nâ”‚             number AS key_2                                                          â”‚\r\nâ”‚         FROM numbers(1000)                                                           â”‚\r\nâ”‚     )                                                                                â”‚\r\nâ”‚ SELECT count()                                                                       â”‚\r\nâ”‚ FROM tbl AS x                                                                        â”‚\r\nâ”‚ CROSS JOIN tbl AS z                                                                  â”‚\r\nâ”‚ WHERE ((key_1 = z.key_1) AND (z.key_2 = 4)) OR ((key_1 = z.key_1) AND (z.key_2 = 1)) â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nEXPLAIN SYNTAX\r\nWITH tbl AS\r\n    (\r\n        SELECT\r\n            intDiv(number, 10) AS key_1,\r\n            number AS key_2\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\n, tbl AS z\r\nWHERE (x.key_1 = z.key_1) AND ((z.key_2 = 4) OR (z.key_2 = 1))\r\n\r\nQuery id: 633699eb-e4a6-486d-81bc-9b00d08da382\r\n\r\nâ”Œâ”€explainâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ WITH tbl AS                                                  â”‚\r\nâ”‚     (                                                        â”‚\r\nâ”‚         SELECT                                               â”‚\r\nâ”‚             intDiv(number, 10) AS key_1,                     â”‚\r\nâ”‚             number AS key_2                                  â”‚\r\nâ”‚         FROM numbers(1000)                                   â”‚\r\nâ”‚     )                                                        â”‚\r\nâ”‚ SELECT count()                                               â”‚\r\nâ”‚ FROM tbl AS x                                                â”‚\r\nâ”‚ ALL INNER JOIN tbl AS z ON key_1 = z.key_1                   â”‚\r\nâ”‚ WHERE (key_1 = z.key_1) AND ((z.key_2 = 4) OR (z.key_2 = 1)) â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\nCan be rewritten as\r\n\r\n```\r\nWITH tbl AS\r\n    (\r\n        SELECT\r\n            intDiv(number, 10) AS key_1,\r\n            number AS key_2\r\n        FROM numbers(1000)\r\n    )\r\nSELECT count()\r\nFROM tbl AS x\r\nINNER JOIN\r\n(\r\n    SELECT *\r\n    FROM tbl\r\n    WHERE (key_2 = 4) OR (key_2 = 1)\r\n) AS z ON x.key_1 = z.key_1\r\nWHERE ((key_1 = z.key_1) AND (z.key_2 = 4)) OR ((key_1 = z.key_1) AND (z.key_2 = 1))\r\n```\r\n\r\n**Additional context**\r\nRelated to https://github.com/ClickHouse/ClickHouse/issues/19856\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21583/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21558","id":826063637,"node_id":"MDU6SXNzdWU4MjYwNjM2Mzc=","number":21558,"title":"ARRAY JOIN query using IN on an array field with a bloomfilter throws an exception.","user":{"login":"dstruck","id":2195318,"node_id":"MDQ6VXNlcjIxOTUzMTg=","avatar_url":"https://avatars.githubusercontent.com/u/2195318?v=4","gravatar_id":"","url":"https://api.github.com/users/dstruck","html_url":"https://github.com/dstruck","followers_url":"https://api.github.com/users/dstruck/followers","following_url":"https://api.github.com/users/dstruck/following{/other_user}","gists_url":"https://api.github.com/users/dstruck/gists{/gist_id}","starred_url":"https://api.github.com/users/dstruck/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dstruck/subscriptions","organizations_url":"https://api.github.com/users/dstruck/orgs","repos_url":"https://api.github.com/users/dstruck/repos","events_url":"https://api.github.com/users/dstruck/events{/privacy}","received_events_url":"https://api.github.com/users/dstruck/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1401233890,"node_id":"MDU6TGFiZWwxNDAxMjMzODkw","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-arrays","name":"comp-arrays","color":"b5bcff","default":false,"description":"Arrays / array joins / higher order"},{"id":1507888214,"node_id":"MDU6TGFiZWwxNTA3ODg4MjE0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-accepted","name":"st-accepted","color":"e5b890","default":false,"description":"The issue is in our backlog, ready to take"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-09T15:00:12Z","updated_at":"2021-07-02T23:57:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\nPerforming a query using `ARRAY JOIN` and `IN` on an array field using a bloomfilter throws an exception. As queries using the function `has` work I suppose this type of query should/could also work?\r\n\r\n**Does it reproduce on recent release?**\r\nTested on v21.2.5.5-stable (2021-03-02).\r\n\r\n**How to reproduce**\r\n\r\nTest table:\r\n\r\n```\r\nCREATE TABLE test (\r\n    id UInt16, \r\n    ts DateTime,\r\n    data Array(String),\r\n    INDEX test_bloom data TYPE bloom_filter GRANULARITY 1\r\n) ENGINE = MergeTree()\r\nPARTITION BY toYYYYMM(ts)\r\nORDER BY id;\r\n\r\nINSERT INTO test VALUES (1, '2021-01-01', ['aaa','bbb']);\r\nINSERT INTO test VALUES (2, '2021-01-01', ['ccc']);\r\n```\r\n\r\nThis query works as intended: `SELECT id FROM test WHERE has(data, 'ccc');`.\r\n\r\nThe query `SELECT id FROM test ARRAY JOIN data WHERE data IN ('aaa');` throws the exception **Code: 130. DB::Exception: Received from localhost:9000. DB::Exception: Array does not start with '[' character.**.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21558/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21557","id":825909356,"node_id":"MDU6SXNzdWU4MjU5MDkzNTY=","number":21557,"title":"EXPLAIN SYNTAX calculation stuck with complex query","user":{"login":"UnamedRus","id":9449405,"node_id":"MDQ6VXNlcjk0NDk0MDU=","avatar_url":"https://avatars.githubusercontent.com/u/9449405?v=4","gravatar_id":"","url":"https://api.github.com/users/UnamedRus","html_url":"https://github.com/UnamedRus","followers_url":"https://api.github.com/users/UnamedRus/followers","following_url":"https://api.github.com/users/UnamedRus/following{/other_user}","gists_url":"https://api.github.com/users/UnamedRus/gists{/gist_id}","starred_url":"https://api.github.com/users/UnamedRus/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/UnamedRus/subscriptions","organizations_url":"https://api.github.com/users/UnamedRus/orgs","repos_url":"https://api.github.com/users/UnamedRus/repos","events_url":"https://api.github.com/users/UnamedRus/events{/privacy}","received_events_url":"https://api.github.com/users/UnamedRus/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":2730601367,"node_id":"MDU6TGFiZWwyNzMwNjAxMzY3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.2-affected","name":"v21.2-affected","color":"c2bfff","default":false,"description":""},{"id":2825253639,"node_id":"MDU6TGFiZWwyODI1MjUzNjM5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.3-affected","name":"v21.3-affected","color":"c2bfff","default":false,"description":""},{"id":2941252908,"node_id":"MDU6TGFiZWwyOTQxMjUyOTA4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.4-affected","name":"v21.4-affected","color":"c2bfff","default":false,"description":""},{"id":3167230064,"node_id":"MDU6TGFiZWwzMTY3MjMwMDY0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.8-affected","name":"v21.8-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-03-09T13:13:20Z","updated_at":"2021-12-25T00:36:25Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**How to reproduce**\r\nClickhouse version 21.2.3, 21.4.1.6198 (from pr https://github.com/ClickHouse/ClickHouse/pull/20392)\r\n```\r\nset max_ast_depth=10000;\r\n```\r\n[tpcds.txt](https://github.com/ClickHouse/ClickHouse/files/6108686/tpcds.txt)\r\n\r\n\r\n**Additional context**\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21557/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21504","id":823874967,"node_id":"MDU6SXNzdWU4MjM4NzQ5Njc=","number":21504,"title":" Bring more results of distributed table query on different machines","user":{"login":"15537261738","id":30434210,"node_id":"MDQ6VXNlcjMwNDM0MjEw","avatar_url":"https://avatars.githubusercontent.com/u/30434210?v=4","gravatar_id":"","url":"https://api.github.com/users/15537261738","html_url":"https://github.com/15537261738","followers_url":"https://api.github.com/users/15537261738/followers","following_url":"https://api.github.com/users/15537261738/following{/other_user}","gists_url":"https://api.github.com/users/15537261738/gists{/gist_id}","starred_url":"https://api.github.com/users/15537261738/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/15537261738/subscriptions","organizations_url":"https://api.github.com/users/15537261738/orgs","repos_url":"https://api.github.com/users/15537261738/repos","events_url":"https://api.github.com/users/15537261738/events{/privacy}","received_events_url":"https://api.github.com/users/15537261738/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-07T09:44:18Z","updated_at":"2021-03-09T07:27:44Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"query: select count(1) from tablename where event_day='2021-03-06'\r\nThe following are the query results of count(1) on different machines\r\ncluster-1: 10494720\r\ncluster-2:10493949 \r\ncluster-3:10502865\r\nLocal table engine for distributed tables is ReplicatedReplacingMergeTree.\r\nThe local table is a materialized view of Kafka.\r\nIs this structure reasonable?\r\nI want to know the reason for this difference\r\nthanks","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21504/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21495","id":823628665,"node_id":"MDExOlB1bGxSZXF1ZXN0NTg2MDQ4Mzk1","number":21495,"title":"Release pull request for branch 21.3","user":{"login":"robot-clickhouse","id":41385210,"node_id":"MDQ6VXNlcjQxMzg1MjEw","avatar_url":"https://avatars.githubusercontent.com/u/41385210?v=4","gravatar_id":"","url":"https://api.github.com/users/robot-clickhouse","html_url":"https://github.com/robot-clickhouse","followers_url":"https://api.github.com/users/robot-clickhouse/followers","following_url":"https://api.github.com/users/robot-clickhouse/following{/other_user}","gists_url":"https://api.github.com/users/robot-clickhouse/gists{/gist_id}","starred_url":"https://api.github.com/users/robot-clickhouse/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/robot-clickhouse/subscriptions","organizations_url":"https://api.github.com/users/robot-clickhouse/orgs","repos_url":"https://api.github.com/users/robot-clickhouse/repos","events_url":"https://api.github.com/users/robot-clickhouse/events{/privacy}","received_events_url":"https://api.github.com/users/robot-clickhouse/received_events","type":"User","site_admin":false},"labels":[{"id":1261360622,"node_id":"MDU6TGFiZWwxMjYxMzYwNjIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/release","name":"release","color":"34d3a1","default":false,"description":"Label for release pull request"},{"id":2107435505,"node_id":"MDU6TGFiZWwyMTA3NDM1NTA1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/submodule%20changed","name":"submodule changed","color":"b7130b","default":false,"description":"At least one submodule changed in this PR."},{"id":2588535996,"node_id":"MDU6TGFiZWwyNTg4NTM1OTk2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/release-lts","name":"release-lts","color":"58e89b","default":false,"description":"LTS release-branch (label name is hardcoded into backport automation)"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-06T11:43:34Z","updated_at":"2022-01-26T11:46:11Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/21495","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21495","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/21495.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/21495.patch","merged_at":null},"body":"\nThis PullRequest is part of ClickHouse release cycle. It's used by CI system only. Don't perform any changes with it.\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21495/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21478","id":823191855,"node_id":"MDU6SXNzdWU4MjMxOTE4NTU=","number":21478,"title":"Distributed query with global join reads redundant columns in second table","user":{"login":"DimasKovas","id":34828390,"node_id":"MDQ6VXNlcjM0ODI4Mzkw","avatar_url":"https://avatars.githubusercontent.com/u/34828390?v=4","gravatar_id":"","url":"https://api.github.com/users/DimasKovas","html_url":"https://github.com/DimasKovas","followers_url":"https://api.github.com/users/DimasKovas/followers","following_url":"https://api.github.com/users/DimasKovas/following{/other_user}","gists_url":"https://api.github.com/users/DimasKovas/gists{/gist_id}","starred_url":"https://api.github.com/users/DimasKovas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DimasKovas/subscriptions","organizations_url":"https://api.github.com/users/DimasKovas/orgs","repos_url":"https://api.github.com/users/DimasKovas/repos","events_url":"https://api.github.com/users/DimasKovas/events{/privacy}","received_events_url":"https://api.github.com/users/DimasKovas/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-05T15:29:24Z","updated_at":"2021-03-09T06:43:18Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the situation**\r\nIn case of distributed query with global join all columns are read from the second table (instead of only required ones).\r\n**How to reproduce**\r\nAdd logging `LOG_DEBUG(log, \"XXX Reading from <storage name> (Query: {})\", DB::serializeAST(*query_info.query));` to read method in StorageDistributed and StorageMergeTree. Then try queries described below.\r\n* Which ClickHouse server version to use\r\nLatest version in Arcadia.\r\n\r\n* `CREATE TABLE` statements for all tables involved\r\n```sql\r\ncreate table \"t0\" (a Int64, b Int64) engine = MergeTree() partition by a order by a;\r\ncreate table \"t1\" (a Int64, b Int64) engine = MergeTree() partition by a order by a;\r\ncreate table \"dist_t0\" (a Int64, b Int64) engine = Distributed(test_shard_localhost, default, t0);\r\n```\r\n* Sample data for all these tables\r\n```sql\r\ninsert into t0 values (0, 0);\r\ninsert into t1 values (0, 0);\r\n```\r\n* Queries to run that lead to slow performance\r\n```sql\r\nselect a from dist_t0 global join t1 using a\r\n\r\nSELECT a\r\nFROM dist_t0\r\nGLOBAL INNER JOIN t1 USING (a)\r\n```\r\n**Expected performance**\r\nRows from the log:\r\n```sql\r\n<Debug> default.t1 (63c0abb0-09ba-4a77-8265-59782a1d0373): XXX Reading from merge tree (Query: SELECT a, b FROM t1)\r\n<Debug> StorageDistributed (dist_t0): XXX Reading from distributed (Query: SELECT a FROM dist_t0 GLOBAL ALL INNER JOIN t1 AS t1 USING (a))\r\n<Debug> default.t0 (841477be-8c67-4299-a2bd-d02c3480412b): XXX Reading from merge tree (Query: SELECT a FROM default.t0 GLOBAL ALL INNER JOIN t1 AS t1 USING (a))\r\n```\r\n2 column were read from t1 ('a' and 'b'), while only 'a' column is requested.\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21478/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21474","id":822987394,"node_id":"MDExOlB1bGxSZXF1ZXN0NTg1NTE5MzE1","number":21474,"title":"Throw exception if removing parts from ZooKeeper fails.","user":{"login":"nvartolomei","id":543193,"node_id":"MDQ6VXNlcjU0MzE5Mw==","avatar_url":"https://avatars.githubusercontent.com/u/543193?v=4","gravatar_id":"","url":"https://api.github.com/users/nvartolomei","html_url":"https://github.com/nvartolomei","followers_url":"https://api.github.com/users/nvartolomei/followers","following_url":"https://api.github.com/users/nvartolomei/following{/other_user}","gists_url":"https://api.github.com/users/nvartolomei/gists{/gist_id}","starred_url":"https://api.github.com/users/nvartolomei/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/nvartolomei/subscriptions","organizations_url":"https://api.github.com/users/nvartolomei/orgs","repos_url":"https://api.github.com/users/nvartolomei/repos","events_url":"https://api.github.com/users/nvartolomei/events{/privacy}","received_events_url":"https://api.github.com/users/nvartolomei/received_events","type":"User","site_admin":false},"labels":[{"id":1302792342,"node_id":"MDU6TGFiZWwxMzAyNzkyMzQy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-bugfix","name":"pr-bugfix","color":"ff4080","default":false,"description":"Pull request with bugfix, not backported by default"}],"state":"open","locked":false,"assignee":{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false},"assignees":[{"login":"tavplubix","id":14847450,"node_id":"MDQ6VXNlcjE0ODQ3NDUw","avatar_url":"https://avatars.githubusercontent.com/u/14847450?v=4","gravatar_id":"","url":"https://api.github.com/users/tavplubix","html_url":"https://github.com/tavplubix","followers_url":"https://api.github.com/users/tavplubix/followers","following_url":"https://api.github.com/users/tavplubix/following{/other_user}","gists_url":"https://api.github.com/users/tavplubix/gists{/gist_id}","starred_url":"https://api.github.com/users/tavplubix/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/tavplubix/subscriptions","organizations_url":"https://api.github.com/users/tavplubix/orgs","repos_url":"https://api.github.com/users/tavplubix/repos","events_url":"https://api.github.com/users/tavplubix/events{/privacy}","received_events_url":"https://api.github.com/users/tavplubix/received_events","type":"User","site_admin":false}],"milestone":null,"comments":3,"created_at":"2021-03-05T11:12:27Z","updated_at":"2022-01-21T11:00:43Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/21474","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21474","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/21474.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/21474.patch","merged_at":null},"body":"This is used for removing part metadata from ZooKeeper when executing\r\nqueue events like `DROP_RANGE` triggered when a user tries to drop a\r\npart or a partition. There are other uses but I'll focus only on this\r\none.\r\n\r\nBefore this change the method was giving up silently if it was unable to\r\nremove parts from ZooKeeper and this behaviour seems to be problematic.\r\nIt could lead to operation being reported as successful at first but\r\ndata reappearing later (very rarely) or \"stuck\" events in replication\r\nqueue.\r\n\r\nHere is one particular scenario which I think we've hit:\r\n\r\n* Execute a DETACH PARTITION\r\n* DROP_RANGE event put in the queue\r\n* Replicas try to execute dropRange but some of them get disconnected\r\n  from ZK and 5 retries aren't enough (ZK is miss-behaving), return code\r\n  (false) is ignored and log pointer advances.\r\n* One of the replica where dropRange failed is restarted.\r\n* checkParts is executed and it finds parts that weren't removed from\r\n  ZK, logs `Removing locally missing part from ZooKeeper and queueing a\r\n  fetch` and puts GET_PART on the queue.\r\n* Few things can happen from here:\r\n  * There is a lagging replica that din't execute DROP_RANGE yet: part will be\r\n    fetched. The other replica will execute DROP_RANGE later and we'll\r\n    get diverging set of parts on replicas.\r\n  * Another replica also silently failed to remove parts from ZK: both\r\n    of them are left with GET_PART in the queue and none of them can\r\n    make progress, logging: `No active replica has part ... or covering\r\n    part`.\r\n\r\nI hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- Improvement\r\n\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n\r\n...","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21474/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21471","id":822940696,"node_id":"MDU6SXNzdWU4MjI5NDA2OTY=","number":21471,"title":"Dictionaries dictGetHierarchy, dictIsIn complex keys support","user":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351463315,"node_id":"MDU6TGFiZWwxMzUxNDYzMzE1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-dictionary","name":"comp-dictionary","color":"b5bcff","default":false,"description":"Dictionaries"}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":1,"created_at":"2021-03-05T10:08:34Z","updated_at":"2021-12-23T08:57:48Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Currently dictionary hierarchy methods `dictGetHierarchy`, `dictIsIn ` supported only for dictionaries with simple key. But there can be use cases where this features can be necessary for complex keys, for example complex key that contains only 1 String attribute.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21471/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21469","id":822825671,"node_id":"MDU6SXNzdWU4MjI4MjU2NzE=","number":21469,"title":"Query fails when it is wrapped with `select * from (...)`","user":{"login":"DimasKovas","id":34828390,"node_id":"MDQ6VXNlcjM0ODI4Mzkw","avatar_url":"https://avatars.githubusercontent.com/u/34828390?v=4","gravatar_id":"","url":"https://api.github.com/users/DimasKovas","html_url":"https://github.com/DimasKovas","followers_url":"https://api.github.com/users/DimasKovas/followers","following_url":"https://api.github.com/users/DimasKovas/following{/other_user}","gists_url":"https://api.github.com/users/DimasKovas/gists{/gist_id}","starred_url":"https://api.github.com/users/DimasKovas/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/DimasKovas/subscriptions","organizations_url":"https://api.github.com/users/DimasKovas/orgs","repos_url":"https://api.github.com/users/DimasKovas/repos","events_url":"https://api.github.com/users/DimasKovas/events{/privacy}","received_events_url":"https://api.github.com/users/DimasKovas/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1507867504,"node_id":"MDU6TGFiZWwxNTA3ODY3NTA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-hold","name":"st-hold","color":"e5b890","default":false,"description":"We've paused the work on issue for some reason"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-05T07:26:46Z","updated_at":"2021-07-03T00:02:48Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the bug**\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\nLatest version in Arcadia.\r\n* `CREATE TABLE` statements for all tables involved\r\n```sql\r\ndi.man.yp-c.yandex.net :) create table t_str (creation_time String) engine = MergeTree() partition by creation_time order by creation_time\r\n\r\nCREATE TABLE t_str\r\n(\r\n    `creation_time` String\r\n)\r\nENGINE = MergeTree\r\nPARTITION BY creation_time\r\nORDER BY creation_time\r\n```\r\n* Sample data for all these tables\r\n```sql\r\ninsert into t_str values ('2020-02-02')\r\n```\r\n* Queries to run that lead to unexpected result\r\nThis query works well:\r\n```sql\r\ndi.man.yp-c.yandex.net :) select 1 as x from t_str where cast('1970-01-01' as date) <= cast((select max('1970-01-01') from numbers(1)) as date)\r\n\r\nSELECT 1 AS x\r\nFROM t_str\r\nWHERE CAST('1970-01-01', 'date') <= CAST(\r\n(\r\n    SELECT max('1970-01-01')\r\n    FROM numbers(1)\r\n), 'date')\r\n\r\nQuery id: 6ff4732e-88e3-4c95-856d-03f7a0ba494b\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.012 sec. \r\n```\r\nBut when it is wrapped with `select * from (...)` it throws an error:\r\n```sql\r\ndi.man.yp-c.yandex.net :) select * from ( select 1 as x from t_str where cast('1970-01-01' as date) <= cast((select max('1970-01-01') from numbers(1)) as date) )\r\n\r\nSELECT *\r\nFROM \r\n(\r\n    SELECT 1 AS x\r\n    FROM t_str\r\n    WHERE CAST('1970-01-01', 'date') <= CAST(\r\n    (\r\n        SELECT max('1970-01-01')\r\n        FROM numbers(1)\r\n    ), 'date')\r\n)\r\n\r\nQuery id: c9c2e5b1-5bc5-4068-a653-a46f9c568cf0\r\n\r\n\r\n0 rows in set. Elapsed: 0.007 sec. \r\n\r\nReceived exception from server (version 21.3.1):\r\nCode: 38. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse date: value is too short: Cannot parse Date from String: while executing 'FUNCTION CAST(_subquery2 : 7, 'date' : 2) -> CAST(_subquery2, 'date') Date : 8'. \r\n```\r\n**Expected behavior**\r\nThe second query should return the same value as the first one.\r\n\r\n**Error message and/or stacktrace**\r\nReceived exception from server (version 21.3.1):\r\nCode: 38. DB::Exception: Received from localhost:9000. DB::Exception: Cannot parse date: value is too short: Cannot parse Date from String: while executing 'FUNCTION CAST(_subquery2 : 7, 'date' : 2) -> CAST(_subquery2, 'date') Date : 8'. \r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21469/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21451","id":822121953,"node_id":"MDU6SXNzdWU4MjIxMjE5NTM=","number":21451,"title":"Found parts with the same min block and with the same max block as the missing part","user":{"login":"BhavyaRajSharma","id":40774184,"node_id":"MDQ6VXNlcjQwNzc0MTg0","avatar_url":"https://avatars.githubusercontent.com/u/40774184?v=4","gravatar_id":"","url":"https://api.github.com/users/BhavyaRajSharma","html_url":"https://github.com/BhavyaRajSharma","followers_url":"https://api.github.com/users/BhavyaRajSharma/followers","following_url":"https://api.github.com/users/BhavyaRajSharma/following{/other_user}","gists_url":"https://api.github.com/users/BhavyaRajSharma/gists{/gist_id}","starred_url":"https://api.github.com/users/BhavyaRajSharma/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BhavyaRajSharma/subscriptions","organizations_url":"https://api.github.com/users/BhavyaRajSharma/orgs","repos_url":"https://api.github.com/users/BhavyaRajSharma/repos","events_url":"https://api.github.com/users/BhavyaRajSharma/events{/privacy}","received_events_url":"https://api.github.com/users/BhavyaRajSharma/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-04T13:14:43Z","updated_at":"2021-06-11T07:42:56Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Problem Statement:**\r\nI was trying to ingest data into clickhouse and getting following warning\r\n\r\n```\r\n_1. Hoping that it will eventually appear as a result of a merge.\r\n2021.03.04 15:29:14.323625 [ 28764 ] {} <Warning> dbpolaris1.visit_local (ReplicatedMergeTreePartCheckThread): Checking part 20180922_70_75_1\r\n2021.03.04 15:29:14.323956 [ 28764 ] {} <Warning> dbpolaris1.visit_local (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 20180922_70_75_1.\r\n2021.03.04 15:29:14.324997 [ 28764 ] {} <Warning> dbpolaris1.visit_local (ReplicatedMergeTreePartCheckThread): Found parts with the same min block and with the same max block as the missing part 20180922_70_75_1. Hoping that it will eventually appear as a result of a merge.\r\n\r\n```\r\n\r\n\t\t\r\n**VISIT CREATE COMMAND:**\r\n\r\n> \t\tCREATE TABLE dbpolaris1.visit\r\n> \t\t(\r\n> \t\t\t`imsi` Int64 CODEC(Delta(8), Default),\r\n> \t\t\t`msisdn` Nullable(String),\r\n> \t\t\t`imei` Nullable(String),\r\n> \t\t\t`visitstartdate` DateTime CODEC(Delta(4), Default),\r\n> \t\t\t`visitenddate` DateTime CODEC(Delta(4), Default),\r\n> \t\t\t`spid` Int64 CODEC(T64, Default),\r\n> \t\t\t`ver` DateTime CODEC(Delta(4), Default)\r\n> \t\t)\r\n> \t\tENGINE = Distributed('API', '', 'visit_local', imsi % 2)\r\n> \r\n\r\n**VISIT_LOCAL CREATE COMMAND:**\r\n\r\n> \t\tCREATE TABLE dbpolaris1.visit_local\r\n> \t\t(\r\n> \t\t\t`imsi` Int64 CODEC(Delta(8), Default),\r\n> \t\t\t`msisdn` Nullable(String),\r\n> \t\t\t`imei` Nullable(String),\r\n> \t\t\t`visitstartdate` DateTime CODEC(Delta(4), Default),\r\n> \t\t\t`visitenddate` DateTime CODEC(Delta(4), Default),\r\n> \t\t\t`spid` Int64 CODEC(T64, Default),\r\n> \t\t\t`ver` DateTime CODEC(Delta(4), Default),\r\n> \t\t\tINDEX imsi_index1 imsi TYPE minmax GRANULARITY 3,\r\n> \t\t\tINDEX visitstartdate_index2 visitstartdate TYPE minmax GRANULARITY 1,\r\n> \t\t\tINDEX visitenddate_index3 visitenddate TYPE minmax GRANULARITY 1,\r\n> \t\t\tINDEX spid_index4 spid TYPE minmax GRANULARITY 1\r\n> \t\t)\r\n> \t\tENGINE = ReplicatedReplacingMergeTree('/clickhouse/tables/{shard1}/visit', '{replica1}', ver)\r\n> \t\tPARTITION BY toYYYYMMDD(visitstartdate)\r\n> \t\tORDER BY (imsi, visitstartdate)\r\n> \r\n\r\n**API CLUSTER:**\r\n\r\n```\r\n<API>\r\n            <shard>\r\n            <internal_replication>true</internal_replication>\r\n                    <replica>\r\n                            <default_database>dbpolaris1</default_database>\r\n                            <host>10.49.3.109</host>\r\n                            <port>9100</port>\r\n                    </replica>\r\n                    <replica>\r\n                            <default_database>dbpolaris1</default_database>\r\n                            <host>10.49.3.111</host>\r\n                            <port>9100</port>\r\n                    </replica>\r\n            </shard>\r\n    </API>\r\n```\r\n\r\n**ClickHouse Version Used**\r\nClickHouse client version 20.12.5.14 (official build).","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21451/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21412","id":820982541,"node_id":"MDU6SXNzdWU4MjA5ODI1NDE=","number":21412,"title":"WITH FILL with interpolation","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-03T10:40:10Z","updated_at":"2021-06-27T12:21:34Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```\r\nSELECT *\r\nFROM values('pos UInt16, val UInt16', (1, 10), (13, 130), (15, 150), (20, 200))\r\nORDER BY pos ASC WITH FILL\r\n\r\nQuery id: 78385df6-a0cf-49e1-bf3b-440ae0b4c626\r\n\r\nâ”Œâ”€posâ”€â”¬â”€valâ”€â”\r\nâ”‚   1 â”‚  10 â”‚\r\nâ”‚   2 â”‚   0 â”‚\r\nâ”‚   3 â”‚   0 â”‚\r\nâ”‚   4 â”‚   0 â”‚\r\nâ”‚   5 â”‚   0 â”‚\r\nâ”‚   6 â”‚   0 â”‚\r\nâ”‚   7 â”‚   0 â”‚\r\nâ”‚   8 â”‚   0 â”‚\r\nâ”‚   9 â”‚   0 â”‚\r\nâ”‚  10 â”‚   0 â”‚\r\nâ”‚  11 â”‚   0 â”‚\r\nâ”‚  12 â”‚   0 â”‚\r\nâ”‚  13 â”‚ 130 â”‚\r\nâ”‚  14 â”‚   0 â”‚\r\nâ”‚  15 â”‚ 150 â”‚\r\nâ”‚  16 â”‚   0 â”‚\r\nâ”‚  17 â”‚   0 â”‚\r\nâ”‚  18 â”‚   0 â”‚\r\nâ”‚  19 â”‚   0 â”‚\r\nâ”‚  20 â”‚ 200 â”‚\r\nâ””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜\r\n\r\n20 rows in set. Elapsed: 0.001 sec. \r\n```\r\n\r\nI want interpolated numbers instead of zeros in val column.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21412/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21389","id":820413528,"node_id":"MDU6SXNzdWU4MjA0MTM1Mjg=","number":21389,"title":"I can CREATE TABLE ... AS ... SETTINGS ... but the settings are ignored.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":2532986061,"node_id":"MDU6TGFiZWwyNTMyOTg2MDYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-query-syntax","name":"comp-query-syntax","color":"b5bcff","default":false,"description":"Relates to query parse / aliases resolution etc."}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-03-02T21:40:26Z","updated_at":"2021-03-03T10:53:02Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"```\r\nminicrawl.ru-central1.internal :) CREATE TABLE minicrawl2 AS minicrawl SETTINGS max_compress_block_size = 65536\r\n\r\nCREATE TABLE minicrawl2 AS minicrawl\r\nSETTINGS max_compress_block_size = 65536\r\n\r\nQuery id: c5a88ed8-cee7-4f1e-97e5-c9e9ac640bbf\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.009 sec. \r\n\r\nminicrawl.ru-central1.internal :) SHOW CREATE TABLE minicrawl2\r\n\r\nSHOW CREATE TABLE minicrawl2\r\n\r\nQuery id: 2061854f-6abc-4bc5-a4a6-fcdd7c27ed7d\r\n\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE TABLE default.minicrawl2\r\n(\r\n    `rank` UInt32,\r\n    `domain` String,\r\n    `log` String CODEC(ZSTD(6)),\r\n    `content` String CODEC(ZSTD(6))\r\n)\r\nENGINE = MergeTree\r\nORDER BY domain\r\nSETTINGS index_granularity = 8192 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.002 sec.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21389/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21386","id":820308205,"node_id":"MDU6SXNzdWU4MjAzMDgyMDU=","number":21386,"title":"SELECT asterisk behaviour","user":{"login":"mzealey","id":6083471,"node_id":"MDQ6VXNlcjYwODM0NzE=","avatar_url":"https://avatars.githubusercontent.com/u/6083471?v=4","gravatar_id":"","url":"https://api.github.com/users/mzealey","html_url":"https://github.com/mzealey","followers_url":"https://api.github.com/users/mzealey/followers","following_url":"https://api.github.com/users/mzealey/following{/other_user}","gists_url":"https://api.github.com/users/mzealey/gists{/gist_id}","starred_url":"https://api.github.com/users/mzealey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzealey/subscriptions","organizations_url":"https://api.github.com/users/mzealey/orgs","repos_url":"https://api.github.com/users/mzealey/repos","events_url":"https://api.github.com/users/mzealey/events{/privacy}","received_events_url":"https://api.github.com/users/mzealey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-02T19:16:11Z","updated_at":"2021-03-02T19:16:11Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"In postgres and many other databases, with tables `t1(a,b)` and `t2(y,z)`, `select * from t1, t2` or `select t1.*, t2.* from t1, t2` would produce results `(a,b,y,z)`. `select t1.*, y from t1,t2` would produce results `(a,b,y)`. In clickhouse currently there is only the option of `select * from t1,t2` which produces `(t1.a,t1.b,t2.y,t2.z)`, or `select a,b,y,z from t1,t2` which produces `(a,b,y,z)` but is really bad to type if you have lots of columns in tables. (I know you shouldn't really do this from normal tables, but in the case where you have a number of CTEs that you want to join together in the final statement it is really userful). For some design reason that I don't really understand, in clickhouse `select t1.*` only works if you have 1 column it won't allow you to select 2 of them.\r\n\r\nSo I'd really like it if `select t1.*, t2.* from t1,t2` produce column name output of `(a,b,y,z)` (ie without the table names prefixed, because they have been explicitly selected by the user from the tables, and allowing asterisk expansion in similarity with other SQL dialects.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21386/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21384","id":820303782,"node_id":"MDU6SXNzdWU4MjAzMDM3ODI=","number":21384,"title":"Int128 max/min misbehavior","user":{"login":"smagellan","id":1594938,"node_id":"MDQ6VXNlcjE1OTQ5Mzg=","avatar_url":"https://avatars.githubusercontent.com/u/1594938?v=4","gravatar_id":"","url":"https://api.github.com/users/smagellan","html_url":"https://github.com/smagellan","followers_url":"https://api.github.com/users/smagellan/followers","following_url":"https://api.github.com/users/smagellan/following{/other_user}","gists_url":"https://api.github.com/users/smagellan/gists{/gist_id}","starred_url":"https://api.github.com/users/smagellan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/smagellan/subscriptions","organizations_url":"https://api.github.com/users/smagellan/orgs","repos_url":"https://api.github.com/users/smagellan/repos","events_url":"https://api.github.com/users/smagellan/events{/privacy}","received_events_url":"https://api.github.com/users/smagellan/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-02T19:10:04Z","updated_at":"2021-03-02T21:58:14Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the unexpected behaviour**\r\nA clear and concise description of what works not as it is supposed to.\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use: 21.2.5 revision 54447\r\n* Which interface to use, if matters: clickhouse-client 21.2.5.5 (official build)\r\n* Queries to run that lead to unexpected result: SELECT CAST(-170141183460469231731687303715884105728, 'Int128') as min_val, CAST(170141183460469231731687303715884105727, 'Int128') as max_val;\r\n* Queries to run that lead to unexpected result: SELECT CAST('-170141183460469231731687303715884105728', 'Nullable(Int128)') as min_val, CAST('170141183460469231731687303715884105727', 'Nullable(Int128)') as max_val;\r\n\r\n**Expected behavior**\r\nFirst query gives min_val = -170141183460469231731687303715884105728, max_val = -170141183460469231731687303715884105728 (-170141183460469231731687303715884105728 and 170141183460469231731687303715884105727 expected). (Compare with this (sane) query: SELECT CAST('-170141183460469231731687303715884105728', 'Int128') as min_val, CAST('170141183460469231731687303715884105727', 'Int128') as max_val;)\r\nSecond query(with Nullable(Int128)) ) gives min_val = NULL and max_val = 170141183460469231731687303715884105727 (-170141183460469231731687303715884105728 and 170141183460469231731687303715884105727 expected)\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21384/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21364","id":819612921,"node_id":"MDU6SXNzdWU4MTk2MTI5MjE=","number":21364,"title":"Implicit distributed table + cluster/local_table definition (RFC)","user":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1634829961,"node_id":"MDU6TGFiZWwxNjM0ODI5OTYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-distributed","name":"comp-distributed","color":"b5bcff","default":false,"description":"Distributed tables"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2021-03-02T04:51:53Z","updated_at":"2021-05-09T19:06:55Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"## Main goal: Provide a way to define cluster, build distributed table with given cluster and simplify local table management.\r\n\r\n\r\n### ImplicitDistributed table creation\r\n\r\n```\r\ncreate table <table_name> (...)\r\nengine ImplicitDistributed(<cluster_name>[, sharding_policy])  <---  cluster provided\r\n```\r\nLocal table names are deduced by <table_name> and <cluster_name>. For simplicity, we ensure that the same database is used. ImplicitDistributed table engine provides two modes for reading (distribute, local)\r\n\r\n1. distribute: it sends queries to all needed shards without modifying the query, but attach the local_table_names for reading via settings (or other mechanisms)\r\n\r\n2. local: it just reads the needed subtables\r\n\r\n### Declarative cluster creation\r\n```\r\ncreate cluster <cluster_name> [on cluster <cluster>]\r\n(\r\n    'endpoint1' [priority, compression],\r\n    'endpoint2' [priority, compression],\r\n    'endpoint3' [priority, compression],\r\n    ...\r\n)\r\nshard_num = 2,\r\nreplica_num = 3,\r\nuser = ...,\r\npassword = ...,\r\nsalt = 0\r\n```\r\nThis will generate a cluster configuration using provided endpoint list. Different shards might share the same endpoint, while any shard (a replica group) will only contain different endpoints. If not possible, error will thrown. Given a fixed list with equal salt, the generated cluster configuration will be the same.\r\n\r\n\r\n### Local table creation\r\n\r\n```\r\ncreate local table [if not exists] db_name.table_name for cluster <cluster_name> [all]\r\n(\r\n    table definition grammar\r\n)\r\nengine ...\r\n```\r\n\r\nLocal table names are deduced using the same logic of `ImplicitDistributed` engine. This command mainly serves the purpose to provide a user-friendly way of setting up local tables. if `all` is specified, `on cluster <cluster_name>` will be used.\r\n\r\n\r\n## Examples\r\n\r\nSuppose we have a `base` cluster definition that contains all nodes hosting query requests.\r\n\r\nstep 0:\r\n\r\n```\r\ncreate cluster example_cluster on cluster base ( 'foo:9000', 'bar:9000`, 'baz:9000' )  shard_num = 2, replica_num = 3\r\n```\r\n\r\nIt generates the following cluster configuration:\r\n\r\n```\r\n<example_cluster>\r\n    <shard>\r\n        <replica>\r\n            <host>foo</host><port>9000</port>\r\n            <host>bar</host><port>9000</port>\r\n            <host>baz</host><port>9000</port>\r\n        </replica>\r\n    </shard>\r\n    <shard>\r\n        <replica>\r\n            <host>bar</host><port>9000</port>\r\n            <host>baz</host><port>9000</port>\r\n            <host>foo</host><port>9000</port>\r\n        </replica>\r\n    </shard>\r\n</example_cluster>\r\n```\r\n\r\nstep 1:\r\n```\r\n    create table distributed_table (i int) engine ImplicitDistributed(example_cluster);\r\n```\r\n\r\nIn this step, `distributed_table` will calculate the local table names of each endpoint, and construct a deterministic mapping. It'll be like this\r\n\r\n```\r\n<example_cluster>\r\n    <shard>\r\n        <replica>\r\n            <host>foo</host><port>9000</port>   --- local table uuid_1\r\n            <host>bar</host><port>9000</port>   --- local table uuid_2\r\n            <host>baz</host><port>9000</port>   --- local table uuid_3\r\n        </replica>\r\n    </shard>\r\n    <shard>\r\n        <replica>\r\n            <host>bar</host><port>9000</port>   --- local table uuid_4\r\n            <host>baz</host><port>9000</port>   --- local table uuid_5\r\n            <host>foo</host><port>9000</port>   --- local table uuid_6\r\n        </replica>\r\n    </shard>\r\n</example_cluster>\r\n```\r\n\r\nstep 2:\r\n```\r\n    select * from distributed_table;\r\n```\r\n\r\nIn this step, `distributed_table` will gather the endpoints to send via our existing mechanism. It will failover unavailable nodes.\r\n\r\nSuppose we've selected `uuid_1` and `uuid_4`. It'll then send query to `foo:9000` and `bar:9000` with these local table names.\r\n\r\nIf we select `uuid_1` and `uuid_6`, it'll find that only `foo:9000` is accessed. Then it sends the query to `foo:9000`, and attach local table names (be it uuid_1 and uuid_6).\r\n\r\nNote, those local table names are deduced deterministicly, and we provide CREATE LOCAL TABLE constructs to help setting up local tables.\r\n\r\n## Possible addons\r\n```\r\nalter cluster <cluster_name> [on cluster] ... modify salt;\r\nalter cluster <cluster_name> [on cluster] ... replace <endpoint_a> with <endpoint_b>;\r\nalter cluster <cluster_name> [on cluster] ... drop <endpoint>;\r\nalter cluster <cluster_name> [on cluster] ... add <endpoint>;\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364/reactions","total_count":10,"+1":8,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":2},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21364/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21363","id":819597705,"node_id":"MDExOlB1bGxSZXF1ZXN0NTgyNjg1MzQ3","number":21363,"title":"Allow to replace partition in the same table","user":{"login":"amosbird","id":5085485,"node_id":"MDQ6VXNlcjUwODU0ODU=","avatar_url":"https://avatars.githubusercontent.com/u/5085485?v=4","gravatar_id":"","url":"https://api.github.com/users/amosbird","html_url":"https://github.com/amosbird","followers_url":"https://api.github.com/users/amosbird/followers","following_url":"https://api.github.com/users/amosbird/following{/other_user}","gists_url":"https://api.github.com/users/amosbird/gists{/gist_id}","starred_url":"https://api.github.com/users/amosbird/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/amosbird/subscriptions","organizations_url":"https://api.github.com/users/amosbird/orgs","repos_url":"https://api.github.com/users/amosbird/repos","events_url":"https://api.github.com/users/amosbird/events{/privacy}","received_events_url":"https://api.github.com/users/amosbird/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-03-02T04:20:05Z","updated_at":"2022-01-24T11:50:34Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/21363","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21363","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/21363.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/21363.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- New Feature\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n\r\nAllow to replace and update partitions in merge tree tables. It's useful to implement partition rollups and atomic data insertion (with staging partitions). This depends on https://github.com/ClickHouse/ClickHouse/pull/21327 . This is for https://github.com/ClickHouse/ClickHouse/issues/18695\r\n\r\nDetailed description / Documentation draft:\r\n\r\n~~`ALTER TABLE <name> REPLACE PARTITION <partition_expr> UPDATE <update_expr> WHERE <filter_expr>`~~\r\n\r\n~~`ALTER TABLE <name> REPLACE PARTITION <partition_expr> UPDATE <update_expr> IN PARTITION <partition_expr>`~~\r\n\r\n`ALTER TABLE <name> REPLACE PARTITION <partition_expr> FROM PARTITION <partition_expr> UPDATE <update_expr>`\r\n\r\nThe mutation will be run in foreground. It'll check if the mutated parts have the target partition_id, or else fail. It'll filling all minmax columns to make sure we correctly update the partition.dat.\r\n\r\nNOTE: Users should avoid using this feature during rolling upgrades, when some replicas don't have this feature.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21363/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21359","id":819395658,"node_id":"MDU6SXNzdWU4MTkzOTU2NTg=","number":21359,"title":"Sharding expression to return multiple shards and dictGet to return multiple entries","user":{"login":"yonzhang","id":2691355,"node_id":"MDQ6VXNlcjI2OTEzNTU=","avatar_url":"https://avatars.githubusercontent.com/u/2691355?v=4","gravatar_id":"","url":"https://api.github.com/users/yonzhang","html_url":"https://github.com/yonzhang","followers_url":"https://api.github.com/users/yonzhang/followers","following_url":"https://api.github.com/users/yonzhang/following{/other_user}","gists_url":"https://api.github.com/users/yonzhang/gists{/gist_id}","starred_url":"https://api.github.com/users/yonzhang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yonzhang/subscriptions","organizations_url":"https://api.github.com/users/yonzhang/orgs","repos_url":"https://api.github.com/users/yonzhang/repos","events_url":"https://api.github.com/users/yonzhang/events{/privacy}","received_events_url":"https://api.github.com/users/yonzhang/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"},{"id":1634829961,"node_id":"MDU6TGFiZWwxNjM0ODI5OTYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-distributed","name":"comp-distributed","color":"b5bcff","default":false,"description":"Distributed tables"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-03-01T23:16:45Z","updated_at":"2021-03-03T08:22:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nWe are considering to manage sharding through dictionary in order to do live resharding.(We had asked this question here, https://github.com/ClickHouse/ClickHouse/issues/10943) \r\n\r\nHere is one typical use case: a user's data might be stored in multiple shards(but this count is still much less than the total number of shards), so in order to query this user's data, sharding expression should return multiple shards. Each user will have a fixed number of buckets and we store the mapping from {user, bucket} to a shard in a dictionary and at query time, the sharding expression can query the dictionary by user only and return multiple shards. \r\n\r\nuser, bucket, shardId\r\nuser_1, 1, 1\r\nuser_1, 2, 1\r\nuser_2, 1, 2\r\nuser_2, 2, 2\r\n\r\nSharding expression example: distinctShardList(dictGet('dict_name', 'attr_name', 'key')) where key is the user mentioned above, dictGet will return multiple entries for this user, and distinctShardList is a new function which returns multiple distinct shards given an integer list.\r\n\r\n**Describe the solution you'd like**\r\nWe need a dictionary to return multiple entries given partial keys, but currently the dictionary requires all keys present at least for COMPLEX_KEY_HASHED dictionary. So is there any existing dictionary which can be queried by partial keys and return multiple entries.\r\n\r\n```\r\nvoid DictionaryStructure::validateKeyTypes(const DataTypes & key_types) const\r\n{\r\n    if (key_types.size() != key->size())\r\n        throw Exception{\"Key structure does not match, expected \" + getKeyDescription(), ErrorCodes::TYPE_MISMATCH};\r\n```\r\n\r\nAlso we need sharding exprssion to return multiple shards, but currently sharding expression requires only a single integer to be returned. See the code in StorageDistributed.cpp\r\n```\r\n if (!type->isValueRepresentedByInteger())\r\n                throw Exception(\"Sharding expression has type \" + type->getName() +\r\n                    \", but should be one of integer type\", ErrorCodes::TYPE_MISMATCH);\r\n\r\n```\r\n\r\n**Describe alternatives you've considered**\r\nIs there any reason behind for below?\r\n1. why dictGet can only support query with all keys present at least for complex_key_hashed dictionary. Are there any issues that we develop a new dictionary to support query with partial keys and return multiple entries?\r\n2. why sharding expression does not allow multiple shards to be returned. Are there any issues that we modify Clickhouse code to support sharding expression to return multiple shards?\r\n\r\n**Additional context**\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21359/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21358","id":819250526,"node_id":"MDU6SXNzdWU4MTkyNTA1MjY=","number":21358,"title":"force_data_skipping_indices='' to disable loading data skipping indexes?","user":{"login":"mzealey","id":6083471,"node_id":"MDQ6VXNlcjYwODM0NzE=","avatar_url":"https://avatars.githubusercontent.com/u/6083471?v=4","gravatar_id":"","url":"https://api.github.com/users/mzealey","html_url":"https://github.com/mzealey","followers_url":"https://api.github.com/users/mzealey/followers","following_url":"https://api.github.com/users/mzealey/following{/other_user}","gists_url":"https://api.github.com/users/mzealey/gists{/gist_id}","starred_url":"https://api.github.com/users/mzealey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzealey/subscriptions","organizations_url":"https://api.github.com/users/mzealey/orgs","repos_url":"https://api.github.com/users/mzealey/repos","events_url":"https://api.github.com/users/mzealey/events{/privacy}","received_events_url":"https://api.github.com/users/mzealey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":6,"created_at":"2021-03-01T20:45:18Z","updated_at":"2021-03-03T02:25:08Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"If I have a big table, a string column with a bloom filter on it, and I know I'm going to hit the most common strings in the table then I know that I don't want to use the bloom index on it - it can take quite a bit of additional time to load prior to starting to run the query. It would be nice if there was an option to disable using any data skipping indexes in this case. I would suggest that `SETTING force_data_skipping_indices=''` might be a good way to flag this?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21358/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21350","id":818972530,"node_id":"MDU6SXNzdWU4MTg5NzI1MzA=","number":21350,"title":"Unexpected growth of memory consumption","user":{"login":"joein","id":22641570,"node_id":"MDQ6VXNlcjIyNjQxNTcw","avatar_url":"https://avatars.githubusercontent.com/u/22641570?v=4","gravatar_id":"","url":"https://api.github.com/users/joein","html_url":"https://github.com/joein","followers_url":"https://api.github.com/users/joein/followers","following_url":"https://api.github.com/users/joein/following{/other_user}","gists_url":"https://api.github.com/users/joein/gists{/gist_id}","starred_url":"https://api.github.com/users/joein/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/joein/subscriptions","organizations_url":"https://api.github.com/users/joein/orgs","repos_url":"https://api.github.com/users/joein/repos","events_url":"https://api.github.com/users/joein/events{/privacy}","received_events_url":"https://api.github.com/users/joein/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1532016596,"node_id":"MDU6TGFiZWwxNTMyMDE2NTk2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/memory","name":"memory","color":"e99695","default":false,"description":"When memory usage is higher than expected"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-01T15:23:53Z","updated_at":"2021-03-12T13:14:58Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We had had some unexpected growth of memory consumption leaded us to exhaustion of memory on the server.\r\nLoad was average for us and there weren't any extra reasons  for memory growth.\r\nNow we'r trying to figure out what it had been and what we should do next.\r\nThe first steps were to limit `max_memory_usage` option. As we suppose, it can help to avoid crashes in the future but it is not a decision for the problem. \r\n\r\nWe'll appreciate, if you can take us a few hints or share some ideas what we can do, i.e. are there any periodic jobs, which can increase memory consumption by 1.5x or something similar?\r\n\r\nI'll attach some memory consumption screenshots and describe our tables below.\r\n<img width=\"1221\" alt=\"1\" src=\"https://user-images.githubusercontent.com/22641570/109518214-184b6780-7abb-11eb-88b6-599beb987d51.png\">\r\n<img width=\"1202\" alt=\"2\" src=\"https://user-images.githubusercontent.com/22641570/109518236-1bdeee80-7abb-11eb-9409-6c31c73045af.png\">\r\n\r\n\r\n\r\n1. This table is both entrypoint for new data and the place from which data is requested later, it has no connection with other tables in scheme.\r\n\r\nCREATE TABLE db.TableA\r\n(\r\n    `field_1` Nullable(Int32),\r\n    `field_2` Nullable(Int32),\r\n    `field_3` Nullable(String),\r\n    `field_4` Nullable(String),\r\n    `field_5` String,\r\n    `field_6` DateTime,\r\n    `field_7` DateTime,\r\n    `field_8` String,\r\n    `field_9` Nullable(Int32),\r\n    `field_10` Nullable(Int32),\r\n    `field_11` Nullable(Int32),\r\n    `field_12` Nullable(Int32),\r\n    `field_13` Nullable(Float64),\r\n    `field_14` Nullable(String),\r\n    `field_15` Nullable(String),\r\n    `field_16` Nullable(Int32),\r\n    `field_17` Nullable(Int64),\r\n    `field_18` Nullable(Int64),\r\n    `field_19` Nullable(Float64),\r\n    `field_20` Nullable(String),\r\n    `field_21` Nullable(Float64),\r\n    `field_22` Nullable(Float64)\r\n)\r\nENGINE = ReplicatedMergeTree('/clickhouse/tables/{shard}/db/TableA', '{replica}')\r\nORDER BY field_6\r\nSETTINGS index_granularity = 8192\r\n\r\n2. We have 2 such tables (the second table has some more fields and it uses  ReplicatedReplacingMergeTree, I think it won't be very helpful to place both of the tables here. Let's call the second table `TableC` further)\r\nCREATE TABLE db.TableB\r\n(\r\n    `A` String,\r\n    `B` String,\r\n    `C` String,\r\n    `D` AggregateFunction(max, Int64),\r\n    `E` AggregateFunction(min, Int64)\r\n)\r\nENGINE = ReplicatedAggregatingMergeTree('/clickhouse/tables/{shard}/db/TableB', '{replica}')\r\nPRIMARY KEY (A, B, C)\r\nORDER BY (A, B, C)\r\nSETTINGS index_granularity = 8192\r\n\r\n3. It is the main entrypoint for data (btw it receives data as protobuf single)\r\nCREATE TABLE db.entrypoint_2\r\n(\r\n   `field_1` String,\r\n   `DE` Int64,\r\n   `field_3` Int32,\r\n   `field_4` Int32,\r\n   `A` String,\r\n   `B` String,\r\n   `C` String,\r\n   `field_8` String,\r\n   `field_9` Int32,\r\n   `field_10` Int32,\r\n   `field_11` Nullable(Int32),\r\n   `field_12` Nullable(Int32),\r\n   `field_13` String,\r\n   `field_14` Nullable(Int32),\r\n   `field_15` Nullable(Int32),\r\n   `field_16` Nullable(String),\r\n   `field_17` Nullable(String),\r\n   `field_18` Nullable(String),\r\n   `field_19` Nullable(String),\r\n   `field_20` Nullable(String),\r\n   `field_21` Nullable(String),\r\n   `field_22` Nullable(String),\r\n   `field_23` Nullable(String),\r\n   `field_24` Nullable(String),\r\n   `field_25` Nullable(Int32),\r\n   `field_26` Nullable(String),\r\n   `field_27` Nullable(Int32),\r\n   `field_28` Nullable(Int32),\r\n   `field_29` Nullable(Int32),\r\n   `field_30` Nullable(Int32),\r\n   `field_31` Nullable(String),\r\n   `field_32` Nullable(String),\r\n   `field_33` Nullable(String),\r\n   `field_34` Nullable(String),\r\n   `field_35` Nullable(Int64),\r\n   `field_36` Nullable(Int64),\r\n   `field_37` Int32,\r\n   `field_38` Nullable(String)\r\n)\r\nENGINE = Null() \r\n\r\n4. This entrypoint does not receive lots of data. (We have one more similar table for TableC)\r\nCREATE TABLE db.entrypoint_3\r\n(\r\n    `DE` Int64,\r\n    `A` String,\r\n    `B` String,\r\n    `C` String\r\n)\r\nENGINE = Null()\r\n\r\n5. We have one more similar MV for TableC\r\nCREATE MATERIALIZED VIEW db.entrypoint_2_mv TO db.TableB\r\n(\r\n  `A` String,\r\n  `B` String,\r\n  `C` String,\r\n  `D` AggregateFunction(max, Int64),\r\n  `E` AggregateFunction(min, Int64)\r\n) AS\r\nSELECT\r\n  _A AS table_b_field_,\r\n  B,\r\n  C,\r\n  maxState(DE) AS D,\r\n  minState(DE) AS E\r\nFROM db.entrypoint_2\r\nWHERE (((A != '') AND (B != '')) OR ((A != '') AND (C != '')) OR ((B != '') AND (C != ''))) AND (DE > 0)\r\nGROUP BY\r\n  _A,\r\n  B,\r\n  C\r\nORDER BY\r\n  _A ASC,\r\n  B ASC,\r\n  C ASC\r\n\r\n6. We have one more similar MV for TableC\r\nCREATE MATERIALIZED VIEW db.entrypoint_3_mv TO db.TableB\r\n(\r\n   `A` String,\r\n   `B` String,\r\n   `C` String,\r\n   `D` AggregateFunction(max, Int64),\r\n   `E` AggregateFunction(min, Int64)\r\n) AS\r\nSELECT\r\n   A,\r\n   B,\r\n   C,\r\n   maxState(DE) AS D,\r\n   minState(DE) AS E\r\nFROM db.entrypoint_3\r\nGROUP BY\r\n   A,\r\n   B,\r\n   C\r\nORDER BY\r\n   A ASC,\r\n   B ASC,\r\n   C ASC\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21350/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21340","id":818794737,"node_id":"MDU6SXNzdWU4MTg3OTQ3Mzc=","number":21340,"title":"TTL, waring reserve space on disk, there is not enough space","user":{"login":"Oinari","id":1106550,"node_id":"MDQ6VXNlcjExMDY1NTA=","avatar_url":"https://avatars.githubusercontent.com/u/1106550?v=4","gravatar_id":"","url":"https://api.github.com/users/Oinari","html_url":"https://github.com/Oinari","followers_url":"https://api.github.com/users/Oinari/followers","following_url":"https://api.github.com/users/Oinari/following{/other_user}","gists_url":"https://api.github.com/users/Oinari/gists{/gist_id}","starred_url":"https://api.github.com/users/Oinari/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Oinari/subscriptions","organizations_url":"https://api.github.com/users/Oinari/orgs","repos_url":"https://api.github.com/users/Oinari/repos","events_url":"https://api.github.com/users/Oinari/events{/privacy}","received_events_url":"https://api.github.com/users/Oinari/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1634815111,"node_id":"MDU6TGFiZWwxNjM0ODE1MTEx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-multidisk","name":"comp-multidisk","color":"b5bcff","default":false,"description":"Storages & policies"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-03-01T11:58:42Z","updated_at":"2021-03-02T14:31:18Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"We are using table with TTL\r\nTTL datetime TO DISK 'default', datetime + toIntervalDay(7) TO DISK 'disk_hdd', datetime + toIntervalMonth(3)\r\n\r\n/var/log/clickhouse-server/clickhouse-server.err.log\r\n`<Warning> default.TABLENAME: Would like to reserve space on disk 'default' by TTL rule of table 'default.TABLENAME' but there is not enough space`\r\n\r\nAnother strange symptoms - the accumulation of data in the 'moving' directory \r\ndu -sh /var/lib/clickhouse/data/default/TABLENAME/moving/\r\n431G\r\n\r\nFound information here \r\nhttps://github.com/ClickHouse/ClickHouse/commit/efd3126b5d7979da7ff79f380fa2ee46c2d54c36\r\nbut I don't see data about these partitions in system.parts.\r\n\r\n\r\nstorage_configuration:\r\n```\r\n    <storage_configuration>\r\n        <default>\r\n            <keep_free_space_bytes>21474836480</keep_free_space_bytes>\r\n        </default>\r\n        <disks>\r\n            <disk_hdd>\r\n                <path>/srv/hdd/metrics/clickhouse/</path>\r\n            </disk_hdd>\r\n        </disks>\r\n        <policies>\r\n            <default>\r\n                <volumes>\r\n                    <default>\r\n                        <disk>default</disk>\r\n                    </default>\r\n                    <disk_hdd_volume1>\r\n                        <disk>disk_hdd</disk>\r\n                    </disk_hdd_volume1>\r\n                </volumes>\r\n            </default>\r\n        </policies>\r\n    </storage_configuration>\r\n```\r\n\r\ndf -h /\r\n```\r\nFilesystem      Size  Used Avail Use% Mounted on\r\n/dev/md125      921G  779G  143G  85% /\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21340/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21338","id":818730112,"node_id":"MDU6SXNzdWU4MTg3MzAxMTI=","number":21338,"title":"findReplicaHavingCoveringPart may create a lot of excessive traffic","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":1401894928,"node_id":"MDU6TGFiZWwxNDAxODk0OTI4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-zookeeper","name":"comp-zookeeper","color":"b5bcff","default":false,"description":"Zookeeper"},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-03-01T10:38:40Z","updated_at":"2022-01-26T11:34:46Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"Here we do full parts listing:\r\nhttps://github.com/ClickHouse/ClickHouse/blob/fea28366735790e4500335a6976941343a412ba7/src/Storages/StorageReplicatedMergeTree.cpp#L3168\r\n\r\nIf number of parts & replicas is big that creates significant amount of traffic between zookeeper and clickhouse. \r\n\r\nI guess we can start with more 'point' lookup by exact part name cluster-wide.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21338/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21335","id":818705259,"node_id":"MDU6SXNzdWU4MTg3MDUyNTk=","number":21335,"title":"Old MySQL, missing column `DATETIME_PRECISION` in INFORMATION_SCHEMA","user":{"login":"anonymous-shy","id":6885437,"node_id":"MDQ6VXNlcjY4ODU0Mzc=","avatar_url":"https://avatars.githubusercontent.com/u/6885437?v=4","gravatar_id":"","url":"https://api.github.com/users/anonymous-shy","html_url":"https://github.com/anonymous-shy","followers_url":"https://api.github.com/users/anonymous-shy/followers","following_url":"https://api.github.com/users/anonymous-shy/following{/other_user}","gists_url":"https://api.github.com/users/anonymous-shy/gists{/gist_id}","starred_url":"https://api.github.com/users/anonymous-shy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/anonymous-shy/subscriptions","organizations_url":"https://api.github.com/users/anonymous-shy/orgs","repos_url":"https://api.github.com/users/anonymous-shy/repos","events_url":"https://api.github.com/users/anonymous-shy/events{/privacy}","received_events_url":"https://api.github.com/users/anonymous-shy/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-03-01T10:08:49Z","updated_at":"2021-10-12T03:32:04Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi, all:\r\nClickhouse Version:ClickHouse client version 21.2.4.6 (official build).\r\nMySQL. Version:Server version: 5.6.35 MySQL Community Server - GPL\r\nAction:\r\ncreate database on Clickhouse with MySQL Engine.\r\ncreate DATABASE Tab1ã€€ENGINE = MySQL('Ip:Host', 'DB', 'user', 'passwd');\r\nError:\r\nReceived exception from server (version 21.2.4):\r\nCode: 501. DB::Exception: Received from bjxg-bd-slave67:19000. DB::Exception: Cannot create MySQL database, because Poco::Exception. Code: 1000, e.code() = 1054, e.displayText() = mysqlxx::BadQuery: Unknown column 'DATETIME_PRECISION' in 'field list' \r\nHow can I fix it?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21335/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21323","id":818411140,"node_id":"MDU6SXNzdWU4MTg0MTExNDA=","number":21323,"title":"`the column name(  table.column_name or  column_name )  return  from join is ambiguous ","user":{"login":"jjtjiang","id":48897688,"node_id":"MDQ6VXNlcjQ4ODk3Njg4","avatar_url":"https://avatars.githubusercontent.com/u/48897688?v=4","gravatar_id":"","url":"https://api.github.com/users/jjtjiang","html_url":"https://github.com/jjtjiang","followers_url":"https://api.github.com/users/jjtjiang/followers","following_url":"https://api.github.com/users/jjtjiang/following{/other_user}","gists_url":"https://api.github.com/users/jjtjiang/gists{/gist_id}","starred_url":"https://api.github.com/users/jjtjiang/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/jjtjiang/subscriptions","organizations_url":"https://api.github.com/users/jjtjiang/orgs","repos_url":"https://api.github.com/users/jjtjiang/repos","events_url":"https://api.github.com/users/jjtjiang/events{/privacy}","received_events_url":"https://api.github.com/users/jjtjiang/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-03-01T03:00:22Z","updated_at":"2021-03-01T03:00:22Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"`the column name  return  from join is ambiguous ,sometimes it   return  the  table.column_name and  sometimes it   return  the  column_name rather than table.column_name;\r\nCREATE TABLE test.a\r\n(\r\n    `id` Int8,\r\n    `name` String\r\n)\r\nENGINE = Memory \r\n\r\n insert into a values (1,'name') ;\r\n \r\n SELECT *\r\nFROM \r\n(\r\n    SELECT *\r\n    FROM a\r\n) AS a1\r\nINNER JOIN \r\n(\r\n    SELECT *\r\n    FROM a\r\n) AS a2 ON a1.id = a2.id\r\nâ”Œâ”€idâ”€â”¬â”€nameâ”€â”¬â”€a2.idâ”€â”¬â”€a2.nameâ”€â”\r\nâ”‚  1 â”‚ name â”‚     1 â”‚ name    â”‚\r\nâ””â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nthis case return column_name;\r\n\r\n\r\nSELECT *\r\nFROM \r\n(\r\n    SELECT *\r\n    FROM a\r\n) AS a1\r\nINNER JOIN \r\n(\r\n    SELECT *\r\n    FROM a\r\n) AS a2 ON a1.id = a2.id\r\nINNER JOIN \r\n(\r\n    SELECT *\r\n    FROM a\r\n) AS a3 ON a2.id = a3.id\r\n\r\nâ”Œâ”€a1.idâ”€â”¬â”€a1.nameâ”€â”¬â”€a2.idâ”€â”¬â”€a2.nameâ”€â”¬â”€a3.idâ”€â”¬â”€a3.nameâ”€â”\r\nâ”‚     1 â”‚ name    â”‚     1 â”‚ name    â”‚     1 â”‚ name    â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nthis case return table.column_name;`\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21323/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21300","id":818077801,"node_id":"MDU6SXNzdWU4MTgwNzc4MDE=","number":21300,"title":"Experiment: GROUP BY optimization with block-local hash tables","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-28T02:07:12Z","updated_at":"2021-11-20T19:31:11Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"If the GROUP BY key is simple (one or several fixed-size columns)\r\nand if there is one simple aggregate function (with state up to 8 bytes),\r\n\r\nif during aggregation the thread-local hash table exceeds the size of L2 cache, \r\ntry to do a loop iteration with \"block-local\" hash table:\r\n\r\naggregate data into temporary hash table and them merge to the common hash table,\r\n\r\nif temporary hash table fits L2 cache and if it's size several times less than the block size, then the method considered beneficial and can be applied in subsequent loop iterations.\r\n\r\nExample query:\r\n\r\n```\r\nSELECT ClientIP AS k, count() FROM test.hits GROUP BY k ORDER BY count() DESC LIMIT 10\r\n```\r\n\r\nThe total number of IP addresses is \r\n```\r\nmilovidov-desktop :) SELECT uniq(ClientIP) FROM test.hits\r\n\r\nâ”Œâ”€uniq(ClientIP)â”€â”\r\nâ”‚         204660 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n```\r\n\r\nSo, aggregation does not fit in L2 cache.\r\n\r\nBut while we process every block of 64K records, there are only up to 10K unique IP addresses per block:\r\n```\r\nSELECT\r\n    blockNumber() AS k,\r\n    uniq(ClientIP)\r\nFROM test.hits\r\nGROUP BY k\r\n\r\nâ”Œâ”€â”€â”€kâ”€â”¬â”€uniq(ClientIP)â”€â”\r\nâ”‚ 116 â”‚           5148 â”‚\r\nâ”‚  66 â”‚           4547 â”‚\r\nâ”‚ 156 â”‚           9570 â”‚\r\nâ”‚  46 â”‚           7769 â”‚\r\nâ”‚  24 â”‚           2270 â”‚\r\nâ”‚ 170 â”‚           7363 â”‚\r\nâ”‚ 114 â”‚           4825 â”‚\r\nâ”‚  68 â”‚           6942 â”‚\r\nâ”‚ 154 â”‚          11033 â”‚\r\nâ”‚  40 â”‚           9188 â”‚\r\nâ”‚  30 â”‚          10215 â”‚\r\n...\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300/reactions","total_count":5,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":4},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21300/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21136","id":815329764,"node_id":"MDU6SXNzdWU4MTUzMjk3NjQ=","number":21136,"title":"Use HTTP trailers.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1481327553,"node_id":"MDU6TGFiZWwxNDgxMzI3NTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-http","name":"comp-http","color":"b5bcff","default":false,"description":"http protocol related"},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":22,"created_at":"2021-02-24T10:15:10Z","updated_at":"2021-04-19T19:24:45Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"If client supports HTTP 1.1, use trailers to provide progress, status and possibly error message.\r\n\r\n**Use case**\r\nThis is the only way to return error in the middle of query processing if HTTP headers have been already sent.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21136/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21110","id":814605989,"node_id":"MDU6SXNzdWU4MTQ2MDU5ODk=","number":21110,"title":"Row Policy - confusing documentation for Restrictive and Permissive.","user":{"login":"MyroTk","id":44327070,"node_id":"MDQ6VXNlcjQ0MzI3MDcw","avatar_url":"https://avatars.githubusercontent.com/u/44327070?v=4","gravatar_id":"","url":"https://api.github.com/users/MyroTk","html_url":"https://github.com/MyroTk","followers_url":"https://api.github.com/users/MyroTk/followers","following_url":"https://api.github.com/users/MyroTk/following{/other_user}","gists_url":"https://api.github.com/users/MyroTk/gists{/gist_id}","starred_url":"https://api.github.com/users/MyroTk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MyroTk/subscriptions","organizations_url":"https://api.github.com/users/MyroTk/orgs","repos_url":"https://api.github.com/users/MyroTk/repos","events_url":"https://api.github.com/users/MyroTk/events{/privacy}","received_events_url":"https://api.github.com/users/MyroTk/received_events","type":"User","site_admin":false},"labels":[{"id":785082162,"node_id":"MDU6TGFiZWw3ODUwODIxNjI=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-documentation","name":"comp-documentation","color":"b5bcff","default":false,"description":"Used to run automatic builds of the documentation"},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-23T16:07:52Z","updated_at":"2021-02-23T22:27:43Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the issue**\r\nThe documentation says `Permissive policy grants access to rows` and `Restrictive policy restricts access to rows` but that's a bit misleading because they do the same thing in terms of permitting access to rows that meet their condition. The rest of the `AS Clause` explains the difference well, but these initial statements are unnecessary.\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21110/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107/events","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21107","id":814453059,"node_id":"MDExOlB1bGxSZXF1ZXN0NTc4NDM5MjUx","number":21107,"title":"Incremental data aggregation in memory","user":{"login":"petuhovskiy","id":8150127,"node_id":"MDQ6VXNlcjgxNTAxMjc=","avatar_url":"https://avatars.githubusercontent.com/u/8150127?v=4","gravatar_id":"","url":"https://api.github.com/users/petuhovskiy","html_url":"https://github.com/petuhovskiy","followers_url":"https://api.github.com/users/petuhovskiy/followers","following_url":"https://api.github.com/users/petuhovskiy/following{/other_user}","gists_url":"https://api.github.com/users/petuhovskiy/gists{/gist_id}","starred_url":"https://api.github.com/users/petuhovskiy/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/petuhovskiy/subscriptions","organizations_url":"https://api.github.com/users/petuhovskiy/orgs","repos_url":"https://api.github.com/users/petuhovskiy/repos","events_url":"https://api.github.com/users/petuhovskiy/events{/privacy}","received_events_url":"https://api.github.com/users/petuhovskiy/received_events","type":"User","site_admin":false},"labels":[{"id":1309674771,"node_id":"MDU6TGFiZWwxMzA5Njc0Nzcx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/pr-feature","name":"pr-feature","color":"007700","default":false,"description":"Pull request with new product feature"},{"id":1807683251,"node_id":"MDU6TGFiZWwxODA3NjgzMjUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/doc-alert","name":"doc-alert","color":"e51068","default":false,"description":"PR where any documentation work is needed or proceeded"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":5,"created_at":"2021-02-23T13:13:26Z","updated_at":"2022-01-14T00:05:34Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"draft":false,"pull_request":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/pulls/21107","html_url":"https://github.com/ClickHouse/ClickHouse/pull/21107","diff_url":"https://github.com/ClickHouse/ClickHouse/pull/21107.diff","patch_url":"https://github.com/ClickHouse/ClickHouse/pull/21107.patch","merged_at":null},"body":"I hereby agree to the terms of the CLA available at: https://yandex.ru/legal/cla/?lang=en\r\n\r\nChangelog category (leave one):\r\n- New Feature\r\n\r\n\r\nChangelog entry (a user-readable short description of the changes that goes to CHANGELOG.md):\r\n\r\nAdded support for incremental data aggregation in memory.\r\n\r\n\r\nDetailed description / Documentation draft:\r\n\r\nThis is a task from Intern tasks 2020/2021 https://github.com/ClickHouse/ClickHouse/issues/15065 . Here is the copied description of this task:\r\n\r\nClickHouse already has support for incremental aggregation (see AggregatingMergeTree). We can provide an alternative way that can sustain higher query rate, can be used for JOINs and dictionaries efficiently, in price of lost persistency.\r\n\r\nWhen ClickHouse executes GROUP BY it creates data structure in memory to hold intermediate data for aggregation. This data structure only lives for query time and is destroyed when query finished. But we can hold aggregation data in memory and allow to incrementally feed more data into it and also allow to query it as key-value table / JOIN with it / use it as a dictionary. Typical usage example is antifraud filter that need to accumulate some statistics to filter data.\r\n\r\n\r\n\r\nThis PR is not ready yet, but here are some thoughts:\r\n\r\nPlanning to create AggregatingMemory table engine, which accepts raw data writes (not aggregated) and passes it to Aggregator, and also allows reads, which read (aggregated) data directly from Aggregator.\r\n\r\nExample of usage with materialized views:\r\n\r\n```sql\r\n-- table with source data\r\nCREATE TABLE test.mv_src ( `UserID` UInt32, `Region` String, `Day` UInt8, `Age` UInt8, `Sex` UInt8 ) ENGINE = MergeTree() ORDER BY (Day, UserID);\r\n\r\n-- table to do aggregation and keep it in memory\r\nCREATE MATERIALIZED VIEW test.mv\r\nENGINE = AggregatingMemory()\r\nAS SELECT\r\n    Region,\r\n    Day,\r\n    avg(Age) AS AvgAge,\r\n    count() AS Visits,\r\n    countIf(Sex = 1) AS Boys,\r\n    uniq(UserID) AS Users\r\nFROM test.mv_src\r\nGROUP BY Region, Day;\r\n\r\n-- after inserting some data into test.mv_src,\r\n-- materialized view will push it to test.mv\r\n\r\n-- this select should return same result as GROUP BY query to original test.mv_src table\r\nSELECT * FROM test.mv;\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107/reactions","total_count":4,"+1":4,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21107/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21100","id":814290968,"node_id":"MDU6SXNzdWU4MTQyOTA5Njg=","number":21100,"title":"how to join 2 tables with both equality and inequality constrains","user":{"login":"poleft","id":31883171,"node_id":"MDQ6VXNlcjMxODgzMTcx","avatar_url":"https://avatars.githubusercontent.com/u/31883171?v=4","gravatar_id":"","url":"https://api.github.com/users/poleft","html_url":"https://github.com/poleft","followers_url":"https://api.github.com/users/poleft/followers","following_url":"https://api.github.com/users/poleft/following{/other_user}","gists_url":"https://api.github.com/users/poleft/gists{/gist_id}","starred_url":"https://api.github.com/users/poleft/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/poleft/subscriptions","organizations_url":"https://api.github.com/users/poleft/orgs","repos_url":"https://api.github.com/users/poleft/repos","events_url":"https://api.github.com/users/poleft/events{/privacy}","received_events_url":"https://api.github.com/users/poleft/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-23T09:46:07Z","updated_at":"2021-02-24T02:02:30Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"while input follow sql, the error \"  DB::Exception: Expected equality or inequality \" occurs\r\nselect t1.a\r\n         ,t1.b\r\n         ,t2.c\r\nfrom t1 left outer join t2\r\non t1.a = t2.a \r\n      and t1.b != t2.b\r\nwhere t2.a = ''\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21100/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21099","id":814279940,"node_id":"MDU6SXNzdWU4MTQyNzk5NDA=","number":21099,"title":"ALTER TABLE UPDATE referencing field from table being updated","user":{"login":"ukutaht","id":3731516,"node_id":"MDQ6VXNlcjM3MzE1MTY=","avatar_url":"https://avatars.githubusercontent.com/u/3731516?v=4","gravatar_id":"","url":"https://api.github.com/users/ukutaht","html_url":"https://github.com/ukutaht","followers_url":"https://api.github.com/users/ukutaht/followers","following_url":"https://api.github.com/users/ukutaht/following{/other_user}","gists_url":"https://api.github.com/users/ukutaht/gists{/gist_id}","starred_url":"https://api.github.com/users/ukutaht/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/ukutaht/subscriptions","organizations_url":"https://api.github.com/users/ukutaht/orgs","repos_url":"https://api.github.com/users/ukutaht/repos","events_url":"https://api.github.com/users/ukutaht/events{/privacy}","received_events_url":"https://api.github.com/users/ukutaht/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-23T09:31:42Z","updated_at":"2021-02-23T09:34:10Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hello. Thanks for the amazing software, I â¤ï¸ Clickhouse.\r\n\r\nWe had a bug in our app code that generated some bad data in the sessions table. I need to regenerate the `exit_page` field for all the historical data in our sessions table (CollapsingMergeTree).\r\n\r\nOur table layout is simpler but similar to the Yandex.Metrica example. Some names are different but you should get the idea `hits -> events, visits -> sessions`. Not that much data yet, less than a billion rows in the sessions table.\r\n\r\nComing from regular SQL, this was my first instinct:\r\n```sql\r\nALTER TABLE sessions UPDATE exit_page=(SELECT anyLast(pathname) FROM events WHERE events.session_id=sessions.session_id ORDER BY timestamp);\r\n```\r\nbut I get the following error:\r\n```\r\nCode: 47. DB::Exception: Received from clickhouse-server:9000. DB::Exception: Missing columns: 'sessions.session_id' while processing query: 'SELECT anyLast(pathname) FROM plausible_dev.events WHERE (domain = 'localtest.me') AND (session_id = sessions.session_id)', required columns: 'domain' 'pathname' 'session_id' 'sessions.session_id', source columns: [...]\r\n```\r\n\r\nIn regular SQL I am used to being able to reference columns from the row that I'm updating. I realize Clickhouse has very different semantics for updating and this might not be supported. Any other ideas how one might go about updating a field like this?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21099/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21096","id":814171121,"node_id":"MDU6SXNzdWU4MTQxNzExMjE=","number":21096,"title":"Define format schema via query","user":{"login":"BigRantLing","id":51715057,"node_id":"MDQ6VXNlcjUxNzE1MDU3","avatar_url":"https://avatars.githubusercontent.com/u/51715057?v=4","gravatar_id":"","url":"https://api.github.com/users/BigRantLing","html_url":"https://github.com/BigRantLing","followers_url":"https://api.github.com/users/BigRantLing/followers","following_url":"https://api.github.com/users/BigRantLing/following{/other_user}","gists_url":"https://api.github.com/users/BigRantLing/gists{/gist_id}","starred_url":"https://api.github.com/users/BigRantLing/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/BigRantLing/subscriptions","organizations_url":"https://api.github.com/users/BigRantLing/orgs","repos_url":"https://api.github.com/users/BigRantLing/repos","events_url":"https://api.github.com/users/BigRantLing/events{/privacy}","received_events_url":"https://api.github.com/users/BigRantLing/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":10,"created_at":"2021-02-23T06:51:38Z","updated_at":"2021-02-25T06:32:33Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nBefore we use KafkaEngine to consume Protobuf data from kafka, we must define the format schema and place the `*.proto` file in the specified directroy (format_schema_path). It's easy for single node. But when we have a cluster, we need to put the schema file on every node of the cluster manually. Of course, this can be implemented by external tools or service, but it's still inconvenient and it might increased workload for developers.  \r\nIt will be easier if we can define the format schema via query.  \r\n\r\n**Describe the solution you'd like**\r\n1. Support the schema definition query like below\r\n ```sql\r\nCREARE SCHEMA IF NOT EXISTS `schema_file`.`type_name` on cluster test_cluster\r\n (\r\n    f1 String, \r\n    f2 UInt32,\r\n    f3 Array(UInt32) \r\n) \r\n Format Protobuf\r\n```  \r\nAfter executing the query , `schema_file.proto` file will be create in the specified directory, and the message type `type_name` will be define in the file.  \r\n\r\n2. Add system table for created schema so that we can check the schema by query.\r\n\r\n**Demo**\r\nWe impleted a simple demo for Protobuf like below.\r\n![mP3Qqn](https://raw.githubusercontent.com/BigRantLing/pics/master/uPic/mP3Qqn.png)","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096/reactions","total_count":3,"+1":2,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":1},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21096/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21084","id":813716426,"node_id":"MDU6SXNzdWU4MTM3MTY0MjY=","number":21084,"title":"Materialized View is inconsistent with source table Row Policy.","user":{"login":"MyroTk","id":44327070,"node_id":"MDQ6VXNlcjQ0MzI3MDcw","avatar_url":"https://avatars.githubusercontent.com/u/44327070?v=4","gravatar_id":"","url":"https://api.github.com/users/MyroTk","html_url":"https://github.com/MyroTk","followers_url":"https://api.github.com/users/MyroTk/followers","following_url":"https://api.github.com/users/MyroTk/following{/other_user}","gists_url":"https://api.github.com/users/MyroTk/gists{/gist_id}","starred_url":"https://api.github.com/users/MyroTk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MyroTk/subscriptions","organizations_url":"https://api.github.com/users/MyroTk/orgs","repos_url":"https://api.github.com/users/MyroTk/repos","events_url":"https://api.github.com/users/MyroTk/events{/privacy}","received_events_url":"https://api.github.com/users/MyroTk/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-22T17:41:32Z","updated_at":"2021-02-23T08:00:09Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nRow policy on the source table does not retroactively change the data on the mat view. As a result, if the data is inserted, then the row policy is created, the outcome is different than if the row policy was created first and then the insert happened.\r\n\r\n**How to reproduce**\r\nClickHouse server version 21.3.1 revision 54447\r\n```\r\naltinity-qa-cosmic2 :) create materialized view mat_pre_pol engine= Memory as select * from table1\r\n\r\nCREATE MATERIALIZED VIEW mat_pre_pol\r\nENGINE = Memory AS\r\nSELECT *\r\nFROM table1\r\n\r\nQuery id: d1945444-8889-4ca0-873e-6e1452e28117\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.013 sec.\r\n\r\naltinity-qa-cosmic2 :) insert into table1 values (1),(2)\r\n\r\nINSERT INTO table1 VALUES\r\n\r\nQuery id: c64ad2eb-ccfd-4a0a-8354-5c53f20fe1c0\r\n\r\nOk.\r\n\r\n2 rows in set. Elapsed: 0.001 sec.\r\n\r\naltinity-qa-cosmic2 :) create row policy pol0 on table1 for select using x=1 to default\r\n\r\nCREATE ROW POLICY pol0 ON table1 FOR SELECT USING x = 1 TO default\r\n\r\nQuery id: 9f66c49b-916e-4d7d-a8d2-ddb3700d848a\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\naltinity-qa-cosmic2 :) select * from table1\r\n\r\nSELECT *\r\nFROM table1\r\n\r\nQuery id: 572ec02e-98fa-45e8-868f-9c8f784be336\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n\r\naltinity-qa-cosmic2 :) select * from mat_pre_pol\r\n\r\nSELECT *\r\nFROM mat_pre_pol\r\n\r\nQuery id: a17e7e47-2e40-43a8-aecd-0b6ee0c2d1c5\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ”‚ 2 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n2 rows in set. Elapsed: 0.001 sec.\r\n```\r\n\r\n```\r\naltinity-qa-cosmic2 :) create row policy pol0 on table1 for select using x=1 to default\r\n\r\nCREATE ROW POLICY pol0 ON table1 FOR SELECT USING x = 1 TO default\r\n\r\nQuery id: 9f66c49b-916e-4d7d-a8d2-ddb3700d848a\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.001 sec.\r\n\r\naltinity-qa-cosmic2 :) create materialized view mat_post_pol engine= Memory as select * from table1\r\n\r\nCREATE MATERIALIZED VIEW mat_post_pol\r\nENGINE = Memory AS\r\nSELECT *\r\nFROM table1\r\n\r\nQuery id: eef8e3cc-4f23-4d8d-8197-02158e8d8a45\r\n\r\nOk.\r\n\r\n0 rows in set. Elapsed: 0.015 sec.\r\n\r\naltinity-qa-cosmic2 :) insert into table1 values (1),(2)\r\n\r\nINSERT INTO table1 VALUES\r\n\r\nQuery id: 91b03b29-2762-448d-ade2-f3e0878a4ba7\r\n\r\nOk.\r\n\r\n2 rows in set. Elapsed: 0.002 sec.\r\n\r\naltinity-qa-cosmic2 :) select * from mat_post_pol\r\n\r\nSELECT *\r\nFROM mat_post_pol\r\n\r\nQuery id: 61c19ee8-3d7a-42c1-be32-97a8f5d649eb\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n\r\n\r\naltinity-qa-cosmic2 :) select * from table1\r\n\r\nSELECT *\r\nFROM table1\r\n\r\nQuery id: 572ec02e-98fa-45e8-868f-9c8f784be336\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21084/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21083","id":813703244,"node_id":"MDU6SXNzdWU4MTM3MDMyNDQ=","number":21083,"title":"Live View don't follow the row policy of the source table. ","user":{"login":"MyroTk","id":44327070,"node_id":"MDQ6VXNlcjQ0MzI3MDcw","avatar_url":"https://avatars.githubusercontent.com/u/44327070?v=4","gravatar_id":"","url":"https://api.github.com/users/MyroTk","html_url":"https://github.com/MyroTk","followers_url":"https://api.github.com/users/MyroTk/followers","following_url":"https://api.github.com/users/MyroTk/following{/other_user}","gists_url":"https://api.github.com/users/MyroTk/gists{/gist_id}","starred_url":"https://api.github.com/users/MyroTk/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MyroTk/subscriptions","organizations_url":"https://api.github.com/users/MyroTk/orgs","repos_url":"https://api.github.com/users/MyroTk/repos","events_url":"https://api.github.com/users/MyroTk/events{/privacy}","received_events_url":"https://api.github.com/users/MyroTk/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2011606513,"node_id":"MDU6TGFiZWwyMDExNjA2NTEz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-rbac","name":"comp-rbac","color":"b5bcff","default":false,"description":"Access control related"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-22T17:24:22Z","updated_at":"2021-02-23T08:00:24Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"**Describe the unexpected behaviour**\r\nWhen selecting from a live view, all rows are shown, regardless of the policies of the source table\r\n\r\n**How to reproduce**\r\nClickHouse server version 21.3.1 revision 54447.\r\n\r\nTables:\r\n```\r\naltinity-qa-cosmic2 :) show create live1\r\n\r\nSHOW CREATE TABLE live1\r\n\r\nQuery id: 084d7788-fb92-466a-8eb5-1a6afca7a693\r\n\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE LIVE VIEW default.live1\r\n(\r\n    `x` Int32\r\n) AS\r\nSELECT *\r\nFROM default.table1 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n\r\naltinity-qa-cosmic2 :) show create table1\r\n\r\nSHOW CREATE TABLE table1\r\n\r\nQuery id: 9b1aa74b-b389-4047-a731-cc55b2bf5af7\r\n\r\nâ”Œâ”€statementâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\r\nâ”‚ CREATE TABLE default.table1\r\n(\r\n    `x` Int32\r\n)\r\nENGINE = Memory â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.001 sec.\r\n```\r\nSelect:\r\n```\r\naltinity-qa-cosmic2 :) select * from table1\r\n\r\nSELECT *\r\nFROM table1\r\n\r\nQuery id: b3294991-5142-4521-95de-1adbf17b33c1\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.004 sec.\r\n\r\naltinity-qa-cosmic2 :) select * from live1\r\n\r\nSELECT *\r\nFROM live1\r\n\r\nQuery id: da1ea439-b971-4ba5-8ce5-791a0d2aecee\r\n\r\nâ”Œâ”€xâ”€â”\r\nâ”‚ 1 â”‚\r\nâ”‚ 2 â”‚\r\nâ””â”€â”€â”€â”˜\r\n\r\n2 rows in set. Elapsed: 0.005 sec.\r\n\r\n```\r\nRow Policy:\r\n```\r\nCREATE ROW POLICY pol0 ON default.table1 FOR SELECT USING x = 1 TO default \r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21083/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21077","id":813580040,"node_id":"MDU6SXNzdWU4MTM1ODAwNDA=","number":21077,"title":"Enable toIPv6(ipv4 domain)","user":{"login":"mzealey","id":6083471,"node_id":"MDQ6VXNlcjYwODM0NzE=","avatar_url":"https://avatars.githubusercontent.com/u/6083471?v=4","gravatar_id":"","url":"https://api.github.com/users/mzealey","html_url":"https://github.com/mzealey","followers_url":"https://api.github.com/users/mzealey/followers","following_url":"https://api.github.com/users/mzealey/following{/other_user}","gists_url":"https://api.github.com/users/mzealey/gists{/gist_id}","starred_url":"https://api.github.com/users/mzealey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzealey/subscriptions","organizations_url":"https://api.github.com/users/mzealey/orgs","repos_url":"https://api.github.com/users/mzealey/repos","events_url":"https://api.github.com/users/mzealey/events{/privacy}","received_events_url":"https://api.github.com/users/mzealey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-22T15:00:54Z","updated_at":"2021-02-22T15:00:54Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Following on from #19518 it would be sensible to allow `toIPv6(ipv4 domain)`. Currently we have to do `toIPv6(IPv4NumToString(ipv4 domain))` which is presumably not very efficient, or some binary hacking which is not going to be fun.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21077/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21075","id":813544346,"node_id":"MDU6SXNzdWU4MTM1NDQzNDY=","number":21075,"title":"Queries with sampling reads more rows / marks than before","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":2266396286,"node_id":"MDU6TGFiZWwyMjY2Mzk2Mjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v20.8-affected","name":"v20.8-affected","color":"c2bfff","default":false,"description":""},{"id":2728482882,"node_id":"MDU6TGFiZWwyNzI4NDgyODgy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.1-affected","name":"v21.1-affected","color":"c2bfff","default":false,"description":""},{"id":2730601367,"node_id":"MDU6TGFiZWwyNzMwNjAxMzY3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.2-affected","name":"v21.2-affected","color":"c2bfff","default":false,"description":""},{"id":2825253639,"node_id":"MDU6TGFiZWwyODI1MjUzNjM5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.3-affected","name":"v21.3-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-02-22T14:19:36Z","updated_at":"2021-07-01T18:18:40Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"```sql\r\nCREATE TABLE e ( id String, time DateTime , m String )\r\nENGINE = MergeTree \r\nPARTITION BY toYYYYMMDD(time)\r\nORDER BY (m, cityHash64(id))\r\nSAMPLE BY cityHash64(id);\r\n\r\ninsert into e select toString(number), \r\n    toDateTime('2020-01-01 00:00:00') + number/100, \r\n    toString(number%103) \r\nfrom numbers(100000000);\r\n\r\nSELECT count()\r\nFROM e\r\nSAMPLE 2 / 100\r\nWHERE time >= toDateTime('2020-01-01 00:00:00') \r\n   AND time <= toDateTime('2020-02-01 00:00:00') \r\n   AND m IN ('42');\r\n\r\n1 rows in set. Elapsed: 0.010 sec. Processed 1.34 million rows, 42.58 MB (135.83 million rows/s., 4.33 GB/s.)\r\n\r\nSELECT count()\r\nFROM e\r\nSAMPLE 2 / 100\r\nWHERE time >= toDateTime('2020-01-01 00:00:00') \r\n   AND time <= toDateTime('2020-02-01 00:00:00') \r\n   AND m IN (select '42');\r\n\r\n1 rows in set. Elapsed: 0.011 sec. Processed 1.34 million rows, 42.58 MB (126.93 million rows/s., 4.05 GB/s.)\r\n\r\n---- expected:\r\n\r\nSELECT count()\r\nFROM e\r\nSAMPLE 2 / 100\r\nWHERE time >= toDateTime('2020-01-01 00:00:00') \r\n   AND time <= toDateTime('2020-02-01 00:00:00') \r\n   AND m = '42';\r\n\r\n1 rows in set. Elapsed: 0.006 sec. Processed 385.02 thousand rows, 12.28 MB (69.88 million rows/s., 2.23 GB/s.)\r\n\r\n21.3.1.6054\r\n\r\n```\r\nit worked in 20.3","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21075/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21067","id":813448975,"node_id":"MDU6SXNzdWU4MTM0NDg5NzU=","number":21067,"title":"Performance regression in 21.2 - query analysis time explosion with several WITH and JOINs","user":{"login":"adrian17","id":4729533,"node_id":"MDQ6VXNlcjQ3Mjk1MzM=","avatar_url":"https://avatars.githubusercontent.com/u/4729533?v=4","gravatar_id":"","url":"https://api.github.com/users/adrian17","html_url":"https://github.com/adrian17","followers_url":"https://api.github.com/users/adrian17/followers","following_url":"https://api.github.com/users/adrian17/following{/other_user}","gists_url":"https://api.github.com/users/adrian17/gists{/gist_id}","starred_url":"https://api.github.com/users/adrian17/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adrian17/subscriptions","organizations_url":"https://api.github.com/users/adrian17/orgs","repos_url":"https://api.github.com/users/adrian17/repos","events_url":"https://api.github.com/users/adrian17/events{/privacy}","received_events_url":"https://api.github.com/users/adrian17/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":2730601367,"node_id":"MDU6TGFiZWwyNzMwNjAxMzY3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.2-affected","name":"v21.2-affected","color":"c2bfff","default":false,"description":""},{"id":2788693937,"node_id":"MDU6TGFiZWwyNzg4NjkzOTM3","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-cte","name":"comp-cte","color":"b5bcff","default":false,"description":"common table expression (WITH ... SELECT)"},{"id":2825253639,"node_id":"MDU6TGFiZWwyODI1MjUzNjM5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.3-affected","name":"v21.3-affected","color":"c2bfff","default":false,"description":""},{"id":2941252908,"node_id":"MDU6TGFiZWwyOTQxMjUyOTA4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v21.4-affected","name":"v21.4-affected","color":"c2bfff","default":false,"description":""}],"state":"open","locked":false,"assignee":{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false},"assignees":[{"login":"kitaisreal","id":22458333,"node_id":"MDQ6VXNlcjIyNDU4MzMz","avatar_url":"https://avatars.githubusercontent.com/u/22458333?v=4","gravatar_id":"","url":"https://api.github.com/users/kitaisreal","html_url":"https://github.com/kitaisreal","followers_url":"https://api.github.com/users/kitaisreal/followers","following_url":"https://api.github.com/users/kitaisreal/following{/other_user}","gists_url":"https://api.github.com/users/kitaisreal/gists{/gist_id}","starred_url":"https://api.github.com/users/kitaisreal/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/kitaisreal/subscriptions","organizations_url":"https://api.github.com/users/kitaisreal/orgs","repos_url":"https://api.github.com/users/kitaisreal/repos","events_url":"https://api.github.com/users/kitaisreal/events{/privacy}","received_events_url":"https://api.github.com/users/kitaisreal/received_events","type":"User","site_admin":false}],"milestone":null,"comments":6,"created_at":"2021-02-22T12:18:03Z","updated_at":"2021-04-27T13:49:52Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the situation**\r\nIn 21.2, several complex test queries became visibly slower - regressing from <1s to 2-3s despite single-digit row counts. Further increasing query complexity results in combinatorial explosion of query analysis time.\r\n\r\nThis might be related to https://github.com/ClickHouse/ClickHouse/issues/20388 .\r\n\r\n**How to reproduce**\r\n\r\nTables (can be empty):\r\n```sql\r\nCREATE TABLE hits (`date` Date, `row_type` Enum8('A' = 1, 'B' = 2, 'C' = 3), `user` UInt32)\r\nENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY date\r\n\r\nCREATE TABLE users_list (`date` Date, `user` UInt32)\r\nENGINE = MergeTree PARTITION BY toYYYYMM(date) ORDER BY date\r\n```\r\n\r\nExample simplified query:\r\n```sql\r\nWITH\r\n    (SELECT count(*) AS users_cnt FROM  (SELECT distinct user FROM users_list WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03')))\r\n    AS with1,\r\n    (SELECT count(*) AS users_cnt FROM  (SELECT distinct user FROM users_list WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03')))\r\n    AS with2\r\nSELECT main.date, averages.users AS average_daily_users, main.users AS users\r\nFROM (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\nAS main\r\nLEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\nAS join1 ON main.date = join1.date\r\nLEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\nAS join2 ON main.date = join2.date\r\nLEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\nAS join3 ON main.date = join3.date\r\nLEFT JOIN \r\n(\r\n    SELECT inner.date as date, avg(users) AS users\r\n    FROM \r\n    (\r\n        WITH\r\n            (SELECT count(*) AS users_cnt FROM  (SELECT distinct user FROM users_list WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03')))\r\n            AS with1,\r\n            (SELECT count(*) AS users_cnt FROM  (SELECT distinct user FROM users_list WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03')))\r\n            AS with2\r\n        SELECT main.date as date, main.users AS users\r\n        FROM (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\n        AS main\r\n        LEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\n        AS join1 ON main.date = join1.date\r\n        LEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\n        AS join2 ON main.date = join2.date\r\n        LEFT JOIN (SELECT date, uniqExact(filtered.user) AS users FROM ( SELECT date, user FROM hits WHERE (1 = 1) AND (date >= '2020-06-01') AND (date <= '2020-06-03') AND (row_type = 'A') ) AS filtered GROUP BY date)\r\n        AS join3 ON main.date = join3.date\r\n    ) AS inner\r\n    WHERE inner.date != '1970-01-01'\r\n    GROUP BY date\r\n) AS averages ON main.date = averages.date\r\n```\r\n\r\nResults:\r\nFirst row is the query above, further rows are with extra added `WITH` entries.\r\n\r\n| WITHs in outer query | WITHs in inner query | time in 21.1.5.4 | time in 21.2.4.6 |\r\n| --- | --- | --- | --- |\r\n| 2 | 2 | 0.080s | 0.8s |\r\n| 3 | 2 | 0.080s | 2.5s |\r\n| 3 | 3 | 0.081s | 7.4s |\r\n\r\nInterestingly, in the last row, even dropping `(1 = 1) AND` from all `WHERE`s reduces execution time by 1s.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21067/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/21047","id":812923057,"node_id":"MDU6SXNzdWU4MTI5MjMwNTc=","number":21047,"title":"Assorted ideas to slightly improve JOINs","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-21T18:50:50Z","updated_at":"2021-02-21T18:52:46Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Only ideas that are rather easy to implement.\r\n\r\n1. Add `IColumn::shrinkToFit` method. It will remove overallocation of columns and save memory (for hash join) up to 2x. Use this method in `HashJoin` unconditionally.\r\n\r\n2. For CROSS JOIN (nested loops): compress blocks in memory if there is large amount of data. Uncompress while joining (repeatedly for every iteration of the outer loop). The implementation is very easy after #20168.\r\n\r\n3. For CROSS JOIN (nested loops): write blocks to tmp directory in Native format (similar to external sorting and external aggregation) if there is large amount of data. Read many times while joining.\r\n\r\n4. Compress blocks for hash join in memory. While joining, maintain LRU cache of uncompressed blocks. Can work good if JOIN is skewed, otherwise questionable.\r\n\r\n5. If the amount of data is large, serialize all records on disk in `RowBinary` format and keep offsets in hash table (we will have 8 bytes per record + key size + hash table overhead instead of keeping all data). While joining, do batch reads with AIO and also maintain small LRU hash table in memory. The performance can be decent (1 million IOPS on modern SSD).\r\n\r\n6. If the amount of data is large, replace HashJoin to SSDCacheDictionary (the performance of SSDCacheDictionary assumed to be decent).\r\n\r\n7. Represent the data structure for right hand side of JOIN or IN as a table for key-value requests. When doing distributed JOIN, instead of usual (broadcast or shuffle) algorithms, do lookup requests over network. Right hand side of distributed JOIN can be represented as a special kind of distributed table with local cache of lookup results. Applicability is limited but can be good for some scenarios: large rhs table but small subset of keys are JOINed.\r\n\r\n8. For INNER and RIGHT JOIN try to use the set of keys of rhs table as an index for lhs table, similar to IN.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047/reactions","total_count":3,"+1":3,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/21047/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20954","id":811657664,"node_id":"MDU6SXNzdWU4MTE2NTc2NjQ=","number":20954,"title":"Cannot attach system's table when restart clickhouser-server ","user":{"login":"talent2333","id":25974296,"node_id":"MDQ6VXNlcjI1OTc0Mjk2","avatar_url":"https://avatars.githubusercontent.com/u/25974296?v=4","gravatar_id":"","url":"https://api.github.com/users/talent2333","html_url":"https://github.com/talent2333","followers_url":"https://api.github.com/users/talent2333/followers","following_url":"https://api.github.com/users/talent2333/following{/other_user}","gists_url":"https://api.github.com/users/talent2333/gists{/gist_id}","starred_url":"https://api.github.com/users/talent2333/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/talent2333/subscriptions","organizations_url":"https://api.github.com/users/talent2333/orgs","repos_url":"https://api.github.com/users/talent2333/repos","events_url":"https://api.github.com/users/talent2333/events{/privacy}","received_events_url":"https://api.github.com/users/talent2333/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""},{"id":1669010018,"node_id":"MDU6TGFiZWwxNjY5MDEwMDE4","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question-answered","name":"question-answered","color":"bfdadc","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-19T03:43:52Z","updated_at":"2021-11-18T04:40:40Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I just change the storage configuraiton in **multiple disks policy** and apply it to some tables.\r\nThen restart my clickhouse-server,but fail.\r\nI encounter the error that i can not attach these tables from metadata files.\r\nrefer to my clickhouse-server.err.log\r\n![image](https://user-images.githubusercontent.com/25974296/108454331-1e656b00-72a7-11eb-86f5-7591c01080fe.png)\r\n\r\nThen I try to add default disk under storage_configuraiton->disks in config.xml\r\nrefer to my config.xml\r\n![image](https://user-images.githubusercontent.com/25974296/108453830-3688ba80-72a6-11eb-97aa-557daa61d3c2.png)\r\n\r\nBut still fail.\r\nrefer to my clickhouse-server.err.log\r\n![image](https://user-images.githubusercontent.com/25974296/108453557-ac405680-72a5-11eb-8e44-cd8a0d230d39.png)\r\nI hope you can tell me how to fix this error,thx","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20954/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20945","id":811504241,"node_id":"MDU6SXNzdWU4MTE1MDQyNDE=","number":20945,"title":"add support  AvroConfluent as Output format","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-18T22:04:15Z","updated_at":"2021-02-19T07:38:19Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"we need AvroConfluent as Output format to write it into Kafka engine","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20945/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20710","id":810455358,"node_id":"MDU6SXNzdWU4MTA0NTUzNTg=","number":20710,"title":"Add a function to compare maps with different order of the same key:value pairs ","user":{"login":"vzakaznikov","id":41681088,"node_id":"MDQ6VXNlcjQxNjgxMDg4","avatar_url":"https://avatars.githubusercontent.com/u/41681088?v=4","gravatar_id":"","url":"https://api.github.com/users/vzakaznikov","html_url":"https://github.com/vzakaznikov","followers_url":"https://api.github.com/users/vzakaznikov/followers","following_url":"https://api.github.com/users/vzakaznikov/following{/other_user}","gists_url":"https://api.github.com/users/vzakaznikov/gists{/gist_id}","starred_url":"https://api.github.com/users/vzakaznikov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vzakaznikov/subscriptions","organizations_url":"https://api.github.com/users/vzakaznikov/orgs","repos_url":"https://api.github.com/users/vzakaznikov/repos","events_url":"https://api.github.com/users/vzakaznikov/events{/privacy}","received_events_url":"https://api.github.com/users/vzakaznikov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-17T19:05:08Z","updated_at":"2021-04-20T00:06:04Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"Need to add function to compare maps with different order of the same key:value pairs.\r\n\r\n**Use case**\r\n\r\nCurrently can't compare if maps have different order of the same key:value pairs.\r\n \r\n```\r\nuser-host :) SELECT map(1,2,3,4) = map(3,4,1,2)\r\n\r\nSELECT {1, 2, 3, 4} = {3, 4, 1, 2}\r\n\r\nQuery id: 0880d259-5312-49bd-b91a-995efe7cda7c\r\nâ”Œâ”€equals(map(1, 2, 3, 4), map(3, 4, 1, 2))â”€â”\r\nâ”‚                                        0 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n1 rows in set. Elapsed: 0.004 sec. \r\n\r\nuser-host :)\r\n```\r\n\r\n**Describe the solution you'd like**\r\nNeed a function that will support comparison of maps where they have the same key:value pairs but in a different order.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20710/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20587","id":809582112,"node_id":"MDU6SXNzdWU4MDk1ODIxMTI=","number":20587,"title":"Approximate GROUP BY with ORDER BY ... LIMIT distributed query.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-16T19:21:18Z","updated_at":"2021-02-16T19:23:29Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"When doing distributed query with\r\n\r\nGROUP BY ... ORDER BY ... LIMIT ...\r\n\r\nwe can get approximate result if perform ORDER BY and LIMIT on every shard and then merge filtered data on the initiating node.\r\n\r\nIt will send less data over network making query tremendously faster. \r\nThe results can be accurate enough and it will be useful to make this behaviour available under a setting.\r\n\r\nThe result will be accurate if top keys appeared to be the same on every shard.\r\nWe can also allow to tune the accuracy by multiplying limit to some value.\r\n\r\nSee also:\r\n\\- `max_rows_to_group_by` and `group_by_overflow_mode = 'any'`;","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20587/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20586","id":809571881,"node_id":"MDU6SXNzdWU4MDk1NzE4ODE=","number":20586,"title":"Functions calculation can be moved after LIMIT","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-16T19:04:44Z","updated_at":"2021-02-16T19:04:44Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"The first query did not execute successfully:\r\n\r\n```\r\nexample.yandex.net :) SELECT arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '\\n'), count() AS c FROM clusterAllReplicas(mtgiga_all, system.trace_log) WHERE event_date >= '2021-01-01' AND trace_type = 'CPU' GROUP BY trace ORDER BY c DESC LIMIT 100 SETTINGS skip_unavailable_shards = 1 \\G\r\n\r\nSELECT \r\n    arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '\\n'),\r\n    count() AS c\r\nFROM clusterAllReplicas(mtgiga_all, system.trace_log)\r\nWHERE (event_date >= '2021-01-01') AND (trace_type = 'CPU')\r\nGROUP BY trace\r\nORDER BY c DESC\r\nLIMIT 100\r\nSETTINGS skip_unavailable_shards = 1\r\n\r\nâ†‘ Progress: 67.78 billion rows, 6.86 TB (329.57 million rows/s., 33.36 GB/s.) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š 98%\r\nReceived exception from server (version 20.8.11):\r\nCode: 241. DB::Exception: Received from localhost:9000. DB::Exception: Memory limit (for query) exceeded: would use 14.07 GiB (attempt to allocate chunk of 4294968384 bytes), maximum: 13.97 GiB. \r\n\r\n0 rows in set. Elapsed: 267.064 sec. Processed 67.78 billion rows, 6.86 TB (253.78 million rows/s., 25.69 GB/s.) \r\n```\r\n\r\nAfter I move LIMIT into subquery it's Ok:\r\n```\r\nexample.yandex.net :) SELECT arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '\\n'), c FROM (SELECT trace, count() AS c FROM clusterAllReplicas(mtgiga_all, system.trace_log) WHERE event_date >= '2021-01-01' AND trace_type = 'CPU' GROUP BY trace ORDER BY c DESC LIMIT 100) SETTINGS skip_unavailable_shards = 1 \\G\r\n\r\nSELECT \r\n    arrayStringConcat(arrayMap(x -> demangle(addressToSymbol(x)), trace), '\\n'),\r\n    c\r\nFROM \r\n(\r\n    SELECT \r\n        trace,\r\n        count() AS c\r\n    FROM clusterAllReplicas(mtgiga_all, system.trace_log)\r\n    WHERE (event_date >= '2021-01-01') AND (trace_type = 'CPU')\r\n    GROUP BY trace\r\n    ORDER BY c DESC\r\n    LIMIT 100\r\n)\r\nSETTINGS skip_unavailable_shards = 1\r\n\r\nRow 1:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nc:                                                                                        3312313\r\n\r\nRow 2:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        2332180\r\n\r\nRow 3:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        2272843\r\n\r\nRow 4:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        2087508\r\n\r\nRow 5:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        1976327\r\n\r\nRow 6:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        1880338\r\n\r\nRow 7:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): sallocx\r\nc:                                                                                        1737007\r\n\r\nRow 8:\r\nâ”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        1678157\r\n\r\nRow 9:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        1621916\r\n\r\nRow 10:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        1320538\r\n\r\nRow 11:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<char8_t>::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        1250724\r\n\r\nRow 12:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        1225631\r\n\r\nRow 13:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        1129279\r\n\r\nRow 14:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): void DB::Set::executeImplCase<DB::SetMethodOneNumber<unsigned short, FixedHashSet<unsigned short, Allocator<true, true> >, false>, false>(DB::SetMethodOneNumber<unsigned short, FixedHashSet<unsigned short, Allocator<true, true> >, false>&, std::__1::vector<DB::IColumn const*, std::__1::allocator<DB::IColumn const*> > const&, DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul>&, bool, unsigned long, DB::PODArray<char8_t, 4096ul, Allocator<false, false>, 15ul, 16ul> const*) const\r\nc:                                                                                        1127890\r\n\r\nRow 15:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        1043651\r\n\r\nRow 16:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        1015684\r\n\r\nRow 17:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator delete(void*, unsigned long)\r\nc:                                                                                        987412\r\n\r\nRow 18:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        977037\r\n\r\nRow 19:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator delete(void*, unsigned long)\r\nc:                                                                                        970672\r\n\r\nRow 20:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        932675\r\n\r\nRow 21:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        918036\r\n\r\nRow 22:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        915962\r\n\r\nRow 23:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::QueryProfilerReal::signalHandler(int, siginfo_t*, void*)\r\nc:                                                                                        904627\r\n\r\nRow 24:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        881437\r\n\r\nRow 25:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): nallocx\r\nc:                                                                                        858912\r\n\r\nRow 26:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator delete(void*, unsigned long)\r\nc:                                                                                        831397\r\n\r\nRow 27:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nc:                                                                                        822176\r\n\r\nRow 28:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::CurrentThread::getMemoryTracker()\r\nc:                                                                                        821204\r\n\r\nRow 29:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::DataTypeArray::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeImpl(unsigned long)\r\nDB::PipelineExecutor::execute(unsigned long)\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        813030\r\n\r\nRow 30:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator new(unsigned long)\r\nc:                                                                                        790941\r\n\r\nRow 31:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        760268\r\n\r\nRow 32:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): sallocx\r\nc:                                                                                        740275\r\n\r\nRow 33:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator delete(void*, unsigned long)\r\nc:                                                                                        728015\r\n\r\nRow 34:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        707181\r\n\r\nRow 35:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): arena_dalloc_bin_junked_locked\r\nc:                                                                                        706596\r\n\r\nRow 36:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        703337\r\n\r\nRow 37:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        693685\r\n\r\nRow 38:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): arena_dalloc_bin_junked_locked\r\nc:                                                                                        685523\r\n\r\nRow 39:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::DataTypeArray::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        684597\r\n\r\nRow 40:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        679654\r\n\r\nRow 41:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        664136\r\n\r\nRow 42:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        644380\r\n\r\nRow 43:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<unsigned long>::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        641596\r\n\r\nRow 44:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<unsigned long>::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        635142\r\n\r\nRow 45:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): nallocx\r\nc:                                                                                        610090\r\n\r\nRow 46:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): nallocx\r\nc:                                                                                        591755\r\n\r\nRow 47:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        561109\r\n\r\nRow 48:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): arena_dalloc_bin_junked_locked\r\nc:                                                                                        542211\r\n\r\nRow 49:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        538622\r\n\r\nRow 50:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): arena_dalloc_bin_junked_locked\r\nc:                                                                                        532725\r\n\r\nRow 51:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        532126\r\n\r\nRow 52:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        520085\r\n\r\nRow 53:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        513711\r\n\r\nRow 54:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeImpl(unsigned long)\r\nDB::PipelineExecutor::execute(unsigned long)\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        503340\r\n\r\nRow 55:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        501146\r\n\r\nRow 56:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeStep(std::__1::atomic<bool>*)\r\nDB::PullingPipelineExecutor::pull(DB::Chunk&)\r\nDB::PullingPipelineExecutor::pull(DB::Block&)\r\nDB::PipelineExecutingBlockInputStream::readImpl()\r\nDB::IBlockInputStream::read()\r\nDB::LazyBlockInputStream::readImpl()\r\nc:                                                                                        494928\r\n\r\nRow 57:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        490787\r\n\r\nRow 58:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        482599\r\n\r\nRow 59:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        479304\r\n\r\nRow 60:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nc:                                                                                        474664\r\n\r\nRow 61:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        464771\r\n\r\nRow 62:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        463907\r\n\r\nRow 63:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        457924\r\n\r\nRow 64:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        454548\r\n\r\nRow 65:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::DataTypeArray::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        452542\r\n\r\nRow 66:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        449990\r\n\r\nRow 67:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): nallocx\r\nc:                                                                                        444792\r\n\r\nRow 68:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        437426\r\n\r\nRow 69:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        430287\r\n\r\nRow 70:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeStep(std::__1::atomic<bool>*)\r\nDB::PullingPipelineExecutor::pull(DB::Chunk&)\r\nDB::PullingPipelineExecutor::pull(DB::Block&)\r\nc:                                                                                        428560\r\n\r\nRow 71:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\n\r\n\r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\n\r\nThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>)\r\n\r\nc:                                                                                        428309\r\n\r\nRow 72:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<char8_t>::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        414655\r\n\r\nRow 73:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): memcpy\r\nlarge_ralloc\r\narena_ralloc\r\nrealloc\r\nAllocator<false, false>::realloc(void*, unsigned long, unsigned long, unsigned long)\r\n\r\nDB::DataTypeString::deserializeBinaryBulk(DB::IColumn&, DB::ReadBuffer&, unsigned long, double) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::continueReadingChain(DB::MergeTreeRangeReader::ReadResult&, unsigned long&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeStep(std::__1::atomic<bool>*)\r\nDB::PullingPipelineExecutor::pull(DB::Chunk&)\r\nDB::PullingPipelineExecutor::pull(DB::Block&)\r\nDB::PipelineExecutingBlockInputStream::readImpl()\r\nDB::IBlockInputStream::read()\r\nDB::LazyBlockInputStream::readImpl()\r\nDB::IBlockInputStream::read()\r\nDB::CreatingSetsTransform::work()\r\n\r\n\r\nDB::PipelineExecutor::executeImpl(unsigned long)\r\nc:                                                                                        413779\r\n\r\nRow 74:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        413305\r\n\r\nRow 75:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        410997\r\n\r\nRow 76:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        408317\r\n\r\nRow 77:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<unsigned long>::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        399713\r\n\r\nRow 78:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::__split_buffer<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName>&>::~__split_buffer()\r\nc:                                                                                        394188\r\n\r\nRow 79:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::free(long)\r\nc:                                                                                        393402\r\n\r\nRow 80:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): CurrentMemoryTracker::alloc(long)\r\nc:                                                                                        371067\r\n\r\nRow 81:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): operator delete(void*, unsigned long)\r\nc:                                                                                        370837\r\n\r\nRow 82:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::Block::safeGetByPosition(unsigned long)\r\nc:                                                                                        363559\r\n\r\nRow 83:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        353907\r\n\r\nRow 84:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        351348\r\n\r\nRow 85:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): re2_st::DFA::AnalyzeSearch(re2_st::DFA::SearchParams*)\r\nc:                                                                                        348707\r\n\r\nRow 86:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): re2_st::DFA::AnalyzeSearch(re2_st::DFA::SearchParams*)\r\nc:                                                                                        347960\r\n\r\nRow 87:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        346862\r\n\r\nRow 88:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        344739\r\n\r\nRow 89:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        343361\r\n\r\nRow 90:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        330615\r\n\r\nRow 91:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnString::serializeValueIntoArena(unsigned long, DB::Arena&, char const*&) const\r\nc:                                                                                        330257\r\n\r\nRow 92:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<char8_t>::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        326634\r\n\r\nRow 93:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): re2_st::Prog::SearchDFA(re2_st::StringPiece const&, re2_st::StringPiece const&, re2_st::Prog::Anchor, re2_st::Prog::MatchKind, re2_st::StringPiece*, bool*, re2_st::SparseSetT<void>*)\r\nc:                                                                                        325477\r\n\r\nRow 94:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): DB::ColumnVector<char8_t>::insertFrom(DB::IColumn const&, unsigned long)\r\nc:                                                                                        323853\r\n\r\nRow 95:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nZSTD_decompressBlock_internal\r\n\r\nZSTD_decompress\r\nDB::CompressionCodecZSTD::doDecompressData(char const*, unsigned int, char*, unsigned int) const\r\nDB::ICompressionCodec::decompress(char const*, unsigned int, char*) const\r\nDB::CompressedReadBufferBase::decompress(char*, unsigned long, unsigned long)\r\nDB::CompressedReadBufferFromFile::nextImpl()\r\nDB::CompressedReadBufferFromFile::seek(unsigned long, unsigned long)\r\nDB::MergeTreeReaderStream::seekToMark(unsigned long)\r\n\r\nDB::IDataType::deserializeBinaryBulkWithMultipleStreams(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&) const\r\nDB::MergeTreeReaderWide::readData(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::IDataType const&, DB::IColumn&, unsigned long, bool, unsigned long, bool)\r\nDB::MergeTreeReaderWide::readRows(unsigned long, bool, unsigned long, std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::DelayedStream::finalize(std::__1::vector<COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::allocator<COW<DB::IColumn>::immutable_ptr<DB::IColumn> > >&)\r\nDB::MergeTreeRangeReader::startReadingChain(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeRangeReader::read(unsigned long, std::__1::deque<DB::MarkRange, std::__1::allocator<DB::MarkRange> >&)\r\nDB::MergeTreeBaseSelectProcessor::readFromPartImpl()\r\nDB::MergeTreeBaseSelectProcessor::generate()\r\nDB::ISource::work()\r\nDB::SourceWithProgress::work()\r\n\r\n\r\nDB::PipelineExecutor::executeStep(std::__1::atomic<bool>*)\r\nDB::PullingPipelineExecutor::pull(DB::Chunk&)\r\nDB::PullingPipelineExecutor::pull(DB::Block&)\r\nDB::PipelineExecutingBlockInputStream::readImpl()\r\nDB::IBlockInputStream::read()\r\nc:                                                                                        323664\r\n\r\nRow 96:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nc:                                                                                        322741\r\n\r\nRow 97:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): \r\nc:                                                                                        288352\r\n\r\nRow 98:\r\nâ”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >::basic_string(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&)\r\nc:                                                                                        288157\r\n\r\nRow 99:\r\nâ”€â”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): malloc_default\r\nc:                                                                                        285577\r\n\r\nRow 100:\r\nâ”€â”€â”€â”€â”€â”€â”€â”€\r\narrayStringConcat(arrayMap(lambda(tuple(x), demangle(addressToSymbol(x))), trace), '\\n'): void DB::PODArrayBase<4ul, 4096ul, Allocator<false, false>, 15ul, 16ul>::alloc<>(unsigned long)\r\nDB::ColumnVector<unsigned int>::insert(DB::Field const&)\r\nDB::ColumnTuple::insert(DB::Field const&)\r\nDB::IDataType::createColumnConst(unsigned long, DB::Field const&) const\r\nDB::ActionsMatcher::visit(DB::ASTLiteral const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&)\r\nDB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ExpressionActions>&, bool)\r\nDB::ExpressionAnalyzer::getConstActions()\r\nDB::evaluateConstantExpression(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&)\r\n\r\nDB::makeExplicitSet(DB::ASTFunction const*, DB::Block const&, bool, DB::Context const&, DB::SizeLimits const&, std::__1::unordered_map<DB::PreparedSetKey, std::__1::shared_ptr<DB::Set>, DB::PreparedSetKey::Hash, std::__1::equal_to<DB::PreparedSetKey>, std::__1::allocator<std::__1::pair<DB::PreparedSetKey const, std::__1::shared_ptr<DB::Set> > > >&)\r\nDB::ActionsMatcher::makeSet(DB::ASTFunction const&, DB::ActionsMatcher::Data&, bool)\r\nDB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&)\r\nDB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&)\r\n\r\nDB::SelectQueryExpressionAnalyzer::appendGroupBy(DB::ExpressionActionsChain&, bool, bool, std::__1::vector<std::__1::shared_ptr<DB::ExpressionActions>, std::__1::allocator<std::__1::shared_ptr<DB::ExpressionActions> > >&)\r\nDB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterInfo> const&, DB::Block const&)\r\nDB::InterpreterSelectQuery::getSampleBlockImpl()\r\n\r\nDB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&)\r\nDB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)\r\nDB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&)\r\nDB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum)\r\n\r\nDB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool)\r\nDB::TCPHandler::runImpl()\r\nDB::TCPHandler::run()\r\nPoco::Net::TCPServerConnection::start()\r\nPoco::Net::TCPServerDispatcher::run()\r\nPoco::PooledThread::run()\r\nc:                                                                                        284370\r\n\r\n100 rows in set. Elapsed: 200.421 sec. Processed 67.78 billion rows, 6.86 TB (338.21 million rows/s., 34.23 GB/s.)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20586/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20500","id":808338335,"node_id":"MDU6SXNzdWU4MDgzMzgzMzU=","number":20500,"title":"MATERILIZED VIEW: select fails when underlying table has different Enum definition","user":{"login":"adiletkabylbekov","id":33948242,"node_id":"MDQ6VXNlcjMzOTQ4MjQy","avatar_url":"https://avatars.githubusercontent.com/u/33948242?v=4","gravatar_id":"","url":"https://api.github.com/users/adiletkabylbekov","html_url":"https://github.com/adiletkabylbekov","followers_url":"https://api.github.com/users/adiletkabylbekov/followers","following_url":"https://api.github.com/users/adiletkabylbekov/following{/other_user}","gists_url":"https://api.github.com/users/adiletkabylbekov/gists{/gist_id}","starred_url":"https://api.github.com/users/adiletkabylbekov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/adiletkabylbekov/subscriptions","organizations_url":"https://api.github.com/users/adiletkabylbekov/orgs","repos_url":"https://api.github.com/users/adiletkabylbekov/repos","events_url":"https://api.github.com/users/adiletkabylbekov/events{/privacy}","received_events_url":"https://api.github.com/users/adiletkabylbekov/received_events","type":"User","site_admin":false},"labels":[{"id":1351290474,"node_id":"MDU6TGFiZWwxMzUxMjkwNDc0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mutations","name":"comp-mutations","color":"b5bcff","default":false,"description":"ALTER UPDATE/DELETE"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2266396286,"node_id":"MDU6TGFiZWwyMjY2Mzk2Mjg2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/v20.8-affected","name":"v20.8-affected","color":"c2bfff","default":false,"description":""},{"id":2572960474,"node_id":"MDU6TGFiZWwyNTcyOTYwNDc0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-alter","name":"comp-alter","color":"b5bcff","default":false,"description":"Problems with alter"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":12,"created_at":"2021-02-15T09:01:50Z","updated_at":"2021-03-21T03:01:31Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"After upgrade from 19.17.9.60 to 20.8.8.2 doesn't work selection over new added Enum values after ALTER column\r\n\r\nCH version: 20.8.8.2\r\n\r\n```\r\nCREATE MATERIALIZED VIEW enum_alter\r\n(\r\n    `date` Date,\r\n    `enum_type` Enum8('none' = 0, 'one' = 1, 'two' = 2),\r\n    `sales_id` UInt32,\r\n    `campaign_id` UInt32\r\n)\r\nENGINE = SummingMergeTree()\r\nPARTITION BY toYYYYMMDD(date)\r\nORDER BY (enum_type, sales_id, campaign_id)\r\nAS SELECT '2021-01-01' as date,\r\n    cast(1, 'Enum8(\\'zero\\'=0, \\'one\\'=1)') AS enum,\r\n    123456 as sales_id,\r\n    1234567 as campaign_id\r\n;\r\n\r\nalter table \".inner.enum_alter\" modify column enum_type Enum8('zero' = 0, 'one' = 1, 'two' = 2);\r\n\r\nselect * from enum_alter where enum_type in ('one','two');\r\nSELECT *\r\nFROM enum_alter\r\nWHERE enum_type IN ('one', 'two')\r\n\r\n\r\nReceived exception from server (version 20.8.8):\r\nCode: 169. DB::Exception: Received from localhost:9000. DB::Exception: Key expression contains comparison between inconvertible types: Enum8('zero' = 0, 'one' = 1, 'two' = 2) and UInt8 inside enum_type IN ('one', 'two'). \r\n\r\n0 rows in set. Elapsed: 0.050 sec. \r\n\r\n\r\n2021.02.15 11:56:58.052028 [ 26487 ] {58b8b912-365d-4763-9a28-1a2ed5b06334} <Error> executeQuery: Code: 169, e.displayText() = DB::Exception: Key expression contains comparison between inconvertible types: Enum8('zero' = 0, 'one' = 1, 'two' = 2) and UInt8 inside enum_type IN ('one', 'two') (version 20.8.8.2 (official build)) (from [::1]:56968) (in query: select * from enum_alter where enum_type in ('one','two');), Stack trace (when copying this message, always include the lines below):\r\n\r\n0. Poco::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0x18b63820 in /usr/bin/clickhouse\r\n1. DB::Exception::Exception(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, int) @ 0xe5b932d in /usr/bin/clickhouse\r\n2. ? @ 0x15f62244 in /usr/bin/clickhouse\r\n3. DB::KeyCondition::traverseAST(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::Block&) @ 0x15f5e240 in /usr/bin/clickhouse\r\n4. DB::KeyCondition::KeyCondition(DB::SelectQueryInfo const&, DB::Context const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::ExpressionActions> const&) @ 0x15f5eafc in /usr/bin/clickhouse\r\n5. DB::MergeTreeDataSelectExecutor::readFromParts(std::__1::vector<std::__1::shared_ptr<DB::IMergeTreeDataPart const>, std::__1::allocator<std::__1::shared_ptr<DB::IMergeTreeDataPart const> > >, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, DB::Context const&, unsigned long, unsigned int, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, long, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, long> > > const*) const @ 0x16029404 in /usr/bin/clickhouse\r\n6. DB::MergeTreeDataSelectExecutor::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, DB::Context const&, unsigned long, unsigned int, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, long, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, long> > > const*) const @ 0x1602ea62 in /usr/bin/clickhouse\r\n7. DB::StorageMergeTree::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, DB::Context const&, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x15d8cd34 in /usr/bin/clickhouse\r\n8. DB::StorageMaterializedView::read(std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, DB::SelectQueryInfo const&, DB::Context const&, DB::QueryProcessingStage::Enum, unsigned long, unsigned int) @ 0x15d7594e in /usr/bin/clickhouse\r\n9. DB::ReadFromStorageStep::ReadFromStorageStep(std::__1::shared_ptr<DB::RWLockImpl::LockHolderImpl>, std::__1::shared_ptr<DB::StorageInMemoryMetadata const>&, DB::SelectQueryOptions, std::__1::shared_ptr<DB::IStorage>, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, DB::SelectQueryInfo const&, std::__1::shared_ptr<DB::Context>, DB::QueryProcessingStage::Enum, unsigned long, unsigned long) @ 0x1655d2cb in /usr/bin/clickhouse\r\n10. DB::InterpreterSelectQuery::executeFetchColumns(DB::QueryProcessingStage::Enum, DB::QueryPlan&, std::__1::shared_ptr<DB::PrewhereInfo> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0x1588b941 in /usr/bin/clickhouse\r\n11. DB::InterpreterSelectQuery::executeImpl(DB::QueryPlan&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>) @ 0x1588f905 in /usr/bin/clickhouse\r\n12. DB::InterpreterSelectQuery::buildQueryPlan(DB::QueryPlan&) @ 0x15891354 in /usr/bin/clickhouse\r\n13. DB::InterpreterSelectWithUnionQuery::buildQueryPlan(DB::QueryPlan&) @ 0x15a0c918 in /usr/bin/clickhouse\r\n14. DB::InterpreterSelectWithUnionQuery::execute() @ 0x15a0caea in /usr/bin/clickhouse\r\n15. ? @ 0x15b95d12 in /usr/bin/clickhouse\r\n16. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0x15b97672 in /usr/bin/clickhouse\r\n17. DB::TCPHandler::runImpl() @ 0x16232ed5 in /usr/bin/clickhouse\r\n18. DB::TCPHandler::run() @ 0x16233c40 in /usr/bin/clickhouse\r\n19. Poco::Net::TCPServerConnection::start() @ 0x18a817db in /usr/bin/clickhouse\r\n20. Poco::Net::TCPServerDispatcher::run() @ 0x18a81ef8 in /usr/bin/clickhouse\r\n21. Poco::PooledThread::run() @ 0x18c005f6 in /usr/bin/clickhouse\r\n22. Poco::ThreadImpl::runnableEntry(void*) @ 0x18bfb9f0 in /usr/bin/clickhouse\r\n23. start_thread @ 0x7dc5 in /usr/lib64/libpthread-2.17.so\r\n24. clone @ 0xf773d in /usr/lib64/libc-2.17.so\r\n```\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20500/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20481","id":808026027,"node_id":"MDU6SXNzdWU4MDgwMjYwMjc=","number":20481,"title":"(idea) Logical fuzzing with SQLite.","user":{"login":"alexey-milovidov","id":18581488,"node_id":"MDQ6VXNlcjE4NTgxNDg4","avatar_url":"https://avatars.githubusercontent.com/u/18581488?v=4","gravatar_id":"","url":"https://api.github.com/users/alexey-milovidov","html_url":"https://github.com/alexey-milovidov","followers_url":"https://api.github.com/users/alexey-milovidov/followers","following_url":"https://api.github.com/users/alexey-milovidov/following{/other_user}","gists_url":"https://api.github.com/users/alexey-milovidov/gists{/gist_id}","starred_url":"https://api.github.com/users/alexey-milovidov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/alexey-milovidov/subscriptions","organizations_url":"https://api.github.com/users/alexey-milovidov/orgs","repos_url":"https://api.github.com/users/alexey-milovidov/repos","events_url":"https://api.github.com/users/alexey-milovidov/events{/privacy}","received_events_url":"https://api.github.com/users/alexey-milovidov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-14T19:15:40Z","updated_at":"2021-02-14T19:16:03Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"Create a table with several fields of randomly selected types among the common SQL types (text, numbers, decimal).\r\n\r\nApply LowCardinality and Nullable randomly.\r\n\r\nPick random table engine. If it is MergeTree, pick random subset of fields for ORDER BY expression and maybe something for PARTITION BY expression. Also pick part types and index granularity at random. Create random secondary indices.\r\n\r\nCreate Distributed, Merge or Buffer tables randomly on top of this table.\r\n\r\nFill the table with random data in order of 100 000 to 10 000 000 records.\r\n\r\nCreate ordinary table in sqlite with the same types (but without LowCardinality), fill it with the same data.\r\n\r\nGenerate simple random queries with GROUP BY (any subset of fields), ORDER BY (must order by every fields in SELECT to get deterministic result) and WHERE (comparison of numeric fields with constants and substring filtering with small LIKE expressions similar to `'%a%'`).\r\n\r\nCompare the results of ClickHouse with SQLite.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20481/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20469","id":807822151,"node_id":"MDU6SXNzdWU4MDc4MjIxNTE=","number":20469,"title":"Function to convert IPv6 to appropriate IPv4/6 string?","user":{"login":"mzealey","id":6083471,"node_id":"MDQ6VXNlcjYwODM0NzE=","avatar_url":"https://avatars.githubusercontent.com/u/6083471?v=4","gravatar_id":"","url":"https://api.github.com/users/mzealey","html_url":"https://github.com/mzealey","followers_url":"https://api.github.com/users/mzealey/followers","following_url":"https://api.github.com/users/mzealey/following{/other_user}","gists_url":"https://api.github.com/users/mzealey/gists{/gist_id}","starred_url":"https://api.github.com/users/mzealey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzealey/subscriptions","organizations_url":"https://api.github.com/users/mzealey/orgs","repos_url":"https://api.github.com/users/mzealey/repos","events_url":"https://api.github.com/users/mzealey/events{/privacy}","received_events_url":"https://api.github.com/users/mzealey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":4,"created_at":"2021-02-13T20:20:40Z","updated_at":"2021-02-16T08:55:37Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Following on from #19518, it would be nice to have a reverse function which operates on an ipv6 address and returns it in ipv4 or ipv6 notation depending on type. So basically `replaceOne(IPv6NumToString(ip), '::ffff:', '')`. This should then allow the `IPv6` type to work pretty much the same as the postgres `inet` type I think?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20469/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20456","id":807604677,"node_id":"MDU6SXNzdWU4MDc2MDQ2Nzc=","number":20456,"title":"limit changes tables order? and makes the query slower","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":756395244,"node_id":"MDU6TGFiZWw3NTYzOTUyNDQ=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/performance","name":"performance","color":"c2e0c6","default":false,"description":null},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-12T23:09:12Z","updated_at":"2021-02-14T01:56:47Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"21.3.1.5944\r\n\r\n```sql\r\ndrop table if exists t ;\r\ndrop table if exists temp ;\r\n\r\nCREATE TABLE t\r\n(   type LowCardinality(String),\r\n    game LowCardinality(String),\r\n    round String,\r\n    trans String,\r\n    casino Int32,\r\n    row_hash UInt64 default cityHash64(casino, game, round, trans, type))\r\nENGINE = MergeTree() ORDER BY (casino, game, round, trans, type);\r\n\r\nCREATE TABLE temp\r\n(   type LowCardinality(String),\r\n    game LowCardinality(String),\r\n    round String,\r\n    trans String,\r\n    casino Int32)\r\nENGINE = MergeTree() ORDER BY (casino, game, round, trans, type);\r\n\r\ninsert into t(casino, game, round, trans, type)\r\nselect number%103, toString(cityHash64(number%999)), toString(number%13), toString(number%4444), toString(number%3) from numbers(3, 50000000);\r\n\r\ninsert into temp(casino, game, round, trans, type)\r\nselect number%103, toString(cityHash64(number%999)), toString(number%13), toString(number%4444), toString(number%3) from numbers(500000);\r\n\r\nSELECT\r\n    temp.casino,\r\n    temp.game,\r\n    temp.round,\r\n    temp.trans,\r\n    temp.type\r\nFROM t\r\nANTI RIGHT JOIN temp ON t.row_hash = cityHash64(temp.casino, temp.game, temp.round, temp.trans, temp.type)\r\n\r\nâ”Œâ”€temp.casinoâ”€â”¬â”€temp.gameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€temp.roundâ”€â”¬â”€temp.transâ”€â”¬â”€temp.typeâ”€â”\r\nâ”‚           1 â”‚ 10577349846663553072 â”‚ 1          â”‚ 1          â”‚ 1         â”‚\r\nâ”‚           2 â”‚ 18198135717204167749 â”‚ 2          â”‚ 2          â”‚ 2         â”‚\r\nâ”‚           0 â”‚ 4761183170873013810  â”‚ 0          â”‚ 0          â”‚ 0         â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n3 rows in set. Elapsed: 0.498 sec. Processed 50.50 million rows, 415.39 MB (101.31 million rows/s., 833.36 MB/s.)\r\n\r\n\r\n---- add  LIMIT 5;\r\n\r\nSELECT\r\n    temp.casino,\r\n    temp.game,\r\n    temp.round,\r\n    temp.trans,\r\n    temp.type\r\nFROM t\r\nANTI RIGHT JOIN temp ON t.row_hash = cityHash64(temp.casino, temp.game, temp.round, temp.trans, temp.type)\r\nLIMIT 5;\r\nâ”Œâ”€temp.casinoâ”€â”¬â”€temp.gameâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€temp.roundâ”€â”¬â”€temp.transâ”€â”¬â”€temp.typeâ”€â”\r\nâ”‚           1 â”‚ 10577349846663553072 â”‚ 1          â”‚ 1          â”‚ 1         â”‚\r\nâ”‚           2 â”‚ 18198135717204167749 â”‚ 2          â”‚ 2          â”‚ 2         â”‚\r\nâ”‚           0 â”‚ 4761183170873013810  â”‚ 0          â”‚ 0          â”‚ 0         â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n3 rows in set. Elapsed: 78.228 sec. Processed 50.50 million rows, 415.39 MB (645.55 thousand rows/s., 5.31 MB/s.)\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20456/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20455","id":807583148,"node_id":"MDU6SXNzdWU4MDc1ODMxNDg=","number":20455,"title":"\"ANTI JOIN + USING\" = incorrect results.","user":{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false},"labels":[{"id":386401505,"node_id":"MDU6TGFiZWwzODY0MDE1MDU=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/bug","name":"bug","color":"ee0701","default":true,"description":"Confirmed user-visible misbehaviour in official release"},{"id":1357578153,"node_id":"MDU6TGFiZWwxMzU3NTc4MTUz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-joins","name":"comp-joins","color":"b5bcff","default":false,"description":"JOINs"},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},"assignees":[{"login":"vdimir","id":7023786,"node_id":"MDQ6VXNlcjcwMjM3ODY=","avatar_url":"https://avatars.githubusercontent.com/u/7023786?v=4","gravatar_id":"","url":"https://api.github.com/users/vdimir","html_url":"https://github.com/vdimir","followers_url":"https://api.github.com/users/vdimir/followers","following_url":"https://api.github.com/users/vdimir/following{/other_user}","gists_url":"https://api.github.com/users/vdimir/gists{/gist_id}","starred_url":"https://api.github.com/users/vdimir/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vdimir/subscriptions","organizations_url":"https://api.github.com/users/vdimir/orgs","repos_url":"https://api.github.com/users/vdimir/repos","events_url":"https://api.github.com/users/vdimir/events{/privacy}","received_events_url":"https://api.github.com/users/vdimir/received_events","type":"User","site_admin":false},{"login":"den-crane","id":19737682,"node_id":"MDQ6VXNlcjE5NzM3Njgy","avatar_url":"https://avatars.githubusercontent.com/u/19737682?v=4","gravatar_id":"","url":"https://api.github.com/users/den-crane","html_url":"https://github.com/den-crane","followers_url":"https://api.github.com/users/den-crane/followers","following_url":"https://api.github.com/users/den-crane/following{/other_user}","gists_url":"https://api.github.com/users/den-crane/gists{/gist_id}","starred_url":"https://api.github.com/users/den-crane/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/den-crane/subscriptions","organizations_url":"https://api.github.com/users/den-crane/orgs","repos_url":"https://api.github.com/users/den-crane/repos","events_url":"https://api.github.com/users/den-crane/events{/privacy}","received_events_url":"https://api.github.com/users/den-crane/received_events","type":"User","site_admin":false}],"milestone":null,"comments":4,"created_at":"2021-02-12T22:19:56Z","updated_at":"2021-02-16T11:13:35Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"The idea of ANTI Join to find rows from one table that does not exists in another.\r\nHere we are looking for rows which exist in B1 but does not exists in T1. (4,5,6)\r\nSo for the T1 ClickHouse should provide only empty (null / 0  / '' ) values.\r\nAlso we use ANTI **RIGHT** to put B1 to the right and put into hash-table and this actually surprisingly works. \r\n\r\n*Vertica for ANTI Join case does not allow to query columns from the second table.* But Vertica has another syntax anyway.\r\n\r\n```sql\r\ncreate table T1(c Int64, d String) Engine = Log;\r\ncreate table B1(c Int64, d String) Engine = Log;\r\ninsert into T1 values(1, '1'), (2, '2'), (3, '3');\r\ninsert into B1 values(1, '1'), (4, '4'), (5, '5'), (6, '6');\r\n\r\nSELECT c, d FROM T1 ANTI RIGHT JOIN B1 USING (c);\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â” -- it's unclear (for me) what should be returned in this case .\r\nâ”‚ 4 â”‚   â”‚    -- But still results for c&d should be the consistent.\r\nâ”‚ 5 â”‚   â”‚\r\nâ”‚ 6 â”‚   â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT * FROM T1 ANTI RIGHT JOIN B1 USING (c);\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”¬â”€B1.dâ”€â”\r\nâ”‚ 4 â”‚   â”‚ 4    â”‚\r\nâ”‚ 5 â”‚   â”‚ 5    â”‚\r\nâ”‚ 6 â”‚   â”‚ 6    â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d FROM T1 ANTI RIGHT JOIN B1 USING (c);\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”       --- it has not sense to request columns from T1 because they always empty, but\r\nâ”‚ 4 â”‚   â”‚                  <-----    using should produces results for the left table IF TABLE IS NOT SPECIFIED.\r\nâ”‚ 5 â”‚   â”‚                      --   but here we requested T1 table, but CH returns results for B1\r\nâ”‚ 6 â”‚   â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d, B1.c, B1.d FROM T1 ANTI RIGHT JOIN B1  USING (c)\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”¬â”€B1.câ”€â”¬â”€B1.dâ”€â”  <----- the result has moved to another column  and the result look correct\r\nâ”‚ 0 â”‚   â”‚    4 â”‚ 4    â”‚           \r\nâ”‚ 0 â”‚   â”‚    5 â”‚ 5    â”‚           \r\nâ”‚ 0 â”‚   â”‚    6 â”‚ 6    â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT  *  FROM T1 ANTI RIGHT JOIN B1 on T1.c = B1.c;\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”¬â”€B1.câ”€â”¬â”€B1.dâ”€â”\r\nâ”‚ 0 â”‚   â”‚    4 â”‚ 4    â”‚\r\nâ”‚ 0 â”‚   â”‚    5 â”‚ 5    â”‚\r\nâ”‚ 0 â”‚   â”‚    6 â”‚ 6    â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT c,d FROM T1 ANTI RIGHT JOIN B1 on T1.c = B1.c;\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”\r\nâ”‚ 0 â”‚   â”‚\r\nâ”‚ 0 â”‚   â”‚\r\nâ”‚ 0 â”‚   â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d FROM T1 ANTI RIGHT JOIN B1 on T1.c = B1.c;\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”\r\nâ”‚ 0 â”‚   â”‚                        <----- let's replace using with on\r\nâ”‚ 0 â”‚   â”‚\r\nâ”‚ 0 â”‚   â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d, B1.c, B1.d FROM T1 ANTI RIGHT JOIN B1 on T1.c = B1.c;  \r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”¬â”€B1.câ”€â”¬â”€B1.dâ”€â”\r\nâ”‚ 0 â”‚   â”‚    4 â”‚ 4    â”‚\r\nâ”‚ 0 â”‚   â”‚    5 â”‚ 5    â”‚\r\nâ”‚ 0 â”‚   â”‚    6 â”‚ 6    â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\n\r\n B1 ANTI LEFT JOIN T1\r\n\r\nSELECT c,d FROM B1 ANTI LEFT JOIN T1 USING (c);\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”\r\nâ”‚ 4 â”‚ 4 â”‚\r\nâ”‚ 5 â”‚ 5 â”‚\r\nâ”‚ 6 â”‚ 6 â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT * FROM B1 ANTI LEFT JOIN T1 USING (c);\r\nâ”Œâ”€câ”€â”¬â”€dâ”€â”¬â”€T1.dâ”€â”\r\nâ”‚ 4 â”‚ 4 â”‚      â”‚\r\nâ”‚ 5 â”‚ 5 â”‚      â”‚\r\nâ”‚ 6 â”‚ 6 â”‚      â”‚\r\nâ””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d FROM B1 ANTI LEFT JOIN T1 USING (c);\r\nâ”Œâ”€T1.câ”€â”¬â”€T1.dâ”€â”\r\nâ”‚    4 â”‚      â”‚\r\nâ”‚    5 â”‚      â”‚\r\nâ”‚    6 â”‚      â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d, B1.c, B1.d FROM B1 ANTI LEFT JOIN T1 USING (c);\r\nâ”Œâ”€T1.câ”€â”¬â”€T1.dâ”€â”¬â”€câ”€â”¬â”€dâ”€â”\r\nâ”‚    4 â”‚      â”‚ 4 â”‚ 4 â”‚\r\nâ”‚    5 â”‚      â”‚ 5 â”‚ 5 â”‚\r\nâ”‚    6 â”‚      â”‚ 6 â”‚ 6 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d FROM B1 ANTI LEFT JOIN T1 on T1.c = B1.c;\r\nâ”Œâ”€T1.câ”€â”¬â”€T1.dâ”€â”\r\nâ”‚    4 â”‚      â”‚\r\nâ”‚    5 â”‚      â”‚\r\nâ”‚    6 â”‚      â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”˜\r\n\r\nSELECT T1.c, T1.d, B1.c, B1.d FROM B1 ANTI LEFT JOIN T1 on T1.c = B1.c;\r\nâ”Œâ”€T1.câ”€â”¬â”€T1.dâ”€â”¬â”€câ”€â”¬â”€dâ”€â”\r\nâ”‚    4 â”‚      â”‚ 4 â”‚ 4 â”‚\r\nâ”‚    5 â”‚      â”‚ 5 â”‚ 5 â”‚\r\nâ”‚    6 â”‚      â”‚ 6 â”‚ 6 â”‚\r\nâ””â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜\r\n\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20455/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20451","id":807526583,"node_id":"MDU6SXNzdWU4MDc1MjY1ODM=","number":20451,"title":"Skip parts when materializing TTL for TTL-expressions that are monotonic functions of columns","user":{"login":"siradjev","id":10959667,"node_id":"MDQ6VXNlcjEwOTU5NjY3","avatar_url":"https://avatars.githubusercontent.com/u/10959667?v=4","gravatar_id":"","url":"https://api.github.com/users/siradjev","html_url":"https://github.com/siradjev","followers_url":"https://api.github.com/users/siradjev/followers","following_url":"https://api.github.com/users/siradjev/following{/other_user}","gists_url":"https://api.github.com/users/siradjev/gists{/gist_id}","starred_url":"https://api.github.com/users/siradjev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/siradjev/subscriptions","organizations_url":"https://api.github.com/users/siradjev/orgs","repos_url":"https://api.github.com/users/siradjev/repos","events_url":"https://api.github.com/users/siradjev/events{/privacy}","received_events_url":"https://api.github.com/users/siradjev/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1351290474,"node_id":"MDU6TGFiZWwxMzUxMjkwNDc0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-mutations","name":"comp-mutations","color":"b5bcff","default":false,"description":"ALTER UPDATE/DELETE"},{"id":1401282669,"node_id":"MDU6TGFiZWwxNDAxMjgyNjY5","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-ttl","name":"comp-ttl","color":"b5bcff","default":false,"description":"TTL"},{"id":2572960474,"node_id":"MDU6TGFiZWwyNTcyOTYwNDc0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-alter","name":"comp-alter","color":"b5bcff","default":false,"description":"Problems with alter"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2021-02-12T20:27:52Z","updated_at":"2021-12-05T18:47:27Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Problem description**\r\nThere is some fundamental problem with current TTL methodology that prevents from using it in production-grade non-cloud environment (when quick elastic resource allocation is not possible) for automated data retention purpose without impact to ongoing operation.  \r\nWhenever TTL expression(s) is (are) modified or \"MATERIALIZE TTL\" is called manually all the parts have to mutate to recalculate expression value. (https://github.com/ClickHouse/ClickHouse/issues/10128#issuecomment-769782785)\r\nEven though there is no need to touch 90% of parts all of them are mutated (it may be with hardlinking, but it's still heavy operation if there are many columns x parts). \r\nThis directly brings down DB for quite a while by creating simply too many mutation entries in replication queue causing picture as below:\r\n* merges are not executed due to \"number_of_free_entries_in_pool_to_lower_max_size_of_merge\":\r\n`Not executing log entry MERGE_PARTS for part xxxxx because source parts size (481.19 MiB) is greater than the current maximum (4.45 MiB).`\r\n* mutations just start failing with \"number_of_free_entries_in_pool_to_execute_mutation\":\r\n`Not executing log entry MUTATE_PART for part xxxxx because source parts size (27.52 KiB) is greater than the current maximum (0.00 B).`\r\n\r\nThat one is probably problem on its own as free pool is checked after getting from queue, and there is always no free space in pool when queue is full. It's resolved after manually changing number_of_free_entries_in_pool_to_execute_mutation=1/0, so that clickhouse does not skips execution of log entries. \r\n\r\nImprovements listed in #10128 under \"A large number of background TTL operations...\" are not sufficient, as they address different aspect of this problem. \r\n\r\n**Typical use cases of using TTL**\r\n* whole table data retention or large column data retention - typical expression is `timestampCol + interval`\r\n* moving cold data to slower storage tiers - typical expression is `timestampCol + interval`\r\n\r\n**High-level functional requirement**\r\n* Adding/modifying TTL expression that is a monotonic function of one or several of partitionkey, or primarykey columns, MUST NOT cause any modifications/mutations of existing parts. Modifications of TTL are normal routine when initial dimensioning is not done. Vast majority of usages in community and clickhouse manual is based on `partitionkey + interval` kind of expression. So this alone shall provide huge improvement\r\n* Modifying TTL expression that is a monotonic function of set of columns when set of columns is not changed, MUST NOT cause any modifications/mutations of existing parts.  E.g. if we had `TTL myDate+INTERVAL 3 day` it should not cause any change when we modify it as `TTL myDate+INTERVAL 4 DAY`. It is normal case to increase/decrease number of days to address input data stream growth. \r\n* materialize_ttl_after_modify default MUST be 0 to prevent huge overhead of reworking of old parts for the remaining cases. If required `ALTER TABLE xxx MODIFY TTL MATERIALIZE` can be used to trigger it. Having it as default 1 in 20.8 (#11042) was simple introduction of dangerous non-backward compatible change without feasible way of \"undoing\" it. \r\n\r\n**Describe the solution you'd like**\r\nWhen expression is monotonic function of column(s), columns min/max themselves are enough. They are already kept for partition key, primary key, so when ttl is added on one of these there is no need to do anything. \r\nWhen ttl_only_drop_parts=0 parts that partially satisfy TTL expression will be mutated, when ttl_only_drop_parts=1 only parts that entirely satisfy TTL expression will be mutated. \r\nIf expression has other columns - their minmax will also be required to be kept. \r\nIf one of the columns used in TTL is removed from expression - there is no need to mutate all parts. \r\nThe only cases when mutation on all parts are really needed:\r\n* expression is non-monotonic to columns used in it\r\n* expression is modified with a new column added that is not partition key and that is not a primary key\r\nIn all other cases parts that don't satisfy TTL expression will be simply skipped after run of MATERIALIZE TTL. \r\n\r\n**Describe alternatives you've considered**\r\nExternal shell scripting. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451/reactions","total_count":7,"+1":7,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20451/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20380","id":807204384,"node_id":"MDU6SXNzdWU4MDcyMDQzODQ=","number":20380,"title":"data race found by test about a race 01502_long_log_tinylog_deadlock_race","user":{"login":"qoega","id":2159081,"node_id":"MDQ6VXNlcjIxNTkwODE=","avatar_url":"https://avatars.githubusercontent.com/u/2159081?v=4","gravatar_id":"","url":"https://api.github.com/users/qoega","html_url":"https://github.com/qoega","followers_url":"https://api.github.com/users/qoega/followers","following_url":"https://api.github.com/users/qoega/following{/other_user}","gists_url":"https://api.github.com/users/qoega/gists{/gist_id}","starred_url":"https://api.github.com/users/qoega/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/qoega/subscriptions","organizations_url":"https://api.github.com/users/qoega/orgs","repos_url":"https://api.github.com/users/qoega/repos","events_url":"https://api.github.com/users/qoega/events{/privacy}","received_events_url":"https://api.github.com/users/qoega/received_events","type":"User","site_admin":false},"labels":[{"id":1955634273,"node_id":"MDU6TGFiZWwxOTU1NjM0Mjcz","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/testing","name":"testing","color":"c9a224","default":false,"description":"Special issue with list of bugs found by CI"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-12T12:40:32Z","updated_at":"2021-04-01T16:33:08Z","closed_at":null,"author_association":"MEMBER","active_lock_reason":null,"body":"**Describe the bug**\r\n01502_long_log_tinylog_deadlock_race \r\n\r\nhttps://clickhouse-test-reports.s3.yandex.net/0/121c5c96053ca07cdc9eca1f9458022400df3e22/functional_stateless_tests_(thread)/stderr.log\r\n\r\nhttps://clickhouse-test-reports.s3.yandex.net/0/121c5c96053ca07cdc9eca1f9458022400df3e22/functional_stateless_tests_(thread).html\r\n\r\n```\r\nWARNING: ThreadSanitizer: data race (pid=165)\r\n  Write of size 8 at 0x7b1c000fec18 by thread T196 (mutexes: write M828516388453652296):\r\n    #0 std::__1::enable_if<(is_move_constructible<DB::StorageLog::Mark*>::value) && (is_move_assignable<DB::StorageLog::Mark*>::value), void>::type std::__1::swap<DB::StorageLog::Mark*>(DB::StorageLog::Mark*&, DB::StorageLog::Mark*&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3934:9 (clickhouse+0x12b26345)\r\n    #1 std::__1::vector<DB::StorageLog::Mark, std::__1::allocator<DB::StorageLog::Mark> >::__swap_out_circular_buffer(std::__1::__split_buffer<DB::StorageLog::Mark, std::__1::allocator<DB::StorageLog::Mark>&>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:955:5 (clickhouse+0x12b26345)\r\n    #2 void std::__1::vector<DB::StorageLog::Mark, std::__1::allocator<DB::StorageLog::Mark> >::__push_back_slow_path<DB::StorageLog::Mark const&>(DB::StorageLog::Mark const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1628:5 (clickhouse+0x12b26345)\r\n    #3 std::__1::vector<DB::StorageLog::Mark, std::__1::allocator<DB::StorageLog::Mark> >::push_back(DB::StorageLog::Mark const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1641:9 (clickhouse+0x12b26345)\r\n    #4 DB::LogBlockOutputStream::writeMarks(std::__1::vector<std::__1::pair<unsigned long, DB::StorageLog::Mark>, std::__1::allocator<std::__1::pair<unsigned long, DB::StorageLog::Mark> > >&&) obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:448:72 (clickhouse+0x12b26345)\r\n    #5 DB::LogBlockOutputStream::write(DB::Block const&) obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:317:5 (clickhouse+0x12b2566a)\r\n    #6 DB::PushingToViewsBlockOutputStream::write(DB::Block const&) obj-x86_64-linux-gnu/../src/DataStreams/PushingToViewsBlockOutputStream.cpp:165:21 (clickhouse+0x122c0bfb)\r\n    #7 DB::AddingDefaultBlockOutputStream::write(DB::Block const&) obj-x86_64-linux-gnu/../src/DataStreams/AddingDefaultBlockOutputStream.cpp:24:13 (clickhouse+0x122bdbf9)\r\n    #8 DB::SquashingBlockOutputStream::finalize() obj-x86_64-linux-gnu/../src/DataStreams/SquashingBlockOutputStream.cpp:30:17 (clickhouse+0x122bbd31)\r\n    #9 DB::SquashingBlockOutputStream::writeSuffix() obj-x86_64-linux-gnu/../src/DataStreams/SquashingBlockOutputStream.cpp:50:5 (clickhouse+0x122bbe59)\r\n    #10 DB::CountingBlockOutputStream::writeSuffix() obj-x86_64-linux-gnu/../src/DataStreams/CountingBlockOutputStream.h:37:67 (clickhouse+0x11f82aaf)\r\n    #11 DB::SinkToOutputStream::onFinish() obj-x86_64-linux-gnu/../src/Processors/Sources/SinkToOutputStream.cpp:22:13 (clickhouse+0x1314b235)\r\n    #12 DB::ISink::prepare() obj-x86_64-linux-gnu/../src/Processors/ISink.cpp:19:9 (clickhouse+0x12f97aaf)\r\n    #13 DB::IProcessor::prepare(std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&) obj-x86_64-linux-gnu/../src/Processors/IProcessor.h:186:128 (clickhouse+0x118508b2)\r\n    #14 DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::unique_lock<std::__1::mutex>) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:215:58 (clickhouse+0x12fddd3c)\r\n    #15 DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::ExecutingGraph::Edge&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:188:16 (clickhouse+0x12fddbb0)\r\n    #16 DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::unique_lock<std::__1::mutex>) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:285:18 (clickhouse+0x12fde384)\r\n    #17 DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::ExecutingGraph::Edge&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:188:16 (clickhouse+0x12fddbb0)\r\n    #18 DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::unique_lock<std::__1::mutex>) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:285:18 (clickhouse+0x12fde384)\r\n    #19 DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::ExecutingGraph::Edge&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:188:16 (clickhouse+0x12fddbb0)\r\n    #20 DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::unique_lock<std::__1::mutex>) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:291:18 (clickhouse+0x12fde3d4)\r\n    #21 DB::PipelineExecutor::tryAddProcessorToStackIfUpdated(DB::ExecutingGraph::Edge&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:188:16 (clickhouse+0x12fddbb0)\r\n    #22 DB::PipelineExecutor::prepareProcessor(unsigned long, unsigned long, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::queue<DB::ExecutingGraph::Node*, std::__1::deque<DB::ExecutingGraph::Node*, std::__1::allocator<DB::ExecutingGraph::Node*> > >&, std::__1::unique_lock<std::__1::mutex>) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:291:18 (clickhouse+0x12fde3d4)\r\n    #23 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:609:26 (clickhouse+0x12fe19b8)\r\n    #24 DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:473:5 (clickhouse+0x12fdf3d8)\r\n    #25 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:807:9 (clickhouse+0x12fdf3d8)\r\n    #26 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:395:9 (clickhouse+0x12fdee2a)\r\n    #27 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:79:24 (clickhouse+0x12fee034)\r\n    #28 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:101:13 (clickhouse+0x12fee034)\r\n    #29 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x12fee034)\r\n    #30 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x12fee034)\r\n    #31 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x12fee034)\r\n    #32 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:178:13 (clickhouse+0x12fee034)\r\n    #33 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12fee034)\r\n    #34 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12fee034)\r\n    #35 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12fee034)\r\n    #36 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12fee034)\r\n    #37 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x8c02735)\r\n    #38 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x8c02735)\r\n    #39 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:247:17 (clickhouse+0x8c02735)\r\n    #40 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:73 (clickhouse+0x8c063d8)\r\n    #41 decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x8c063d8)\r\n    #42 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x8c063d8)\r\n    #43 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x8c063d8)\r\n\r\n  Previous read of size 8 at 0x7b1c000fec18 by thread T240:\r\n    #0 std::__1::vector<DB::StorageLog::Mark, std::__1::allocator<DB::StorageLog::Mark> >::operator[](unsigned long) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1550:18 (clickhouse+0x12b2a2b6)\r\n    #1 DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)::operator()(DB::IDataType::SubstreamPath const&) const obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:184:26 (clickhouse+0x12b2a2b6)\r\n    #2 decltype(std::__1::forward<DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)&>(fp)(std::__1::forward<DB::IDataType::SubstreamPath const&>(fp0))) std::__1::__invoke<DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)&, DB::IDataType::SubstreamPath const&>(DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)&, DB::IDataType::SubstreamPath const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12b2a2b6)\r\n    #3 DB::ReadBuffer* std::__1::__invoke_void_return_wrapper<DB::ReadBuffer*>::__call<DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)&, DB::IDataType::SubstreamPath const&>(DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&)&, DB::IDataType::SubstreamPath const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:317:16 (clickhouse+0x12b2a2b6)\r\n    #4 std::__1::__function::__default_alloc_func<DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&), DB::ReadBuffer* (DB::IDataType::SubstreamPath const&)>::operator()(DB::IDataType::SubstreamPath const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12b2a2b6)\r\n    #5 DB::ReadBuffer* std::__1::__function::__policy_invoker<DB::ReadBuffer* (DB::IDataType::SubstreamPath const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&)::$_0::operator()(bool) const::'lambda'(DB::IDataType::SubstreamPath const&), DB::ReadBuffer* (DB::IDataType::SubstreamPath const&)> >(std::__1::__function::__policy_storage const*, DB::IDataType::SubstreamPath const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12b2a2b6)\r\n    #6 std::__1::__function::__policy_func<DB::ReadBuffer* (DB::IDataType::SubstreamPath const&)>::operator()(DB::IDataType::SubstreamPath const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x11de1248)\r\n    #7 std::__1::function<DB::ReadBuffer* (DB::IDataType::SubstreamPath const&)>::operator()(DB::IDataType::SubstreamPath const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x11de1248)\r\n    #8 DB::IDataType::deserializeBinaryBulkWithMultipleStreamsImpl(DB::IColumn&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const obj-x86_64-linux-gnu/../src/DataTypes/IDataType.cpp:296:31 (clickhouse+0x11de1248)\r\n    #9 DB::IDataType::deserializeBinaryBulkWithMultipleStreams(COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, DB::IDataType::DeserializeBinaryBulkSettings&, std::__1::shared_ptr<DB::IDataType::DeserializeBinaryBulkState>&, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >*) const obj-x86_64-linux-gnu/../src/DataTypes/IDataType.cpp:327:5 (clickhouse+0x11de1432)\r\n    #10 DB::LogSource::readData(DB::NameAndTypePair const&, COW<DB::IColumn>::immutable_ptr<DB::IColumn>&, unsigned long, std::__1::unordered_map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, COW<DB::IColumn>::immutable_ptr<DB::IColumn>, std::__1::hash<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::equal_to<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, COW<DB::IColumn>::immutable_ptr<DB::IColumn> > > >&) obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:200:11 (clickhouse+0x12b253a4)\r\n    #11 DB::LogSource::generate() obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:135:13 (clickhouse+0x12b24596)\r\n    #12 DB::ISource::tryGenerate() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:79:18 (clickhouse+0x12f9b164)\r\n    #13 DB::ISource::work() obj-x86_64-linux-gnu/../src/Processors/ISource.cpp:53:26 (clickhouse+0x12f9ae01)\r\n    #14 DB::SourceWithProgress::work() obj-x86_64-linux-gnu/../src/Processors/Sources/SourceWithProgress.cpp:36:30 (clickhouse+0x1314feaa)\r\n    #15 DB::executeJob(DB::IProcessor*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:79:20 (clickhouse+0x12fe3564)\r\n    #16 DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:96:13 (clickhouse+0x12fe3564)\r\n    #17 decltype(std::__1::forward<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(fp)()) std::__1::__invoke<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12fe3564)\r\n    #18 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&>(DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12fe3564)\r\n    #19 std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12fe3564)\r\n    #20 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<DB::PipelineExecutor::addJob(DB::ExecutingGraph::Node*)::$_0, void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12fe3564)\r\n    #21 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x12fe15d1)\r\n    #22 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x12fe15d1)\r\n    #23 DB::PipelineExecutor::executeStepImpl(unsigned long, unsigned long, std::__1::atomic<bool>*) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:580:17 (clickhouse+0x12fe15d1)\r\n    #24 DB::PipelineExecutor::executeSingleThread(unsigned long, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:473:5 (clickhouse+0x12fe3ae4)\r\n    #25 DB::PipelineExecutor::executeImpl(unsigned long)::$_4::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:771:21 (clickhouse+0x12fe3ae4)\r\n    #26 decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(fp)()) std::__1::__invoke_constexpr<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x12fe3ae4)\r\n    #27 decltype(auto) std::__1::__apply_tuple_impl<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x12fe3ae4)\r\n    #28 decltype(auto) std::__1::apply<DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x12fe3ae4)\r\n    #29 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:178:13 (clickhouse+0x12fe3ae4)\r\n    #30 decltype(std::__1::forward<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12fe3ae4)\r\n    #31 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'()&>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12fe3ae4)\r\n    #32 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12fe3ae4)\r\n    #33 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12fe3ae4)\r\n    #34 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x8c02735)\r\n    #35 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x8c02735)\r\n    #36 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:247:17 (clickhouse+0x8c02735)\r\n    #37 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:73 (clickhouse+0x8c063d8)\r\n    #38 decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x8c063d8)\r\n    #39 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x8c063d8)\r\n    #40 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x8c063d8)\r\n\r\n  Location is heap block of size 112 at 0x7b1c000febc0 allocated by thread T358:\r\n    #0 operator new(unsigned long) <null> (clickhouse+0x8ba48e7)\r\n    #1 void* std::__1::__libcpp_operator_new<unsigned long>(unsigned long) obj-x86_64-linux-gnu/../contrib/libcxx/include/new:235:10 (clickhouse+0x12b31b55)\r\n    #2 std::__1::__libcpp_allocate(unsigned long, unsigned long) obj-x86_64-linux-gnu/../contrib/libcxx/include/new:261:10 (clickhouse+0x12b31b55)\r\n    #3 std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*> >::allocate(unsigned long) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:840:38 (clickhouse+0x12b31b55)\r\n    #4 std::__1::allocator_traits<std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*> > >::allocate(std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*> >&, unsigned long) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:468:21 (clickhouse+0x12b31b55)\r\n    #5 std::__1::unique_ptr<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*>, std::__1::__tree_node_destructor<std::__1::allocator<std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*> > > > std::__1::__tree<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, std::__1::__map_value_compare<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData> > >::__construct_node<std::__1::piecewise_construct_t const&, std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>, std::__1::tuple<> >(std::__1::piecewise_construct_t const&, std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>&&, std::__1::tuple<>&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__tree:2133:23 (clickhouse+0x12b31b55)\r\n    #6 std::__1::pair<std::__1::__tree_iterator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, std::__1::__tree_node<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, void*>*, long>, bool> std::__1::__tree<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, std::__1::__map_value_compare<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData>, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, true>, std::__1::allocator<std::__1::__value_type<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData> > >::__emplace_unique_key_args<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::piecewise_construct_t const&, std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>, std::__1::tuple<> >(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::piecewise_construct_t const&, std::__1::tuple<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&>&&, std::__1::tuple<>&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__tree:2096:29 (clickhouse+0x12b31b55)\r\n    #7 std::__1::map<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, DB::StorageLog::ColumnData, std::__1::less<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >, std::__1::allocator<std::__1::pair<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const, DB::StorageLog::ColumnData> > >::operator[](std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/map:1521:20 (clickhouse+0x12b2d72e)\r\n    #8 DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6::operator()(DB::IDataType::SubstreamPath const&, DB::IDataType const&) const obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:510:40 (clickhouse+0x12b2d72e)\r\n    #9 decltype(std::__1::forward<DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6&>(fp)(std::__1::forward<DB::IDataType::SubstreamPath const&>(fp0), std::__1::forward<DB::IDataType const&>(fp0))) std::__1::__invoke<DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6&, DB::IDataType::SubstreamPath const&, DB::IDataType const&>(DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6&, DB::IDataType::SubstreamPath const&, DB::IDataType const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12b2d72e)\r\n    #10 void std::__1::__invoke_void_return_wrapper<void>::__call<DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6&, DB::IDataType::SubstreamPath const&, DB::IDataType const&>(DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6&, DB::IDataType::SubstreamPath const&, DB::IDataType const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12b2d72e)\r\n    #11 std::__1::__function::__default_alloc_func<DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6, void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)>::operator()(DB::IDataType::SubstreamPath const&, DB::IDataType const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12b2d72e)\r\n    #12 void std::__1::__function::__policy_invoker<void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::StorageLog::addFiles(DB::NameAndTypePair const&)::$_6, void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)> >(std::__1::__function::__policy_storage const*, DB::IDataType::SubstreamPath const&, DB::IDataType const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12b2d72e)\r\n    #13 std::__1::__function::__policy_func<void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)>::operator()(DB::IDataType::SubstreamPath const&, DB::IDataType const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x8e437a0)\r\n    #14 std::__1::function<void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)>::operator()(DB::IDataType::SubstreamPath const&, DB::IDataType const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x8e437a0)\r\n    #15 DB::IDataType::enumerateStreamsImpl(std::__1::function<void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)> const&, DB::IDataType::SubstreamPath&) const obj-x86_64-linux-gnu/../src/DataTypes/IDataType.h:282:9 (clickhouse+0x8e437a0)\r\n    #16 DB::IDataType::enumerateStreams(std::__1::function<void (DB::IDataType::SubstreamPath const&, DB::IDataType const&)> const&, DB::IDataType::SubstreamPath&) const obj-x86_64-linux-gnu/../src/DataTypes/IDataType.cpp:243:9 (clickhouse+0x11de0f91)\r\n    #17 DB::StorageLog::addFiles(DB::NameAndTypePair const&) obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:520:18 (clickhouse+0x12b27ca2)\r\n    #18 DB::StorageLog::StorageLog(std::__1::shared_ptr<DB::IDisk>, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool, unsigned long) obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:492:9 (clickhouse+0x12b2768b)\r\n    #19 std::__1::shared_ptr<DB::StorageLog> ext::shared_ptr_helper<DB::StorageLog>::create<std::__1::shared_ptr<DB::IDisk>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool const&, DB::SettingFieldNumber<unsigned long> >(std::__1::shared_ptr<DB::IDisk>&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::StorageID const&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool const&, DB::SettingFieldNumber<unsigned long>&&) obj-x86_64-linux-gnu/../base/common/../ext/shared_ptr_helper.h:19:39 (clickhouse+0x12b2dbe7)\r\n    #20 DB::registerStorageLog(DB::StorageFactory&)::$_8::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../src/Storages/StorageLog.cpp:725:16 (clickhouse+0x12b2dbe7)\r\n    #21 decltype(std::__1::forward<DB::registerStorageLog(DB::StorageFactory&)::$_8&>(fp)(std::__1::forward<DB::StorageFactory::Arguments const&>(fp0))) std::__1::__invoke<DB::registerStorageLog(DB::StorageFactory&)::$_8&, DB::StorageFactory::Arguments const&>(DB::registerStorageLog(DB::StorageFactory&)::$_8&, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12b2dbe7)\r\n    #22 std::__1::shared_ptr<DB::IStorage> std::__1::__invoke_void_return_wrapper<std::__1::shared_ptr<DB::IStorage> >::__call<DB::registerStorageLog(DB::StorageFactory&)::$_8&, DB::StorageFactory::Arguments const&>(DB::registerStorageLog(DB::StorageFactory&)::$_8&, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:317:16 (clickhouse+0x12b2dbe7)\r\n    #23 std::__1::__function::__default_alloc_func<DB::registerStorageLog(DB::StorageFactory&)::$_8, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12b2dbe7)\r\n    #24 std::__1::shared_ptr<DB::IStorage> std::__1::__function::__policy_invoker<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::__call_impl<std::__1::__function::__default_alloc_func<DB::registerStorageLog(DB::StorageFactory&)::$_8, std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)> >(std::__1::__function::__policy_storage const*, DB::StorageFactory::Arguments const&) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12b2dbe7)\r\n    #25 std::__1::__function::__policy_func<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x1298134b)\r\n    #26 std::__1::function<std::__1::shared_ptr<DB::IStorage> (DB::StorageFactory::Arguments const&)>::operator()(DB::StorageFactory::Arguments const&) const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x1298134b)\r\n    #27 DB::StorageFactory::get(DB::ASTCreateQuery const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, DB::Context&, DB::ColumnsDescription const&, DB::ConstraintsDescription const&, bool) const obj-x86_64-linux-gnu/../src/Storages/StorageFactory.cpp:183:16 (clickhouse+0x1298134b)\r\n    #28 DB::InterpreterCreateQuery::doCreateTable(DB::ASTCreateQuery&, DB::InterpreterCreateQuery::TableProperties const&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:927:42 (clickhouse+0x1223867e)\r\n    #29 DB::InterpreterCreateQuery::createTable(DB::ASTCreateQuery&) obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:843:20 (clickhouse+0x12235ce1)\r\n    #30 DB::InterpreterCreateQuery::execute() obj-x86_64-linux-gnu/../src/Interpreters/InterpreterCreateQuery.cpp:1136:16 (clickhouse+0x1223a7bf)\r\n    #31 DB::executeQueryImpl(char const*, char const*, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool, DB::ReadBuffer*) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:546:32 (clickhouse+0x126ea209)\r\n    #32 DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) obj-x86_64-linux-gnu/../src/Interpreters/executeQuery.cpp:904:30 (clickhouse+0x126e8ab8)\r\n    #33 DB::TCPHandler::runImpl() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:267:24 (clickhouse+0x12f2edc5)\r\n    #34 DB::TCPHandler::run() obj-x86_64-linux-gnu/../src/Server/TCPHandler.cpp:1459:9 (clickhouse+0x12f3db67)\r\n    #35 Poco::Net::TCPServerConnection::start() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerConnection.cpp:43:3 (clickhouse+0x15a1ac72)\r\n    #36 Poco::Net::TCPServerDispatcher::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:113:19 (clickhouse+0x15a1b39e)\r\n    #37 Poco::PooledThread::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:199:14 (clickhouse+0x15b83d71)\r\n    #38 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x15b8230f)\r\n    #39 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x15b80b17)\r\n\r\n  Mutex M828516388453652296 is already destroyed.\r\n\r\n  Thread T196 'QueryPipelineEx' (tid=17484, running) created by thread T183 at:\r\n    #0 pthread_create <null> (clickhouse+0x8b15d0b)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x8c059d0)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'(), void>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x8c059d0)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:35 (clickhouse+0x8c0135a)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:160:5 (clickhouse+0x8c01a87)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:162:38 (clickhouse+0x12fe389e)\r\n    #6 void std::__1::allocator<ThreadFromGlobalPool>::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886:28 (clickhouse+0x12fe389e)\r\n    #7 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::__construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::integral_constant<bool, true>, std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:519:21 (clickhouse+0x12fe389e)\r\n    #8 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:481:14 (clickhouse+0x12fe389e)\r\n    #9 void std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::__construct_one_at_end<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:926:5 (clickhouse+0x12fdf0e8)\r\n    #10 ThreadFromGlobalPool& std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::emplace_back<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1684:9 (clickhouse+0x12fdf0e8)\r\n    #11 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:755:21 (clickhouse+0x12fdf0e8)\r\n    #12 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:395:9 (clickhouse+0x12fdee2a)\r\n    #13 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:79:24 (clickhouse+0x12fee034)\r\n    #14 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:101:13 (clickhouse+0x12fee034)\r\n    #15 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x12fee034)\r\n    #16 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x12fee034)\r\n    #17 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x12fee034)\r\n    #18 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:178:13 (clickhouse+0x12fee034)\r\n    #19 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12fee034)\r\n    #20 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12fee034)\r\n    #21 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12fee034)\r\n    #22 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12fee034)\r\n    #23 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x8c02735)\r\n    #24 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x8c02735)\r\n    #25 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:247:17 (clickhouse+0x8c02735)\r\n    #26 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:73 (clickhouse+0x8c063d8)\r\n    #27 decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x8c063d8)\r\n    #28 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x8c063d8)\r\n    #29 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x8c063d8)\r\n\r\n  Thread T240 'QueryPipelineEx' (tid=17528, running) created by thread T234 at:\r\n    #0 pthread_create <null> (clickhouse+0x8b15d0b)\r\n    #1 std::__1::__libcpp_thread_create(unsigned long*, void* (*)(void*), void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/__threading_support:509:10 (clickhouse+0x8c059d0)\r\n    #2 std::__1::thread::thread<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'(), void>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:307:16 (clickhouse+0x8c059d0)\r\n    #3 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:35 (clickhouse+0x8c0135a)\r\n    #4 ThreadPoolImpl<std::__1::thread>::scheduleOrThrow(std::__1::function<void ()>, int, unsigned long) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:160:5 (clickhouse+0x8c01a87)\r\n    #5 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:162:38 (clickhouse+0x12fe389e)\r\n    #6 void std::__1::allocator<ThreadFromGlobalPool>::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/memory:886:28 (clickhouse+0x12fe389e)\r\n    #7 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::__construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::integral_constant<bool, true>, std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:519:21 (clickhouse+0x12fe389e)\r\n    #8 void std::__1::allocator_traits<std::__1::allocator<ThreadFromGlobalPool> >::construct<ThreadFromGlobalPool, DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(std::__1::allocator<ThreadFromGlobalPool>&, ThreadFromGlobalPool*, DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/__memory/allocator_traits.h:481:14 (clickhouse+0x12fe389e)\r\n    #9 void std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::__construct_one_at_end<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:926:5 (clickhouse+0x12fdf0e8)\r\n    #10 ThreadFromGlobalPool& std::__1::vector<ThreadFromGlobalPool, std::__1::allocator<ThreadFromGlobalPool> >::emplace_back<DB::PipelineExecutor::executeImpl(unsigned long)::$_4>(DB::PipelineExecutor::executeImpl(unsigned long)::$_4&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/vector:1684:9 (clickhouse+0x12fdf0e8)\r\n    #11 DB::PipelineExecutor::executeImpl(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:755:21 (clickhouse+0x12fdf0e8)\r\n    #12 DB::PipelineExecutor::execute(unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PipelineExecutor.cpp:395:9 (clickhouse+0x12fdee2a)\r\n    #13 DB::threadFunction(DB::PullingAsyncPipelineExecutor::Data&, std::__1::shared_ptr<DB::ThreadGroupStatus>, unsigned long) obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:79:24 (clickhouse+0x12fee034)\r\n    #14 DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0::operator()() const obj-x86_64-linux-gnu/../src/Processors/Executors/PullingAsyncPipelineExecutor.cpp:101:13 (clickhouse+0x12fee034)\r\n    #15 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(fp)()) std::__1::__invoke_constexpr<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3682:1 (clickhouse+0x12fee034)\r\n    #16 decltype(auto) std::__1::__apply_tuple_impl<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1415:1 (clickhouse+0x12fee034)\r\n    #17 decltype(auto) std::__1::apply<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&, std::__1::tuple<>&) obj-x86_64-linux-gnu/../contrib/libcxx/include/tuple:1424:1 (clickhouse+0x12fee034)\r\n    #18 ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()::operator()() obj-x86_64-linux-gnu/../src/Common/ThreadPool.h:178:13 (clickhouse+0x12fee034)\r\n    #19 decltype(std::__1::forward<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(fp)()) std::__1::__invoke<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x12fee034)\r\n    #20 void std::__1::__invoke_void_return_wrapper<void>::__call<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'()&>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/__functional_base:348:9 (clickhouse+0x12fee034)\r\n    #21 std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()>::operator()() obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:1608:12 (clickhouse+0x12fee034)\r\n    #22 void std::__1::__function::__policy_invoker<void ()>::__call_impl<std::__1::__function::__default_alloc_func<ThreadFromGlobalPool::ThreadFromGlobalPool<DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0>(DB::PullingAsyncPipelineExecutor::pull(DB::Chunk&, unsigned long)::$_0&&)::'lambda'(), void ()> >(std::__1::__function::__policy_storage const*) obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2089:16 (clickhouse+0x12fee034)\r\n    #23 std::__1::__function::__policy_func<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2221:16 (clickhouse+0x8c02735)\r\n    #24 std::__1::function<void ()>::operator()() const obj-x86_64-linux-gnu/../contrib/libcxx/include/functional:2560:12 (clickhouse+0x8c02735)\r\n    #25 ThreadPoolImpl<std::__1::thread>::worker(std::__1::__list_iterator<std::__1::thread, void*>) obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:247:17 (clickhouse+0x8c02735)\r\n    #26 void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()::operator()() const obj-x86_64-linux-gnu/../src/Common/ThreadPool.cpp:124:73 (clickhouse+0x8c063d8)\r\n    #27 decltype(std::__1::forward<void>(fp)(std::__1::forward<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(fp0)...)) std::__1::__invoke<void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(void&&, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()&&...) obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3676:1 (clickhouse+0x8c063d8)\r\n    #28 void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>(std::__1::tuple<void, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()>&, std::__1::__tuple_indices<>) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:280:5 (clickhouse+0x8c063d8)\r\n    #29 void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, void ThreadPoolImpl<std::__1::thread>::scheduleImpl<void>(std::__1::function<void ()>, int, std::__1::optional<unsigned long>)::'lambda1'()> >(void*) obj-x86_64-linux-gnu/../contrib/libcxx/include/thread:291:5 (clickhouse+0x8c063d8)\r\n\r\n  Thread T358 'TCPHandler' (tid=25045, running) created by thread T40 at:\r\n    #0 pthread_create <null> (clickhouse+0x8b15d0b)\r\n    #1 Poco::ThreadImpl::startImpl(Poco::SharedPtr<Poco::Runnable, Poco::ReferenceCounter, Poco::ReleasePolicy<Poco::Runnable> >) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:202:6 (clickhouse+0x15b805b7)\r\n    #2 Poco::Thread::start(Poco::Runnable&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:128:2 (clickhouse+0x15b81cfc)\r\n    #3 Poco::PooledThread::start() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:85:10 (clickhouse+0x15b860b7)\r\n    #4 Poco::ThreadPool::getThread() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:461:14 (clickhouse+0x15b860b7)\r\n    #5 Poco::ThreadPool::startWithPriority(Poco::Thread::Priority, Poco::Runnable&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/ThreadPool.cpp:365:2 (clickhouse+0x15b86497)\r\n    #6 Poco::Net::TCPServerDispatcher::enqueue(Poco::Net::StreamSocket const&) obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServerDispatcher.cpp:147:17 (clickhouse+0x15a1b91a)\r\n    #7 Poco::Net::TCPServer::run() obj-x86_64-linux-gnu/../contrib/poco/Net/src/TCPServer.cpp:148:21 (clickhouse+0x15a1a5b7)\r\n    #8 Poco::(anonymous namespace)::RunnableHolder::run() obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread.cpp:55:11 (clickhouse+0x15b8230f)\r\n    #9 Poco::ThreadImpl::runnableEntry(void*) obj-x86_64-linux-gnu/../contrib/poco/Foundation/src/Thread_POSIX.cpp:345:27 (clickhouse+0x15b80b17)\r\n\r\nSUMMARY: ThreadSanitizer: data race obj-x86_64-linux-gnu/../contrib/libcxx/include/type_traits:3934:9 in std::__1::enable_if<(is_move_constructible<DB::StorageLog::Mark*>::value) && (is_move_assignable<DB::StorageLog::Mark*>::value), void>::type std::__1::swap<DB::StorageLog::Mark*>(DB::StorageLog::Mark*&, DB::StorageLog::Mark*&)\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20380/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20343","id":806614738,"node_id":"MDU6SXNzdWU4MDY2MTQ3Mzg=","number":20343,"title":"INSERT SELECT: SETTINGS clause after FORMAT can lead to parsing issues","user":{"login":"joein","id":22641570,"node_id":"MDQ6VXNlcjIyNjQxNTcw","avatar_url":"https://avatars.githubusercontent.com/u/22641570?v=4","gravatar_id":"","url":"https://api.github.com/users/joein","html_url":"https://github.com/joein","followers_url":"https://api.github.com/users/joein/followers","following_url":"https://api.github.com/users/joein/following{/other_user}","gists_url":"https://api.github.com/users/joein/gists{/gist_id}","starred_url":"https://api.github.com/users/joein/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/joein/subscriptions","organizations_url":"https://api.github.com/users/joein/orgs","repos_url":"https://api.github.com/users/joein/repos","events_url":"https://api.github.com/users/joein/events{/privacy}","received_events_url":"https://api.github.com/users/joein/received_events","type":"User","site_admin":false},"labels":[{"id":1401255404,"node_id":"MDU6TGFiZWwxNDAxMjU1NDA0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-formats","name":"comp-formats","color":"b5bcff","default":false,"description":"Input / output formats"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""},{"id":2532986061,"node_id":"MDU6TGFiZWwyNTMyOTg2MDYx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-query-syntax","name":"comp-query-syntax","color":"b5bcff","default":false,"description":"Relates to query parse / aliases resolution etc."}],"state":"open","locked":false,"assignee":{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false},"assignees":[{"login":"vitlibar","id":45142681,"node_id":"MDQ6VXNlcjQ1MTQyNjgx","avatar_url":"https://avatars.githubusercontent.com/u/45142681?v=4","gravatar_id":"","url":"https://api.github.com/users/vitlibar","html_url":"https://github.com/vitlibar","followers_url":"https://api.github.com/users/vitlibar/followers","following_url":"https://api.github.com/users/vitlibar/following{/other_user}","gists_url":"https://api.github.com/users/vitlibar/gists{/gist_id}","starred_url":"https://api.github.com/users/vitlibar/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/vitlibar/subscriptions","organizations_url":"https://api.github.com/users/vitlibar/orgs","repos_url":"https://api.github.com/users/vitlibar/repos","events_url":"https://api.github.com/users/vitlibar/events{/privacy}","received_events_url":"https://api.github.com/users/vitlibar/received_events","type":"User","site_admin":false}],"milestone":null,"comments":11,"created_at":"2021-02-11T17:55:43Z","updated_at":"2021-11-19T18:21:09Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I send protobuf messages (Protobuf format) to ClickHouse  via Python(3.8.1). On some messages I receive exceptions 'unrecognized tokens' which aren't expected. The odd thing is that exception's appearance depends on the context of protobuf messages (and especially with negative integers). It will be shown in the examples below.\r\n\r\nSerialize messages via google.protobuf(3.13.0), add length which is calculated via the same module.\r\nThese messages were serialized by me and I can deserialize it by Python means, so I assume that they are correct. \r\n\r\nI use clickhouse 20.11.4.13 (yandex/clickhouse-server:20.11 docker image). Also I tried to use clickhouse-server:21.2.2.8 and there is the same error.\r\n\r\nIt doesn't look like problem in protobuf serializing because the very first time when I received such errors were with messages, which were serialized by Java client.\r\n\r\nThe following statements were produced by `show create table` commands in `clickhouse-client`:\r\n```\r\nCREATE TABLE arbitrary_db.some_entrypoint\r\n(\r\n    `field_1` String,\r\n    `field_2` Int64,\r\n    `field_3` Int32,\r\n    `field_4` Int32,\r\n    `field_5` Int32,\r\n    `field_6` String,\r\n    `field_7` String,\r\n    `field_8` String,\r\n    `field_9` String,\r\n    `field_10` Int32,\r\n    `field_11` Int32,\r\n    `field_12` Nullable(Int32),\r\n    `field_13` Nullable(Int32),\r\n    `field_14` String,\r\n    `field_16_field_18` Nullable(Int32),\r\n    `field_16_field_19` Nullable(Int32),\r\n    `field_16_field_20` Nullable(String),\r\n    `field_16_field_21` Nullable(String),\r\n    `field_16_field_22` Nullable(String),\r\n    `field_16_field_23` Nullable(String),\r\n    `field_16_field_24` Nullable(String),\r\n    `field_16_field_25` Nullable(String),\r\n    `field_16_field_266` Nullable(String),\r\n    `field_16_field_27` Nullable(String),\r\n    `field_16_field_28` Nullable(String),\r\n    `field_17_field_29` Nullable(Int32),\r\n    `field_17_field_30` Nullable(String),\r\n    `field_17_field_31` Nullable(Int32),\r\n    `field_17_field_32` Nullable(Int32),\r\n    `field_17_field_33` Nullable(Int32),\r\n    `field_17_field_34` Nullable(Int32),\r\n    `field_17_field_35` Nullable(String),\r\n    `field_17_field_36` Nullable(String),\r\n    `field_17_field_37` Nullable(String),\r\n    `field_17_field_38` Nullable(String),\r\n    `field_17_field_39` Nullable(Int64),\r\n    `field_17_field_40` Nullable(Int64),\r\n    `field_17_field_41` Nullable(String)\r\n)\r\nENGINE = Null() \r\n\r\nCREATE TABLE arbitrary_db.some_aggregation\r\n(\r\n    `final_6` String,\r\n    `final_7` String,\r\n    `final_8` String,\r\n    `max_final_2` AggregateFunction(max, Int64),\r\n    `min_final_2` AggregateFunction(min, Int64)\r\n)\r\nENGINE = AggregatingMergeTree()\r\nPRIMARY KEY (final_6, final_7, final_8)\r\nORDER BY (final_6, final_7, final_8)\r\nSETTINGS index_granularity = 8192\r\n\r\nCREATE MATERIALIZED VIEW arbitrary_db.some_aggregation_mv TO arbitrary_db.some_aggregation\r\n(\r\n    `final_6` String,\r\n    `final_7` String,\r\n    `final_8` String,\r\n    `max_final_2` AggregateFunction(max, Int64),\r\n    `min_final_2` AggregateFunction(min, Int64)\r\n) AS\r\nSELECT\r\n    field_6 AS final_6,\r\n    field_7 AS final_7,\r\n    field_8 AS final_8,\r\n    maxState(field_2) AS max_final_2,\r\n    minState(field_2) AS min_final_2\r\nFROM arbitrary_db.some_entrypoint\r\nWHERE (((field_8 != '') AND (field_7 != '')) OR ((field_8 != '') AND (field_6 != '')) OR ((field_7 != '') AND (field_6 != ''))) AND (field_2 > 0)\r\nGROUP BY\r\n    field_6,\r\n    field_7,\r\n    field_8\r\nORDER BY\r\n    field_6 ASC,\r\n    field_7 ASC,\r\n    field_8 ASC\r\n```\r\n\r\n\r\n**Error message**\r\nCode: 62, e.displayText() = DB::Exception: Syntax error: failed at position 177 ('') (line 2, col 2): ï¿½ï¿½ï¿½ï¿½:\t123456789B\r\n                                                                                                                                          87654322342ï¿½\r\n                                                                                                                                                       ï¿½Ì£ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. Unrecognized token (version 20.11.4.13 (official build))\r\n\r\n**Examples of input and output**\r\n\r\n\r\n```\r\nfield_2: 1606918072\r\nfield_7: \"123456789\"\r\nfield_8: \"87654322342\"\r\nfield_17 {\r\n  field_31: -1062672876\r\n  field_33: 4\r\n  field_34: 6\r\n}\r\n\r\nb'0\\x10\\xb8\\xbf\\x9e\\xfe\\x05:\\t123456789B\\x0b87654322342\\xaa\\x06\\x0f \\x94\\xcc\\xa3\\x85\\xfc\\xff\\xff\\xff\\xff\\x010\\x048\\x06'\r\nRESPONSE TEXT \r\nSTATUS CODE 200\r\n```\r\n\r\nRemaining only negative value in the submessage leads to an exception.\r\n```\r\nfield_2: 1606918072\r\nfield_7: \"123456789\"\r\nfield_8: \"87654322342\"\r\nfield_17 {\r\n  field_31: -1062672876\r\n}\r\n\r\nb',\\x10\\xb8\\xbf\\x9e\\xfe\\x05:\\t123456789B\\x0b87654322342\\xaa\\x06\\x0b \\x94\\xcc\\xa3\\x85\\xfc\\xff\\xff\\xff\\xff\\x01'\r\nRESPONSE TEXT Code: 62, e.displayText() = DB::Exception: Syntax error: failed at position 177 ('') (line 2, col 2): ï¿½ï¿½ï¿½ï¿½:\t123456789B\r\n                                                                                                                                          87654322342ï¿½\r\n                                                                                                                                                       ï¿½Ì£ï¿½ï¿½ï¿½ï¿½ï¿½ï¿½. Unrecognized token (version 20.11.4.13 (official build))\r\n\r\nSTATUS CODE 400\r\n```\r\n\r\nThe same message as previous but without one field (`field_7`) from main message.\r\n```\r\nfield_2: 1606918072\r\nfield_8: \"87654322342\"\r\nfield_17 {\r\n  field_31: -1062672876\r\n}\r\n\r\nb'!\\x10\\xb8\\xbf\\x9e\\xfe\\x05B\\x0b87654322342\\xaa\\x06\\x0b \\x94\\xcc\\xa3\\x85\\xfc\\xff\\xff\\xff\\xff\\x01'\r\nRESPONSE TEXT \r\nSTATUS CODE 200\r\n```\r\n\r\nReturned `field_7` and made `field_31` value positive.\r\n```\r\nfield_2: 1606918072\r\nfield_7: \"123456789\"\r\nfield_8: \"87654322342\"\r\nfield_17 {\r\n  field_31: 1062672876\r\n}\r\n\r\nb\"'\\x10\\xb8\\xbf\\x9e\\xfe\\x05:\\t123456789B\\x0b87654322342\\xaa\\x06\\x06 \\xec\\xb3\\xdc\\xfa\\x03\"\r\nRESPONSE TEXT \r\nSTATUS CODE 200\r\n```\r\n\r\nI also tried to skip broken messages via ` input_format_allow_errors_num=10`, `input_format_allow_errors_ratio=1` options to INSERT query but it didn't get me desired results.\r\n\r\nIn attachments you can find code snippets that I used to reproduce the unexpected behaviour (.py files, requirements for python, .proto files and compiled from it .py files).\r\n\r\n[issue.zip](https://github.com/ClickHouse/ClickHouse/files/5967432/issue.zip)\r\n\r\n\r\n\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20343/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20340","id":806586712,"node_id":"MDU6SXNzdWU4MDY1ODY3MTI=","number":20340,"title":"Irrelevant exception messages in clickhouse-server.log","user":{"login":"SaltTan","id":20357526,"node_id":"MDQ6VXNlcjIwMzU3NTI2","avatar_url":"https://avatars.githubusercontent.com/u/20357526?v=4","gravatar_id":"","url":"https://api.github.com/users/SaltTan","html_url":"https://github.com/SaltTan","followers_url":"https://api.github.com/users/SaltTan/followers","following_url":"https://api.github.com/users/SaltTan/following{/other_user}","gists_url":"https://api.github.com/users/SaltTan/gists{/gist_id}","starred_url":"https://api.github.com/users/SaltTan/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/SaltTan/subscriptions","organizations_url":"https://api.github.com/users/SaltTan/orgs","repos_url":"https://api.github.com/users/SaltTan/repos","events_url":"https://api.github.com/users/SaltTan/events{/privacy}","received_events_url":"https://api.github.com/users/SaltTan/received_events","type":"User","site_admin":false},"labels":[{"id":845247686,"node_id":"MDU6TGFiZWw4NDUyNDc2ODY=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/usability","name":"usability","color":"ebf28c","default":false,"description":""},{"id":1478073551,"node_id":"MDU6TGFiZWwxNDc4MDczNTUx","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/comp-replication","name":"comp-replication","color":"b5bcff","default":false,"description":"Replicated tables"},{"id":1507888214,"node_id":"MDU6TGFiZWwxNTA3ODg4MjE0","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-accepted","name":"st-accepted","color":"e5b890","default":false,"description":"The issue is in our backlog, ready to take"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":1,"created_at":"2021-02-11T17:17:16Z","updated_at":"2021-05-17T17:05:06Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"After a replica fails to fetch a part from a host, the exception comes up every time the replica wants to download anything else from that host.\r\nThe exception seems to be just an annoying message, fetching requests are able to complete successfully.\r\nI found this while investigating the behaviour of 'optimize_on_insert' which created a good amount of empty parts in my environment, and these parts were immediately deleted by the empty parts cleaner.\r\n```\r\nHost1\r\n\r\n-- Insert an empty part\r\n2021.02.10 20:07:02.656343 [ 13601 ] {f596ba73-bc01-4323-8f38-6e3590644100} <Debug> db.table1 (Replicated OutputStream): Wrote block with 0 rows\r\n2021.02.10 20:07:02.681045 [ 13601 ] {f596ba73-bc01-4323-8f38-6e3590644100} <Trace> db.table1: Renaming temporary part tmp_insert_202102_34590_34590_0 to 202102_54133_54133_0.\r\n-- Drop the empty part\r\n2021.02.10 20:07:03.042327 [ 13614 ] {} <Trace> db.table1: Will try to insert a log entry to DROP_RANGE for part: 202102_54133_54133_0\r\n2021.02.10 20:07:03.083379 [ 13603 ] {} <Debug> db.table1: There is no part 202102_54133_54133_0 in ZooKeeper, it was only in filesystem\r\n\r\n-- Replica wants to download the part, but it's already deleted\r\n2021.02.10 20:07:03.125134 [ 9274 ] {} <Trace> InterserverIOHTTPHandler-factory: HTTP Request for InterserverIOHTTPHandler-factory. Method: POST, Address: [::ffff:10.246.48.2]:51842, User-Agent: (none), Content Type: , Transfer Encoding: identity, X-Forwarded-For: (none)\r\n2021.02.10 20:07:03.125188 [ 9274 ] {} <Trace> InterserverIOHTTPHandler: Request URI: /?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable1%2Freplicas%2Fhost1&part=202102_54133_54133_0&client_protocol_version=5&compress=false\r\n2021.02.10 20:07:03.125198 [ 9274 ] {} <Trace> db.table1 (Replicated PartsService): Sending part 202102_54133_54133_0\r\n2021.02.10 20:07:03.125257 [ 13609 ] {} <Warning> db.table1 (ReplicatedMergeTreePartCheckThread): Checking part 202102_54133_54133_0\r\n2021.02.10 20:07:03.125288 [ 9274 ] {} <Error> InterserverIOHTTPHandler: Code: 232, e.displayText() = DB::Exception: No part 202102_54133_54133_0 in table, Stack trace (when copying this message, always include the lines below):\r\n2021.02.10 20:07:03.125718 [ 13609 ] {} <Warning> db.table1 (ReplicatedMergeTreePartCheckThread): Checking if anyone has a part covering 202102_54133_54133_0.\r\n2021.02.10 20:07:03.126727 [ 13609 ] {} <Error> db.table1 (ReplicatedMergeTreePartCheckThread): No replica has part covering 202102_54133_54133_0 and a merge is impossible: we didn't find smaller parts with either the same min block or the same max block.\r\n2021.02.10 20:07:03.126733 [ 13609 ] {} <Error> db.table1 (ReplicatedMergeTreePartCheckThread): Missing part 202102_54133_54133_0 is not in our queue.\r\n\r\nHost2\r\n\r\n-- Replica wants to download the part, but it's already deleted\r\n2021.02.10 20:07:03.052819 [ 25538 ] {} <Debug> db.table1: Fetching part 202102_54133_54133_0 from /clickhouse/tables/table1/replicas/host1\r\n2021.02.10 20:07:03.085504 [ 25538 ] {} <Trace> ReadWriteBufferFromHTTP: Sending request to http://host1:9009/?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable1%2Freplicas%2Fhost1&part=202102_54133_54133_0&client_protocol_version=5&compress=false\r\n2021.02.10 20:07:03.136022 [ 25538 ] {} <Error> db.table1: auto DB::StorageReplicatedMergeTree::processQueueEntry(ReplicatedMergeTreeQueue::SelectedEntryPtr)::(anonymous class)::operator()(DB::StorageReplicatedMergeTree::LogEntryPtr &) const: Code: 86, e.displayText() = DB::Exception: Received error from remote server /?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable1%2Freplicas%2Fhost1&part=202102_54133_54133_0&client_protocol_version=5&compress=false. HTTP status code: 500 Internal Server Error, body: Code: 232, e.displayText() = DB::Exception: No part 202102_54133_54133_0 in table, Stack trace (when copying this message, always include the lines below):\r\n\r\n-- The next request to download a part from a different table reports the same exception (the table and part name in the exception are completely different)\r\n2021.02.10 20:07:03.195136 [ 25517 ] {} <Debug> db.table2: Fetching part 202102_54658_54707_10 from /clickhouse/tables/table2/replicas/host1\r\n2021.02.10 20:07:03.220531 [ 25517 ] {} <Trace> HTTPCommon: Failed communicating with host1 with error 'Received error from remote server /?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable1%2Freplicas%2Fhost1&part=202102_54133_54133_0&client_protocol_version=5&compress=false. HTTP status code: 500 Internal Server Error, body: Code: 232, e.displayText() = DB::Exception: No part 202102_54133_54133_0 in table, Stack trace (when copying this message, always include the lines below):\r\n-- However the request completes successfully\r\n2021.02.10 20:07:03.220552 [ 25517 ] {} <Trace> ReadWriteBufferFromHTTP: Sending request to http://host1:9009/?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable2%2Freplicas%2Fhost1&part=202102_54658_54707_10&client_protocol_version=5&compress=false\r\n2021.02.10 20:07:03.372064 [ 25517 ] {} <Debug> DiskLocal: Reserving 1.00 MiB on disk `default`, having unreserved 944.95 GiB.\r\n2021.02.10 20:07:03.373431 [ 25517 ] {} <Trace> db.table2: Renaming temporary part tmp_fetch_202102_54658_54707_10 to 202102_54658_54707_10.\r\n2021.02.10 20:07:03.512730 [ 25517 ] {} <Debug> db.table2: Fetched part 202102_54658_54707_10 from /clickhouse/tables/table2/replicas/host1\r\n\r\n-- Next day, still the same irrelevant exception on the second replica\r\n2021.02.11 15:35:03.095160 [ 25575 ] {} <Debug> db.table3: Fetching part all_180775_181336_123 from /clickhouse/tables/table3/replicas/host1\r\n2021.02.11 15:35:03.127568 [ 25575 ] {} <Trace> HTTPCommon: Failed communicating with host1 with error 'Received error from remote server /?endpoint=DataPartsExchange%3A%2Fclickhouse%2Ftables%2Ftable1%2Freplicas%2Fhost1&part=202102_54133_54133_0&client_protocol_version=5&compress=false. HTTP status code: 500 Internal Server Error, body: Code: 232, e.displayText() = DB::Exception: No part 202102_54133_54133_0 in table, Stack trace (when copying this message, always include the lines below):\r\n```\r\nThe messages disappears after a restart of the replica.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340/reactions","total_count":1,"+1":1,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20340/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20254","id":804760863,"node_id":"MDU6SXNzdWU4MDQ3NjA4NjM=","number":20254,"title":"Optimize (select * from table [where clause]) subqueries usage for distributed engine","user":{"login":"siradjev","id":10959667,"node_id":"MDQ6VXNlcjEwOTU5NjY3","avatar_url":"https://avatars.githubusercontent.com/u/10959667?v=4","gravatar_id":"","url":"https://api.github.com/users/siradjev","html_url":"https://github.com/siradjev","followers_url":"https://api.github.com/users/siradjev/followers","following_url":"https://api.github.com/users/siradjev/following{/other_user}","gists_url":"https://api.github.com/users/siradjev/gists{/gist_id}","starred_url":"https://api.github.com/users/siradjev/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/siradjev/subscriptions","organizations_url":"https://api.github.com/users/siradjev/orgs","repos_url":"https://api.github.com/users/siradjev/repos","events_url":"https://api.github.com/users/siradjev/events{/privacy}","received_events_url":"https://api.github.com/users/siradjev/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":7,"created_at":"2021-02-09T17:33:26Z","updated_at":"2021-02-20T07:54:28Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Use case**\r\nMany of BI engines and tools generate query syntax as \r\n`select sum(col1) as xxx, sum(col2) as yyy from (select * from table) where dim1='UIfilter' and dim2='UIvalue'`\r\nBased on our test with distributed engine, it does quite well with pushing where conditions and skipping unnecessary columns from remote hosts in cluster.\r\nHowever, there is no optimization for running the aggregation remotely and all the data is first fetched to initiator and then aggregation is performed. This causes somewhat notable performance degradation when raw data is large. \r\n \r\n**Describe the solution you'd like**\r\nDo remote aggregation by default, or support query level hint or distributed engine setting to allow pushing groupings to remote hosts in case when distributed table is used in simple subquery. \r\n\r\nRepro of the problem:\r\n```\r\ndrop table if exists default.tst ON CLUSTER 'clustername';\r\ndrop table if exists default.dist_tst ON CLUSTER 'clustername';\r\nCREATE TABLE default.tst ON CLUSTER 'clustername'\r\n(x Int32,y Int32, val Float32)\r\nENGINE = ReplicatedMergeTree('/clickhouse/clustername/default_tst', '{replica}') \r\nORDER BY (x);\r\nCREATE TABLE default.dist_tst (x Int32,y Int32, val Float32) ENGINE = Distributed('clustername', 'default', 'tst', rand());\r\n```\r\n\r\nExample of query:\r\n```\r\nselect x, sum(y) from default.dist_tst where y<5 group by x;\r\n```\r\nTrace from remote shard:\r\n```\r\n...\r\n 2021.02.09 18:19:05.731394 [ 1728 ] {615abb66-d4a0-4678-a4e3-b543ea575db3} <Debug> executeQuery: (from [::ffff:192.168.3.1]:44680, initial_query_id: badaff6f-8d29-4158-9d3e-f4cf8ad9b141) SELECT x, sum(y) FROM default.tst WHERE y < 5 GROUP BY x\r\n...\r\n```\r\n\r\nRunning the same query using (select * from table subquery)\r\n```\r\nselect x, sum(y) from (select * from default.dist_tst) where y<5 group by x;\r\n```\r\nTrace from remote shard:\r\n```\r\n...\r\n2021.02.09 18:19:05.925762 [ 1728 ] {4f01e972-3640-4b63-afd1-351d0ba970f3} <Debug> executeQuery: (from [::ffff:192.168.3.1]:44680, initial_query_id: 7f47c13b-c4c0-40f5-8d06-eeb268671af8) SELECT tst.x, tst.y FROM default.tst WHERE y < 5\r\n...\r\n```\r\n\r\n\r\n**Describe alternatives you've considered**\r\nRewriting all 80% of BI tools in the world, or sticking to the rest 20% :)\r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20254/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20225","id":804344395,"node_id":"MDU6SXNzdWU4MDQzNDQzOTU=","number":20225,"title":"Leading zeroes cause number equality to blow up","user":{"login":"macobo","id":148820,"node_id":"MDQ6VXNlcjE0ODgyMA==","avatar_url":"https://avatars.githubusercontent.com/u/148820?v=4","gravatar_id":"","url":"https://api.github.com/users/macobo","html_url":"https://github.com/macobo","followers_url":"https://api.github.com/users/macobo/followers","following_url":"https://api.github.com/users/macobo/following{/other_user}","gists_url":"https://api.github.com/users/macobo/gists{/gist_id}","starred_url":"https://api.github.com/users/macobo/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/macobo/subscriptions","organizations_url":"https://api.github.com/users/macobo/orgs","repos_url":"https://api.github.com/users/macobo/repos","events_url":"https://api.github.com/users/macobo/events{/privacy}","received_events_url":"https://api.github.com/users/macobo/received_events","type":"User","site_admin":false},"labels":[{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"},{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-02-09T08:52:08Z","updated_at":"2021-02-10T14:01:45Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Describe the bug**\r\nClickhouse allows comparing numbers to ints. For example, all of the following queries work:\r\n\r\n```\r\nSELECT 1 = '1'\r\nSELECT 4324 = '4324'\r\nSELECT 2.1 = '2.1'\r\nSELECT 2.0 = '2'\r\nSELECT 0.2 = '0.2'\r\n```\r\n\r\nAnd so on.\r\n\r\nHowever the following query blows up:\r\n\r\n```\r\nSELECT 123 = '0123'\r\n```\r\n\r\nThis behavior works in:\r\n1. Postgres\r\n2. Python: `int(\"0123\") == 123`\r\n3. Javascript: `parseInt(\"0123\") === 123`\r\n\r\n**Does it reproduce on recent release?**\r\nYes, using latest docker image\r\n\r\n**How to reproduce**\r\n\r\n```\r\nSELECT 123 = '0123'\r\n```\r\n\r\n**Expected behavior**\r\nReturn 1 / true\r\n\r\n**Error message and/or stacktrace**\r\n\r\n```\r\nDB::Exception: Cannot convert string 0123 to type UInt8. Stack trace:\r\n\r\n0. DB::Exception::Exception<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > >(int, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >&&) @ 0xc0ad5e0 in /usr/bin/clickhouse\r\n1. ? @ 0xdec12ba in /usr/bin/clickhouse\r\n2. DB::convertFieldToType(DB::Field const&, DB::IDataType const&, DB::IDataType const*) @ 0xdebffe4 in /usr/bin/clickhouse\r\n3. DB::FunctionComparison<DB::EqualsOp, DB::NameEquals>::executeWithConstString(DB::FunctionArguments&, unsigned long, DB::IColumn const*, DB::IColumn const*, std::__1::shared_ptr<DB::IDataType const> const&, std::__1::shared_ptr<DB::IDataType const> const&, unsigned long) const @ 0xa8ebf08 in /usr/bin/clickhouse\r\n4. DB::FunctionComparison<DB::EqualsOp, DB::NameEquals>::executeImpl(DB::FunctionArguments&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, unsigned long, unsigned long) const @ 0xa8dce1c in /usr/bin/clickhouse\r\n5. DB::ExecutableFunctionAdaptor::execute(std::__1::vector<DB::ColumnWithTypeAndName, std::__1::allocator<DB::ColumnWithTypeAndName> >&, std::__1::vector<unsigned long, std::__1::allocator<unsigned long> > const&, unsigned long, unsigned long, bool) @ 0x9255f8f in /usr/bin/clickhouse\r\n6. DB::ActionsDAG::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool) @ 0xdb9903b in /usr/bin/clickhouse\r\n7. DB::ScopeStack::addFunction(std::__1::shared_ptr<DB::IFunctionOverloadResolver> const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, bool) @ 0xdc3b956 in /usr/bin/clickhouse\r\n8. DB::ActionsMatcher::visit(DB::ASTFunction const&, std::__1::shared_ptr<DB::IAST> const&, DB::ActionsMatcher::Data&) @ 0xdc3eed9 in /usr/bin/clickhouse\r\n9. DB::InDepthNodeVisitor<DB::ActionsMatcher, true, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) @ 0xdc1acba in /usr/bin/clickhouse\r\n10. DB::InDepthNodeVisitor<DB::ActionsMatcher, true, std::__1::shared_ptr<DB::IAST> const>::visit(std::__1::shared_ptr<DB::IAST> const&) @ 0xdc1ad31 in /usr/bin/clickhouse\r\n11. DB::ExpressionAnalyzer::getRootActions(std::__1::shared_ptr<DB::IAST> const&, bool, std::__1::shared_ptr<DB::ActionsDAG>&, bool) @ 0xdc1aa46 in /usr/bin/clickhouse\r\n12. DB::SelectQueryExpressionAnalyzer::appendSelect(DB::ExpressionActionsChain&, bool) @ 0xdc21d86 in /usr/bin/clickhouse\r\n13. DB::ExpressionAnalysisResult::ExpressionAnalysisResult(DB::SelectQueryExpressionAnalyzer&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&, bool, bool, bool, std::__1::shared_ptr<DB::FilterInfo> const&, DB::Block const&) @ 0xdc26803 in /usr/bin/clickhouse\r\n14. DB::InterpreterSelectQuery::getSampleBlockImpl() @ 0xdbd2294 in /usr/bin/clickhouse\r\n15. ? @ 0xdbcd155 in /usr/bin/clickhouse\r\n16. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, std::__1::shared_ptr<DB::IBlockInputStream> const&, std::__1::optional<DB::Pipe>, std::__1::shared_ptr<DB::IStorage> const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&, std::__1::shared_ptr<DB::StorageInMemoryMetadata const> const&) @ 0xdbc875b in /usr/bin/clickhouse\r\n17. DB::InterpreterSelectQuery::InterpreterSelectQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdbc717d in /usr/bin/clickhouse\r\n18. DB::InterpreterSelectWithUnionQuery::InterpreterSelectWithUnionQuery(std::__1::shared_ptr<DB::IAST> const&, DB::Context const&, DB::SelectQueryOptions const&, std::__1::vector<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> >, std::__1::allocator<std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > > > const&) @ 0xdd71a54 in /usr/bin/clickhouse\r\n19. DB::InterpreterFactory::get(std::__1::shared_ptr<DB::IAST>&, DB::Context&, DB::QueryProcessingStage::Enum) @ 0xdb462f7 in /usr/bin/clickhouse\r\n20. ? @ 0xdecae30 in /usr/bin/clickhouse\r\n21. DB::executeQuery(std::__1::basic_string<char, std::__1::char_traits<char>, std::__1::allocator<char> > const&, DB::Context&, bool, DB::QueryProcessingStage::Enum, bool) @ 0xdec9e9d in /usr/bin/clickhouse\r\n22. DB::TCPHandler::runImpl() @ 0xe56a3b6 in /usr/bin/clickhouse\r\n23. DB::TCPHandler::run() @ 0xe5771b7 in /usr/bin/clickhouse\r\n24. Poco::Net::TCPServerConnection::start() @ 0x10d4f71f in /usr/bin/clickhouse\r\n25. Poco::Net::TCPServerDispatcher::run() @ 0x10d5112e in /usr/bin/clickhouse\r\n26. Poco::PooledThread::run() @ 0x10e823e9 in /usr/bin/clickhouse\r\n27. Poco::ThreadImpl::runnableEntry(void*) @ 0x10e7e31a in /usr/bin/clickhouse\r\n28. start_thread @ 0x9609 in /usr/lib/x86_64-linux-gnu/libpthread-2.31.so\r\n29. clone @ 0x122293 in /usr/lib/x86_64-linux-gnu/libc-2.31.so\r\n```","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20225/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20222","id":804302300,"node_id":"MDU6SXNzdWU4MDQzMDIzMDA=","number":20222,"title":"Add tables parts/size/rows metrics","user":{"login":"dmmarkov","id":10596076,"node_id":"MDQ6VXNlcjEwNTk2MDc2","avatar_url":"https://avatars.githubusercontent.com/u/10596076?v=4","gravatar_id":"","url":"https://api.github.com/users/dmmarkov","html_url":"https://github.com/dmmarkov","followers_url":"https://api.github.com/users/dmmarkov/followers","following_url":"https://api.github.com/users/dmmarkov/following{/other_user}","gists_url":"https://api.github.com/users/dmmarkov/gists{/gist_id}","starred_url":"https://api.github.com/users/dmmarkov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/dmmarkov/subscriptions","organizations_url":"https://api.github.com/users/dmmarkov/orgs","repos_url":"https://api.github.com/users/dmmarkov/repos","events_url":"https://api.github.com/users/dmmarkov/events{/privacy}","received_events_url":"https://api.github.com/users/dmmarkov/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-02-09T07:46:17Z","updated_at":"2021-02-09T12:58:55Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\nI'd like to see size of tables, number of parts and number of rows in tables. But I don't want to use extra exporter which can extract this information from system.parts.\r\n\r\n**Describe the solution you'd like**\r\nIt maybe some kind of async metrics which can be captured by prometheus. \r\n\r\n**Additional context**\r\nI know that exists https://github.com/ClickHouse/clickhouse_exporter which can scrape this values but it will be better if clickhouse can also export this part of values via metrics\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20222/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20219","id":804150619,"node_id":"MDU6SXNzdWU4MDQxNTA2MTk=","number":20219,"title":"User Friendly Cloud Table Design (RFC)","user":{"login":"yiguolei","id":9208457,"node_id":"MDQ6VXNlcjkyMDg0NTc=","avatar_url":"https://avatars.githubusercontent.com/u/9208457?v=4","gravatar_id":"","url":"https://api.github.com/users/yiguolei","html_url":"https://github.com/yiguolei","followers_url":"https://api.github.com/users/yiguolei/followers","following_url":"https://api.github.com/users/yiguolei/following{/other_user}","gists_url":"https://api.github.com/users/yiguolei/gists{/gist_id}","starred_url":"https://api.github.com/users/yiguolei/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/yiguolei/subscriptions","organizations_url":"https://api.github.com/users/yiguolei/orgs","repos_url":"https://api.github.com/users/yiguolei/repos","events_url":"https://api.github.com/users/yiguolei/events{/privacy}","received_events_url":"https://api.github.com/users/yiguolei/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1507886722,"node_id":"MDU6TGFiZWwxNTA3ODg2NzIy","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/st-discussion","name":"st-discussion","color":"e5b890","default":false,"description":"The story requires discussion /research / expert help / design & decomposition before will be taken"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":8,"created_at":"2021-02-09T02:51:33Z","updated_at":"2021-08-13T01:40:41Z","closed_at":null,"author_association":"CONTRIBUTOR","active_lock_reason":null,"body":"# Problem\r\nCurrent distributed table has the following problems:\r\n\r\n- Each table can only have one shard on each node, so the number of shard in a table is bound to the number of nodes, but when number of node is expanded, the number of shards has been determined when the table is created, so the system could not move one shard to other node to rebalance data. Currently, the existing solution is to add an empty shard to the newly added node, and then write the incremental data to the new node. This method has the following disadvantages:\r\n\t- It is okay for the general merge tree table, but for some replacing merge tree tables, there is nothing you can do;\r\n\t- Historical data cannot be migrated, many users stop writing and then copy historical data to the new shard, which is very unfriendly to business;\r\n- The replication relationship between replicas is actually the data replication relationship between nodes, because a table has only one shard on a node, and when there are 3 nodes in the system, there is no way to make 2 shards and 2 replicas. , 4 nodes are required; in addition, since the replica is defined at the cluster level, there is no way to create multiple tables in a cluster with different number of replicas;\r\n- The definition of the shard and the definition of the replica are all in the configuration file, which needs to be manually modified by the user. The configuration file needs to be manually modified when the node expands and shrinks, and the maintenance cost is relatively high.\r\n- Each local table needs the user to specify the path on ZK, and the user needs to ensure that no multiple tables point to the same ZK path. In fact, it is difficult for users to understand why ZK  is needed because zk is actually a module inside the system.\r\n\r\n# New Cloud Table \r\n\r\n## Create table\r\n\r\n```\r\nCREATE TABLE [IF NOT EXISTS] [db.]table_name [ON CLUSTER cluster]\r\n(\r\n    name1 [type1] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|ALIAS expr1] [compression_codec] [TTL expr1],\r\n    name2 [type2] [NULL|NOT NULL] [DEFAULT|MATERIALIZED|ALIAS expr2] [compression_codec] [TTL expr2],\r\n    ...\r\n) ENGINE = MergeTree()\r\n \r\n[Distributed by sharding key]\r\nSettings vshard_num = 3, replica_num = 2ï¼Œassign_vshard_policy='random'ï¼›\r\n```\r\n\r\n- vshard_num: indicates how many vshards are created\r\n- replica_num: number of replcias for every vshard\r\n- assign_vshard_policy is a distribution strategy from vshard to node. At the initial stage, you can consider assigning it randomly.\r\n- sharding key definition\r\n\r\nBoth related local table and distributed table is called during create table. vshard is a local table actually. But it is different from current shard, because vshard num is not related with node num and a distributed table could have more than one vshard on a single node.\r\n\r\nreplica_num is related with node num, for example, if there are only 2 nodes, you can not create a vshard with 3 replicas.\r\n\r\n\r\n## Insert \r\n\r\n```\r\nINSERT INTO [db.]table [(c1, c2, c3)] VALUES (v11, v12, v13), (v21, v22, v23), ...\r\nSettings vshard_id = 2\r\n```\r\n\r\n\r\nUser does not need a local table here, just specify the distributed table name and vshard id, and the system automatically routes the data to specific vshard;\r\nIf shard_id is not specified, then the data will be allocated according to the sharding strategy in the table's sharding definition.\r\n\r\n## Query\r\n\r\n- Case 1: Just use distributed table\r\n\r\n```\r\nSELECT * FROM table_name\r\n```\r\n\r\n- Case 2: Sometimes, a query has to use local table to do colocated join, it has to use local table name in the query. In this case user could use distributed_table_name[v_shardid] instead of local table name.\r\n\r\n```\r\nSELECT * FROM cluster(`cluster_name`, view(SELECT a, b, c FROM table_name[v_shardid]))\r\n```\r\n\r\n## Rebalance Data\r\n\r\n```\r\nALTER TABLE table_name\r\nRELOCATE shard_id to node_ip:port\r\n```\r\n\r\nIf a new node is added to the cluster, the administrator could use migrate function to move some vshards to the new node.\r\n\r\nLater, you can consider introducing a coordinator node to maintain the load of each node in the cluster, and dynamically call the relocate command to complete the rebalance work.\r\n\r\n# Solutions\r\n![image](https://user-images.githubusercontent.com/9208457/107309084-c23f6180-6ac4-11eb-9eee-de0ff67af1ce.png)\r\n\r\nIn order to achieve the above functions, and to minimize the changes to the current CK code and maintain compatibility, the rough design scheme is as follows:\r\n\r\n- When the user creates a distributed table, a distributed table is created on each machine in the cluster;\r\n- When creating a distributed table, the user can specify the number of vshards, and the system directly distributes the vshards to multiple machines according to a certain distribution algorithm;\r\n- Each vshard corresponds to a local table. Since user could not access vshard using the vshard name directly, so it can be directly idized, its name is a uuid. So that multiple local tables belonging to the same distributed table can be stored on one node.\r\n- Store the following metadata information on zk:\r\n\t- Mapping from distributed table to vshard;\r\n\t- Mapping from vshard to replicas;\r\n\t- Mapping from replicas to nodes;\r\n\t- zk path for each replica;\r\n- When reading data, directly rewrite the vshard query on a certain node into a merge table function; when the merge table function is executed, it calculates the list of vshards to be read locally based on the metadata in ZK.\r\n- When inserting data, insert directly into vshard;\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219/reactions","total_count":6,"+1":6,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20219/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20209","id":803596171,"node_id":"MDU6SXNzdWU4MDM1OTYxNzE=","number":20209,"title":"How to improve query perfermance on system.parts","user":{"login":"Qsimple","id":28128661,"node_id":"MDQ6VXNlcjI4MTI4NjYx","avatar_url":"https://avatars.githubusercontent.com/u/28128661?v=4","gravatar_id":"","url":"https://api.github.com/users/Qsimple","html_url":"https://github.com/Qsimple","followers_url":"https://api.github.com/users/Qsimple/followers","following_url":"https://api.github.com/users/Qsimple/following{/other_user}","gists_url":"https://api.github.com/users/Qsimple/gists{/gist_id}","starred_url":"https://api.github.com/users/Qsimple/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/Qsimple/subscriptions","organizations_url":"https://api.github.com/users/Qsimple/orgs","repos_url":"https://api.github.com/users/Qsimple/repos","events_url":"https://api.github.com/users/Qsimple/events{/privacy}","received_events_url":"https://api.github.com/users/Qsimple/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-08T14:05:12Z","updated_at":"2021-02-18T13:25:19Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Hi dear dev team:\r\n  We are developing some Table parts monitor by \"SELECT xxx FROM system.parts\". As time goes on, the stored data gets bigger and bigger, and parts directory gets more and more. The query on system.parts gets slower and slower.   \r\n we consume 6s to process 58k rows with sql \"SELECT COUNT(1) FROM system.parts\"\r\n\r\n  I think the reason is the parts fetch the parts info from OS file system when ingest an query rather then store some data independent.  \r\n  In those scenes, any suggestion to improve the query perfermance on system.parts?\r\n                                                                                                                                               Tks, :)","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20209/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20200","id":803314004,"node_id":"MDU6SXNzdWU4MDMzMTQwMDQ=","number":20200,"title":"Prometheus endpoint improvements","user":{"login":"filimonov","id":1549571,"node_id":"MDQ6VXNlcjE1NDk1NzE=","avatar_url":"https://avatars.githubusercontent.com/u/1549571?v=4","gravatar_id":"","url":"https://api.github.com/users/filimonov","html_url":"https://github.com/filimonov","followers_url":"https://api.github.com/users/filimonov/followers","following_url":"https://api.github.com/users/filimonov/following{/other_user}","gists_url":"https://api.github.com/users/filimonov/gists{/gist_id}","starred_url":"https://api.github.com/users/filimonov/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/filimonov/subscriptions","organizations_url":"https://api.github.com/users/filimonov/orgs","repos_url":"https://api.github.com/users/filimonov/repos","events_url":"https://api.github.com/users/filimonov/events{/privacy}","received_events_url":"https://api.github.com/users/filimonov/received_events","type":"User","site_admin":false},"labels":[{"id":386401508,"node_id":"MDU6TGFiZWwzODY0MDE1MDg=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/help%20wanted","name":"help wanted","color":"128A0C","default":true,"description":null},{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null},{"id":1365579236,"node_id":"MDU6TGFiZWwxMzY1NTc5MjM2","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/minor","name":"minor","color":"FFF8F8","default":false,"description":"Priority: minor"}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-08T08:13:05Z","updated_at":"2021-06-27T13:06:22Z","closed_at":null,"author_association":"COLLABORATOR","active_lock_reason":null,"body":"- [x] https://github.com/ClickHouse/ClickHouse/issues/10571 Prometheus metrics contain illegal character sequence, ensure correct type is reported (some metrics are floats, see https://github.com/ClickHouse/clickhouse_exporter/issues/44 )\r\n- [ ] https://github.com/ClickHouse/ClickHouse/issues/10043 Export sql query results to prometheus (and may be to system.asynchronious_metrics) ? \r\n- [x] https://github.com/ClickHouse/ClickHouse/issues/19570 system.parts (and other system tables): fill only requested columns \r\n- [ ] custom metric naming schemas (compat layer for https://github.com/ClickHouse/clickhouse_exporter https://github.com/Altinity/clickhouse-operator/blob/a112229168babd83abfc8eb2f2bef9dee19e94e6/pkg/apis/metrics/exporter.go ) \r\n- [ ] add system.errors\r\n- [ ] configurable constant labels like: type='development',source='clickhouse' \r\n- [ ] create some 'standrard' set of dashboard / alerts. \r\n\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20200/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20172","id":802851959,"node_id":"MDU6SXNzdWU4MDI4NTE5NTk=","number":20172,"title":" Cannot create MySQL database, because Code: 69, e.displayText() = DB::Exception: FixedString size must be positive,","user":{"login":"imysm","id":48442505,"node_id":"MDQ6VXNlcjQ4NDQyNTA1","avatar_url":"https://avatars.githubusercontent.com/u/48442505?v=4","gravatar_id":"","url":"https://api.github.com/users/imysm","html_url":"https://github.com/imysm","followers_url":"https://api.github.com/users/imysm/followers","following_url":"https://api.github.com/users/imysm/following{/other_user}","gists_url":"https://api.github.com/users/imysm/gists{/gist_id}","starred_url":"https://api.github.com/users/imysm/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/imysm/subscriptions","organizations_url":"https://api.github.com/users/imysm/orgs","repos_url":"https://api.github.com/users/imysm/repos","events_url":"https://api.github.com/users/imysm/events{/privacy}","received_events_url":"https://api.github.com/users/imysm/received_events","type":"User","site_admin":false},"labels":[{"id":386401510,"node_id":"MDU6TGFiZWwzODY0MDE1MTA=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/question","name":"question","color":"bfdadc","default":true,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":3,"created_at":"2021-02-07T04:01:20Z","updated_at":"2021-05-20T09:19:34Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"Clickhouse Version:ClickHouse client version 21.1.2.15 (official build).\r\nMySQL.  Version:Server version: 8.0.21 MySQL Community Server - GPL\r\nAction:\r\ncreate database on Clickhouse with MySQL Engine.\r\n`create DATABASE mysql_dcdb_13ã€€ENGINE = MySQL('10.2.1.3:8809', 'dcdb', 'dc', 'Dc%A0dc1234');`\r\nError:\r\n`Received exception from server (version 21.1.2):\r\nCode: 501. DB::Exception: Received from localhost:9000. DB::Exception: Cannot create MySQL database, because Code: 69, e.displayText() = DB::Exception: FixedString size must be positive,`","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20172/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20170","id":802837648,"node_id":"MDU6SXNzdWU4MDI4Mzc2NDg=","number":20170,"title":"Code: 349. DB::Exception: Received from 9.222.24.118:9000. DB::Exception: Can not insert NULL data into non-nullable column \"income_source\": While executing HDFS. ","user":{"login":"biandan8","id":14833308,"node_id":"MDQ6VXNlcjE0ODMzMzA4","avatar_url":"https://avatars.githubusercontent.com/u/14833308?v=4","gravatar_id":"","url":"https://api.github.com/users/biandan8","html_url":"https://github.com/biandan8","followers_url":"https://api.github.com/users/biandan8/followers","following_url":"https://api.github.com/users/biandan8/following{/other_user}","gists_url":"https://api.github.com/users/biandan8/gists{/gist_id}","starred_url":"https://api.github.com/users/biandan8/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/biandan8/subscriptions","organizations_url":"https://api.github.com/users/biandan8/orgs","repos_url":"https://api.github.com/users/biandan8/repos","events_url":"https://api.github.com/users/biandan8/events{/privacy}","received_events_url":"https://api.github.com/users/biandan8/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-07T02:25:20Z","updated_at":"2021-02-07T14:46:53Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"(you don't have to strictly follow this form)\r\n\r\n**Describe the bug**\r\nA clear and concise description of what works not as it is supposed to.\r\nwhen i insert the data from hdfs with the orc formatï¼Œi recieve the error\"Code: 349. DB::Exception: Received from 9.222.24.118:9000. DB::Exception: Can not insert NULL data into non-nullable column \"income_source\": While executing HDFS. \",but the column is defined by  Nullable(Int64)\r\n**Does it reproduce on recent release?**\r\n[The list of releases](https://github.com/ClickHouse/ClickHouse/blob/master/utils/list-versions/version_date.tsv)\r\n\r\n**How to reproduce**\r\n* Which ClickHouse server version to use\r\n* Which interface to use, if matters\r\n* Non-default settings, if any\r\n* `CREATE TABLE` statements for all tables involved\r\n* Sample data for all these tables, use [clickhouse-obfuscator](https://github.com/ClickHouse/ClickHouse/blob/master/programs/obfuscator/Obfuscator.cpp#L42-L80) if necessary\r\n* Queries to run that lead to unexpected result\r\n\r\n**Expected behavior**\r\nA clear and concise description of what you expected to happen.\r\n\r\n**Error message and/or stacktrace**\r\nIf applicable, add screenshots to help explain your problem.\r\n\r\n**Additional context**\r\nAdd any other context about the problem here.\r\n","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20170/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20166","id":802809620,"node_id":"MDU6SXNzdWU4MDI4MDk2MjA=","number":20166,"title":"EOF error using python driver","user":{"login":"MaxJPowers","id":23011178,"node_id":"MDQ6VXNlcjIzMDExMTc4","avatar_url":"https://avatars.githubusercontent.com/u/23011178?v=4","gravatar_id":"","url":"https://api.github.com/users/MaxJPowers","html_url":"https://github.com/MaxJPowers","followers_url":"https://api.github.com/users/MaxJPowers/followers","following_url":"https://api.github.com/users/MaxJPowers/following{/other_user}","gists_url":"https://api.github.com/users/MaxJPowers/gists{/gist_id}","starred_url":"https://api.github.com/users/MaxJPowers/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/MaxJPowers/subscriptions","organizations_url":"https://api.github.com/users/MaxJPowers/orgs","repos_url":"https://api.github.com/users/MaxJPowers/repos","events_url":"https://api.github.com/users/MaxJPowers/events{/privacy}","received_events_url":"https://api.github.com/users/MaxJPowers/received_events","type":"User","site_admin":false},"labels":[{"id":1575085465,"node_id":"MDU6TGFiZWwxNTc1MDg1NDY1","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/unexpected%20behaviour","name":"unexpected behaviour","color":"e088ca","default":false,"description":""}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":2,"created_at":"2021-02-06T22:57:23Z","updated_at":"2021-02-11T16:25:26Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"I have Clickhouse version 20.8.3.18 and python3 installed on a vm stress testing Cache dictionaries. After a certain number of entries the query using clickhouse_driver, I'll get the error\r\n```\r\nUnexpected EOF while reading bytes\r\n``` \r\nIs this an error due to the driver/python related or due to the cache being maxed on the system. For example this happens on a file size 203 columns and 10000 rows on a machine with 32Gb of RAM and 256Gb of SSD memory, a csv file of around 66Mb which seems quite small for such an error. The query I'm running is:\r\n```\r\nSELECT  \r\n    dictGet('CacheDictionary', 'date', toUInt64(number)) AS date, \r\n    SUM(dictGet('CacheDictionary', 'filterColumn', toUInt64(number))) AS val, \r\n    AVG(dictGet('CacheDictionary', 'filterColumn', toUInt64(number))) AS avg \r\nFROM numbers(1, 10000) \r\nGROUP BY date\r\n```\r\nAn example entry of the csv file is:\r\n```\r\n20000,2021-02-05,6867,0.5314826651111791,OA9SMRN54LC3MTDW,D6S8AYXZ3JVSHPCY,12UQV1JR87MT00EP,3WBT23MA2QN6URA7,YGKJR5577BP6S3AD,2T90WPW1REOZA0L9,JQG8Z6FXXIX2788M,OAOVV1YX3A6HKQV8,FISBMOAHEXHAAKEY,XAULW5F90T3VEMUL,RAAZ5TM5XL7GRC1F,B16JEGDHXUXFI2R9,DETSZ7BR45CRAIA7,Z2X53PAQYCSBHPU3,SRISC0ZLWXC2DP34,KO2M3044JX5JCB74,ML776REFIX3Z1L78,ND6PXBOR135SWFSB,ZF4K45N2AIGFAK0L,RFE3EHCKC5EPYE2V,NJKM5T8UUD5NRDPX,O57IQW0670LP00I9,F0EBZ3BXHPETCFSY,RUZ7VH2IM0DIZ4UC,08BP467WG7ROEHTJ,9LSTNLUA240T2K4D,5L4PIRKMK746QW5Q,2VX3SER8ULU93NZG,Z0MZ9C3TTPR6WFDV,KB32XWCR67AWGSIB,PDM8QJ34X4EOTVN1,P7TUVP8Q1YF9S746,YDFDBCG6S2EXYPNW,55RN0F4UMGF3ABQZ,RRF895J8LQSLI48U,54OQWCJODIEQLRQF,D5ZJPGAG7CCO4LWA,UQDWEXPI184UUJQD,3QF6QAS32ITRL8JH,FPQ324RO04LNVAMO,ZJ6QCWNQCBQOE7F5,6OWVEVWHNSZILC6E,GIUD29OIFF3LUCCX,VGBJHKW32BUNUSDH,908TDRODVZIIC5O8,UCIU38BXEREJMO4M,5LKJ23ER4CKUZ88J,A1GBKPPM10L8X5RM,BB3SAVWF3CNBDXHO,279MIC1OXTDS2PFP,J6UVFJE8RGFK4LDN,3CE12GT27GX0WVWU,PNNTRLDFVJQ0TCRK,MI7XOHWUQX3W938H,LKZPV4K0BA6OE3R0,YJMLI82UBLSZWP7U,JORNKD1MSVECXBRF,CO5KKJIL1FHEYA11,GXVXWDOI538WCLC0,OPODB2R2ITSX0E6J,3VE7SOJZL3DKIES7,5LPXB17GJ94S86HL,UQ0DZVUDMBD39LC3,KSSVOBUKMZC7T89M,P6YL0WW22NOM5A36,RA46SZF4ZLO5YWUM,TUTMJ34X4040USXX,09HPKJAD58P3FVMP,DM0NJVFYKR2653HH,HP869NM4Y2EBE3ND,RVKP40RPBOPB6RPQ,WI3QXYA5XIWJUFUK,770L6U5KAEPKKJC1,2H0XNUDM41QBAZWB,8AWJ2Y7RB9F2WTT0,Y6T3PIPLU3FCBZCU,CY8SCO15RNUWQU2B,DRC88XH21J9ADT6Z,MLZ2JN7F8MXVBHBI,2YSUVHRL4V0EVHXF,Y0U12EBQSEVE6W6X,A6RRJY191S0JOXJH,4F12P4K0SJ6EDKSD,THCRJ2ZEXGM1RUM4,PF0OUAULUNIW0W9X,EK1249WXC0C2KKY8,11WEDAAJL7BL4T4U,4K8OP1WXSN1MIXPF,8D0WNN1672A6WK07,5RLYH7K00ZSR1LL2,EKEXBG87U1X6UOLL,YWK3V1F7MTAF9T19,XZ8ZF0XO5V8TCBPS,A3RX8X8A8I11Z8X3,77P2Q5WRSTL4ERAI,00BGNPDYFSVG5F81,5KTUM76C42VTP4I7,TA933GZZN8OQ20QJ,612WNQ74RDHMBWX3,D41HNOBPX11GFYWO,OGR4A0EPCSS00XL6,QIOH165Y5JGKJMFC,TF2R9TFC5TJN2PER,TYNXWI46H7I83O77,JMD5DOEV4U628SDK,D7ECJH43FEC77UCJ,FKA9AT5J20QI3MQP,7QSU0I8VRRLUMD7R,6OJ1O2XI2QJXP6W2,UD2QVJXNUFRCAO43,GS3TZUW8U6Z8EWWQ,QD79GBSO6D6GCAZ1,GQ5TUY2FMJSNMTRK,OGOYL2PD64E2DOOQ,Q733OU5P7J7SAFS1,GBS7MV5QOMQ4E89N,SB8MIQ1P37HMQZBJ,Z6G96BM7FL4150H3,05PS81HW528971RM,6F3KFLYT0345GI43,G65CDWEORNH3OUCY,12F43L99AZ84PDWR,GQQVWMTMS471WAWD,F1DFWRJ1F9M9MUTT,1M734H07IQAW49Q3,OPSRG5J7370227XE,BIPNR22KFF71MKQN,PV7DWGCQF5551FKT,YPGQVGUP37MRJY2B,RILKP96QV69WBW2D,4RXDCJURAVCQEGLX,XGIPC0AK1K0I6KDP,HMSE306L5NAK62LC,YAZHMS2UHGMWIB44,RZCAVUM45YTNV23T,3B7K07XPRTE8OMW1,FTP48ED5DQ4K3DM8,WW419RRJ2WU1F15L,85FWD49J0ARSUGI9,4U4768ANPCJ46K5P,EJ24BNUA6OZMUDEL,6Z27W6BN36GO8QWU,5AMZ4UU819GSI454,KMNIEJ2V5PI83KGP,APT4CYG8M5FM0BSW,IME5VRP08W468DZE,6BT4W0ZAW6C7993L,DRD6Q4P8BZVDG37U,2R1OEWQFV5J597AF,CKS41A6PXKVYICAG,OQYZ9UOQRVS3LLTF,JA3PZSAXFCJVZVLB,J23BP73T6GNC0Z08,GWOJXMXDVHCRE51Y,I826DE6KEVQK2PFC,6FF5LWM61KCM4C9K,P16P80EIX2X87OZO,O5GEOEO72CDV4GAX,UMKFUKMV6U0L5PM5,U64YI4G53LR3SC6J,CLML8KPAL697KYYJ,LMH2W0STEJ5H2J2S,AL61EP61ZR3GOPN3,Z3AEUMZSX4MQJ6M6,IS5RFEWIJ8XHYNK0,TNE1BS4JYN280PIF,67IER2YS6N2XHEW1,63P3O4X42T2INRT4,XYV043108XRK7Y4S,RW0HN600K0GQXF4Y,BZ1ZE6IBB4B72A81,QHAINYDIZX7838YI,7FFCKG3XJSZ2DIHJ,DF6C1OMPC1ETFPDZ,1EJ3EW0TXKVBC88R,WX6HG8FD021VFZ2S,W4OB9NZRODSTM96M,6GDA3L5CLBPVTPWQ,1Y4U7BL9UHPBJVIX,Y31SUUZ0JF2AXZWO,PL2I18PA0SVXG85E,TEY1HC97QMZ5YXMI,T49EVLLM43AI4OG3,0SDNMLWY85Z7NENX,4446QKGO8UL6RERT,IMEAM22I51GT4ZHY,HUCLC93NIUG0C5R0,5VPBRUUVMBXP7HJY,XCOOPM3JU5VHQ94T,3LRZGAF451G9XDIN,Y6VIN1E31NYRLA2N,RAROO2EM5Q9NJRG9,NUQ2QJ9M6T5KRCHK,WQKKQK8UBB30GRWI,20SOMMKD08FYAENW,1G9K4UFWAI8Q7Z8K,XLG898A4MQXZHVYR,FPT67A7VDLVZEWYH,6DQ6417FF07FORXZ,10RUAPY5KGAYBZZD\r\n```\r\nHere's part of the code trying to find the maximum number of cache items stored, along with the queries executed for each. In `selectBenchmark` the `string` correspond to the query above. The parameters for each are fairly self explanatory (the xmlFile is the dictionary created in `/etc/lib/clickhouse-server`).\r\n```\r\ndef cacheMaxItems(csvRead, xmlFile, benchmarkType, columnStepSize, rowStepSize):\r\n    maxCache = []\r\n    os.system('rm -f ' + csvRead)\r\n    os.system('bash /root/restartCH.sh')\r\n    for j in range(1, 13):\r\n        outputCSV = '/root/results' + benchmarkType + '/cacheResults' + str(j*columnStepSize) + '.csv'  \r\n        with open(outputCSV, 'w') as fp:\r\n            wr = csv.writer(fp)  \r\n            wr.writerow([benchmarkType + ': Number of rows', 'Loading time', 'Mean', 'Variance', 'Skewness', 'Number of Columns: ' + str(j*columnStepSize)])\r\n        for i in range(1, 10000):\r\n            if i%5 == 0:\r\n                os.system('bash /root/restartCH.sh')\r\n            createCSV(10000, j*columnStepSize, csvRead)\r\n            try:\r\n                clickhouseDictionary(rowStepSize*i*j*columnStepSize, j*columnStepSize, xmlFile, csvRead, 'Cache')\r\n                if benchmarkType == 'Random':\r\n                    results = selectBenchmark(i*rowStepSize, j*columnStepSize, 'Random', 'Cache')\r\n                elif benchmarkType == 'Consecutive':\r\n                    results = selectBenchmark(i*rowStepSize, j*columnStepSize, 'Consecutive', 'Cache')\r\n                elif benchmarkType == 'CPU':\r\n                    results = selectBenchmark(i*rowStepSize, j*columnStepSize, 'CPU', 'Cache')\r\n                results.insert(0, i*rowStepSize)\r\n                with open(outputCSV, 'a') as fp:\r\n                    wr = csv.writer(fp)  \r\n                    wr.writerow(results)\r\n\r\n                print('Successfully loaded and queried cache of size ' + str(rowStepSize*i*j*columnStepSize) + '.')\r\n            except Exception as ex:\r\n                print(ex)\r\n                os.system('rm -f ' + csvRead)\r\n                os.system('bash /root/restartCH.sh')\r\n                maxCache.append([j*columnStepSize, (i-1)*rowStepSize])\r\n                print(maxCache)\r\n                break\r\n    return maxCache\r\n```\r\n```\r\ndef selectBenchmark(numberOfRows, numberOfColumns, benchmarkType, dictType):\r\n    client = Client('localhost', port=9000, database='system')\r\n    client.execute('SYSTEM RELOAD DICTIONARY ' + dictType + 'Dictionary')\r\n    loadingTime = client.last_query.elapsed\r\n    client.execute('SELECT dictGet(\\'' + dictType + 'Dictionary\\', \\'random0\\', toUInt64(1))', query_id=str(uuid.uuid4()))\r\n    loadingTime += client.last_query.elapsed\r\n    loop = True\r\n    counter = 0\r\n    j=0\r\n    while loop:\r\n        times = []\r\n        for i in range(0, 31):\r\n            query_id = str(uuid.uuid4())\r\n            string = stringGen(numberOfRows, numberOfColumns, benchmarkType, dictType)\r\n            client.execute(string, query_id = query_id)\r\n            times.append(client.last_query.elapsed)  \r\n        if max(times) > loadingTime:\r\n            loadingTime = max(times)\r\n        stats = transformedMLE(times)\r\n        redactedTimes = [x for x in times if (stats[0]-3*np.sqrt(stats[1])) < x < (stats[0]+3*np.sqrt(stats[1]))]\r\n        if len(times) - len(redactedTimes) <= 3:\r\n            loop = False\r\n        elif j > 15:\r\n            print('High variance query')\r\n            loop = False\r\n        j+=1\r\n    result = transformedMLE(redactedTimes)\r\n    loadingTime = loadingTime - result[0]\r\n    result.insert(0, loadingTime)\r\n    client.disconnect()\r\n    return result\r\n```\r\nThe restartCH.sh file is\r\n```\r\nservice clickhouse-server forcerestart\r\n```\r\nas the cache overflow often blocks the `restart` command. \r\nThere is no output to the server error logs indicating that this is a problem with the python driver, perhaps reading the large amounts of data being returned. I also get the 'Killed' python output which also points towards cache issues, which is to be expected as I'm benchmarking cache dictionaries. Quick side note but how do you increase the timeout for loading clickhouse dictionaries, it seems to be 60000ms and I can't see where to change it in config.xml.","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20166/timeline","performed_via_github_app":null},{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157","repository_url":"https://api.github.com/repos/ClickHouse/ClickHouse","labels_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157/labels{/name}","comments_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157/comments","events_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157/events","html_url":"https://github.com/ClickHouse/ClickHouse/issues/20157","id":802762254,"node_id":"MDU6SXNzdWU4MDI3NjIyNTQ=","number":20157,"title":"Store data-skipping indexes on separate partition?","user":{"login":"mzealey","id":6083471,"node_id":"MDQ6VXNlcjYwODM0NzE=","avatar_url":"https://avatars.githubusercontent.com/u/6083471?v=4","gravatar_id":"","url":"https://api.github.com/users/mzealey","html_url":"https://github.com/mzealey","followers_url":"https://api.github.com/users/mzealey/followers","following_url":"https://api.github.com/users/mzealey/following{/other_user}","gists_url":"https://api.github.com/users/mzealey/gists{/gist_id}","starred_url":"https://api.github.com/users/mzealey/starred{/owner}{/repo}","subscriptions_url":"https://api.github.com/users/mzealey/subscriptions","organizations_url":"https://api.github.com/users/mzealey/orgs","repos_url":"https://api.github.com/users/mzealey/repos","events_url":"https://api.github.com/users/mzealey/events{/privacy}","received_events_url":"https://api.github.com/users/mzealey/received_events","type":"User","site_admin":false},"labels":[{"id":520905553,"node_id":"MDU6TGFiZWw1MjA5MDU1NTM=","url":"https://api.github.com/repos/ClickHouse/ClickHouse/labels/feature","name":"feature","color":"1d76db","default":false,"description":null}],"state":"open","locked":false,"assignee":null,"assignees":[],"milestone":null,"comments":0,"created_at":"2021-02-06T18:41:06Z","updated_at":"2021-02-06T18:41:06Z","closed_at":null,"author_association":"NONE","active_lock_reason":null,"body":"**Use case**\r\n\r\nWe have some very large tables that are infrequently accessed stored on hdd storage. We have added some data-skipping indexes (bloom filters) to speed up common search queries. It works well. However when doing the initial query it has to load the entire index. Testing on nvme shows we can easily process this at 2-3GB/sec, much higher than our hdd speed. Would it be possible to store the skipping indexes on separate volume/partition to speed this up?","reactions":{"url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157/reactions","total_count":0,"+1":0,"-1":0,"laugh":0,"hooray":0,"confused":0,"heart":0,"rocket":0,"eyes":0},"timeline_url":"https://api.github.com/repos/ClickHouse/ClickHouse/issues/20157/timeline","performed_via_github_app":null}]
